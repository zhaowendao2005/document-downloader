---
url: https://js.langchain.com.cn/docs/production/callbacks/
crawled_at: 2025-06-22T02:00:26.045821
---

äº‹ä»¶ / å›žè°ƒ
LangChain æä¾›äº†ä¸€ä¸ªå›žè°ƒç³»ç»Ÿï¼Œå…è®¸ä½ åœ¨ LLM åº”ç”¨ç¨‹åºçš„å„ä¸ªé˜¶æ®µä¸­è¿›è¡Œé’©å­å¤„ç†ã€‚è¿™å¯¹äºŽè®°å½•æ—¥å¿—ã€
ç›‘è§†
ã€
æµåª’ä½“
å’Œå…¶ä»–ä»»åŠ¡éžå¸¸æœ‰ç”¨ã€‚
ä½ å¯ä»¥é€šè¿‡ API ä¸­ä½¿ç”¨çš„
callbacks
å‚æ•°æ¥è®¢é˜…è¿™äº›äº‹ä»¶ã€‚æ­¤æ–¹æ³•æŽ¥å—ä¸€ä¸ªå¤„ç†ç¨‹åºå¯¹è±¡çš„åˆ—è¡¨ï¼Œè¿™äº›å¯¹è±¡åº”è¯¥å®žçŽ°
API æ–‡æ¡£
ä¸­æè¿°çš„ä¸€ä¸ªæˆ–å¤šä¸ªæ–¹æ³•ã€‚
æ·±å…¥äº†è§£
â€‹
ðŸ“„ï¸
åˆ›å»ºå›žè°ƒå¤„ç†ç¨‹åº
åˆ›å»ºè‡ªå®šä¹‰å¤„ç†ç¨‹åº
ðŸ“„ï¸
è‡ªå®šä¹‰Chainsä¸­çš„å›žè°ƒ
LangChainæ—¨åœ¨å¯æ‰©å±•ã€‚ æ‚¨å¯ä»¥å°†è‡ªå·±çš„è‡ªå®šä¹‰Chainså’ŒAgentsæ·»åŠ åˆ°åº“ä¸­ã€‚ æœ¬é¡µå°†å‘æ‚¨å±•ç¤ºå¦‚ä½•å°†å›žè°ƒæ·»åŠ åˆ°è‡ªå®šä¹‰çš„Chainså’ŒAgentsä¸­ã€‚
å¦‚ä½•ä½¿ç”¨å›žè°ƒ
â€‹
åœ¨ API ä¸­çš„å¤§å¤šæ•°å¯¹è±¡ä¸Šï¼ˆ
Chains
ã€
Models
ã€
Tools
ã€
Agents
ç­‰)éƒ½æä¾›äº†
callbacks
å‚æ•°ï¼Œå®ƒæœ‰ä¸¤ä¸ªä¸åŒçš„ç”¨æ³•:
æž„é€ å™¨å›žè°ƒ
â€‹
åœ¨æž„é€ å‡½æ•°ä¸­å®šä¹‰ï¼Œå¦‚
new LLMChain({ callbacks: [handler] })
ï¼Œå°†ç”¨äºŽè¯¥å¯¹è±¡ä¸Šè¿›è¡Œçš„æ‰€æœ‰è°ƒç”¨ï¼Œå¹¶ä¸”ä»…é€‚ç”¨äºŽè¯¥å¯¹è±¡æœ¬èº«ã€‚ä¾‹å¦‚ï¼Œå¦‚æžœä½ å°†å¤„ç†ç¨‹åºä¼ é€’ç»™
LLMChain
æž„é€ å‡½æ•°ï¼Œåˆ™ä¸ä¼šè¢«è¿žæŽ¥åˆ°è¯¥é“¾ä¸Šçš„æ¨¡åž‹ä½¿ç”¨ã€‚
import
{
ConsoleCallbackHandler
}
from
"langchain/callbacks"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
const
llm
=
new
OpenAI
(
{
temperature
:
0
,
// This handler will be used for all calls made with this LLM.
callbacks
:
[
new
ConsoleCallbackHandler
(
)
]
,
}
)
;
è¯·æ±‚å›žè°ƒ
â€‹
åœ¨å‘å‡ºè¯·æ±‚çš„
call()
/
run()
/
apply()
æ–¹æ³•ä¸­å®šä¹‰ï¼Œä¾‹å¦‚
chain.call({ input: '...' }ï¼Œ [handler])
ï¼Œå°†ä»…ç”¨äºŽè¯¥ç‰¹å®šè¯·æ±‚åŠå…¶åŒ…å«çš„æ‰€æœ‰å­è¯·æ±‚ï¼ˆä¾‹å¦‚ï¼Œå¯¹ LLMChain çš„è°ƒç”¨ä¼šè§¦å‘å¯¹æ¨¡åž‹çš„è°ƒç”¨ï¼Œè¯¥æ¨¡åž‹ä½¿ç”¨åœ¨
call()
æ–¹æ³•ä¸­ä¼ é€’çš„ç›¸åŒå¤„ç†ç¨‹åº)ã€‚
import
{
ConsoleCallbackHandler
}
from
"langchain/callbacks"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
const
llm
=
new
OpenAI
(
{
temperature
:
0
,
}
)
;
// This handler will be used only for this call.
const
response
=
await
llm
.
call
(
"1 + 1 ="
,
undefined
,
[
new
ConsoleCallbackHandler
(
)
,
]
)
;
è¯¦ç»†æ¨¡å¼
â€‹
verbose
å‚æ•°å¯ç”¨äºŽAPIä¸­çš„å¤§éƒ¨åˆ†å¯¹è±¡ï¼ˆé“¾æŽ¥ï¼Œæ¨¡åž‹ï¼Œå·¥å…·ï¼Œä»£ç†ç­‰)ä½œä¸ºæž„é€ å‚æ•°ã€‚ä¾‹å¦‚ï¼Œ
new LLMChain({ verbose: true })
ï¼Œå®ƒç›¸å½“äºŽå°†
callbacks
å‚æ•°ä¼ é€’ç»™è¯¥å¯¹è±¡å’Œæ‰€æœ‰å­å¯¹è±¡çš„
ConsoleCallbackHandler
ã€‚è¿™å¯¹äºŽè°ƒè¯•éžå¸¸æœ‰ç”¨ï¼Œå› ä¸ºå®ƒä¼šå°†æ‰€æœ‰äº‹ä»¶è®°å½•åœ¨æŽ§åˆ¶å°ä¸Šã€‚æ‚¨è¿˜å¯ä»¥é€šè¿‡è®¾ç½®çŽ¯å¢ƒå˜é‡
LANGCHAIN_VERBOSE=true
æ¥ä¸ºæ•´ä¸ªåº”ç”¨ç¨‹åºå¯ç”¨è¯¦ç»†æ¨¡å¼ã€‚
import
{
PromptTemplate
}
from
"langchain/prompts"
;
import
{
LLMChain
}
from
"langchain/chains"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
const
chain
=
new
LLMChain
(
{
llm
:
new
OpenAI
(
{
temperature
:
0
}
)
,
prompt
:
PromptTemplate
.
fromTemplate
(
"Hello, world!"
)
,
// This will enable logging of all Chain *and* LLM events to the console.
verbose
:
true
,
}
)
;
ä½ ä½•æ—¶éœ€è¦ä½¿ç”¨å®ƒä»¬ï¼Ÿ
â€‹
æž„é€ å‡½æ•°å›žè°ƒæœ€é€‚ç”¨äºŽè¯¸å¦‚æ—¥å¿—è®°å½•ï¼Œç›‘è§†ç­‰ç”¨ä¾‹ï¼Œè¿™äº›ç”¨ä¾‹ä¸ç‰¹å®šäºŽå•ä¸ªè¯·æ±‚ï¼Œè€Œæ˜¯é€‚ç”¨äºŽæ•´ä¸ªé“¾ã€‚ä¾‹å¦‚ï¼Œå¦‚æžœæ‚¨è¦è®°å½•æ‰€æœ‰å‘é€åˆ°LLMChainçš„è¯·æ±‚ï¼Œåˆ™åº”å°†å¤„ç†ç¨‹åºä¼ é€’ç»™æž„é€ å‡½æ•°ã€‚
è¯·æ±‚å›žè°ƒæœ€é€‚ç”¨äºŽæµå¼ä¼ è¾“ç­‰ç”¨ä¾‹ï¼Œå…¶ä¸­æ‚¨éœ€è¦å°†å•ä¸ªè¯·æ±‚çš„è¾“å‡ºæµåˆ°ç‰¹å®šçš„websocketè¿žæŽ¥æˆ–å…¶ä»–ç±»ä¼¼çš„ç”¨ä¾‹ã€‚ä¾‹å¦‚ï¼Œå¦‚æžœæ‚¨æƒ³å°†å•ä¸ªè¯·æ±‚çš„è¾“å‡ºæµåˆ°websocketï¼Œåˆ™åº”å°†å¤„ç†ç¨‹åºä¼ é€’ç»™
call()
æ–¹æ³•ã€‚
ä½¿ç”¨ç¤ºä¾‹
â€‹
å†…ç½®å¤„ç†ç¨‹åº
â€‹
LangChainæä¾›äº†ä¸€äº›å†…ç½®å¤„ç†ç¨‹åºï¼Œå¯ç”¨äºŽå…¥é—¨ã€‚è¿™äº›å¯åœ¨
langchain/callbacks
æ¨¡å—ä¸­ä½¿ç”¨ã€‚æœ€åŸºæœ¬çš„å¤„ç†ç¨‹åºæ˜¯
ConsoleCallbackHandler
ï¼Œåªéœ€å°†æ‰€æœ‰äº‹ä»¶è®°å½•åˆ°æŽ§åˆ¶å°å³å¯ã€‚åœ¨å°†
verbose
æ ‡å¿—è®¾ç½®ä¸º
true
çš„æƒ…å†µä¸‹ï¼Œ
ConsoleCallbackHandler
å°†åœ¨ä¸æ˜¾å¼ä¼ é€’çš„æƒ…å†µä¸‹è¢«è°ƒç”¨ã€‚
import
{
ConsoleCallbackHandler
}
from
"langchain/callbacks"
;
import
{
LLMChain
}
from
"langchain/chains"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
PromptTemplate
}
from
"langchain/prompts"
;
export
const
run
=
async
(
)
=>
{
const
handler
=
new
ConsoleCallbackHandler
(
)
;
const
llm
=
new
OpenAI
(
{
temperature
:
0
,
callbacks
:
[
handler
]
}
)
;
const
prompt
=
PromptTemplate
.
fromTemplate
(
"1 + {number} ="
)
;
const
chain
=
new
LLMChain
(
{
prompt
,
llm
,
callbacks
:
[
handler
]
}
)
;
const
output
=
await
chain
.
call
(
{
number
:
2
}
)
;
/*
Entering new llm_chain chain...
Finished chain.
*/
console
.
log
(
output
)
;
/*
{ text: ' 3\n\n3 - 1 = 2' }
*/
// The non-enumerable key `__run` contains the runId.
console
.
log
(
output
.
__run
)
;
/*
{ runId: '90e1f42c-7cb4-484c-bf7a-70b73ef8e64b' }
*/
}
;
One-off handlers
â€‹
æ‚¨å¯ä»¥é€šè¿‡å°†æ™®é€šå¯¹è±¡ä¼ é€’ç»™
callbacks
å‚æ•°æ¥åˆ›å»ºä¸€ä¸ªä¸´æ—¶å¤„ç†ç¨‹åºã€‚è¯¥å¯¹è±¡åº”å®žçŽ°
CallbackHandlerMethods
æŽ¥å£ã€‚å¦‚æžœæ‚¨éœ€è¦åˆ›å»ºä¸€ä¸ªä»…ç”¨äºŽå•ä¸ªè¯·æ±‚çš„å¤„ç†ç¨‹åºï¼Œè¿™å°†éžå¸¸æœ‰ç”¨ï¼Œä¾‹å¦‚æµå¼ä¼ è¾“LLM / Agent /ç­‰çš„è¾“å‡ºåˆ°WebSocketã€‚
import
{
OpenAI
}
from
"langchain/llms/openai"
;
// To enable streaming, we pass in `streaming: true` to the LLM constructor.
// Additionally, we pass in a handler for the `handleLLMNewToken` event.
const
chat
=
new
OpenAI
(
{
maxTokens
:
25
,
streaming
:
true
,
}
)
;
const
response
=
await
chat
.
call
(
"Tell me a joke."
,
undefined
,
[
{
handleLLMNewToken
(
token
:
string
)
{
console
.
log
(
{
token
}
)
;
}
,
}
,
]
)
;
console
.
log
(
response
)
;
/*
{ token: '\n' }
{ token: '\n' }
{ token: 'Q' }
{ token: ':' }
{ token: ' Why' }
{ token: ' did' }
{ token: ' the' }
{ token: ' chicken' }
{ token: ' cross' }
{ token: ' the' }
{ token: ' playground' }
{ token: '?' }
{ token: '\n' }
{ token: 'A' }
{ token: ':' }
{ token: ' To' }
{ token: ' get' }
{ token: ' to' }
{ token: ' the' }
{ token: ' other' }
{ token: ' slide' }
{ token: '.' }
Q: Why did the chicken cross the playground?
A: To get to the other slide.
*/
å¤šä¸ªå¤„ç†ç¨‹åº
â€‹
æˆ‘ä»¬åœ¨
CallbackManager
ç±»ä¸Šæä¾›äº†ä¸€ç§æ–¹æ³•ï¼Œå…è®¸æ‚¨åˆ›å»ºä¸€ä¸ªä¸´æ—¶å¤„ç†ç¨‹åºã€‚å¦‚æžœæ‚¨éœ€è¦åˆ›å»ºä¸€ä¸ªä»…ç”¨äºŽå•ä¸ªè¯·æ±‚çš„å¤„ç†ç¨‹åºï¼Œè¿™å°†éžå¸¸æœ‰ç”¨ï¼Œä¾‹å¦‚æµå¼ä¼ è¾“LLM / Agent /ç­‰çš„è¾“å‡ºåˆ°WebSocketã€‚
This is a more complete example that passes a
CallbackManager
to a ChatModel, and LLMChain, a Tool, and an Agent.
import
{
LLMChain
}
from
"langchain/chains"
;
import
{
AgentExecutor
,
ZeroShotAgent
}
from
"langchain/agents"
;
import
{
BaseCallbackHandler
}
from
"langchain/callbacks"
;
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
Calculator
}
from
"langchain/tools/calculator"
;
import
{
AgentAction
}
from
"langchain/schema"
;
export
const
run
=
async
(
)
=>
{
// You can implement your own callback handler by extending BaseCallbackHandler
class
CustomHandler
extends
BaseCallbackHandler
{
name
=
"custom_handler"
;
handleLLMNewToken
(
token
:
string
)
{
console
.
log
(
"token"
,
{
token
}
)
;
}
handleLLMStart
(
llm
:
{
name
:
string
}
,
_prompts
:
string
[
]
)
{
console
.
log
(
"handleLLMStart"
,
{
llm
}
)
;
}
handleChainStart
(
chain
:
{
name
:
string
}
)
{
console
.
log
(
"handleChainStart"
,
{
chain
}
)
;
}
handleAgentAction
(
action
:
AgentAction
)
{
console
.
log
(
"handleAgentAction"
,
action
)
;
}
handleToolStart
(
tool
:
{
name
:
string
}
)
{
console
.
log
(
"handleToolStart"
,
{
tool
}
)
;
}
}
const
handler1
=
new
CustomHandler
(
)
;
// Additionally, you can use the `fromMethods` method to create a callback handler
const
handler2
=
BaseCallbackHandler
.
fromMethods
(
{
handleLLMStart
(
llm
,
_prompts
:
string
[
]
)
{
console
.
log
(
"handleLLMStart: I'm the second handler!!"
,
{
llm
}
)
;
}
,
handleChainStart
(
chain
)
{
console
.
log
(
"handleChainStart: I'm the second handler!!"
,
{
chain
}
)
;
}
,
handleAgentAction
(
action
)
{
console
.
log
(
"handleAgentAction"
,
action
)
;
}
,
handleToolStart
(
tool
)
{
console
.
log
(
"handleToolStart"
,
{
tool
}
)
;
}
,
}
)
;
// You can restrict callbacks to a particular object by passing it upon creation
const
model
=
new
ChatOpenAI
(
{
temperature
:
0
,
callbacks
:
[
handler2
]
,
// this will issue handler2 callbacks related to this model
streaming
:
true
,
// needed to enable streaming, which enables handleLLMNewToken
}
)
;
const
tools
=
[
new
Calculator
(
)
]
;
const
agentPrompt
=
ZeroShotAgent
.
createPrompt
(
tools
)
;
const
llmChain
=
new
LLMChain
(
{
llm
:
model
,
prompt
:
agentPrompt
,
callbacks
:
[
handler2
]
,
// this will issue handler2 callbacks related to this chain
}
)
;
const
agent
=
new
ZeroShotAgent
(
{
llmChain
,
allowedTools
:
[
"search"
]
,
}
)
;
const
agentExecutor
=
AgentExecutor
.
fromAgentAndTools
(
{
agent
,
tools
,
}
)
;
/*
* When we pass the callback handler to the agent executor, it will be used for all
* callbacks related to the agent and all the objects involved in the agent's
* execution, in this case, the Tool, LLMChain, and LLM.
*
* The `handler2` callback handler will only be used for callbacks related to the
* LLMChain and LLM, since we passed it to the LLMChain and LLM objects upon creation.
*/
const
result
=
await
agentExecutor
.
call
(
{
input
:
"What is 2 to the power of 8"
,
}
,
[
handler1
]
)
;
// this is needed to see handleAgentAction
/*
handleChainStart { chain: { name: 'agent_executor' } }
handleChainStart { chain: { name: 'llm_chain' } }
handleChainStart: I'm the second handler!! { chain: { name: 'llm_chain' } }
handleLLMStart { llm: { name: 'openai' } }
handleLLMStart: I'm the second handler!! { llm: { name: 'openai' } }
token { token: '' }
token { token: 'I' }
token { token: ' can' }
token { token: ' use' }
token { token: ' the' }
token { token: ' calculator' }
token { token: ' tool' }
token { token: ' to' }
token { token: ' solve' }
token { token: ' this' }
token { token: '.\n' }
token { token: 'Action' }
token { token: ':' }
token { token: ' calculator' }
token { token: '\n' }
token { token: 'Action' }
token { token: ' Input' }
token { token: ':' }
token { token: ' ' }
token { token: '2' }
token { token: '^' }
token { token: '8' }
token { token: '' }
handleAgentAction {
tool: 'calculator',
toolInput: '2^8',
log: 'I can use the calculator tool to solve this.\n' +
'Action: calculator\n' +
'Action Input: 2^8'
}
handleToolStart { tool: { name: 'calculator' } }
handleChainStart { chain: { name: 'llm_chain' } }
handleChainStart: I'm the second handler!! { chain: { name: 'llm_chain' } }
handleLLMStart { llm: { name: 'openai' } }
handleLLMStart: I'm the second handler!! { llm: { name: 'openai' } }
token { token: '' }
token { token: 'That' }
token { token: ' was' }
token { token: ' easy' }
token { token: '!\n' }
token { token: 'Final' }
token { token: ' Answer' }
token { token: ':' }
token { token: ' ' }
token { token: '256' }
token { token: '' }
*/
console
.
log
(
result
)
;
/*
{
output: '256',
__run: { runId: '26d481a6-4410-4f39-b74d-f9a4f572379a' }
}
*/
}
;