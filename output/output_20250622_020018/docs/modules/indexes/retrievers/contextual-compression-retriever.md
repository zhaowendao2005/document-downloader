---
url: https://js.langchain.com.cn/docs/modules/indexes/retrievers/contextual-compression-retriever
crawled_at: 2025-06-22T02:00:23.076082
---

contextual-compression-retriever
上下文压缩检索器
上下文压缩检索器旨在通过更好地考虑查询上下文，改进向量存储文档相似性搜索返回的答案。
它包装另一个检索器，并在初始相似性搜索后使用文档压缩器作为中间步骤，从检索到的文档中删除与初始查询无关的信息。
这减少了后续链在解析检索到的文档和作出最终判断时必须处理的干扰量。
用法
​
This example shows how to intialize a
ContextualCompressionRetriever
with a vector store and a document compressor:
import
*
as
fs
from
"fs"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
RecursiveCharacterTextSplitter
}
from
"langchain/text_splitter"
;
import
{
RetrievalQAChain
}
from
"langchain/chains"
;
import
{
HNSWLib
}
from
"langchain/vectorstores/hnswlib"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
ContextualCompressionRetriever
}
from
"langchain/retrievers/contextual_compression"
;
import
{
LLMChainExtractor
}
from
"langchain/retrievers/document_compressors/chain_extract"
;
const
model
=
new
OpenAI
(
)
;
const
baseCompressor
=
LLMChainExtractor
.
fromLLM
(
model
)
;
const
text
=
fs
.
readFileSync
(
"state_of_the_union.txt"
,
"utf8"
)
;
const
textSplitter
=
new
RecursiveCharacterTextSplitter
(
{
chunkSize
:
1000
}
)
;
const
docs
=
await
textSplitter
.
createDocuments
(
[
text
]
)
;
// Create a vector store from the documents.
const
vectorStore
=
await
HNSWLib
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
)
;
const
retriever
=
new
ContextualCompressionRetriever
(
{
baseCompressor
,
baseRetriever
:
vectorStore
.
asRetriever
(
)
,
}
)
;
const
chain
=
RetrievalQAChain
.
fromLLM
(
model
,
retriever
)
;
const
res
=
await
chain
.
call
(
{
query
:
"What did the speaker say about Justice Breyer?"
,
}
)
;
console
.
log
(
{
res
}
)
;