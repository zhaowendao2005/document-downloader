---
url: https://js.langchain.com.cn/docs/modules/memory/examples/buffer_memory_chat
crawled_at: 2025-06-22T02:00:24.826897
---

使用缓冲内存与聊天模型翻译的中文结果
本示例介绍如何将聊天特定的内存类与聊天模型配合使用。翻译的中文结果
需要注意的关键点是，设置
returnMessages: true
会使内存返回聊天消息列表而不是字符串。翻译的中文结果
import
{
ConversationChain
}
from
"langchain/chains"
;
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
ChatPromptTemplate
,
HumanMessagePromptTemplate
,
SystemMessagePromptTemplate
,
MessagesPlaceholder
,
}
from
"langchain/prompts"
;
import
{
BufferMemory
}
from
"langchain/memory"
;
export
const
run
=
async
(
)
=>
{
const
chat
=
new
ChatOpenAI
(
{
temperature
:
0
}
)
;
const
chatPrompt
=
ChatPromptTemplate
.
fromPromptMessages
(
[
SystemMessagePromptTemplate
.
fromTemplate
(
"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know."
)
,
new
MessagesPlaceholder
(
"history"
)
,
HumanMessagePromptTemplate
.
fromTemplate
(
"{input}"
)
,
]
)
;
const
chain
=
new
ConversationChain
(
{
memory
:
new
BufferMemory
(
{
returnMessages
:
true
,
memoryKey
:
"history"
}
)
,
prompt
:
chatPrompt
,
llm
:
chat
,
}
)
;
const
response
=
await
chain
.
call
(
{
input
:
"hi! whats up?"
,
}
)
;
console
.
log
(
response
)
;
}
;