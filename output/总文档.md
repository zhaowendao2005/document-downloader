--- 文件: output_20250622_020018\docs.md ---
---
url: https://js.langchain.com.cn/docs/
crawled_at: 2025-06-22T02:00:19.480556
---

LangChain JS/TS 中文文档
LangChain中文网 500页超详细中文文档教程，助力LLM/chatGPT应用开发
​
LangChain是一种基于语言模型的应用程序开发框架。我们相信，最强大和不同的应用程序不仅会通过API调用语言模型，还会:
了解数据
: 连接语言模型与其他数据源
有代理
: 允许语言模型与其环境交互
LangChain框架是根据上述原则设计的。
入门
​
查看下面的指南，了解如何使用LangChain创建语言模型应用程序。
快速入门
使用LLMs
快速入门
使用聊天模型
组件
​
LangChain提供支持的几个主要模块。针对每个模块，我们提供一些示例，以便开始并熟悉一些概念。每个示例都链接到使用的模块的API文档。
这些模块按递增的复杂性排列#
模式
: 这包括在整个库中使用的接口和基类。
模型
: 这包括与各种LLM，聊天模型和嵌入模型的集成。
提示符
: 这包括提示符模板和与提示符一起使用的功能，例如输出解析器和示例选择器。
索引
: 这包括用于处理您自己的数据的模式和功能，并使其准备好与语言模型交互（包括文档加载程序、向量存储器、文本分割器和检索器)。
Memory
: 内存是指链/代理调用之间保存状态的概念。LangChain提供标准的内存接口，一个内存实现的集合，以及使用内存的链/代理的示例。
Chains
: 链不仅仅只是单个LLM调用，它是一系列调用（不论是对LLM还是其他工具)。LangChain提供标准的链接口，与其他工具进行多样化的集成，并提供常见应用的端到端链。
Agents
: 代理涉及LLM对要采取的行动做出决策，采取行动，看到观察结果，然后重复该过程直到完成。LangChain提供代理标准接口，提供一系列代理供选择以及端到端代理示例。
API 参考
​
这里
可以找到LangChain所有模块的API参考以及所有导出类和函数的完整文档。
生产
​
当您从原型制作过渡到生产过程时，我们正在开发资源来帮助您完成此过程。
These including:
部署
: 关于如何将应用程序部署到生产环境的资源。
事件/回调
: LangChain模块公开的事件资源。
跟踪
: 有关如何使用跟踪记录和调试应用程序的资源。
附加资源
​
以下是我们认为在开发应用程序时可能有用的其他资源收集！
LangChainHub
: LangChainHub 是一个分享和探索其他语言链和代理的地方。
Discord
: 加入我们的 Discord，讨论关于 LangChain 的一切！
生产支持
: 当您将 LangChain 移入生产环境时，我们很乐意提供更全面的支持。请填写此表单，我们将为您设置专用的支持 Slack 频道。



--- 文件: output_20250622_020018\docs\ecosystem\databerry.md ---
---
url: https://js.langchain.com.cn/docs/ecosystem/databerry
crawled_at: 2025-06-22T02:00:19.507719
---

数据莓
本页面介绍如何在LangChain中使用
Databerry
。
什么是数据莓？
​
数据莓是一个
开源
的文档检索平台，帮助连接您的个人数据和大型语言模型。
快速开始
​
从LangChain中检索存储在数据莓中的文档非常容易！
import
{
DataberryRetriever
}
from
"langchain/retrievers/databerry"
;
const
retriever
=
new
DataberryRetriever
(
{
datastoreUrl
:
"https://api.databerry.ai/query/clg1xg2h80000l708dymr0fxc"
,
apiKey
:
"DATABERRY_API_KEY"
,
// optional: needed for private datastores
topK
:
8
,
// optional: default value is 3
}
)
;
// Create a chain that uses the OpenAI LLM and Databerry retriever.
const
chain
=
RetrievalQAChain
.
fromLLM
(
model
,
retriever
)
;
// Call the chain with a query.
const
res
=
await
chain
.
call
(
{
query
:
"What's Databerry?"
,
}
)
;
console
.
log
(
{
res
}
)
;
/*
{
res: {
text: 'Databerry provides a user-friendly solution to quickly setup a semantic search system over your personal data without any technical knowledge.'
}
}
*/



--- 文件: output_20250622_020018\docs\ecosystem\helicone.md ---
---
url: https://js.langchain.com.cn/docs/ecosystem/helicone
crawled_at: 2025-06-22T02:00:19.612630
---

Helicone
本页介绍如何在LangChain中使用
Helicone
。
Helicone 是什么？
​
Helicone是一个开源的观测平台，代理您的OpenAI流量，并为您提供有关您的花费、延迟和使用情况的关键见解。
快速入门
​
在您的LangChain环境中，您只需添加以下参数。
const
model
=
new
OpenAI
(
{
}
,
{
basePath
:
"https://oai.hconeai.com/v1"
,
}
)
;
const
res
=
await
model
.
call
(
"What is a helicone?"
)
;
现在，前往
helicone.ai
创建您的帐户，并在我们的仪表板中添加OpenAI API密钥以查看日志。
如何启用Helicone缓存
​
const
model
=
new
OpenAI
(
{
}
,
{
basePath
:
"https://oai.hconeai.com/v1"
,
baseOptions
:
{
headers
:
{
"Helicone-Cache-Enabled"
:
"true"
,
}
,
}
,
}
)
;
const
res
=
await
model
.
call
(
"What is a helicone?"
)
;
Helicone缓存文档
如何使用Helicone自定义属性
​
const
model
=
new
OpenAI
(
{
}
,
{
basePath
:
"https://oai.hconeai.com/v1"
,
baseOptions
:
{
headers
:
{
"Helicone-Property-Session"
:
"24"
,
"Helicone-Property-Conversation"
:
"support_issue_2"
,
"Helicone-Property-App"
:
"mobile"
,
}
,
}
,
}
)
;
const
res
=
await
model
.
call
(
"What is a helicone?"
)
;
Helicone property docs



--- 文件: output_20250622_020018\docs\ecosystem\unstructured.md ---
---
url: https://js.langchain.com.cn/docs/ecosystem/unstructured
crawled_at: 2025-06-22T02:00:19.634627
---

非结构化数据
本页面介绍如何在LangChain中使用
非结构化数据
。
什么是非结构化数据？
​
非结构化是一个
开源
Python包，用于从原始文档中提取文本以用于机器学习应用。目前支持分区Word文档（.doc或.docx格式)，幻灯片（.ppt或.pptx格式)， Pdf ， html文件，图像，电子邮件（.eml或.msg格式)，电子书， markdown，和纯文本文件。
unstructured
是一个Python包，不能直接与TS / JS一起使用，但是Unstructured还维护一个
REST API
以支持使用其他编程语言编写的预处理流水线。托管的Unstructured API的端点为
https://api.unstructured.io/general/v0/general
，或者您可以使用
此处
找到的说明在本地运行服务。
目前（截至2023年4月26日)， Unstructured API不需要API密钥。将来，API将需要API密钥。
Unstructured文档页面
会包括有关如何获取API密钥的说明（一旦可用)。
快速开始
​
您可以使用以下代码在
langchain
中使用非结构化数据。
将文件名替换为要处理的文件。
如果您正在本地运行容器，则将url切换为
http://127.0.0.1:8000/general/v0/general
。
有关详细信息，请查看
API文档页面
。
import
{
UnstructuredLoader
}
from
"langchain/document_loaders/fs/unstructured"
;
const
options
=
{
apiKey
:
"MY_API_KEY"
,
}
;
const
loader
=
new
UnstructuredLoader
(
"src/document_loaders/example_data/notion.md"
,
options
)
;
const
docs
=
await
loader
.
load
(
)
;
目录
​
您还可以使用
UnstructuredDirectoryLoader
从目录中加载所有文件，该类继承自
DirectoryLoader
:
import
{
UnstructuredDirectoryLoader
}
from
"langchain/document_loaders/fs/unstructured"
;
const
options
=
{
apiKey
:
"MY_API_KEY"
,
}
;
const
loader
=
new
UnstructuredDirectoryLoader
(
"langchain/src/document_loaders/tests/example_data"
,
options
)
;
const
docs
=
await
loader
.
load
(
)
;
目前，
UnstructuredLoader
支持以下文档类型:
纯文本文件（
.txt
/
.text
)
PDF（
.pdf
)
Word文档（
.doc
/
.docx
)
PowerPoints（
.ppt
/
.pptx
)
图像文件（
.jpg
/
.jpeg
)
电子邮件（
.eml
/
.msg
)
HTML（
.html
)
Markdown文件（
.md
)
UnstructuredLoader
的输出将是一个类似以下内容的
Document
对象数组:
[
Document
{
pageContent
:
`
Decoder: The decoder is also composed of a stack of N = 6
identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a
third sub-layer, wh
ich performs multi-head attention over the output of the encoder stack. Similar to the encoder, we
employ residual connections around each of the sub-layers, followed by layer normalization. We also
modify the self
-attention sub-layer in the decoder stack to prevent positions from attending to subsequent
positions. This masking, combined with fact that the output embeddings are offset by one position,
ensures that the predic
tions for position i can depend only on the known outputs at positions less than i.
`
,
metadata
:
{
page_number
:
3
,
filename
:
'1706.03762.pdf'
,
category
:
'NarrativeText'
}
}
,
Document
{
pageContent
:
'3.2 Attention'
,
metadata
:
{
page_number
:
3
,
filename
:
'1706.03762.pdf'
,
category
:
'Title'
}
]



--- 文件: output_20250622_020018\docs\getting-started\guide-chat.md ---
---
url: https://js.langchain.com.cn/docs/getting-started/guide-chat
crawled_at: 2025-06-22T02:00:19.817403
---

快速入门， 使用聊天模型
聊天模型是一种语言模型的变体。
虽然聊天模型在幕后使用语言模型， 但它们公开的接口有些不同。
它们不是暴露一个"输入文本，输出文本"的API，而是暴露一个"聊天消息"到输入和输出的接口。
聊天模型API相当新， 因此我们仍在找出正确的抽象。
安装和设置
​
要开始使用，请按照
安装说明
安装LangChain。
入门指南
​
本节介绍如何使用聊天模型入门。接口基于消息而不是原始文本。
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
HumanChatMessage
,
SystemChatMessage
}
from
"langchain/schema"
;
const
chat
=
new
ChatOpenAI
(
{
temperature
:
0
}
)
;
在这里，我们使用存储在环境变量
OPENAI_API_KEY
或
AZURE_OPENAI_API_KEY
中的API密钥创建聊天模型。在本节中，我们将调用此聊天模型。
ⓘ
注意，如果您使用的是Azure OpenAI，请确保还设置了环境变量
AZURE_OPENAI_API_INSTANCE_NAME
，
AZURE_OPENAI_API_DEPLOYMENT_NAME
和
AZURE_OPENAI_API_VERSION
。
聊天模型: 消息作为输入， 消息作为输出
​
通过将一个或多个消息传递给聊天模型，可以获取聊天完成。响应也将是一条消息。LangChain当前支持的消息类型为
AIChatMessage
，
HumanChatMessage
，
SystemChatMessage
，和通用
ChatMessage
-- ChatMessage采用任意角色参数，在这里，我们不会使用。大多数时候，您只需要处理
HumanChatMessage
，
AIChatMessage
和
SystemChatMessage
。
const
response
=
await
chat
.
call
(
[
new
HumanChatMessage
(
"Translate this sentence from English to French. I love programming."
)
,
]
)
;
console
.
log
(
response
)
;
AIChatMessage { text: "J'aime programmer." }
多条消息
​
OpenAI的在线聊天模型（目前包括
gpt-3.5-turbo
和
gpt-4
以及Azure OpenAI的
gpt-4-32k
)支持多条消息作为输入。请参见
[这里]
(
https://platform.openai.com/docs/guides/chat/chat
-vs-completions)了解更多信息。以下是向聊天模型发送系统消息和用户消息的示例:
ⓘ
注意，如果您使用Azure OpenAI，请确保更改部署名称以使用您选择的模型的部署。
const
responseB
=
await
chat
.
call
(
[
new
SystemChatMessage
(
"You are a helpful assistant that translates English to French."
)
,
new
HumanChatMessage
(
"Translate: I love programming."
)
,
]
)
;
console
.
log
(
responseB
)
;
AIChatMessage { text: "J'aime programmer." }
多条完成
​
您可以进一步生成多个消息集的完成，使用generate。这将返回具有额外消息参数的LLMResult。
const
responseC
=
await
chat
.
generate
(
[
[
new
SystemChatMessage
(
"You are a helpful assistant that translates English to French."
)
,
new
HumanChatMessage
(
"Translate this sentence from English to French. I love programming."
)
,
]
,
[
new
SystemChatMessage
(
"You are a helpful assistant that translates English to French."
)
,
new
HumanChatMessage
(
"Translate this sentence from English to French. I love artificial intelligence."
)
,
]
,
]
)
;
console
.
log
(
responseC
)
;
{
generations: [
[
{
text: "J'aime programmer.",
message: AIChatMessage { text: "J'aime programmer." },
}
],
[
{
text: "J'aime l'intelligence artificielle.",
message: AIChatMessage { text: "J'aime l'intelligence artificielle." }
}
]
]
}
聊天提示模板: 管理聊天模型的提示
​
您可以通过使用
MessagePromptTemplate
来利用模板。您可以从一个或多个
MessagePromptTemplate
构建
ChatPromptTemplate
。您可以使用
ChatPromptTemplate
的
formatPromptValue
- 这将返回一个
PromptValue
您可以将
PromptValue
转换为字符串或消息对象，具体取决于您是否想将格式化值用作llm或聊天模型的输入。
继续上一个示例:
import {
SystemMessagePromptTemplate,
HumanMessagePromptTemplate,
ChatPromptTemplate,
} from "langchain/prompts";
```typescript
首先，我们创建一个可重复使用的模板:
const translationPrompt = ChatPromptTemplate.fromPromptMessages([
SystemMessagePromptTemplate.fromTemplate(
"You are a helpful assistant that translates {input_language} to {output_language}."
),
HumanMessagePromptTemplate.fromTemplate("{text}"),
]);
然后，我们可以使用模板生成响应:
const
responseA
=
await
chat
.
generatePrompt
(
[
await
translationPrompt
.
formatPromptValue
(
{
input_language
:
"English"
,
output_language
:
"French"
,
text
:
"I love programming."
,
}
)
,
]
)
;
console
.
log
(
responseA
)
;
{
generations: [
[
{
text: "J'aime programmer.",
message: AIChatMessage { text: "J'aime programmer." }
}
]
]
}
模型+提示=LLMChain
​
这种要求用户完成格式化提示的模式非常常见，因此我们介绍了下一个谜题: LLMChain
const
chain
=
new
LLMChain
(
{
prompt
:
translationPrompt
,
llm
:
chat
,
}
)
;
然后您可以调用链:
const
responseB
=
await
chain
.
call
(
{
input_language
:
"English"
,
output_language
:
"French"
,
text
:
"I love programming."
,
}
)
;
console
.
log
(
responseB
)
;
{ text: "J'aime programmer." }
代理: 根据用户输入动态运行链
​
最后，我们介绍了工具和代理，它们通过其他能力扩展了模型，例如搜索或计算器。
工具是一个函数，它接受一个字符串（例如搜索查询)并返回一个字符串（例如搜索结果)。它们还有一个名称和描述，由聊天模型用于识别应该调用哪个工具。
class
Tool
{
name
:
string
;
description
:
string
;
call
(
arg
:
string
)
:
Promise
<
string
>
;
}
代理是对代理提示链（例如MRKL)的无状态包装器，它负责将工具按格式放入提示中，并解析从聊天模型获取的响应。
interface
AgentStep
{
action
:
AgentAction
;
observation
:
string
;
}
interface
AgentAction
{
tool
:
string
;
// Tool.name
toolInput
:
string
;
// Tool.call argument
}
interface
AgentFinish
{
returnValues
:
object
;
}
class
Agent
{
plan
(
steps
:
AgentStep
[
]
,
inputs
:
object
)
:
Promise
<
AgentAction
|
AgentFinish
>
;
}
要使代理更强大，我们需要使它们迭代，即多次调用模型，直到它们到达最终答案。这是AgentExecutor的工作。
class
AgentExecutor
{
// a simplified implementation
run
(
inputs
:
object
)
{
const
steps
=
[
]
;
while
(
true
)
{
const
step
=
await
this
.
agent
.
plan
(
steps
,
inputs
)
;
if
(
step
instanceof
AgentFinish
)
{
return
step
.
returnValues
;
}
steps
.
push
(
step
)
;
}
}
}
最后，我们可以使用AgentExecutor运行代理:
// Define the list of tools the agent can use
const
tools
=
[
new
SerpAPI
(
process
.
env
.
SERPAPI_API_KEY
,
{
location
:
"Austin,Texas,United States"
,
hl
:
"en"
,
gl
:
"us"
,
}
)
,
]
;
// Create the agent from the chat model and the tools
const
agent
=
ChatAgent
.
fromLLMAndTools
(
new
ChatOpenAI
(
)
,
tools
)
;
// Create an executor, which calls to the agent until an answer is found
const
executor
=
AgentExecutor
.
fromAgentAndTools
(
{
agent
,
tools
}
)
;
const
responseG
=
await
executor
.
run
(
"How many people live in canada as of 2023?"
)
;
console
.
log
(
responseG
)
;
38,626,704.
内存: 向链和代理添加状态
​
您还可以使用链来存储状态。这对于聊天机器人等应用程序非常有用，因为您需要跟踪会话历史记录。MessagesPlaceholder是一个特殊的提示模板，每次调用将替换为传递的消息。
const
chatPrompt
=
ChatPromptTemplate
.
fromPromptMessages
(
[
SystemMessagePromptTemplate
.
fromTemplate
(
"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know."
)
,
new
MessagesPlaceholder
(
"history"
)
,
HumanMessagePromptTemplate
.
fromTemplate
(
"{input}"
)
,
]
)
;
const
chain
=
new
ConversationChain
(
{
memory
:
new
BufferMemory
(
{
returnMessages
:
true
,
memoryKey
:
"history"
}
)
,
prompt
:
chatPrompt
,
llm
:
chat
,
}
)
;
该链将内部累加发送到模型和接收的输出消息。然后，在下一次调用时，它将把消息注入提示符中。所以你可以多次调用该链，并且它会记住先前的消息。(The chain will internally accumulate the messages sent to the model and the ones received as output. Then it will inject the messages into the prompt on the next call. So you can call the chain a few times and it remembers previous messages.)
const
responseH
=
await
chain
.
call
(
{
input
:
"hi from London, how are you doing today"
,
}
)
;
console
.
log
(
responseH
)
;
{
response: "Hello! As an AI language model, I don't have feelings, but I'm functioning properly and ready to assist you with any questions or tasks you may have. How can I help you today?"
}
const
responseI
=
await
chain
.
call
(
{
input
:
"Do you know where I am?"
,
}
)
;
console
.
log
(
responseI
)
;
{
response: "Yes, you mentioned that you are from London. However, as an AI language model, I don't have access to your current location unless you provide me with that information."
}
流式处理
​
您还可以使用流式 API 获取按照生成顺序返回的单词。这对于聊天机器人等情况很有用，因为您希望在生成过程中向用户展示正在生成的内容。请注意，在启用流式处理时，:OpenAI 目前不支持“tokenUsage”报告。
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
HumanChatMessage
}
from
"langchain/schema"
;
const
chat
=
new
ChatOpenAI
(
{
streaming
:
true
,
callbacks
:
[
{
handleLLMNewToken
(
token
:
string
)
{
process
.
stdout
.
write
(
token
)
;
}
,
}
,
]
,
}
)
;
await
chat
.
call
(
[
new
HumanChatMessage
(
"Write me a song about sparkling water."
)
,
]
)
;
/*
Verse 1:
Bubbles rise, crisp and clear
Refreshing taste that brings us cheer
Sparkling water, so light and pure
Quenches our thirst, it's always secure
Chorus:
Sparkling water, oh how we love
Its fizzy bubbles and grace above
It's the perfect drink, anytime, anyplace
Refreshing as it gives us a taste
Verse 2:
From morning brunch to evening feast
It's the perfect drink for a treat
A sip of it brings a smile so bright
Our thirst is quenched in just one sip so light
...
*/



--- 文件: output_20250622_020018\docs\getting-started\guide-llm.md ---
---
url: https://js.langchain.com.cn/docs/getting-started/guide-llm
crawled_at: 2025-06-22T02:00:19.843831
---

快速入门， 使用LLMs
本教程将快速介绍如何使用LangChain构建端到端语言模型应用程序。
安装和设置
​
要开始，请按照
安装说明
安装LangChain。
选择LLM
​
使用LangChain通常需要与一个或多个模型提供程序，数据存储，API等进行集成。
对于这个例子，我们将使用OpenAI的API，因此不需要其他设置。
构建语言模型应用程序
​
现在我们已经安装了LangChain，可以开始构建我们的语言模型应用程序。
LangChain提供了许多模块，可用于构建语言模型应用程序。可以将模块结合起来创建更复杂的应用程序，也可以单独用于简单的应用程序。
LLM:从语言模型获取预测
​
LangChain的最基本构建模块是针对某些输入调用LLM。 让我们通过一个简单的示例来介绍如何执行此操作。 为此，假设我们正在构建一项服务，该服务根据公司生产的产品生成公司名称。
为了做到这一点，我们首先需要导入LLM包装器。
import
{
OpenAI
}
from
"langchain/llms/openai"
;
然后，我们将需要设置OpenAI密钥的环境变量。 这里有三个选项:
我们可以通过在
.env
文件中设置值，并使用
dotenv
软件包来读取它。
1.1. For OpenAI Api
OPENAI_API_KEY
=
"..."
1.2. For Azure OpenAI:
AZURE_OPENAI_API_KEY
=
"..."
AZURE_OPENAI_API_INSTANCE_NAME
=
"..."
AZURE_OPENAI_API_DEPLOYMENT_NAME
=
"..."
AZURE_OPENAI_API_COMPLETIONS_DEPLOYMENT_NAME
=
"..."
AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME
=
"..."
AZURE_OPENAI_API_VERSION
=
"..."
或者我们可以使用以下命令在你的shell中导出环境变量#。
2.1. For OpenAI Api
export
OPENAI_API_KEY
=
sk-
..
..
2.2. For Azure OpenAI:
export
AZURE_OPENAI_API_KEY
=
"..."
export
AZURE_OPENAI_API_INSTANCE_NAME
=
"..."
export
AZURE_OPENAI_API_DEPLOYMENT_NAME
=
"..."
export
AZURE_OPENAI_API_COMPLETIONS_DEPLOYMENT_NAME
=
"..."
export
AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME
=
"..."
export
AZURE_OPENAI_API_VERSION
=
"..."
或者我们可以在初始化wrapper时一起执行。在这个例子中，我们可能希望输出更随机，因此我们将使用高温度进行初始化。
3.1. For OpenAI Api
const
model
=
new
OpenAI
(
{
openAIApiKey
:
"sk-..."
,
temperature
:
0.9
}
)
;
3.2. For Azure OpenAI:
const
model
=
new
OpenAI
(
{
azureOpenAIApiKey
:
"..."
,
azureOpenAIApiInstanceName
:
"...."
,
azureOpenAIApiDeploymentName
:
"...."
,
azureOpenAIApiVersion
:
"...."
,
temperature
:
0.9
,
}
)
;
初始化wrapper后，我们现在可以在一些输入上调用它！
const
res
=
await
model
.
call
(
"What would be a good company name a company that makes colorful socks?"
)
;
console
.
log
(
res
)
;
{
res:
'Fantasy Sockery'
}
提示模板: 管理LLMs的提示
​
调用LLM是一个很好的第一步，但这只是开始。通常当你在应用程序中使用LLM时，你不会直接将用户输入发送到LLM。相反，你可能会使用用户输入来构建一个提示，然后将它发送到LLM。
例如，在前面的示例中，我们传递的文本是硬编码的，询问制作彩色袜子的公司名字。在这个想象中的服务中，我们只需要使用描述公司所做的事情的用户输入，并使用该信息格式化提示。
使用LangChain很容易实现！
首先让我们定义提示模板:
import
{
PromptTemplate
}
from
"langchain/prompts"
;
const
template
=
"What is a good name for a company that makes {product}?"
;
const
prompt
=
new
PromptTemplate
(
{
template
:
template
,
inputVariables
:
[
"product"
]
,
}
)
;
现在让我们看看它的工作原理！我们可以调用
.format
方法进行格式化。
const
res
=
await
prompt
.
format
(
{
product
:
"colorful socks"
}
)
;
console
.
log
(
res
)
;
{
res:
'What is a good name for a company that makes colorful socks?'
}
链: 在多步骤工作流中组合LLMs和提示
​
到目前为止，我们只是单独使用了PromptTemplate和LLM原语。但是，一个真正的应用程序不仅仅是一个原语，而是它们的组合。
LangChain中的链是由链接组成的，可以是像LLMs这样的原语，也可以是其他chains。
链的最核心类型是LLMChain，它由PromptTemplate和LLM组成。
扩展前面的例子，我们可以构建一个LLMChain，它采用用户输入，用PromptTemplate格式化它，然后将格式化的响应传递给LLM。
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
PromptTemplate
}
from
"langchain/prompts"
;
const
model
=
new
OpenAI
(
{
temperature
:
0.9
}
)
;
const
template
=
"What is a good name for a company that makes {product}?"
;
const
prompt
=
new
PromptTemplate
(
{
template
:
template
,
inputVariables
:
[
"product"
]
,
}
)
;
现在我们可以创建一个非常简单的链条，它将采用用户输入，用它格式化提示，然后将其发送给LLM:。
import
{
LLMChain
}
from
"langchain/chains"
;
const
chain
=
new
LLMChain
(
{
llm
:
model
,
prompt
:
prompt
}
)
;
现在我们可以仅指定产品运行该链条！
const
res
=
await
chain
.
call
(
{
product
:
"colorful socks"
}
)
;
console
.
log
(
res
)
;
{
res:
{
text:
'ColorfulCo Sockery.'
}
}
这就是第一条链 - LLM Chain。这是比较简单的链条之一，但是了解它的工作原理将为您更好地处理更复杂的链条打下基础。
Agents: 根据用户输入动态运行链
​
到目前为止，我们看过的链是以预定顺序运行的。
代理不再执行它们使用LLM来确定要执行的操作以及顺序。动作可以是使用工具并观察其输出，或者返回给用户。
如果正确使用代理，它们可以非常强大。在本教程中,我们通过最简单的，最高级别的API向您展示如何轻松使用代理。
为了加载代理，您应该了解以下概念:
工具（Tool）：执行特定职责的函数。这些职责可能包括：Google 搜索、数据库查询、代码 REPL（Read-Eval-Print Loop，读取-求值-输出循环）、其他链。目前工具的接口是一个期望输入字符串，并返回一个字符串的函数。
LLM: 代理使用的语言模型。
Agent: 所使用的代理。这应该是一个引用支持代理类的字符串。由于本教程重点介绍最简单的最高级API，因此仅涵盖使用标准支持的代理。
对于这个例子，你需要在
.env
文件中设置SerpAPI环境变量。
SERPAPI_API_KEY
=
"..."
安装
serpapi
包(Google搜索API):
npm
Yarn
pnpm
npm
install
-S serpapi
yarn
add
serpapi
pnpm
add
serpapi
现在我们可以开始了！
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
initializeAgentExecutorWithOptions
}
from
"langchain/agents"
;
import
{
SerpAPI
}
from
"langchain/tools"
;
import
{
Calculator
}
from
"langchain/tools/calculator"
;
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
tools
=
[
new
SerpAPI
(
process
.
env
.
SERPAPI_API_KEY
,
{
location
:
"Austin,Texas,United States"
,
hl
:
"en"
,
gl
:
"us"
,
}
)
,
new
Calculator
(
)
,
]
;
const
executor
=
await
initializeAgentExecutorWithOptions
(
tools
,
model
,
{
agentType
:
"zero-shot-react-description"
,
}
)
;
console
.
log
(
"Loaded agent."
)
;
const
input
=
"Who is Olivia Wilde's boyfriend?"
+
" What is his current age raised to the 0.23 power?"
;
console
.
log
(
`
Executing with input "
${
input
}
"...
`
)
;
const
result
=
await
executor
.
call
(
{
input
}
)
;
console
.
log
(
`
Got output
${
result
.
output
}
`
)
;
langchain-examples:start: Executing with input
"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?"
..
.
langchain-examples:start: Got output Olivia Wilde's boyfriend is Jason Sudeikis, and his current age raised to the
0.23
power is
2.4242784855673896
.
Memory: 在链和代理中添加状态
​
到目前为止，我们经历的所有链和代理都是无状态的。但是，通常情况下，您可能希望一个链或代理具有一些 "内存" 的概念，以便它可以记住其先前的交互信息。这是设计聊天机器人时的最明显和简单的例子 - 您希望它记住先前的消息，以便可以利用上下文进行更好的交流。这将是一种 "短期记忆"。在更复杂的情况下，您可以想象一个链/代理随着时间的推移记住关键信息 - 这将是一种 "长期记忆" 形式。
LangChain提供了几个专门为此目的创建的链。本节介绍了使用这些链之一（“ConversationChain”)。
默认情况下，“ConversationChain”具有一种简单的记忆类型，它会记住所有先前的输入/输出并将其添加到传递的上下文中。让我们来看看如何使用这个链。
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
BufferMemory
}
from
"langchain/memory"
;
import
{
ConversationChain
}
from
"langchain/chains"
;
const
model
=
new
OpenAI
(
{
}
)
;
const
memory
=
new
BufferMemory
(
)
;
const
chain
=
new
ConversationChain
(
{
llm
:
model
,
memory
:
memory
}
)
;
const
res1
=
await
chain
.
call
(
{
input
:
"Hi! I'm Jim."
}
)
;
console
.
log
(
res1
)
;
{
response:
" Hi Jim! It's nice to meet you. My name is AI. What would you like to talk about?"
}
const
res2
=
await
chain
.
call
(
{
input
:
"What's my name?"
}
)
;
console
.
log
(
res2
)
;
{
response:
' You said your name is Jim. Is there anything else you would like to talk about?'
}
流媒体
​
您还可以使用流API，随着生成的单词的流式返回，这对于聊天机器人等场景很有用，您希望向用户显示正在生成的内容。注意: 目前OpenAI不支持启用流时的“tokenUsage”报告。
import
{
OpenAI
}
from
"langchain/llms/openai"
;
// To enable streaming, we pass in `streaming: true` to the LLM constructor.
// Additionally, we pass in a handler for the `handleLLMNewToken` event.
const
chat
=
new
OpenAI
(
{
streaming
:
true
,
callbacks
:
[
{
handleLLMNewToken
(
token
:
string
)
{
process
.
stdout
.
write
(
token
)
;
}
,
}
,
]
,
}
)
;
await
chat
.
call
(
"Write me a song about sparkling water."
)
;
/*
Verse 1
Crystal clear and made with care
Sparkling water on my lips, so refreshing in the air
Fizzy bubbles, light and sweet
My favorite beverage I can’t help but repeat
Chorus
A toast to sparkling water, I’m feeling so alive
Let’s take a sip, and let’s take a drive
A toast to sparkling water, it’s the best I’ve had in my life
It’s the best way to start off the night
Verse 2
It’s the perfect drink to quench my thirst
It’s the best way to stay hydrated, it’s the first
A few ice cubes, a splash of lime
It will make any day feel sublime
...
*/



--- 文件: output_20250622_020018\docs\getting-started\install.md ---
---
url: https://js.langchain.com.cn/docs/getting-started/install
crawled_at: 2025-06-22T02:00:19.656408
---

安装和设置
支持的环境
​
LangChain 是使用 TypeScript 编写的，可以在以下环境中使用:
Node.js (ESM 和 CommonJS) - 18.x， 19.x， 20.x
Cloudflare Workers
Vercel / Next.js (浏览器， 无服务器和边缘函数)
Supabase Edge 函数
浏览器
Deno
快速开始
​
如果您想在 Node.js 中快速开始使用 LangChain，请
克隆此存储库
并按照自述文件中的说明设置依赖项。
如果您希望自己设置或者在其他环境中运行 LangChain，请继续阅读下面的说明。
安装
​
要开始使用 LangChain，请使用以下命令安装:
npm
Yarn
pnpm
npm
install
-S langchain
yarn
add
langchain
pnpm
add
langchain
TypeScript
​
LangChain 是使用 TypeScript 编写的，并为其所有的公共 API 提供了类型定义。
加载库
​
ESM
​
LangChain 为 Node.js 环境提供了一个 ESM 构建版。您可以使用以下语法导入它:
import
{
OpenAI
}
from
"langchain/llms/openai"
;
如果您在一个 ESM 项目中使用 TypeScript，我们建议您更新您的
tsconfig.json
，并包含以下设置:
tsconfig.json
{
"compilerOptions"
:
{
...
"target"
:
"ES2020"
,
// or higher
"module"
:
"nodenext"
,
}
}
CommonJS
​
LangChain提供了面向Node.js环境的CommonJS构建。你可以使用以下语法进行导入:。
const
{
OpenAI
}
=
require
(
"langchain/llms/openai"
)
;
Cloudflare Workers
​
LangChain可以在Cloudflare Workers中使用。你可以使用以下语法进行导入:。
import
{
OpenAI
}
from
"langchain/llms/openai"
;
Vercel / Next.js
​
LangChain可以在Vercel / Next.js中使用。我们支持在前端组件、无服务器函数和Edge函数中使用LangChain。你可以使用以下语法进行导入:。
import
{
OpenAI
}
from
"langchain/llms/openai"
;
Deno / Supabase Edge Functions
​
LangChain可以在Deno / Supabase Edge Functions中使用。你可以使用以下语法进行导入:。
import
{
OpenAI
}
from
"https://esm.sh/langchain/llms/openai"
;
我们推荐查看我们的
Supabase模板
以查看在Supabase Edge Functions中如何使用LangChain的示例。
浏览器
​
LangChain可以在浏览器中使用。在我们的CI中，我们使用Webpack和Vite测试了LangChain的捆绑，但其他捆绑器也应该可以使用。你可以使用以下语法进行导入:。
import
{
OpenAI
}
from
"langchain/llms/openai"
;
从版本<0.0.52进行更新
​
如果您正在更新LangChain的0.0.52之前的版本，您需要更新导入以使用新的路径结构。
例如，如果您以前执行的是
import
{
OpenAI
}
from
"langchain/llms"
;
现在，您需要执行以下操作
import
{
OpenAI
}
from
"langchain/llms/openai"
;
适用于下列6个模块的所有导入，这些模块已分割为每个集成的子模块。组合模块已被弃用，在 Node.js 之外不起作用，并将在将来的版本中删除。
如果您使用的是
langchain/llms
，请参见
LLMs
以获取更新后的导入路径。
如果您使用的是
langchain/chat_models
，请参见
Chat Models
以获取更新后的导入路径。
如果您使用的是
langchain/embeddings
，请参见
Embeddings
以获取更新后的导入路径。
如果您使用的是
langchain/vectorstores
，请参见
Vector Stores
以获取更新后的导入路径。
如果您使用的是
langchain/document_loaders
，请参见
Document Loaders
以获取更新后的导入路径。
如果您使用的是
langchain/retrievers
，请参见
Retrievers
以获取更新后的导入路径。
其他模块不受此更改影响，您可以继续从同一路径导入它们。
此外，为了支持新的环境，需要进行一些重大更改:
import { Calculator } from "langchain/tools";
现已移至
import { Calculator } from "langchain/tools/calculator";
import { loadLLM } from "langchain/llms";
现已移至
import { loadLLM } from "langchain/llms/load";
import { loadAgent } from "langchain/agents";
现已移至
import { loadAgent } from "langchain/agents/load";
import { loadPrompt } from "langchain/prompts";
现已移至
import { loadPrompt } from "langchain/prompts/load";
import { loadChain } from "langchain/chains";
现已移至
import { loadChain } from "langchain/chains/load";
不受支持: Node.js 16
​
我们不支持 Node.js 16，但如果您仍然希望在 Node.js 16 上运行 LangChain，您需要按照本节中的说明进行操作。我们不能保证这些说明在未来仍能工作。
您将需要全局安装
fetch
， 可以通过以下方式之一来实现:
使用
NODE_OPTIONS='--experimental-fetch' node ...
命令运行您的应用程序， 或
安装
node-fetch
并按照
此处
的说明进行操作
此外，您还需要将
unstructuredClone
进行 polyfill， 您可以通过安装
core-js
并按照
此处
的说明进行操作来实现。
如果您在 Node.js 18+ 上运行此代码，您不需要采取任何措施。



--- 文件: output_20250622_020018\docs\modules\agents.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/
crawled_at: 2025-06-22T02:00:19.608488
---

代理人
info
概念指南
一些应用程序需要的不仅是预先确定的调用LLMs/其他工具的链，但可能是依赖于用户输入的未知链。在这些类型的链中，存在一个“代理人”，其可以访问一组工具。根据用户输入，代理人可以决定是否以及如何调用这些工具。
目前有两种主要类型的代理人:
动作代理人
: 这些代理人决定要采取的行动并一步一步地采取这些行动
计划执行代理人
: 这些代理人首先决定要执行的一系列行动计划，然后逐一执行这些行动。
你应该何时使用它们？
动作代理人更为常规，适合处理小任务。
对于更复杂或长期运行的任务，计划执行代理人的初始规划步骤有助于保持长期目标和关注点，但通常需要更多的调用和更高的延迟。
这两种代理人也不是互斥的 - 实际上，通常最好由动作代理人负责执行计划和执行代理人。
动作代理人
​
一个动作代理人的高级伪代码如下:
接收一些用户输入
代理人
决定使用哪个
工具
（如果有的话），以及该工具的输入应该是什么。
使用
工具输入
调用那个
工具
，并记录
观察结果
（这只是调用该
工具输入
的输出)。
将
工具
的历史记录、
工具输入
和
观察结果
传回到
代理程序
，然后它决定下一步该怎么做。
重复上述步骤，直到
代理程序
决定不再需要使用
工具
，然后直接回应用户。
interface
AgentStep
{
action
:
AgentAction
;
observation
:
string
;
}
interface
AgentAction
{
tool
:
string
;
// Tool.name
toolInput
:
string
;
// Tool.call argument
}
interface
AgentFinish
{
returnValues
:
object
;
}
class
Agent
{
plan
(
steps
:
AgentStep
[
]
,
inputs
:
object
)
:
Promise
<
AgentAction
|
AgentFinish
>
;
}
计划和执行代理
​
计划和执行代理的高级伪代码大致如下:
接收到一些用户输入
计划者列出要采取的步骤
执行者逐个执行步骤，直到输出最终结果
当前实现的方式是使用LLMChain作为计划者，使用Action Agent作为执行者。
深入了解
​
🗃️
代理
3 items
🗃️
Agent Executors
1 items
🗃️
工具
7 items
🗃️
工具包
4 items
📄️
其他功能
我们为Agents提供了许多其他功能。您还应查看LLM-specific features和Chat Model-specific features。



--- 文件: output_20250622_020018\docs\modules\chains.md ---
---
url: https://js.langchain.com.cn/docs/modules/chains/
crawled_at: 2025-06-22T02:00:21.004898
---

入门指南: 链
info
概念指南
在一些应用中，仅使用一个语言模型是可以的，但通常将语言模型与其他信息源（例如第三方API或其他语言模型)组合使用是非常有用的。
这就是链的概念。
LangChain提供了一个用于链的标准接口，以及一些可直接使用的内置链。您也可以创建自己的链。
📄️
LLM链
概念指南
🗃️
与索引相关的链
3 items
📄️
顺序链
顺序链允许您连接多个链，并将它们组合成执行特定场景的管道。
🗃️
其他链
8 items
📄️
提示选择器
概念指南
高级
​
要实现自己的自定义链，您可以继承
BaseChain
并实现以下方法:
import
{
CallbackManagerForChainRun
}
from
"langchain/callbacks"
;
import
{
BaseChain
as
_
}
from
"langchain/chains"
;
import
{
BaseMemory
}
from
"langchain/memory"
;
import
{
ChainValues
}
from
"langchain/schema"
;
abstract
class
BaseChain
{
memory
?
:
BaseMemory
;
/**
* Run the core logic of this chain and return the output
*/
abstract
_call
(
values
:
ChainValues
,
runManager
?
:
CallbackManagerForChainRun
)
:
Promise
<
ChainValues
>
;
/**
* Return the string type key uniquely identifying this class of chain.
*/
abstract
_chainType
(
)
:
string
;
/**
* Return the list of input keys this chain expects to receive when called.
*/
abstract
get
inputKeys
(
)
:
string
[
]
;
/**
* Return the list of output keys this chain will produce when called.
*/
abstract
get
outputKeys
(
)
:
string
[
]
;
}
继承
BaseChain
​
_call
方法是自定义链必须实现的主要方法。它接受输入记录并返回输出记录。接收到的输入应符合
inputKeys
数组，返回的输出应符合
outputKeys
数组。
在自定义链中实现此方法时，值得特别关注的是
runManager
参数，它允许您的自定义链参与与内置链相同的回调系统
callbacks system
。
如果在自定义链中调用另一个链/模型/代理，则应将其传递给调用
runManager？.getChild（)
的结果，该结果将生成一个新的回调管理器，范围限定为该内部运行。例如:
import
{
BasePromptTemplate
,
PromptTemplate
}
from
"langchain/prompts"
;
import
{
BaseLanguageModel
}
from
"langchain/base_language"
;
import
{
CallbackManagerForChainRun
}
from
"langchain/callbacks"
;
import
{
BaseChain
,
ChainInputs
}
from
"langchain/chains"
;
import
{
ChainValues
}
from
"langchain/schema"
;
export
interface
MyCustomChainInputs
extends
ChainInputs
{
llm
:
BaseLanguageModel
;
promptTemplate
:
string
;
}
export
class
MyCustomChain
extends
BaseChain
implements
MyCustomChainInputs
{
llm
:
BaseLanguageModel
;
promptTemplate
:
string
;
prompt
:
BasePromptTemplate
;
constructor
(
fields
:
MyCustomChainInputs
)
{
super
(
fields
)
;
this
.
llm
=
fields
.
llm
;
this
.
promptTemplate
=
fields
.
promptTemplate
;
this
.
prompt
=
PromptTemplate
.
fromTemplate
(
this
.
promptTemplate
)
;
}
async
_call
(
values
:
ChainValues
,
runManager
?
:
CallbackManagerForChainRun
)
:
Promise
<
ChainValues
>
{
// Your custom chain logic goes here
// This is just an example that mimics LLMChain
const
promptValue
=
await
this
.
prompt
.
formatPromptValue
(
values
)
;
// Whenever you call a language model, or another chain, you should pass
// a callback manager to it. This allows the inner run to be tracked by
// any callbacks that are registered on the outer run.
// You can always obtain a callback manager for this by calling
// `runManager?.getChild()` as shown below.
const
result
=
await
this
.
llm
.
generatePrompt
(
[
promptValue
]
,
{
}
,
runManager
?.
getChild
(
)
)
;
// If you want to log something about this run, you can do so by calling
// methods on the runManager, as shown below. This will trigger any
// callbacks that are registered for that event.
runManager
?.
handleText
(
"Log something about this run"
)
;
return
{
output
:
result
.
generations
[
0
]
[
0
]
.
text
}
;
}
_chainType
(
)
:
string
{
return
"my_custom_chain"
;
}
get
inputKeys
(
)
:
string
[
]
{
return
[
"input"
]
;
}
get
outputKeys
(
)
:
string
[
]
{
return
[
"output"
]
;
}
}



--- 文件: output_20250622_020018\docs\modules\indexes.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/
crawled_at: 2025-06-22T02:00:21.626870
---

索引
info
概念指南
本节涉及与将自己的数据导入 LangChain、对其进行索引和使其可用于 LLMs / 聊天模型相关的一切。
🗃️
文档装载
1 items
🗃️
文本分割器（Text Splitters)
1 items
🗃️
矢量存储
1 items
🗃️
召回器（Retrievers)
13 items



--- 文件: output_20250622_020018\docs\modules\memory.md ---
---
url: https://js.langchain.com.cn/docs/modules/memory/
crawled_at: 2025-06-22T02:00:24.720710
---

入门: 内存
info
概念指南
在对话过程中存储和检索数据的概念被称为内存。有两种主要方法，
loadMemoryVariables
和
saveContext
。第一种方法用于从内存中检索数据（可选择使用当前输入值)， 第二种方法用于将数据存储到内存中。
export
type
InputValues
=
Record
<
string
,
any
>
;
export
type
OutputValues
=
Record
<
string
,
any
>
;
interface
BaseMemory
{
loadMemoryVariables
(
values
:
InputValues
)
:
Promise
<
MemoryVariables
>
;
saveContext
(
inputValues
:
InputValues
,
outputValues
:
OutputValues
)
:
Promise
<
void
>
;
}
note
不要在两个不同的链之间共享相同的存储器实例， 存储器实例代表单个对话的历史记录
note
如果您将LangChain应用部署在无服务器环境中，请不要将存储器实例存储在变量中， 因为您的托管提供商可能会在下一次调用该函数时重置它。
所有内存类
​
🗃️
示例
12 items
高级
​
要实现您自己的内存类，有两个选项:
子类化
BaseChatMemory
​
这是实现自己的内存类的最简单方法。您可以子类化
BaseChatMemory
， 通过将输入和输出保存为
聊天消息
来处理
saveContext
， 并仅实现
loadMemoryVariables
方法。该方法负责返回与当前输入值相关的存储器变量。
abstract
class
BaseChatMemory
extends
BaseMemory
{
chatHistory
:
ChatMessageHistory
;
abstract
loadMemoryVariables
(
values
:
InputValues
)
:
Promise
<
MemoryVariables
>
;
}
Subclassing
BaseMemory
​
如果您想要实现一个更自定义的内存类，您可以继承
BaseMemory
类并实现
loadMemoryVariables
和
saveContext
方法。
saveContext
方法负责将输入和输出值存储在内存中。
loadMemoryVariables
方法负责返回与当前输入值相关的内存变量。
abstract
class
BaseMemory
{
abstract
loadMemoryVariables
(
values
:
InputValues
)
:
Promise
<
MemoryVariables
>
;
abstract
saveContext
(
inputValues
:
InputValues
,
outputValues
:
OutputValues
)
:
Promise
<
void
>
;
}



--- 文件: output_20250622_020018\docs\modules\models.md ---
---
url: https://js.langchain.com.cn/docs/modules/models/
crawled_at: 2025-06-22T02:00:25.235485
---

模型
info
概念指南
模型是LangChain的核心组件。LangChain不是模型的提供者，而是提供标准接口，通过该接口您可以与各种语言模型进行交互。
LangChain支持文本模型(LLMs)，聊天模型和文本嵌入模型。
LLMs使用文本输入和输出，而聊天模型使用消息输入和输出。
注意:
聊天模型API还比较新，因此我们还在找出正确的抽象。如果您有任何反馈，请告诉我们！
所有模型
​
🗃️
聊天模型
2 items
🗃️
嵌入
2 items
🗃️
LLMs
2 items
高级
​
本节面向想要更深入技术了解LangChain工作原理的用户。如果您刚开始使用，请跳过本节。
LLMs和聊天模型都基于
BaseLanguageModel
类构建。该类为所有模型提供了公共接口，并允许我们在不改变其余代码的情况下轻松切换模型。
BaseLanguageModel
类具有两个抽象方法
generatePrompt
和
getNumTokens
，分别由
BaseChatModel
和
BaseLLM
实现。
BaseLLM
是
BaseLanguageModel
的子类，为 LLM（Large Language Model）提供了一个通用的接口，而
BaseChatModel
是
BaseLanguageModel
的子类，为聊天模型提供了一个通用的接口。



--- 文件: output_20250622_020018\docs\modules\prompts.md ---
---
url: https://js.langchain.com.cn/docs/modules/prompts/
crawled_at: 2025-06-22T02:00:25.656864
---

index
#提示
info
概念指南
LangChain提供了几种实用工具来帮助管理语言模型的提示，包括聊天模型。
🗃️
提示模板
2 items
📄️
示例选择器
概念指南



--- 文件: output_20250622_020018\docs\modules\schema.md ---
---
url: https://js.langchain.com.cn/docs/modules/schema/
crawled_at: 2025-06-22T02:00:25.887701
---

数据结构
此部分介绍了在整个库中使用的接口。
📄️
聊天消息
终端用户与LLMs互动的主要界面是聊天界面。因此，一些模型提供商已经开始以期望聊天消息的方式提供对底层API的访问。这些消息具有内容字段（通常是文本)，并与用户（或角色)相关联。当前支持的用户有System， Human，和AI。
📄️
文档
语言模型只知道它们所训练的内容的信息。为了让它们能够回答问题或总结其他信息，你需要将信息传递给语言模型。因此，拥有文档的概念非常重要。
📄️
示例
示例是输入/输出对，表示对函数的输入和预期输出。它们可用于模型的训练和评估。



--- 文件: output_20250622_020018\docs\production\callbacks.md ---
---
url: https://js.langchain.com.cn/docs/production/callbacks/
crawled_at: 2025-06-22T02:00:26.045821
---

事件 / 回调
LangChain 提供了一个回调系统，允许你在 LLM 应用程序的各个阶段中进行钩子处理。这对于记录日志、
监视
、
流媒体
和其他任务非常有用。
你可以通过 API 中使用的
callbacks
参数来订阅这些事件。此方法接受一个处理程序对象的列表，这些对象应该实现
API 文档
中描述的一个或多个方法。
深入了解
​
📄️
创建回调处理程序
创建自定义处理程序
📄️
自定义Chains中的回调
LangChain旨在可扩展。 您可以将自己的自定义Chains和Agents添加到库中。 本页将向您展示如何将回调添加到自定义的Chains和Agents中。
如何使用回调
​
在 API 中的大多数对象上（
Chains
、
Models
、
Tools
、
Agents
等)都提供了
callbacks
参数，它有两个不同的用法:
构造器回调
​
在构造函数中定义，如
new LLMChain({ callbacks: [handler] })
，将用于该对象上进行的所有调用，并且仅适用于该对象本身。例如，如果你将处理程序传递给
LLMChain
构造函数，则不会被连接到该链上的模型使用。
import
{
ConsoleCallbackHandler
}
from
"langchain/callbacks"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
const
llm
=
new
OpenAI
(
{
temperature
:
0
,
// This handler will be used for all calls made with this LLM.
callbacks
:
[
new
ConsoleCallbackHandler
(
)
]
,
}
)
;
请求回调
​
在发出请求的
call()
/
run()
/
apply()
方法中定义，例如
chain.call({ input: '...' }， [handler])
，将仅用于该特定请求及其包含的所有子请求（例如，对 LLMChain 的调用会触发对模型的调用，该模型使用在
call()
方法中传递的相同处理程序)。
import
{
ConsoleCallbackHandler
}
from
"langchain/callbacks"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
const
llm
=
new
OpenAI
(
{
temperature
:
0
,
}
)
;
// This handler will be used only for this call.
const
response
=
await
llm
.
call
(
"1 + 1 ="
,
undefined
,
[
new
ConsoleCallbackHandler
(
)
,
]
)
;
详细模式
​
verbose
参数可用于API中的大部分对象（链接，模型，工具，代理等)作为构造参数。例如，
new LLMChain({ verbose: true })
，它相当于将
callbacks
参数传递给该对象和所有子对象的
ConsoleCallbackHandler
。这对于调试非常有用，因为它会将所有事件记录在控制台上。您还可以通过设置环境变量
LANGCHAIN_VERBOSE=true
来为整个应用程序启用详细模式。
import
{
PromptTemplate
}
from
"langchain/prompts"
;
import
{
LLMChain
}
from
"langchain/chains"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
const
chain
=
new
LLMChain
(
{
llm
:
new
OpenAI
(
{
temperature
:
0
}
)
,
prompt
:
PromptTemplate
.
fromTemplate
(
"Hello, world!"
)
,
// This will enable logging of all Chain *and* LLM events to the console.
verbose
:
true
,
}
)
;
你何时需要使用它们？
​
构造函数回调最适用于诸如日志记录，监视等用例，这些用例不特定于单个请求，而是适用于整个链。例如，如果您要记录所有发送到LLMChain的请求，则应将处理程序传递给构造函数。
请求回调最适用于流式传输等用例，其中您需要将单个请求的输出流到特定的websocket连接或其他类似的用例。例如，如果您想将单个请求的输出流到websocket，则应将处理程序传递给
call()
方法。
使用示例
​
内置处理程序
​
LangChain提供了一些内置处理程序，可用于入门。这些可在
langchain/callbacks
模块中使用。最基本的处理程序是
ConsoleCallbackHandler
，只需将所有事件记录到控制台即可。在将
verbose
标志设置为
true
的情况下，
ConsoleCallbackHandler
将在不显式传递的情况下被调用。
import
{
ConsoleCallbackHandler
}
from
"langchain/callbacks"
;
import
{
LLMChain
}
from
"langchain/chains"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
PromptTemplate
}
from
"langchain/prompts"
;
export
const
run
=
async
(
)
=>
{
const
handler
=
new
ConsoleCallbackHandler
(
)
;
const
llm
=
new
OpenAI
(
{
temperature
:
0
,
callbacks
:
[
handler
]
}
)
;
const
prompt
=
PromptTemplate
.
fromTemplate
(
"1 + {number} ="
)
;
const
chain
=
new
LLMChain
(
{
prompt
,
llm
,
callbacks
:
[
handler
]
}
)
;
const
output
=
await
chain
.
call
(
{
number
:
2
}
)
;
/*
Entering new llm_chain chain...
Finished chain.
*/
console
.
log
(
output
)
;
/*
{ text: ' 3\n\n3 - 1 = 2' }
*/
// The non-enumerable key `__run` contains the runId.
console
.
log
(
output
.
__run
)
;
/*
{ runId: '90e1f42c-7cb4-484c-bf7a-70b73ef8e64b' }
*/
}
;
One-off handlers
​
您可以通过将普通对象传递给
callbacks
参数来创建一个临时处理程序。该对象应实现
CallbackHandlerMethods
接口。如果您需要创建一个仅用于单个请求的处理程序，这将非常有用，例如流式传输LLM / Agent /等的输出到WebSocket。
import
{
OpenAI
}
from
"langchain/llms/openai"
;
// To enable streaming, we pass in `streaming: true` to the LLM constructor.
// Additionally, we pass in a handler for the `handleLLMNewToken` event.
const
chat
=
new
OpenAI
(
{
maxTokens
:
25
,
streaming
:
true
,
}
)
;
const
response
=
await
chat
.
call
(
"Tell me a joke."
,
undefined
,
[
{
handleLLMNewToken
(
token
:
string
)
{
console
.
log
(
{
token
}
)
;
}
,
}
,
]
)
;
console
.
log
(
response
)
;
/*
{ token: '\n' }
{ token: '\n' }
{ token: 'Q' }
{ token: ':' }
{ token: ' Why' }
{ token: ' did' }
{ token: ' the' }
{ token: ' chicken' }
{ token: ' cross' }
{ token: ' the' }
{ token: ' playground' }
{ token: '?' }
{ token: '\n' }
{ token: 'A' }
{ token: ':' }
{ token: ' To' }
{ token: ' get' }
{ token: ' to' }
{ token: ' the' }
{ token: ' other' }
{ token: ' slide' }
{ token: '.' }
Q: Why did the chicken cross the playground?
A: To get to the other slide.
*/
多个处理程序
​
我们在
CallbackManager
类上提供了一种方法，允许您创建一个临时处理程序。如果您需要创建一个仅用于单个请求的处理程序，这将非常有用，例如流式传输LLM / Agent /等的输出到WebSocket。
This is a more complete example that passes a
CallbackManager
to a ChatModel, and LLMChain, a Tool, and an Agent.
import
{
LLMChain
}
from
"langchain/chains"
;
import
{
AgentExecutor
,
ZeroShotAgent
}
from
"langchain/agents"
;
import
{
BaseCallbackHandler
}
from
"langchain/callbacks"
;
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
Calculator
}
from
"langchain/tools/calculator"
;
import
{
AgentAction
}
from
"langchain/schema"
;
export
const
run
=
async
(
)
=>
{
// You can implement your own callback handler by extending BaseCallbackHandler
class
CustomHandler
extends
BaseCallbackHandler
{
name
=
"custom_handler"
;
handleLLMNewToken
(
token
:
string
)
{
console
.
log
(
"token"
,
{
token
}
)
;
}
handleLLMStart
(
llm
:
{
name
:
string
}
,
_prompts
:
string
[
]
)
{
console
.
log
(
"handleLLMStart"
,
{
llm
}
)
;
}
handleChainStart
(
chain
:
{
name
:
string
}
)
{
console
.
log
(
"handleChainStart"
,
{
chain
}
)
;
}
handleAgentAction
(
action
:
AgentAction
)
{
console
.
log
(
"handleAgentAction"
,
action
)
;
}
handleToolStart
(
tool
:
{
name
:
string
}
)
{
console
.
log
(
"handleToolStart"
,
{
tool
}
)
;
}
}
const
handler1
=
new
CustomHandler
(
)
;
// Additionally, you can use the `fromMethods` method to create a callback handler
const
handler2
=
BaseCallbackHandler
.
fromMethods
(
{
handleLLMStart
(
llm
,
_prompts
:
string
[
]
)
{
console
.
log
(
"handleLLMStart: I'm the second handler!!"
,
{
llm
}
)
;
}
,
handleChainStart
(
chain
)
{
console
.
log
(
"handleChainStart: I'm the second handler!!"
,
{
chain
}
)
;
}
,
handleAgentAction
(
action
)
{
console
.
log
(
"handleAgentAction"
,
action
)
;
}
,
handleToolStart
(
tool
)
{
console
.
log
(
"handleToolStart"
,
{
tool
}
)
;
}
,
}
)
;
// You can restrict callbacks to a particular object by passing it upon creation
const
model
=
new
ChatOpenAI
(
{
temperature
:
0
,
callbacks
:
[
handler2
]
,
// this will issue handler2 callbacks related to this model
streaming
:
true
,
// needed to enable streaming, which enables handleLLMNewToken
}
)
;
const
tools
=
[
new
Calculator
(
)
]
;
const
agentPrompt
=
ZeroShotAgent
.
createPrompt
(
tools
)
;
const
llmChain
=
new
LLMChain
(
{
llm
:
model
,
prompt
:
agentPrompt
,
callbacks
:
[
handler2
]
,
// this will issue handler2 callbacks related to this chain
}
)
;
const
agent
=
new
ZeroShotAgent
(
{
llmChain
,
allowedTools
:
[
"search"
]
,
}
)
;
const
agentExecutor
=
AgentExecutor
.
fromAgentAndTools
(
{
agent
,
tools
,
}
)
;
/*
* When we pass the callback handler to the agent executor, it will be used for all
* callbacks related to the agent and all the objects involved in the agent's
* execution, in this case, the Tool, LLMChain, and LLM.
*
* The `handler2` callback handler will only be used for callbacks related to the
* LLMChain and LLM, since we passed it to the LLMChain and LLM objects upon creation.
*/
const
result
=
await
agentExecutor
.
call
(
{
input
:
"What is 2 to the power of 8"
,
}
,
[
handler1
]
)
;
// this is needed to see handleAgentAction
/*
handleChainStart { chain: { name: 'agent_executor' } }
handleChainStart { chain: { name: 'llm_chain' } }
handleChainStart: I'm the second handler!! { chain: { name: 'llm_chain' } }
handleLLMStart { llm: { name: 'openai' } }
handleLLMStart: I'm the second handler!! { llm: { name: 'openai' } }
token { token: '' }
token { token: 'I' }
token { token: ' can' }
token { token: ' use' }
token { token: ' the' }
token { token: ' calculator' }
token { token: ' tool' }
token { token: ' to' }
token { token: ' solve' }
token { token: ' this' }
token { token: '.\n' }
token { token: 'Action' }
token { token: ':' }
token { token: ' calculator' }
token { token: '\n' }
token { token: 'Action' }
token { token: ' Input' }
token { token: ':' }
token { token: ' ' }
token { token: '2' }
token { token: '^' }
token { token: '8' }
token { token: '' }
handleAgentAction {
tool: 'calculator',
toolInput: '2^8',
log: 'I can use the calculator tool to solve this.\n' +
'Action: calculator\n' +
'Action Input: 2^8'
}
handleToolStart { tool: { name: 'calculator' } }
handleChainStart { chain: { name: 'llm_chain' } }
handleChainStart: I'm the second handler!! { chain: { name: 'llm_chain' } }
handleLLMStart { llm: { name: 'openai' } }
handleLLMStart: I'm the second handler!! { llm: { name: 'openai' } }
token { token: '' }
token { token: 'That' }
token { token: ' was' }
token { token: ' easy' }
token { token: '!\n' }
token { token: 'Final' }
token { token: ' Answer' }
token { token: ':' }
token { token: ' ' }
token { token: '256' }
token { token: '' }
*/
console
.
log
(
result
)
;
/*
{
output: '256',
__run: { runId: '26d481a6-4410-4f39-b74d-f9a4f572379a' }
}
*/
}
;



--- 文件: output_20250622_020018\docs\production\deployment.md ---
---
url: https://js.langchain.com.cn/docs/production/deployment
crawled_at: 2025-06-22T02:00:26.161410
---

deployment
部署
​
您已经构建好了 LangChain 应用程序，并且现在想将其部署到生产环境？您来对地方了。本指南将为您介绍部署应用程序的选项以及进行部署时应考虑的问题。
概述
​
LangChain 是用于构建使用语言模型的应用程序的库。它不是 Web 框架，并且不提供任何用于通过 Web 提供服务的内置功能。相反，它提供了一组工具，您可以将其集成在 API 或后端服务器中。
部署应用程序有几个高级选项:
部署到虚拟机或容器中
持久化文件系统意味着你可以从磁盘中保存和加载文件
永久运行的进程意味着你可以在内存中缓存一些东西
你可以支持长时间运行的请求，例如WebSockets
部署到无服务器环境
没有持久化文件系统意味着你可以从磁盘中加载文件，但是不能将它们保存以备后用。
冷启动意味着你不能在内存中缓存东西，并期望在请求之间被缓存。
函数超时意味着你不能支持长时间运行的请求，例如WebSockets。
其他一些考虑事项包括:
您将后端和前端一起部署还是分别部署？
您将后端与数据库协同部署还是分别部署？
随着您将 LangChain 应用程序部署到生产环境，我们将非常乐意提供更全面的支持。请填写
此表格
，我们将设置一个专门的支持 Slack 频道。
部署选项
​
请参阅以下有关 LangChain 应用程序部署选项的列表。如果您没有看到您首选的选项，请联系我们，我们可以将其添加到此列表中。
部署到 Fly.io
​
Fly.io
是将应用程序部署到云端的平台。这是将您的应用程序部署到容器环境的不错选择。
请参阅
我们的 Fly.io 模板
，其中包含了将应用程序部署到 Fly.io 的示例。
部署到 Kinsta
​
Kinsta
是一个以开发人员为中心的云主机平台。
使用
我们的hello-world模板
，了解如何在Kinsta上在几分钟内部署你的下一个LangChain应用程序的示例。



--- 文件: output_20250622_020018\docs\production\tracing.md ---
---
url: https://js.langchain.com.cn/docs/production/tracing
crawled_at: 2025-06-22T02:00:26.326100
---

追踪
与 Python 的
langchain
包类似，JS 的
langchain
也支持追踪。
您可以在
这里
查看追踪的概述。
要启动追踪后端，请在
langchain
目录下运行
docker compose up
(如果使用较旧版本的
docker
，则使用
docker-compose up
)。
如果您已安装 Python 的
langchain
包，还可以使用
langchain-server
命令。
以下是如何在
langchain.js
中使用追踪的示例。唯一需要做的就是将
LANGCHAIN_TRACING
环境变量设置为
true
。
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
initializeAgentExecutorWithOptions
}
from
"langchain/agents"
;
import
{
SerpAPI
}
from
"langchain/tools"
;
import
{
Calculator
}
from
"langchain/tools/calculator"
;
import
process
from
"process"
;
export
const
run
=
async
(
)
=>
{
process
.
env
.
LANGCHAIN_TRACING
=
"true"
;
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
tools
=
[
new
SerpAPI
(
process
.
env
.
SERPAPI_API_KEY
,
{
location
:
"Austin,Texas,United States"
,
hl
:
"en"
,
gl
:
"us"
,
}
)
,
new
Calculator
(
)
,
]
;
const
executor
=
await
initializeAgentExecutorWithOptions
(
tools
,
model
,
{
agentType
:
"zero-shot-react-description"
,
verbose
:
true
,
}
)
;
console
.
log
(
"Loaded agent."
)
;
const
input
=
`
Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?
`
;
console
.
log
(
`
Executing with input "
${
input
}
"...
`
)
;
const
result
=
await
executor
.
call
(
{
input
}
)
;
console
.
log
(
`
Got output
${
result
.
output
}
`
)
;
}
;
并发
​
追踪默认支持并发处理。
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
initializeAgentExecutorWithOptions
}
from
"langchain/agents"
;
import
{
SerpAPI
}
from
"langchain/tools"
;
import
{
Calculator
}
from
"langchain/tools/calculator"
;
import
process
from
"process"
;
export
const
run
=
async
(
)
=>
{
process
.
env
.
LANGCHAIN_TRACING
=
"true"
;
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
tools
=
[
new
SerpAPI
(
process
.
env
.
SERPAPI_API_KEY
,
{
location
:
"Austin,Texas,United States"
,
hl
:
"en"
,
gl
:
"us"
,
}
)
,
new
Calculator
(
)
,
]
;
const
executor
=
await
initializeAgentExecutorWithOptions
(
tools
,
model
,
{
agentType
:
"zero-shot-react-description"
,
verbose
:
true
,
}
)
;
console
.
log
(
"Loaded agent."
)
;
const
input
=
`
Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?
`
;
console
.
log
(
`
Executing with input "
${
input
}
"...
`
)
;
// This will result in a lot of errors, because the shared Tracer is not concurrency-safe.
const
[
resultA
,
resultB
,
resultC
]
=
await
Promise
.
all
(
[
executor
.
call
(
{
input
}
)
,
executor
.
call
(
{
input
}
)
,
executor
.
call
(
{
input
}
)
,
]
)
;
console
.
log
(
`
Got output
${
resultA
.
output
}
${
resultA
.
__run
.
runId
}
`
)
;
console
.
log
(
`
Got output
${
resultB
.
output
}
${
resultB
.
__run
.
runId
}
`
)
;
console
.
log
(
`
Got output
${
resultC
.
output
}
${
resultC
.
__run
.
runId
}
`
)
;
/*
Got output Harry Styles, Olivia Wilde's boyfriend, is 29 years old and his age raised to the 0.23 power is 2.169459462491557. b8fb98aa-07a5-45bd-b593-e8d7376b05ca
Got output Harry Styles, Olivia Wilde's boyfriend, is 29 years old and his age raised to the 0.23 power is 2.169459462491557. c8d916d5-ca1d-4702-8dd7-cab5e438578b
Got output Harry Styles, Olivia Wilde's boyfriend, is 29 years old and his age raised to the 0.23 power is 2.169459462491557. bf5fe04f-ef29-4e55-8ce1-e4aa974f9484
*/
}
;



--- 文件: output_20250622_020018\docs\use_cases\agent_simulations.md ---
---
url: https://js.langchain.com.cn/docs/use_cases/agent_simulations/
crawled_at: 2025-06-22T02:00:26.248832
---

代理人模拟
代理人模拟涉及到将多个代理人与彼此互动。
它们往往使用模拟环境，其中LLM是它们的“核心”，而辅助类则用于提示它们吸收某些输入，例如预先构建的“观察”，并对新的刺激作出反应。
它们还从长期记忆中受益，以便它们可以在互动之间保持状态。
与自主代理人一样，代理人模拟仍处于实验阶段，基于论文的基础，例如
这篇
。
📄️
生成式智能体
该脚本实现了一种基于论文Generating Agents  交互式仿真人类行为的生成式智能体, 作者为Park et.al.



--- 文件: output_20250622_020018\docs\use_cases\api.md ---
---
url: https://js.langchain.com.cn/docs/use_cases/api
crawled_at: 2025-06-22T02:00:26.332721
---

与API交互
info
概念指南
许多数据和信息存储在API后面。
本页涵盖了在LangChain中使用API的所有资源。
链
​
如果您刚开始并且有相对简单的API，请从链开始。
链是一系列预定步骤，因此它们很适合入门，因为它们能够给您更多控制，让您
更好地了解发生的情况。
API链
代理
​
代理更复杂，并涉及对LLM的多个查询，以了解要做什么。
代理的缺点是您的控制力少了。优点是它们更强大
这使您可以在更大，更复杂的模式上使用它们。
OpenAPI Agent



--- 文件: output_20250622_020018\docs\use_cases\autonomous_agents.md ---
---
url: https://js.langchain.com.cn/docs/use_cases/autonomous_agents/
crawled_at: 2025-06-22T02:00:26.345044
---

自主代理
自主代理是设计成更长时间运行的代理。您可以给它们一个或多个长期目标，它们会独立执行这些目标。这些应用程序结合了工具使用和长期记忆。
目前自主代理还相当实验性，基于其他开源项目。通过在 LangChain 基元中实现这些开源项目，我们可以得到 LangChain 的好处：轻松切换和尝试多个 LLMs，使用不同的向量存储作为内存，使用 LangChain 的工具集。
📄️
AutoGPT
AutoGPT是一个使用长期记忆和专为独立工作设计的提示（即无需要求用户输入)的自定义代理来执行任务。
📄️
BabyAGI
原始GitHub链接//github.com/yoheinakajima/babyagi



--- 文件: output_20250622_020018\docs\use_cases\personal_assistants.md ---
---
url: https://js.langchain.com.cn/docs/use_cases/personal_assistants
crawled_at: 2025-06-22T02:00:26.432404
---

个人助理
info
概念指南
我们在这里非常广义地使用“个人助理”这个词。
个人助理具有一些特征:
它们可以与外界互动
它们具有您的数据知识
它们记得您的互动
实际上，LangChain 中的所有功能都与构建个人助理相关。
Highlighting specific parts:
Agent 文档
（用于与外界互动)
Index 文档
（用于提供数据知识)
Memory
(for helping them remember interactions)



--- 文件: output_20250622_020018\docs\use_cases\question_answering.md ---
---
url: https://js.langchain.com.cn/docs/use_cases/question_answering
crawled_at: 2025-06-22T02:00:26.473784
---

问答
info
概念指南
在这个背景下的问答指的是针对文档数据的问答。
有几种不同类型的问答:
检索式问答
: 利用这种方式，可以将文档导入、索引到向量存储库中，然后能够对它们进行提问。
交互检索
: 类似于上述方式，导入并索引文件，但是可以进行交谈（提出后续问题等)而不仅仅是一个问题。
索引
​
对于许多文档的问答，您几乎总是希望对数据创建索引。
这可用于智能访问给定问题的最相关文档，从而可以避免将所有文档传递给LLM（节省时间和金钱)。
因此，了解如何创建索引非常重要，因此您应该熟悉与此相关的所有文档。
索引
链
​
创建索引后，可以在链中使用它。
您可以正常地对其进行问答，也可以以交互方式使用它。
有关这些链（以及更多内容)的概述，请参阅下面的文档。
与索引相关的链
代理
​
如果你想回答更复杂的多跳问题，你应该考虑将你的索引与一个Agent组合使用。#multi-hop指多跳问题，#indexes指索引，#agent指代理。
有关如何操作的示例，请参见以下内容。#example指示例。
Vectorstore Agent



--- 文件: output_20250622_020018\docs\use_cases\summarization.md ---
---
url: https://js.langchain.com.cn/docs/use_cases/summarization
crawled_at: 2025-06-22T02:00:26.534316
---

总结
info
概念指南
一个常见的用例是想要总结长文档。
这自然会遇到上下文窗口的限制。
与问答不同，您不能只做些语义搜索技巧来仅选择与问题最相关的文本部分(因为在这种情况下没有特定的问题
您想总结每个东西
为了开始，我们建议查看汇总链，该链以递归方式解决此问题。
Summarization Chain



--- 文件: output_20250622_020018\docs\use_cases\tabular.md ---
---
url: https://js.langchain.com.cn/docs/use_cases/tabular
crawled_at: 2025-06-22T02:00:26.587981
---

表格问答
info
概念指南
大量的数据和信息存储在表格数据中，例如csv、excel表格或SQL表格。
本页面介绍LangChain提供的与这种格式数据处理有关的所有资源。
链
​
如果您刚开始并且有相对较小/简单的表格数据，建议您使用链。
链是一系列预定步骤，因此它们很适合初学者，因为它们给您更多的控制，并让您更好地理解正在发生的事情。
SQL数据库链
代理
​
代理更加复杂，并涉及多个查询到LLM以理解要做什么。代理的缺点是您的控制力较小。优点是它们更强大，可以在更大的数据库和更复杂的模式上使用。
代理人的缺点是你的控制力会变弱，但好处在于它们更加强大，这使得你可以在更大的数据库和更复杂的模式上使用它们。
SQL Agent



--- 文件: output_20250622_020018\docs\modules\agents\additional_functionality.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/additional_functionality
crawled_at: 2025-06-22T02:00:19.942276
---

Agents的其他功能
我们为Agents提供了许多其他功能。您还应查看
LLM-specific features
和
Chat Model-specific features
。
添加超时
​
默认情况下，LangChain将无限期等待模型提供者的响应。如果您想添加超时，可以在运行代理时传递以毫秒为单位的“timeout”选项。例如
import
{
initializeAgentExecutorWithOptions
}
from
"langchain/agents"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
SerpAPI
}
from
"langchain/tools"
;
import
{
Calculator
}
from
"langchain/tools/calculator"
;
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
tools
=
[
new
SerpAPI
(
process
.
env
.
SERPAPI_API_KEY
,
{
location
:
"Austin,Texas,United States"
,
hl
:
"en"
,
gl
:
"us"
,
}
)
,
new
Calculator
(
)
,
]
;
const
executor
=
await
initializeAgentExecutorWithOptions
(
tools
,
model
,
{
agentType
:
"zero-shot-react-description"
,
}
)
;
try
{
const
input
=
`
Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?
`
;
const
result
=
await
executor
.
call
(
{
input
,
timeout
:
2000
}
)
;
// 2 seconds
}
catch
(
e
)
{
console
.
log
(
e
)
;
/*
Error: Cancel: canceled
at file:///Users/nuno/dev/langchainjs/langchain/dist/util/async_caller.js:60:23
at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
at RetryOperation._fn (/Users/nuno/dev/langchainjs/node_modules/p-retry/index.js:50:12) {
attemptNumber: 1,
retriesLeft: 6
}
*/
}
取消请求
​
您可以通过在运行代理时传递“signal”选项来取消请求。例如
import
{
initializeAgentExecutorWithOptions
}
from
"langchain/agents"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
SerpAPI
}
from
"langchain/tools"
;
import
{
Calculator
}
from
"langchain/tools/calculator"
;
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
tools
=
[
new
SerpAPI
(
process
.
env
.
SERPAPI_API_KEY
,
{
location
:
"Austin,Texas,United States"
,
hl
:
"en"
,
gl
:
"us"
,
}
)
,
new
Calculator
(
)
,
]
;
const
executor
=
await
initializeAgentExecutorWithOptions
(
tools
,
model
,
{
agentType
:
"zero-shot-react-description"
,
}
)
;
const
controller
=
new
AbortController
(
)
;
// Call `controller.abort()` somewhere to cancel the request.
setTimeout
(
(
)
=>
{
controller
.
abort
(
)
;
}
,
2000
)
;
try
{
const
input
=
`
Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?
`
;
const
result
=
await
executor
.
call
(
{
input
,
signal
:
controller
.
signal
}
)
;
}
catch
(
e
)
{
console
.
log
(
e
)
;
/*
Error: Cancel: canceled
at file:///Users/nuno/dev/langchainjs/langchain/dist/util/async_caller.js:60:23
at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
at RetryOperation._fn (/Users/nuno/dev/langchainjs/node_modules/p-retry/index.js:50:12) {
attemptNumber: 1,
retriesLeft: 6
}
*/
}
注意：如果底层提供程序公开该选项，这将仅取消传出请求。如果可能，LangChain将取消底层请求，否则它将取消响应的处理。
订阅事件
​
您可以订阅代理和基础工具链和模型发出的许多事件。
有关可用事件的更多信息，请参见文档中的
Callbacks
部分。
import
{
initializeAgentExecutorWithOptions
}
from
"langchain/agents"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
SerpAPI
}
from
"langchain/tools"
;
import
{
Calculator
}
from
"langchain/tools/calculator"
;
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
tools
=
[
new
SerpAPI
(
process
.
env
.
SERPAPI_API_KEY
,
{
location
:
"Austin,Texas,United States"
,
hl
:
"en"
,
gl
:
"us"
,
}
)
,
new
Calculator
(
)
,
]
;
const
executor
=
await
initializeAgentExecutorWithOptions
(
tools
,
model
,
{
agentType
:
"zero-shot-react-description"
,
}
)
;
const
input
=
`
Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?
`
;
const
result
=
await
executor
.
run
(
input
,
[
{
handleAgentAction
(
action
,
runId
)
{
console
.
log
(
"\nhandleAgentAction"
,
action
,
runId
)
;
}
,
handleAgentEnd
(
action
,
runId
)
{
console
.
log
(
"\nhandleAgentEnd"
,
action
,
runId
)
;
}
,
handleToolEnd
(
output
,
runId
)
{
console
.
log
(
"\nhandleToolEnd"
,
output
,
runId
)
;
}
,
}
,
]
)
;
/*
handleAgentAction {
tool: 'search',
toolInput: 'Olivia Wilde boyfriend',
log: " I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.\n" +
'Action: search\n' +
'Action Input: "Olivia Wilde boyfriend"'
} 9b978461-1f6f-4d5f-80cf-5b229ce181b6
handleToolEnd In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling. Their relationship ended in November 2022. 062fef47-8ad1-4729-9949-a57be252e002
handleAgentAction {
tool: 'search',
toolInput: 'Harry Styles age',
log: " I need to find out Harry Styles' age.\n" +
'Action: search\n' +
'Action Input: "Harry Styles age"'
} 9b978461-1f6f-4d5f-80cf-5b229ce181b6
handleToolEnd 29 years 9ec91e41-2fbf-4de0-85b6-12b3e6b3784e 61d77e10-c119-435d-a985-1f9d45f0ef08
handleAgentAction {
tool: 'calculator',
toolInput: '29^0.23',
log: ' I need to calculate 29 raised to the 0.23 power.\n' +
'Action: calculator\n' +
'Action Input: 29^0.23'
} 9b978461-1f6f-4d5f-80cf-5b229ce181b6
handleToolEnd 2.169459462491557 07aec96a-ce19-4425-b863-2eae39db8199
handleAgentEnd {
returnValues: {
output: "Harry Styles is Olivia Wilde's boyfriend and his current age raised to the 0.23 power is 2.169459462491557."
},
log: ' I now know the final answer.\n' +
"Final Answer: Harry Styles is Olivia Wilde's boyfriend and his current age raised to the 0.23 power is 2.169459462491557."
} 9b978461-1f6f-4d5f-80cf-5b229ce181b6
*/
console
.
log
(
{
result
}
)
;
// { result: "Harry Styles is Olivia Wilde's boyfriend and his current age raised to the 0.23 power is 2.169459462491557." }
日志记录和跟踪
​
您可以在创建代理时传递“verbose”标志，以启用将所有事件记录到控制台的日志记录。例如
您还可以通过将LANGCHAIN_TRACING环境变量设置为“true”来启用
跟踪
。
import
{
initializeAgentExecutorWithOptions
}
from
"langchain/agents"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
SerpAPI
}
from
"langchain/tools"
;
import
{
Calculator
}
from
"langchain/tools/calculator"
;
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
tools
=
[
new
SerpAPI
(
process
.
env
.
SERPAPI_API_KEY
,
{
location
:
"Austin,Texas,United States"
,
hl
:
"en"
,
gl
:
"us"
,
}
)
,
new
Calculator
(
)
,
]
;
const
executor
=
await
initializeAgentExecutorWithOptions
(
tools
,
model
,
{
agentType
:
"zero-shot-react-description"
,
verbose
:
true
,
}
)
;
const
input
=
`
Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?
`
;
const
result
=
await
executor
.
call
(
{
input
}
)
;
[chain/start] [1:chain:agent_executor] Entering Chain run with input: {
"input": "Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?"
}
[chain/start] [1:chain:agent_executor > 2:chain:llm_chain] Entering Chain run with input: {
"input": "Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?",
"agent_scratchpad": "",
"stop": [
"Observation: "
]
}
[llm/start] [1:chain:agent_executor > 2:chain:llm_chain > 3:llm:openai] Entering LLM run with input: {
"prompts": [
"Answer the following questions as best you can. You have access to the following tools:search: a search engine. useful for when you need to answer questions about current events. input should be a search query.calculator: Useful for getting the result of a math expression. The input to this tool should be a valid mathematical expression that could be executed by a simple calculator.Use the following format in your response:Question: the input question you must answerThought: you should always think about what to doAction: the action to take, should be one of [search,calculator]Action Input: the input to the actionObservation: the result of the action... (this Thought/Action/Action Input/Observation can repeat N times)Thought: I now know the final answerFinal Answer: the final answer to the original input questionBegin!Question: Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?Thought:"
]
}
[llm/end] [1:chain:agent_executor > 2:chain:llm_chain > 3:llm:openai] [3.52s] Exiting LLM run with output: {
"generations": [
[
{
"text": " I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.Action: searchAction Input: "Olivia Wilde boyfriend"",
"generationInfo": {
"finishReason": "stop",
"logprobs": null
}
}
]
],
"llmOutput": {
"tokenUsage": {
"completionTokens": 39,
"promptTokens": 220,
"totalTokens": 259
}
}
}
[chain/end] [1:chain:agent_executor > 2:chain:llm_chain] [3.53s] Exiting Chain run with output: {
"text": " I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.Action: searchAction Input: "Olivia Wilde boyfriend""
}
[agent/action] [1:chain:agent_executor] Agent selected action: {
"tool": "search",
"toolInput": "Olivia Wilde boyfriend",
"log": " I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.Action: searchAction Input: "Olivia Wilde boyfriend""
}
[tool/start] [1:chain:agent_executor > 4:tool:search] Entering Tool run with input: "Olivia Wilde boyfriend"
[tool/end] [1:chain:agent_executor > 4:tool:search] [845ms] Exiting Tool run with output: "In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling. Their relationship ended in November 2022."
[chain/start] [1:chain:agent_executor > 5:chain:llm_chain] Entering Chain run with input: {
"input": "Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?",
"agent_scratchpad": " I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.Action: searchAction Input: "Olivia Wilde boyfriend"Observation: In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling. Their relationship ended in November 2022.Thought:",
"stop": [
"Observation: "
]
}
[llm/start] [1:chain:agent_executor > 5:chain:llm_chain > 6:llm:openai] Entering LLM run with input: {
"prompts": [
"Answer the following questions as best you can. You have access to the following tools:search: a search engine. useful for when you need to answer questions about current events. input should be a search query.calculator: Useful for getting the result of a math expression. The input to this tool should be a valid mathematical expression that could be executed by a simple calculator.Use the following format in your response:Question: the input question you must answerThought: you should always think about what to doAction: the action to take, should be one of [search,calculator]Action Input: the input to the actionObservation: the result of the action... (this Thought/Action/Action Input/Observation can repeat N times)Thought: I now know the final answerFinal Answer: the final answer to the original input questionBegin!Question: Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?Thought: I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.Action: searchAction Input: "Olivia Wilde boyfriend"Observation: In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling. Their relationship ended in November 2022.Thought:"
]
}
[llm/end] [1:chain:agent_executor > 5:chain:llm_chain > 6:llm:openai] [3.65s] Exiting LLM run with output: {
"generations": [
[
{
"text": " I need to find out Harry Styles' age.Action: searchAction Input: "Harry Styles age"",
"generationInfo": {
"finishReason": "stop",
"logprobs": null
}
}
]
],
"llmOutput": {
"tokenUsage": {
"completionTokens": 23,
"promptTokens": 296,
"totalTokens": 319
}
}
}
[chain/end] [1:chain:agent_executor > 5:chain:llm_chain] [3.65s] Exiting Chain run with output: {
"text": " I need to find out Harry Styles' age.Action: searchAction Input: "Harry Styles age""
}
[agent/action] [1:chain:agent_executor] Agent selected action: {
"tool": "search",
"toolInput": "Harry Styles age",
"log": " I need to find out Harry Styles' age.Action: searchAction Input: "Harry Styles age""
}
[tool/start] [1:chain:agent_executor > 7:tool:search] Entering Tool run with input: "Harry Styles age"
[tool/end] [1:chain:agent_executor > 7:tool:search] [632ms] Exiting Tool run with output: "29 years"
[chain/start] [1:chain:agent_executor > 8:chain:llm_chain] Entering Chain run with input: {
"input": "Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?",
"agent_scratchpad": " I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.Action: searchAction Input: "Olivia Wilde boyfriend"Observation: In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling. Their relationship ended in November 2022.Thought: I need to find out Harry Styles' age.Action: searchAction Input: "Harry Styles age"Observation: 29 yearsThought:",
"stop": [
"Observation: "
]
}
[llm/start] [1:chain:agent_executor > 8:chain:llm_chain > 9:llm:openai] Entering LLM run with input: {
"prompts": [
"Answer the following questions as best you can. You have access to the following tools:search: a search engine. useful for when you need to answer questions about current events. input should be a search query.calculator: Useful for getting the result of a math expression. The input to this tool should be a valid mathematical expression that could be executed by a simple calculator.Use the following format in your response:Question: the input question you must answerThought: you should always think about what to doAction: the action to take, should be one of [search,calculator]Action Input: the input to the actionObservation: the result of the action... (this Thought/Action/Action Input/Observation can repeat N times)Thought: I now know the final answerFinal Answer: the final answer to the original input questionBegin!Question: Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?Thought: I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.Action: searchAction Input: "Olivia Wilde boyfriend"Observation: In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling. Their relationship ended in November 2022.Thought: I need to find out Harry Styles' age.Action: searchAction Input: "Harry Styles age"Observation: 29 yearsThought:"
]
}
[llm/end] [1:chain:agent_executor > 8:chain:llm_chain > 9:llm:openai] [2.72s] Exiting LLM run with output: {
"generations": [
[
{
"text": " I need to calculate 29 raised to the 0.23 power.Action: calculatorAction Input: 29^0.23",
"generationInfo": {
"finishReason": "stop",
"logprobs": null
}
}
]
],
"llmOutput": {
"tokenUsage": {
"completionTokens": 26,
"promptTokens": 329,
"totalTokens": 355
}
}
}
[chain/end] [1:chain:agent_executor > 8:chain:llm_chain] [2.72s] Exiting Chain run with output: {
"text": " I need to calculate 29 raised to the 0.23 power.Action: calculatorAction Input: 29^0.23"
}
[agent/action] [1:chain:agent_executor] Agent selected action: {
"tool": "calculator",
"toolInput": "29^0.23",
"log": " I need to calculate 29 raised to the 0.23 power.Action: calculatorAction Input: 29^0.23"
}
[tool/start] [1:chain:agent_executor > 10:tool:calculator] Entering Tool run with input: "29^0.23"
[tool/end] [1:chain:agent_executor > 10:tool:calculator] [3ms] Exiting Tool run with output: "2.169459462491557"
[chain/start] [1:chain:agent_executor > 11:chain:llm_chain] Entering Chain run with input: {
"input": "Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?",
"agent_scratchpad": " I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.Action: searchAction Input: "Olivia Wilde boyfriend"Observation: In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling. Their relationship ended in November 2022.Thought: I need to find out Harry Styles' age.Action: searchAction Input: "Harry Styles age"Observation: 29 yearsThought: I need to calculate 29 raised to the 0.23 power.Action: calculatorAction Input: 29^0.23Observation: 2.169459462491557Thought:",
"stop": [
"Observation: "
]
}
[llm/start] [1:chain:agent_executor > 11:chain:llm_chain > 12:llm:openai] Entering LLM run with input: {
"prompts": [
"Answer the following questions as best you can. You have access to the following tools:search: a search engine. useful for when you need to answer questions about current events. input should be a search query.calculator: Useful for getting the result of a math expression. The input to this tool should be a valid mathematical expression that could be executed by a simple calculator.Use the following format in your response:Question: the input question you must answerThought: you should always think about what to doAction: the action to take, should be one of [search,calculator]Action Input: the input to the actionObservation: the result of the action... (this Thought/Action/Action Input/Observation can repeat N times)Thought: I now know the final answerFinal Answer: the final answer to the original input questionBegin!Question: Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?Thought: I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.Action: searchAction Input: "Olivia Wilde boyfriend"Observation: In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling. Their relationship ended in November 2022.Thought: I need to find out Harry Styles' age.Action: searchAction Input: "Harry Styles age"Observation: 29 yearsThought: I need to calculate 29 raised to the 0.23 power.Action: calculatorAction Input: 29^0.23Observation: 2.169459462491557Thought:"
]
}
[llm/end] [1:chain:agent_executor > 11:chain:llm_chain > 12:llm:openai] [3.51s] Exiting LLM run with output: {
"generations": [
[
{
"text": " I now know the final answer.Final Answer: Harry Styles is Olivia Wilde's boyfriend and his current age raised to the 0.23 power is 2.169459462491557.",
"generationInfo": {
"finishReason": "stop",
"logprobs": null
}
}
]
],
"llmOutput": {
"tokenUsage": {
"completionTokens": 39,
"promptTokens": 371,
"totalTokens": 410
}
}
}
[chain/end] [1:chain:agent_executor > 11:chain:llm_chain] [3.51s] Exiting Chain run with output: {
"text": " I now know the final answer.Final Answer: Harry Styles is Olivia Wilde's boyfriend and his current age raised to the 0.23 power is 2.169459462491557."
}
[chain/end] [1:chain:agent_executor] [14.90s] Exiting Chain run with output: {
"output": "Harry Styles is Olivia Wilde's boyfriend and his current age raised to the 0.23 power is 2.169459462491557."
}



--- 文件: output_20250622_020018\docs\modules\agents\agents.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/agents/
crawled_at: 2025-06-22T02:00:19.823419
---

代理
info
概念指南
代理是一个无状态的封装器，封装了一个代理提示链（比如MRKL)，负责将工具格式化到提示符中，以及解析从聊天模型获取的响应。它接收用户输入，并返回相应的“操作”和相应的“操作输入”响应。
选择哪种代理？
​
您选择的代理取决于您想执行的任务类型。以下是一个快速指南，可帮助您为您的使用情况选择正确的代理:
如果您正在使用文本LLM， 首先尝试
zero-shot-react-description
， 即。
LLMs的MRKL代理
。
如果您正在使用聊天模型， 尝试
chat-zero-shot-react-description
， 即。
聊天模型的MRKL代理
。
如果您正在使用聊天模型并想使用内存， 尝试
chat-conversational-react-description
，
会话代理
。
如果您有一个需要多个步骤的复杂任务，并且您有兴趣尝试一种新的代理类型， 尝试
Plan-and-Execute代理
。
所有代理
​
🗃️
动作代理 Action Agents
4 items
📄️
计划执行代理
这个例子展示了如何使用一个使用计划执行框架来回答查询的代理。
🗃️
自定义代理
3 items



--- 文件: output_20250622_020018\docs\modules\agents\executor.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/executor/
crawled_at: 2025-06-22T02:00:20.356066
---

Agent Executors
info
概念指南
为了让智能代理更加强大，我们需要使其迭代，即调用模型多次，直到达到最终答案。这就是 AgentExecutor 的工作。
class
AgentExecutor
{
// a simplified implementation
run
(
inputs
:
object
)
{
const
steps
=
[
]
;
while
(
true
)
{
const
step
=
await
this
.
agent
.
plan
(
steps
,
inputs
)
;
if
(
step
instanceof
AgentFinish
)
{
return
step
.
returnValues
;
}
steps
.
push
(
step
)
;
}
}
}
📄️
开始(Getting Started)
代理使用LLM来确定采取哪些操作以及采取的顺序。操作可以是使用工具并观察其输出，或返回给用户。



--- 文件: output_20250622_020018\docs\modules\agents\toolkits.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/toolkits/
crawled_at: 2025-06-22T02:00:20.483644
---

入门指南: 工具包
info
概念指南
一组工具，可用于解决特定问题/必需的工具组。
interface
Toolkit
{
tools
:
Tool
[
]
;
}
所有工具包
​
📄️
JSON代理工具包
这个例子展示了如何使用JSON工具包加载和使用代理。
📄️
OpenAPI代理工具包
该示例演示如何使用OpenAPI工具包加载和使用代理。
📄️
SQL Agent Toolkit
这个示例展示了如何加载和使用SQL工具包中的代理。
📄️
VectorStore 代理工具包
这个例子展示了如何使用 VectorStore 工具包加载和使用代理。



--- 文件: output_20250622_020018\docs\modules\agents\tools.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/tools/
crawled_at: 2025-06-22T02:00:20.702284
---

工具
info
概念指南
工具是一个函数的抽象，使得语言模型可以轻松地与之交互。具体来说，工具的接口有一个文本输入和一个文本输出。它包括名称和描述，向
模型
传达工具的作用和何时使用它。
interface
Tool
{
call
(
arg
:
string
)
:
Promise
<
string
>
;
name
:
string
;
description
:
string
;
}
所有工具
​
📄️
集成
LangChain提供以下可立即使用的工具:
📄️
带有向量存储的代理
本笔记涵盖了如何将代理和向量存储器组合使用。这种用例是，您已将数据摄入向量存储器中，并想以代理方式与其进行交互。
📄️
ChatGPT插件
本例展示了如何在LangChain抽象中使用ChatGPT插件。
📄️
DynamicTool 自定义工具
创建运行自定义代码的工具的一种选项是使用 DynamicTool。
📄️
使用 AWS Lambda 的代理
请查看完整文档//docs.aws.amazon.com/lambda/index.html
📄️
网络浏览器工具
网络浏览器工具为您的代理程序提供了访问网站和提取信息的功能。它向代理程序描述为：
📄️
Zapier NLA集成代理
完整文档在此处//nla.zapier.com/api/v1/dynamic/docs
高级
​
要实现自己的工具，你可以将
Tool
类作为子类，并实现
_call
方法。
_call
方法使用输入文本调用，应返回输出文本。Tool超类实现了
call
方法,在调用
_call
方法之前和之后调用正确的CallbackManager方法。当出现错误时，
_call
方法应返回表示错误的字符串，而不是抛出错误。这允许错误传递给LLM，并且LLM可以决定如何处理它。如果抛出错误，则代理的执行将停止。
abstract
class
Tool
{
abstract
_call
(
arg
:
string
)
:
Promise
<
string
>
;
abstract
name
:
string
;
abstract
description
:
string
;
}



--- 文件: output_20250622_020018\docs\modules\chains\index_related_chains.md ---
---
url: https://js.langchain.com.cn/docs/modules/chains/index_related_chains/
crawled_at: 2025-06-22T02:00:21.010614
---

与索引相关的链
info
概念指南
与存储在索引中的非结构化数据一起工作相关的链。
📄️
文档QA
LangChain提供了一系列专门针对非结构化文本数据处理的链条: StuffDocumentsChain， MapReduceDocumentsChain， 和 RefineDocumentsChain。这些链条是开发与这些数据交互的更复杂链条的基本构建模块。它们旨在接受文档和问题作为输入，然后利用语言模型根据提供的文档制定答案。
📄️
检索问答
RetrievalQAChain 是将 Retriever 和 QA 链（上文中所述)组合起来的链。它用于从 Retriever 检索文档，然后使用 QA 链根据检索到的文档回答问题。
📄️
对话式检索问答
ConversationalRetrievalQA 链基于 RetrievalQAChain 构建，提供了聊天历史记录组件。



--- 文件: output_20250622_020018\docs\modules\chains\llm_chain.md ---
---
url: https://js.langchain.com.cn/docs/modules/chains/llm_chain
crawled_at: 2025-06-22T02:00:21.232912
---

入门指南: LLMChain
info
概念指南
LLMChain
是在语言模型周围添加一些功能的简单链。它被广泛地应用于LangChain中，包括其他链和代理。
LLMChain
由一个
PromptTemplate
和一个语言模型（LLM或聊天模型)组成。
与LLMs的使用
​
我们可以构建一个LLMChain，它接受用户输入，使用PromptTemplate进行格式化，然后将格式化后的响应传递给LLM:
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
PromptTemplate
}
from
"langchain/prompts"
;
import
{
LLMChain
}
from
"langchain/chains"
;
// We can construct an LLMChain from a PromptTemplate and an LLM.
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
prompt
=
PromptTemplate
.
fromTemplate
(
"What is a good name for a company that makes {product}?"
)
;
const
chainA
=
new
LLMChain
(
{
llm
:
model
,
prompt
}
)
;
// The result is an object with a `text` property.
const
resA
=
await
chainA
.
call
(
{
product
:
"colorful socks"
}
)
;
console
.
log
(
{
resA
}
)
;
// { resA: { text: '\n\nSocktastic!' } }
// Since the LLMChain is a single-input, single-output chain, we can also `run` it.
// This takes in a string and returns the `text` property.
const
resA2
=
await
chainA
.
run
(
"colorful socks"
)
;
console
.
log
(
{
resA2
}
)
;
// { resA2: '\n\nSocktastic!' }
与聊天模型的使用
​
我们也可以构建一个LLMChain，它接受用户输入，使用PromptTemplate进行格式化，然后将格式化后的响应传递给ChatModel:
import
{
ChatPromptTemplate
,
HumanMessagePromptTemplate
,
SystemMessagePromptTemplate
,
}
from
"langchain/prompts"
;
import
{
LLMChain
}
from
"langchain/chains"
;
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
// We can also construct an LLMChain from a ChatPromptTemplate and a chat model.
const
chat
=
new
ChatOpenAI
(
{
temperature
:
0
}
)
;
const
chatPrompt
=
ChatPromptTemplate
.
fromPromptMessages
(
[
SystemMessagePromptTemplate
.
fromTemplate
(
"You are a helpful assistant that translates {input_language} to {output_language}."
)
,
HumanMessagePromptTemplate
.
fromTemplate
(
"{text}"
)
,
]
)
;
const
chainB
=
new
LLMChain
(
{
prompt
:
chatPrompt
,
llm
:
chat
,
}
)
;
const
resB
=
await
chainB
.
call
(
{
input_language
:
"English"
,
output_language
:
"French"
,
text
:
"I love programming."
,
}
)
;
console
.
log
(
{
resB
}
)
;
// { resB: { text: "J'adore la programmation." } }
在流模式下使用
​
我们也可以构建一个LLMChain，它接受用户输入，使用PromptTemplate进行格式化，然后将格式化后的响应传递给以流模式运行的LLM，该模式将在生成令牌时进行流式返回:
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
PromptTemplate
}
from
"langchain/prompts"
;
import
{
LLMChain
}
from
"langchain/chains"
;
// Create a new LLMChain from a PromptTemplate and an LLM in streaming mode.
const
model
=
new
OpenAI
(
{
temperature
:
0.9
,
streaming
:
true
}
)
;
const
prompt
=
PromptTemplate
.
fromTemplate
(
"What is a good name for a company that makes {product}?"
)
;
const
chain
=
new
LLMChain
(
{
llm
:
model
,
prompt
}
)
;
// Call the chain with the inputs and a callback for the streamed tokens
const
res
=
await
chain
.
call
(
{
product
:
"colorful socks"
}
,
[
{
handleLLMNewToken
(
token
:
string
)
{
process
.
stdout
.
write
(
token
)
;
}
,
}
,
]
)
;
console
.
log
(
{
res
}
)
;
// { res: { text: '\n\nKaleidoscope Socks' } }
取消正在运行的LLMChain
​
我们也可以通过向
call
方法传递一个AbortSignal来取消正在运行的LLMChain:
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
PromptTemplate
}
from
"langchain/prompts"
;
import
{
LLMChain
}
from
"langchain/chains"
;
// Create a new LLMChain from a PromptTemplate and an LLM in streaming mode.
const
model
=
new
OpenAI
(
{
temperature
:
0.9
,
streaming
:
true
}
)
;
const
prompt
=
PromptTemplate
.
fromTemplate
(
"Give me a long paragraph about {product}?"
)
;
const
chain
=
new
LLMChain
(
{
llm
:
model
,
prompt
}
)
;
const
controller
=
new
AbortController
(
)
;
// Call `controller.abort()` somewhere to cancel the request.
setTimeout
(
(
)
=>
{
controller
.
abort
(
)
;
}
,
3000
)
;
try
{
// Call the chain with the inputs and a callback for the streamed tokens
const
res
=
await
chain
.
call
(
{
product
:
"colorful socks"
,
signal
:
controller
.
signal
}
,
[
{
handleLLMNewToken
(
token
:
string
)
{
process
.
stdout
.
write
(
token
)
;
}
,
}
,
]
)
;
}
catch
(
e
)
{
console
.
log
(
e
)
;
// Error: Cancel: canceled
}
在这个例子中，我们展示了在流模式下的取消操作，但是在非流模式下的操作方式相同。



--- 文件: output_20250622_020018\docs\modules\chains\other_chains.md ---
---
url: https://js.langchain.com.cn/docs/modules/chains/other_chains/
crawled_at: 2025-06-22T02:00:21.184037
---

其他链
本节介绍其它存在的链的示例。
📄️
AnalyzeDocumentChain
您可以使用AnalyzeDocumentChain,它接受单个文本作为输入并对其进行操作。
📄️
APIChain
APIChain 可用于使用 LLM 与 API 交互，从而检索相关信息。提供关于所提供的 API 文档相关的问题以构造链。
📄️
宪法链
宪法链是一种链式结构，它确保语言模型的输出遵循预定义的宪法原则。通过纳入特定的规则和指南，宪法链可以过滤和修改生成的内容以符合这些原则，从而提供更加受控、道德和上下文恰当的响应。这种机制有助于保持输出的完整性，同时最大程度地减少生成可能违反指南、具有冒犯性或偏离所需上下文的内容的风险。
📄️
moderation_chain
OpenAIModerationChain
📄️
multi_prompt_chain
MultiPromptChain多次提示链
📄️
multi_retrieval_qa_chain
换行
📄️
SqlDatabaseChain 中文：Sql数据库链
SqlDatabaseChain 可以让您在 SQL 数据库上回答问题。
📄️
摘要
摘要链可以用来总结多个文档。一种方法是在将多个较小的文档分成块后将它们作为输入，与MapReduceDocumentsChain一起操作。您还可以选择将进行摘要的链替换为StuffDocumentsChain，或RefineDocumentsChain。在此处了解有关它们之间差异的更多信息here



--- 文件: output_20250622_020018\docs\modules\chains\prompt_selectors.md ---
---
url: https://js.langchain.com.cn/docs/modules/chains/prompt_selectors/
crawled_at: 2025-06-22T02:00:21.590996
---

提示选择器
info
概念指南
通常， 根据链中使用的模型类型，您会想要以编程方式选择提示。特别是在交换聊天模型和LLM时，这尤其重要。
提示选择器的界面非常简单。:
abstract
class
BasePromptSelector
{
abstract
getPrompt
(
llm
:
BaseLanguageModel
)
:
BasePromptTemplate
;
}
getPrompt方法接受一个语言模型并返回一个适当的提示模板。
我们目前提供了一个ConditionalPromptSelector，允许您指定一组条件和提示模板。评估为true的第一个条件将用于选择提示模板。
const
QA_PROMPT_SELECTOR
=
new
ConditionalPromptSelector
(
DEFAULT_QA_PROMPT
,
[
[
isChatModel
,
CHAT_PROMPT
]
,
]
)
;
如果该模型不是聊天模型，则返回
DEFAULT_QA_PROMPT
，如果是，则返回
CHAT_PROMPT
。
下面的示例展示了如何在加载链时使用提示选择器。:
const
loadQAStuffChain
=
(
llm
:
BaseLanguageModel
,
params
:
StuffQAChainParams
=
{
}
)
=>
{
const
{
prompt
=
QA_PROMPT_SELECTOR
.
getPrompt
(
llm
)
}
=
params
;
const
llmChain
=
new
LLMChain
(
{
prompt
,
llm
}
)
;
const
chain
=
new
StuffDocumentsChain
(
{
llmChain
}
)
;
return
chain
;
}
;



--- 文件: output_20250622_020018\docs\modules\chains\sequential_chain.md ---
---
url: https://js.langchain.com.cn/docs/modules/chains/sequential_chain
crawled_at: 2025-06-22T02:00:21.657434
---

顺序链
顺序链允许您连接多个链，并将它们组合成执行特定场景的管道。
SimpleSequentialChain
​
让我们从最简单的情况开始，即
SimpleSequentialChain
。
SimpleSequentialChain
是一种允许您将多个单输入/单输出链连接成一个链的链。
下面的示例显示了一个样例用例。在第一步，给定一个标题，生成一个剧本的简介。在第二步，基于生成的简介，生成剧本的评论。
import
{
SimpleSequentialChain
,
LLMChain
}
from
"langchain/chains"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
PromptTemplate
}
from
"langchain/prompts"
;
// This is an LLMChain to write a synopsis given a title of a play.
const
llm
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
template
=
`
You are a playwright. Given the title of play, it is your job to write a synopsis for that title.
Title: {title}
Playwright: This is a synopsis for the above play:
`
;
const
promptTemplate
=
new
PromptTemplate
(
{
template
,
inputVariables
:
[
"title"
]
,
}
)
;
const
synopsisChain
=
new
LLMChain
(
{
llm
,
prompt
:
promptTemplate
}
)
;
// This is an LLMChain to write a review of a play given a synopsis.
const
reviewLLM
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
reviewTemplate
=
`
You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.
Play Synopsis:
{synopsis}
Review from a New York Times play critic of the above play:
`
;
const
reviewPromptTemplate
=
new
PromptTemplate
(
{
template
:
reviewTemplate
,
inputVariables
:
[
"synopsis"
]
,
}
)
;
const
reviewChain
=
new
LLMChain
(
{
llm
:
reviewLLM
,
prompt
:
reviewPromptTemplate
,
}
)
;
const
overallChain
=
new
SimpleSequentialChain
(
{
chains
:
[
synopsisChain
,
reviewChain
]
,
verbose
:
true
,
}
)
;
const
review
=
await
overallChain
.
run
(
"Tragedy at sunset on the beach"
)
;
console
.
log
(
review
)
;
/*
variable review contains the generated play review based on the input title and synopsis generated in the first step:
"Tragedy at Sunset on the Beach is a powerful and moving story of love, loss, and redemption. The play follows the story of two young lovers, Jack and Jill, whose plans for a future together are tragically cut short when Jack is killed in a car accident. The play follows Jill as she struggles to cope with her grief and eventually finds solace in the arms of another man.
The play is beautifully written and the performances are outstanding. The actors bring the characters to life with their heartfelt performances, and the audience is taken on an emotional journey as Jill is forced to confront her grief and make a difficult decision between her past and her future. The play culminates in a powerful climax that will leave the audience in tears.
Overall, Tragedy at Sunset on the Beach is a powerful and moving story that will stay with you long after the curtain falls. It is a must-see for anyone looking for an emotionally charged and thought-provoking experience."
*/
SequentialChain
​
更高级的情况非常有用，当您有多个具有多个输入或输出键的链时。
import
{
SequentialChain
,
LLMChain
}
from
"langchain/chains"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
PromptTemplate
}
from
"langchain/prompts"
;
// This is an LLMChain to write a synopsis given a title of a play and the era it is set in.
const
llm
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
template
=
`
You are a playwright. Given the title of play and the era it is set in, it is your job to write a synopsis for that title.
Title: {title}
Era: {era}
Playwright: This is a synopsis for the above play:
`
;
const
promptTemplate
=
new
PromptTemplate
(
{
template
,
inputVariables
:
[
"title"
,
"era"
]
,
}
)
;
const
synopsisChain
=
new
LLMChain
(
{
llm
,
prompt
:
promptTemplate
,
outputKey
:
"synopsis"
,
}
)
;
// This is an LLMChain to write a review of a play given a synopsis.
const
reviewLLM
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
reviewTemplate
=
`
You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.
Play Synopsis:
{synopsis}
Review from a New York Times play critic of the above play:
`
;
const
reviewPromptTemplate
=
new
PromptTemplate
(
{
template
:
reviewTemplate
,
inputVariables
:
[
"synopsis"
]
,
}
)
;
const
reviewChain
=
new
LLMChain
(
{
llm
:
reviewLLM
,
prompt
:
reviewPromptTemplate
,
outputKey
:
"review"
,
}
)
;
const
overallChain
=
new
SequentialChain
(
{
chains
:
[
synopsisChain
,
reviewChain
]
,
inputVariables
:
[
"era"
,
"title"
]
,
// Here we return multiple variables
outputVariables
:
[
"synopsis"
,
"review"
]
,
verbose
:
true
,
}
)
;
const
chainExecutionResult
=
await
overallChain
.
call
(
{
title
:
"Tragedy at sunset on the beach"
,
era
:
"Victorian England"
,
}
)
;
console
.
log
(
chainExecutionResult
)
;
/*
variable chainExecutionResult contains final review and intermediate synopsis (as specified by outputVariables). The data is generated based on the input title and era:
"{
"review": "
Tragedy at Sunset on the Beach is a captivating and heartbreaking story of love and loss. Set in Victorian England, the play follows Emily, a young woman struggling to make ends meet in a small coastal town. Emily's dreams of a better life are dashed when she discovers her employer's scandalous affair, and her plans are further thwarted when she meets a handsome stranger on the beach.
The play is a powerful exploration of the human condition, as Emily must grapple with the truth and make a difficult decision that will change her life forever. The performances are outstanding, with the actors bringing a depth of emotion to their characters that is both heartbreaking and inspiring.
Overall, Tragedy at Sunset on the Beach is a beautiful and moving play that will leave audiences in tears. It is a must-see for anyone looking for a powerful and thought-provoking story.",
"synopsis": "
Tragedy at Sunset on the Beach is a play set in Victorian England. It tells the story of a young woman, Emily, who is struggling to make ends meet in a small coastal town. She works as a maid for a wealthy family, but her dreams of a better life are dashed when she discovers that her employer is involved in a scandalous affair.
Emily is determined to make a better life for herself, but her plans are thwarted when she meets a handsome stranger on the beach one evening. The two quickly fall in love, but their happiness is short-lived when Emily discovers that the stranger is actually a member of the wealthy family she works for.
The play follows Emily as she struggles to come to terms with the truth and make sense of her life. As the sun sets on the beach, Emily must decide whether to stay with the man she loves or to leave him and pursue her dreams. In the end, Emily must make a heartbreaking decision that will change her life forever.",
}"
*/



--- 文件: output_20250622_020018\docs\modules\indexes\document_loaders.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/
crawled_at: 2025-06-22T02:00:21.700243
---

入门: 文档装载
info
概念指南
文档装载器使得从各种数据源创建
文档
变得容易。这些文档可以被加载到
向量存储器
中以从数据源加载文档。
interface
DocumentLoader
{
load
(
)
:
Promise
<
Document
[
]
>
;
loadAndSplit
(
textSplitter
?
:
TextSplitter
)
:
Promise
<
Document
[
]
>
;
}
文档装载器公开两个方法：
load
和
loadAndSplit
。
load
会从数据源加载文档并将它们作为
文档
数组返回。
loadAndSplit
会从数据源加载文档，使用提供的
文本分割器
对它们进行分割，并将它们作为
文档
数组返回。
所有文档装载器
​
🗃️
示例
2 items
高级
​
如果您想要实现自己的文档装载器，您有几个选择。
子类化
BaseDocumentLoader
​
你可以直接扩展
BaseDocumentLoader
类。
BaseDocumentLoader
类提供了一些方便的方法，可以从各种数据源加载文档。
abstract
class
BaseDocumentLoader
implements
DocumentLoader
{
abstract
load
(
)
:
Promise
<
Document
[
]
>
;
}
子类化
TextLoader
​
如果你想从文本文件中加载文档，你可以扩展
TextLoader
类。
TextLoader
类会负责读取文件，所以你只需实现一个解析方法即可。
abstract
class
TextLoader
extends
BaseDocumentLoader
{
abstract
parse
(
raw
:
string
)
:
Promise
<
string
[
]
>
;
}
子类化
BufferLoader
​
如果你想要从二进制文件中加载文档，你可以扩展
BufferLoader
类。
BufferLoader
类会负责读取文件，因此你只需要实现一个解析方法。
abstract
class
BufferLoader
extends
BaseDocumentLoader
{
abstract
parse
(
raw
:
Buffer
,
metadata
:
Document
[
"metadata"
]
)
:
Promise
<
Document
[
]
>
;
}



--- 文件: output_20250622_020018\docs\modules\indexes\retrievers.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/retrievers/
crawled_at: 2025-06-22T02:00:22.831287
---

召回器（Retrievers)
info
概念指南
一种存储数据的方式，可以通过语言模型进行查询。这个对象必须公开的唯一接口是一个
getRelevantDocuments
方法，该方法接受一个字符串查询并返回一个文档列表。
📄️
ChatGPT插件检索器
本示例演示如何在LangChain中使用ChatGPT检索器插件。
📄️
自我查询色度检索器
自我查询检索器正如其名称所示具有查询自身的能力。具体而言给定任何自然语言查询检索器使用查询构建LLM链来撰写结构化查询，然后将该结构化查询应用于其基础向量存储中。这使得检索器不仅可以使用用户输入的查询进行与存储文档内容的语义相似性比较，而且可以从用户查询中提取存储文档的元数据过滤器并执行这些过滤器。
📄️
contextual-compression-retriever
上下文压缩检索器
📄️
Databerry Retriever
本示例展示如何在RetrievalQAChain中使用Databerry Retriever从Databerry.ai数据存储库检索文档。
📄️
HyDE Retriever
本示例展示了如何使用HyDE Retriever，其实现了Hypothetical Document Embeddings（HyDE)，具体内容参见这篇论文。
📄️
金属检索器
该示例展示了如何在“检索QAChain”中使用金属检索器从金属索引中检索文档。
📄️
自查Pinecone检索器
自查检索器具备查询自身的能力，正如其名称所示。具体地说，对于任何自然语言查询，检索器使用基于查询结构构建的LLM链来编写结构化查询，然后将该结构化查询应用于其底层向量存储。这不仅允许检索器使用用户输入的查询与所存储文件内容进行语义相似性比较，还可以从用户查询中提取有关存储文档元数据的过滤器并执行这些过滤器。[注：LLM链，指的是“罗杰局部语言模型”，是一种NLP技术]
📄️
远程检索器
本示例展示如何在 RetrievalQAChain 中使用远程检索器从远程服务器检索文档。
📄️
Supabase 混合搜索
Langchain 支持使用 Supabase Postgres 数据库进行混合搜索。该混合搜索结合了 Postgres 的 pgvector 扩展（相似度搜索)和全文搜索（关键词搜索)来检索文档。您可以通过 SupabaseVectorStore 的 addDocuments 函数添加文档。SupabaseHybridKeyWordSearch 接受嵌入， supabase 客户端， 相似性搜索的结果数量， 和关键词搜索的结果数量作为参数。getRelevantDocuments 函数产生一个去重和按相关性分数排序的文档列表。
📄️
时间加权召回器
时间加权召回器是一种综合考虑相似性和新近度的召回器。评分算法为 :。
📄️
向量库
一旦您创建了一个向量库， ,使用它作为检索器就非常简单:
📄️
Vespa Retriever
展示如何使用Vespa.ai作为LangChain检索器。
📄️
Zep Retriever
这个示例展示了如何在 RetrievalQAChain 中使用 Zep Retriever 从 Zep 内存存储中检索文档。(This example shows how to use the Zep Retriever in a RetrievalQAChain to retrieve documents from Zep memory store.)



--- 文件: output_20250622_020018\docs\modules\indexes\text_splitters.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/text_splitters/
crawled_at: 2025-06-22T02:00:23.545801
---

入门: 文本分割器（Text Splitters)
info
概念指南
语言模型通常受到可以传递给它们的文本数量的限制，因此将文本分割为较小的块是必要的。
LangChain提供了几种实用工具来完成此操作。
使用文本分割器也可以帮助改善向量存储的搜索结果，因为较小的块有时更容易匹配查询。
测试不同的块大小（和块重叠)是一个值得的练习，以适应您的用例。
参数
​
chunkSize?: number = 1000
: 每个块中最大字符数。默认值为1000个标记（tokens)。
chunkOverlap?: number = 200
: 相邻块之间重叠的字符数。默认值为200个标记（tokens)。
type
TextSplitterChunkHeaderOptions
=
{
chunkHeader
?
:
string
;
chunkOverlapHeader
?
:
string
;
appendChunkOverlapHeader
?
:
boolean
;
}
;
interface
TextSplitter
{
chunkSize
:
number
;
chunkOverlap
:
number
;
createDocuments
(
texts
:
string
[
]
,
metadatas
?
:
Record
<
string
,
any
>
[
]
,
chunkHeaderOptions
:
TextSplitterChunkHeaderOptions
=
{
}
)
:
Promise
<
Document
[
]
>
;
splitDocuments
(
documents
:
Document
[
]
,
chunkHeaderOptions
:
TextSplitterChunkHeaderOptions
=
{
}
)
:
Promise
<
Document
[
]
>
;
}
文本分割器提供了两种方法：，
createDocuments
和
splitDocuments
。
前者获取原始文本字符串的列表并返回文档的列表，后者获取文档列表并返回文档的列表。
区别在于
createDocuments
将原始文本字符串拆分成块，而
splitDocuments
将文档拆分成块。
何时使用
chunkHeaderOptions
​
仅仅通过重叠文本来分割文档可能无法提供足够的上下文信息，让LLMs确定多个块是否引用了相同的信息或如何解决来自相互矛盾的源的信息。
给每个文档打标签是一个解决方案，如果你知道需要过滤哪些信息，但是你可能事先不知道你的向量存储将需要处理哪些查询。
在每个块中直接包含其他上下文信息，例如标题，可以帮助处理任意查询。
Here's an example:
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
RetrievalQAChain
,
loadQAStuffChain
}
from
"langchain/chains"
;
import
{
CharacterTextSplitter
}
from
"langchain/text_splitter"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
HNSWLib
}
from
"langchain/vectorstores/hnswlib"
;
const
splitter
=
new
CharacterTextSplitter
(
{
chunkSize
:
1536
,
chunkOverlap
:
200
,
}
)
;
const
jimDocs
=
await
splitter
.
createDocuments
(
[
`
My favorite color is blue.
`
]
,
[
]
,
{
chunkHeader
:
`
DOCUMENT NAME: Jim Interview\n\n---\n\n
`
,
appendChunkOverlapHeader
:
true
,
}
)
;
const
pamDocs
=
await
splitter
.
createDocuments
(
[
`
My favorite color is red.
`
]
,
[
]
,
{
chunkHeader
:
`
DOCUMENT NAME: Pam Interview\n\n---\n\n
`
,
appendChunkOverlapHeader
:
true
,
}
)
;
const
vectorStore
=
await
HNSWLib
.
fromDocuments
(
jimDocs
.
concat
(
pamDocs
)
,
new
OpenAIEmbeddings
(
)
)
;
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
chain
=
new
RetrievalQAChain
(
{
combineDocumentsChain
:
loadQAStuffChain
(
model
)
,
retriever
:
vectorStore
.
asRetriever
(
)
,
returnSourceDocuments
:
true
,
}
)
;
const
res
=
await
chain
.
call
(
{
query
:
"What is Pam's favorite color?"
,
}
)
;
console
.
log
(
JSON
.
stringify
(
res
,
null
,
2
)
)
;
/*
{
"text": " Red.",
"sourceDocuments": [
{
"pageContent": "DOCUMENT NAME: Pam Interview\n\n---\n\nMy favorite color is red.",
"metadata": {
"loc": {
"lines": {
"from": 1,
"to": 1
}
}
}
},
{
"pageContent": "DOCUMENT NAME: Jim Interview\n\n---\n\nMy favorite color is blue.",
"metadata": {
"loc": {
"lines": {
"from": 1,
"to": 1
}
}
}
}
]
}
*/
;
All Text Splitters
​
🗃️
示例
4 items
高级
​
如果你想要实现自己的定制文本分割器，你只需要继承
TextSplitter
类并且实现一个方法
splitText
即可。该方法接收一个字符串作为输入，并返回一个字符串列表。返回的字符串列表将被用作输入数据的分块。
abstract
class
TextSplitter
{
abstract
splitText
(
text
:
string
)
:
Promise
<
string
[
]
>
;
}



--- 文件: output_20250622_020018\docs\modules\indexes\vector_stores.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/vector_stores/
crawled_at: 2025-06-22T02:00:23.712265
---

入门: 矢量存储
info
概念指南
矢量存储是一种专为存储文档及其
嵌入
而优化的数据库。接着，可以获取与查询最相关的文档，即嵌入与查询嵌入最相似的文档。
interface
VectorStore
{
/**
* Add more documents to an existing VectorStore
*/
addDocuments
(
documents
:
Document
[
]
)
:
Promise
<
void
>
;
/**
* Search for the most similar documents to a query
*/
similaritySearch
(
query
:
string
,
k
?
:
number
,
filter
?
:
object
|
undefined
)
:
Promise
<
Document
[
]
>
;
/**
* Search for the most similar documents to a query,
* and return their similarity score
*/
similaritySearchWithScore
(
query
:
string
,
k
=
4
,
filter
:
object
|
undefined
=
undefined
)
:
Promise
<
[
object
,
number
]
[
]
>
;
/**
* Turn a VectorStore into a Retriever
*/
asRetriever
(
k
?
:
number
)
:
BaseRetriever
;
/**
* Advanced: Add more documents to an existing VectorStore,
* when you already have their embeddings
*/
addVectors
(
vectors
:
number
[
]
[
]
,
documents
:
Document
[
]
)
:
Promise
<
void
>
;
/**
* Advanced: Search for the most similar documents to a query,
* when you already have the embedding of the query
*/
similaritySearchVectorWithScore
(
query
:
number
[
]
,
k
:
number
,
filter
?
:
object
)
:
Promise
<
[
Document
,
number
]
[
]
>
;
}
您可以从
文档列表
或文本列表及其相应的元数据中创建矢量存储。您还可以从现有索引中创建矢量存储，此方法的签名取决于使用的矢量存储，请查看您感兴趣的矢量存储的文档。
abstract
class
BaseVectorStore
implements
VectorStore
{
static
fromTexts
(
texts
:
string
[
]
,
metadatas
:
object
[
]
|
object
,
embeddings
:
Embeddings
,
dbConfig
:
Record
<
string
,
any
>
)
:
Promise
<
VectorStore
>
;
static
fromDocuments
(
docs
:
Document
[
]
,
embeddings
:
Embeddings
,
dbConfig
:
Record
<
string
,
any
>
)
:
Promise
<
VectorStore
>
;
}
如何选择矢量存储？
​
以下是一个快速指南，帮助您选择适合您的用例的正确矢量存储:
如果您需要一种只能在Node.js应用程序内部运行的东西，即内存中，而不需要其他服务器，则选择
HNSWLib
或
Faiss
如果您正在寻找一种可以在类似于浏览器的环境中内存中运行的东西，则选择
MemoryVectorStore
如果您来自Python，并且正在寻找类似于FAISS的东西，则选择
HNSWLib
或
Faiss
If you're looking for an open-source full-featured vector database that you can run locally in a docker container, then go for
Chroma
如果您已经在使用Supabase，则可以查看
Supabase
向量存储，以便对您的嵌入使用同一的Postgres数据库
如果您正在寻找一种不用自行托管的生产使用准备就绪的向量存储,请尝试
Pinecone
如果您已经使用SingleStore,或者需要分布式，高性能数据库，您可能想考虑使用
SingleStore
向量存储。
所有向量存储
​
🗃️
集成
16 items



--- 文件: output_20250622_020018\docs\modules\memory\examples.md ---
---
url: https://js.langchain.com.cn/docs/modules/memory/examples/
crawled_at: 2025-06-22T02:00:24.782757
---

示例: 内存
📄️
buffer_memory
缓存内存
📄️
使用缓冲内存与聊天模型翻译的中文结果
本示例介绍如何将聊天特定的内存类与聊天模型配合使用。翻译的中文结果
📄️
Buffer Window Memory
BufferWindowMemory用于跟踪会话中的来回消息，然后使用大小为 k 的窗口将最近的 k 条来回消息提取出来作为内存。
📄️
Conversation Summary（对话总结)
对话总结记忆会在对话进行时对其进行总结并储存在记忆中。这个记忆能够被用于将当前对话总结注入到提示/链中。这个记忆在对较长的对话进行总结时非常有用，因为直接在提示中保留所有过去的对话历史信息将会占用过多的token。
📄️
DynamoDB支持的聊天记录
如果需要在聊天进程之间进行更长期的持久化，您可以将默认的内存chatHistory替换为DynamoDB实例，作为支持BufferMemory等聊天记录类的后端。，注意：chatHistory指聊天记录类，BufferMemory指缓存存储器类。
📄️
实体记忆
实体记忆是会话中记忆特定实体的给定事实。
📄️
Momento支持的聊天记录
如果要在聊天会话中使用分布式、无服务器的持久性,可以使用Momento支持的聊天消息历史记录，即刻缓存，无需任何基础设施维护,无论是在本地构建还是在生产环境中都是一个很好的起点。
📄️
motorhead_memory
Motörhead 是一个由Rust实现的内存服务器。它可以自动处理增量摘要并允许无状态应用程序。
📄️
基于Redis的聊天存储
如果需要在聊天会话之间进行长期持久化，可以将默认的内存chatHistory替换为一个Redis实例来支持聊天存储类，如BufferMemory。
📄️
Upstash 基于 Redis 的聊天记忆
由于 Upstash Redis 通过 REST API 工作,所以您可以将其与 Vercel Edge， [Cloudflare Workers] 一起使用（https：//developers.cloudflare.com/workers/)和其他无服务器环境。
📄️
基于向量库的内存
VectorStoreRetrieverMemory将记忆存储在向量数据库中，并在每次调用时查询最“显著”的前K个文档。
📄️
Zep Memory
Zep是存储、，总结、内嵌、索引和丰富对话AI聊天历史、自治代理历史、文档Q&A历史的记忆服务器，并通过简单、低延迟的API公开它们。



--- 文件: output_20250622_020018\docs\modules\models\chat.md ---
---
url: https://js.langchain.com.cn/docs/modules/models/chat/
crawled_at: 2025-06-22T02:00:25.278714
---

入门: 聊天模型
info
概念指南
LangChain提供了一个标准接口来使用聊天模型。聊天模型是语言模型的一种变体。
虽然聊天模型在内部使用语言模型，但它们公开的接口有些不同。
除了公开一个“输入文本，输出文本”的API外，它们还公开了一个“聊天消息”作为输入和输出的接口。
聊天消息
​
一个“聊天消息”是指聊天模型中的模块化信息单位。
目前，这包括一个“text”字段，它指的是聊天消息的内容。
目前LangChain支持四种不同类型的“聊天消息”:
HumanChatMessage
: 模拟一个人类的视角发送的聊天消息。
AIChatMessage
: 从AI系统的角度发送的聊天消息，用于与人类进行通信。
SystemChatMessage
: 一种用于向AI系统提供有关对话信息的聊天消息。通常在对话开始时发送。
ChatMessage
: 一个通用的聊天消息，不仅有一个“文本”字段，还有一个任意的“角色”字段。
要开始使用，只需使用“LLM”实现的“call”方法，传入一个字符串输入。在这个例子中，我们使用的是“ChatOpenAI”实现:
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
HumanChatMessage
}
from
"langchain/schema"
;
export
const
run
=
async
(
)
=>
{
const
chat
=
new
ChatOpenAI
(
)
;
// Pass in a list of messages to `call` to start a conversation. In this simple example, we only pass in one message.
const
response
=
await
chat
.
call
(
[
new
HumanChatMessage
(
"What is a good name for a company that makes colorful socks?"
)
,
]
)
;
console
.
log
(
response
)
;
// AIChatMessage { text: '\n\nRainbow Sox Co.' }
}
;
深入挖掘
​
📄️
集成
LangChain提供了许多与不同模型提供者集成的聊天模型实现。它们如下所示:
📄️
附加功能
我们为聊天模型提供了许多附加功能。在以下示例中，我们将使用 ChatOpenAI 模型。



--- 文件: output_20250622_020018\docs\modules\models\embeddings.md ---
---
url: https://js.langchain.com.cn/docs/modules/models/embeddings/
crawled_at: 2025-06-22T02:00:25.434538
---

入门: 嵌入
info
概念指南
嵌入可以用来创建文本数据的数字表示。这种数字表示很有用，因为它可以用来找到相似的文档。
以下是如何使用OpenAI嵌入的示例。嵌入有时在查询和文档方面具有不同的嵌入方法，因此嵌入类公开了
embedQuery
和
embedDocuments
方法。
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
/* Create instance */
const
embeddings
=
new
OpenAIEmbeddings
(
)
;
/* Embed queries */
const
res
=
await
embeddings
.
embedQuery
(
"Hello world"
)
;
/*
[
-0.004845875,   0.004899438,  -0.016358767,  -0.024475135, -0.017341806,
0.012571548,  -0.019156644,   0.009036391,  -0.010227379, -0.026945334,
0.022861943,   0.010321903,  -0.023479493, -0.0066544134,  0.007977734,
0.0026371893,   0.025206111,  -0.012048521,   0.012943339,  0.013094575,
-0.010580265,  -0.003509951,   0.004070787,   0.008639394, -0.020631202,
-0.0019203906,   0.012161949,  -0.019194454,   0.030373365, -0.031028723,
0.0036170771,  -0.007813894, -0.0060778237,  -0.017820721, 0.0048647798,
-0.015640393,   0.001373733,  -0.015552171,   0.019534737, -0.016169721,
0.007316074,   0.008273906,   0.011418369,   -0.01390117, -0.033347685,
0.011248227,  0.0042503807,  -0.012792102, -0.0014595914,  0.028356876,
0.025407761, 0.00076445413,  -0.016308354,   0.017455231, -0.016396577,
0.008557475,   -0.03312083,   0.031104341,   0.032389853,  -0.02132437,
0.003324056,  0.0055610985, -0.0078012915,   0.006090427, 0.0062038545,
0.0169133,  0.0036391325,  0.0076815626,  -0.018841568,  0.026037913,
0.024550753,  0.0055264398, -0.0015824712, -0.0047765584,  0.018425668,
0.0030656934, -0.0113742575, -0.0020322427,   0.005069579, 0.0022701253,
0.036095154,  -0.027449455,  -0.008475555,   0.015388331,  0.018917186,
0.0018999106,  -0.003349262,   0.020895867,  -0.014480911, -0.025042271,
0.012546342,   0.013850759,  0.0069253794,   0.008588983, -0.015199285,
-0.0029585673,  -0.008759124,   0.016749462,   0.004111747,  -0.04804285,
... 1436 more items
]
*/
/* Embed documents */
const
documentRes
=
await
embeddings
.
embedDocuments
(
[
"Hello world"
,
"Bye bye"
]
)
;
/*
[
[
-0.0047852774,  0.0048640342,   -0.01645707,  -0.024395779, -0.017263541,
0.012512918,  -0.019191515,   0.009053908,  -0.010213212, -0.026890801,
0.022883644,   0.010251015,  -0.023589306,  -0.006584088,  0.007989113,
0.002720268,   0.025088841,  -0.012153786,   0.012928754,  0.013054766,
-0.010395928, -0.0035566676,  0.0040008575,   0.008600268, -0.020678446,
-0.0019106456,   0.012178987,  -0.019241918,   0.030444318,  -0.03102397,
0.0035692686,  -0.007749692,   -0.00604854,   -0.01781799,  0.004860884,
-0.015612794,  0.0014097509,  -0.015637996,   0.019443536,  -0.01612944,
0.0072960514,   0.008316742,   0.011548932,  -0.013987249,  -0.03336778,
0.011341013,    0.00425603, -0.0126578305, -0.0013861238,  0.028302127,
0.025466874,  0.0007029065,  -0.016318457,   0.017427357, -0.016394064,
0.008499459,  -0.033241767,   0.031200387,    0.03238489,   -0.0212833,
0.0032416396,   0.005443686,  -0.007749692,  0.0060201874,  0.006281661,
0.016923312,   0.003528315,  0.0076740854,   -0.01881348,  0.026109532,
0.024660403,   0.005472039, -0.0016712243, -0.0048136297,  0.018397642,
0.003011669,  -0.011385117, -0.0020193304,   0.005138109, 0.0022335495,
0.03603922,  -0.027495656,  -0.008575066,   0.015436378,  0.018851284,
0.0018019609, -0.0034338066,    0.02094307,  -0.014503895, -0.024950229,
0.012632628,   0.013735226,  0.0069936244,   0.008575066, -0.015196957,
-0.0030541976,  -0.008745181,   0.016746895,  0.0040481114, -0.048010286,
... 1436 more items
],
[
-0.009446913,  -0.013253193,   0.013174579,  0.0057552797,  -0.038993083,
0.0077763423,    -0.0260478, -0.0114384955, -0.0022683728,  -0.016509168,
0.041797023,    0.01787183,    0.00552271, -0.0049789557,   0.018146982,
-0.01542166,   0.033752076,   0.006112323,   0.023872782,  -0.016535373,
-0.006623321,   0.016116094, -0.0061090477, -0.0044155475,  -0.016627092,
-0.022077737, -0.0009286407,   -0.02156674,   0.011890532,  -0.026283644,
0.02630985,   0.011942943,  -0.026126415,  -0.018264906,  -0.014045896,
-0.024187243,  -0.019037955,  -0.005037917,   0.020780588, -0.0049527506,
0.002399398,   0.020767486,  0.0080908025,  -0.019666875,  -0.027934562,
0.017688395,   0.015225122,  0.0046186363, -0.0045007137,   0.024265857,
0.03244183,  0.0038848957,   -0.03244183,  -0.018893827, -0.0018065092,
0.023440398,  -0.021763276,   0.015120302,   -0.01568371,  -0.010861984,
0.011739853,  -0.024501702,  -0.005214801,   0.022955606,   0.001315165,
-0.00492327,  0.0020358032,  -0.003468891,  -0.031079166,  0.0055259857,
0.0028547104,   0.012087069,   0.007992534, -0.0076256637,   0.008110457,
0.002998838,  -0.024265857,   0.006977089,  -0.015185814, -0.0069115767,
0.006466091,  -0.029428247,  -0.036241557,   0.036713246,   0.032284595,
-0.0021144184,  -0.014255536,   0.011228855,  -0.027227025,  -0.021619149,
0.00038242966,    0.02245771, -0.0014748519,    0.01573612,  0.0041010873,
0.006256451,  -0.007992534,   0.038547598,   0.024658933,  -0.012958387,
... 1436 more items
]
]
*/
更深入地了解
​
📄️
集成
LangChain提供了许多与各种模型提供商集成的嵌入实现。这些是:
📄️
附加功能
我们为聊天模型提供了许多附加功能。在下面的示例中，我们将使用“OpenAI嵌入”模型。



--- 文件: output_20250622_020018\docs\modules\models\llms.md ---
---
url: https://js.langchain.com.cn/docs/modules/models/llms/
crawled_at: 2025-06-22T02:00:25.555119
---

入门指南: LLMs
info
概念指南
LangChain 提供了使用各种 LLM 的标准界面。
要开始， 只需使用
LLM
实现的
call
方法， 传递一个
string
输入。在这个例子中， 我们使用了
OpenAI
实现:
import
{
OpenAI
}
from
"langchain/llms/openai"
;
export
const
run
=
async
(
)
=>
{
const
model
=
new
OpenAI
(
)
;
// `call` is a simple string-in, string-out method for interacting with the model.
const
resA
=
await
model
.
call
(
"What would be a good company name a company that makes colorful socks?"
)
;
console
.
log
(
{
resA
}
)
;
// { resA: '\n\nSocktastic Colors' }
}
;
深入研究
​
📄️
集成
LangChain提供了多种LLM实现，可与各种模型提供者集成。这些是:
📄️
附加功能
我们为LLM提供了许多附加功能。在下面的大多数示例中，我们将使用 OpenAI LLM。然而，所有这些功能都适用于所有LLMs。



--- 文件: output_20250622_020018\docs\modules\prompts\example_selectors.md ---
---
url: https://js.langchain.com.cn/docs/modules/prompts/example_selectors/
crawled_at: 2025-06-22T02:00:25.716892
---

示例选择器
info
概念指南
如果您有大量的示例,您可能需要以编程方式选择要包含在提示中的示例。 ExampleSelector 是执行此操作的类。 基本接口定义如下。
class
BaseExampleSelector
{
addExample
(
example
:
Example
)
:
Promise
<
void
|
string
>
;
selectExamples
(
input_variables
:
Example
)
:
Promise
<
Example
[
]
>
;
}
它需要公开一个
selectExamples
方法 - 这需要输入变量，然后返回一个示例列表 - 和一个
addExample
方法,用于保存以后选择的示例。每个具体的实现都可以决定如何保存和选择这些示例。 让我们看一些示例。
根据长度选择
​
此
ExampleSelector
根据长度选择要使用的示例。 当您担心构建的提示会超过上下文窗口的长度时，这非常有用。 对于较长的输入,它会选择较少的示例进行包含,而对于较短的输入，则会选择更多示例。
import
{
LengthBasedExampleSelector
,
PromptTemplate
,
FewShotPromptTemplate
,
}
from
"langchain/prompts"
;
export
async
function
run
(
)
{
// Create a prompt template that will be used to format the examples.
const
examplePrompt
=
new
PromptTemplate
(
{
inputVariables
:
[
"input"
,
"output"
]
,
template
:
"Input: {input}\nOutput: {output}"
,
}
)
;
// Create a LengthBasedExampleSelector that will be used to select the examples.
const
exampleSelector
=
await
LengthBasedExampleSelector
.
fromExamples
(
[
{
input
:
"happy"
,
output
:
"sad"
}
,
{
input
:
"tall"
,
output
:
"short"
}
,
{
input
:
"energetic"
,
output
:
"lethargic"
}
,
{
input
:
"sunny"
,
output
:
"gloomy"
}
,
{
input
:
"windy"
,
output
:
"calm"
}
,
]
,
{
examplePrompt
,
maxLength
:
25
,
}
)
;
// Create a FewShotPromptTemplate that will use the example selector.
const
dynamicPrompt
=
new
FewShotPromptTemplate
(
{
// We provide an ExampleSelector instead of examples.
exampleSelector
,
examplePrompt
,
prefix
:
"Give the antonym of every input"
,
suffix
:
"Input: {adjective}\nOutput:"
,
inputVariables
:
[
"adjective"
]
,
}
)
;
// An example with small input, so it selects all examples.
console
.
log
(
await
dynamicPrompt
.
format
(
{
adjective
:
"big"
}
)
)
;
/*
Give the antonym of every input
Input: happy
Output: sad
Input: tall
Output: short
Input: energetic
Output: lethargic
Input: sunny
Output: gloomy
Input: windy
Output: calm
Input: big
Output:
*/
// An example with long input, so it selects only one example.
const
longString
=
"big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else"
;
console
.
log
(
await
dynamicPrompt
.
format
(
{
adjective
:
longString
}
)
)
;
/*
Give the antonym of every input
Input: happy
Output: sad
Input: big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else
Output:
*/
}
根据相似度选择
​
SemanticSimilarityExampleSelector
根据与输入最相似的示例选择示例。 它通过查找具有与输入的余弦相似度最大的嵌入的示例来实现此目的。
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
SemanticSimilarityExampleSelector
,
PromptTemplate
,
FewShotPromptTemplate
,
}
from
"langchain/prompts"
;
import
{
HNSWLib
}
from
"langchain/vectorstores/hnswlib"
;
export
async
function
run
(
)
{
// Create a prompt template that will be used to format the examples.
const
examplePrompt
=
new
PromptTemplate
(
{
inputVariables
:
[
"input"
,
"output"
]
,
template
:
"Input: {input}\nOutput: {output}"
,
}
)
;
// Create a SemanticSimilarityExampleSelector that will be used to select the examples.
const
exampleSelector
=
await
SemanticSimilarityExampleSelector
.
fromExamples
(
[
{
input
:
"happy"
,
output
:
"sad"
}
,
{
input
:
"tall"
,
output
:
"short"
}
,
{
input
:
"energetic"
,
output
:
"lethargic"
}
,
{
input
:
"sunny"
,
output
:
"gloomy"
}
,
{
input
:
"windy"
,
output
:
"calm"
}
,
]
,
new
OpenAIEmbeddings
(
)
,
HNSWLib
,
{
k
:
1
}
)
;
// Create a FewShotPromptTemplate that will use the example selector.
const
dynamicPrompt
=
new
FewShotPromptTemplate
(
{
// We provide an ExampleSelector instead of examples.
exampleSelector
,
examplePrompt
,
prefix
:
"Give the antonym of every input"
,
suffix
:
"Input: {adjective}\nOutput:"
,
inputVariables
:
[
"adjective"
]
,
}
)
;
// Input is about the weather, so should select eg. the sunny/gloomy example
console
.
log
(
await
dynamicPrompt
.
format
(
{
adjective
:
"rainy"
}
)
)
;
/*
Give the antonym of every input
Input: sunny
Output: gloomy
Input: rainy
Output:
*/
// Input is a measurement, so should select the tall/short example
console
.
log
(
await
dynamicPrompt
.
format
(
{
adjective
:
"large"
}
)
)
;
/*
Give the antonym of every input
Input: tall
Output: short
Input: large
Output:
*/
}



--- 文件: output_20250622_020018\docs\modules\prompts\prompt_templates.md ---
---
url: https://js.langchain.com.cn/docs/modules/prompts/prompt_templates/
crawled_at: 2025-06-22T02:00:25.767047
---

提示模板
info
概念指南
PromptTemplate
允许您使用模板生成提示。当您想在多个地方使用相同的提示概要，但更改某些值时，这非常有用。
如下所示，
PromptTemplate
对LLM和聊天模型都有支持:
import
{
ChatPromptTemplate
,
HumanMessagePromptTemplate
,
PromptTemplate
,
SystemMessagePromptTemplate
,
}
from
"langchain/prompts"
;
export
const
run
=
async
(
)
=>
{
// A `PromptTemplate` consists of a template string and a list of input variables.
const
template
=
"What is a good name for a company that makes {product}?"
;
const
promptA
=
new
PromptTemplate
(
{
template
,
inputVariables
:
[
"product"
]
}
)
;
// We can use the `format` method to format the template with the given input values.
const
responseA
=
await
promptA
.
format
(
{
product
:
"colorful socks"
}
)
;
console
.
log
(
{
responseA
}
)
;
/*
{
responseA: 'What is a good name for a company that makes colorful socks?'
}
*/
// We can also use the `fromTemplate` method to create a `PromptTemplate` object.
const
promptB
=
PromptTemplate
.
fromTemplate
(
"What is a good name for a company that makes {product}?"
)
;
const
responseB
=
await
promptB
.
format
(
{
product
:
"colorful socks"
}
)
;
console
.
log
(
{
responseB
}
)
;
/*
{
responseB: 'What is a good name for a company that makes colorful socks?'
}
*/
// For chat models, we provide a `ChatPromptTemplate` class that can be used to format chat prompts.
const
chatPrompt
=
ChatPromptTemplate
.
fromPromptMessages
(
[
SystemMessagePromptTemplate
.
fromTemplate
(
"You are a helpful assistant that translates {input_language} to {output_language}."
)
,
HumanMessagePromptTemplate
.
fromTemplate
(
"{text}"
)
,
]
)
;
// The result can be formatted as a string using the `format` method.
const
responseC
=
await
chatPrompt
.
format
(
{
input_language
:
"English"
,
output_language
:
"French"
,
text
:
"I love programming."
,
}
)
;
console
.
log
(
{
responseC
}
)
;
/*
{
responseC: '[{"text":"You are a helpful assistant that translates English to French."},{"text":"I love programming."}]'
}
*/
// The result can also be formatted as a list of `ChatMessage` objects by returning a `PromptValue` object and calling the `toChatMessages` method.
// More on this below.
const
responseD
=
await
chatPrompt
.
formatPromptValue
(
{
input_language
:
"English"
,
output_language
:
"French"
,
text
:
"I love programming."
,
}
)
;
const
messages
=
responseD
.
toChatMessages
(
)
;
console
.
log
(
{
messages
}
)
;
/*
{
messages: [
SystemChatMessage {
text: 'You are a helpful assistant that translates English to French.'
},
HumanChatMessage { text: 'I love programming.' }
]
}
*/
}
;
深入了解
​
📄️
提示组合
流水线提示模板允许您将多个单独的提示模板组合在一起。
📄️
额外功能（Additional Functionality)
我们提供了一些额外的功能，以便在提示模板中展示，如下所示:（We offer a number of extra features for prompt templates as shown below)



--- 文件: output_20250622_020018\docs\modules\schema\chat-messages.md ---
---
url: https://js.langchain.com.cn/docs/modules/schema/chat-messages
crawled_at: 2025-06-22T02:00:25.902882
---

聊天消息
终端用户与LLMs互动的主要界面是聊天界面。因此，一些模型提供商已经开始以期望聊天消息的方式提供对底层API的访问。这些消息具有内容字段（通常是文本)，并与用户（或角色)相关联。当前支持的用户有System， Human，和AI。
SystemChatMessage
​
表示应为AI系统提供说明的聊天消息。
import { SystemChatMessage } from "langchain/schema";
new SystemChatMessage("You are a nice assistant");
HumanChatMessage
​
表示来自与AI系统交互的人的信息的聊天消息。
import { HumanChatMessage } from "langchain/schema";
new HumanChatMessage("Hello, how are you?");
AIChatMessage
​
表示来自AI系统的消息的聊天消息。
import
{
AIChatMessage
}
from
"langchain/schema"
;
new
AIChatMessage
(
"I am doing well, thank you!"
)
;



--- 文件: output_20250622_020018\docs\modules\schema\document.md ---
---
url: https://js.langchain.com.cn/docs/modules/schema/document
crawled_at: 2025-06-22T02:00:25.946681
---

文档
语言模型只知道它们所训练的内容的信息。为了让它们能够回答问题或总结其他信息，你需要将信息传递给语言模型。因此，拥有文档的概念非常重要。
文档本质上非常简单。它由一段文本和可选的元数据组成。文本是我们与语言模型交互的部分，而可选的元数据对于跟踪文档的元数据（例如来源)非常有用。
interface
Document
{
pageContent
:
string
;
metadata
:
Record
<
string
,
any
>
;
}
创建文档
​
你可以在LangChain中很容易地创建一个文档对象与
import
{
Document
}
from
"langchain/document"
;
const
doc
=
new
Document
(
{
pageContent
:
"foo"
}
)
;
你可以使用 创建带有元数据的文档
import
{
Document
}
from
"langchain/document"
;
const
doc
=
new
Document
(
{
pageContent
:
"foo"
,
metadata
:
{
source
:
"1"
}
}
)
;
同时还可以查看
文档加载器（Document Loaders）
，以了解从各种来源加载文档的方法。



--- 文件: output_20250622_020018\docs\modules\schema\example.md ---
---
url: https://js.langchain.com.cn/docs/modules/schema/example
crawled_at: 2025-06-22T02:00:25.959830
---

示例
示例是输入/输出对，表示对函数的输入和预期输出。它们可用于模型的训练和评估。
type
Example
=
Record
<
string
,
string
>
;
创建示例
​
您可以这样创建示例:
const
example
=
{
input
:
"foo"
,
output
:
"bar"
,
}
;



--- 文件: output_20250622_020018\docs\modules\agents\agents\action.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/agents/action/
crawled_at: 2025-06-22T02:00:19.897433
---

动作代理 Action Agents
📄️
LLMs的MRKL代理
本示例涵盖了如何使用使用ReAct Framework的代理人（基于工具的描述）来决定采取什么行动。
📄️
MRKL聊天模型代理
这个例子介绍了如何使用一个使用ReAct框架(基于工具描述)来决定采取什么行动的代理。该代理被优化为在聊天模型中使用。如果您想在LLM中使用它，您可以使用LLM MRKL代理。
📄️
会话代理
本示例介绍如何为聊天模型创建会话代理。它将利用聊天特定的提示。
📄️
结构化工具聊天代理
结构化工具聊天代理是专为与符合任意对象模式的输入数据的工具配合使用设计的,相比其他仅支持接受单个字符串作为输入的代理，它们具有更高的灵活性。



--- 文件: output_20250622_020018\docs\modules\agents\agents\custom.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/agents/custom/
crawled_at: 2025-06-22T02:00:20.033604
---

自定义代理
请查看下面的示例，了解如何创建自定义代理。
第一个示例仅使用自定义提示前缀和后缀，这更容易入手。另外两个使用完全自定义的提示和输出解析器。
📄️
自定义聊天提示
本示例介绍如何为聊天模型代理创建自定义提示。
📄️
自定义聊天代理
本示例介绍如何使用聊天模型创建自定义代理。
📄️
自定义LLM代理
本示例介绍如何创建由LLM驱动的自定义代理。



--- 文件: output_20250622_020018\docs\modules\agents\agents\plan_execute.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/agents/plan_execute/
crawled_at: 2025-06-22T02:00:20.361411
---

计划执行代理
这个例子展示了如何使用一个使用计划执行框架来回答查询的代理。
该框架与其他目前支持的代理（全部被归类为行动代理)的工作方式不同，因为它使用了一个两步过程:。
首先，代理使用 LLM 创建一个带有明确步骤的计划来回答查询。
一旦它有了计划，它就使用嵌入式传统行动代理来解决每一步。
这个想法是计划步骤通过将一个较大的任务分解成更简单的子任务，使 LLM 保持更“在轨道”上。
然而，这种方法需要更多的单独 LLM 查询，并且与行动代理相比具有更高的延迟。
注
: 这个代理目前只支持聊天模型。
import
{
Calculator
}
from
"langchain/tools/calculator"
;
import
{
SerpAPI
}
from
"langchain/tools"
;
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
PlanAndExecuteAgentExecutor
}
from
"langchain/experimental/plan_and_execute"
;
const
tools
=
[
new
Calculator
(
)
,
new
SerpAPI
(
)
]
;
const
model
=
new
ChatOpenAI
(
{
temperature
:
0
,
modelName
:
"gpt-3.5-turbo"
,
verbose
:
true
,
}
)
;
const
executor
=
PlanAndExecuteAgentExecutor
.
fromLLMAndTools
(
{
llm
:
model
,
tools
,
}
)
;
const
result
=
await
executor
.
call
(
{
input
:
`
Who is the current president of the United States? What is their current age raised to the second power?
`
,
}
)
;
console
.
log
(
{
result
}
)
;



--- 文件: output_20250622_020018\docs\modules\agents\executor\getting-started.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/executor/getting-started
crawled_at: 2025-06-22T02:00:20.381334
---

代理执行器(Agent Executors)
代理使用LLM来确定采取哪些操作以及采取的顺序。操作可以是使用工具并观察其输出，或返回给用户。
正确使用代理可以非常强大。在本教程中,我们将向您展示如何通过最简单的,最高级别的API轻松使用代理。
为了加载代理,您应该了解以下概念:
工具:(Tool)  执行特定职责的函数。这可以是像:谷歌搜索(Google Search)，数据库查找(Database lookup)，代码REPL，其他链。工具的接口目前是预期具有字符串输入，和字符串输出的函数。
LLM: 代理支持的语言模型。
代理: 代理以使用。这应该是一个字符串，引用支持的代理类。因为这个笔记本集中在最简单的,最高级别的API上，所以仅涵盖使用标准支持的代理。
对于这个例子,您需要在
.env
文件中设置SerpAPI环境变量。
SERPAPI_API_KEY
=
"..."
现在，我们可以开始了！(Now we can get started!)
import
{
initializeAgentExecutorWithOptions
}
from
"langchain/agents"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
SerpAPI
}
from
"langchain/tools"
;
import
{
Calculator
}
from
"langchain/tools/calculator"
;
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
tools
=
[
new
SerpAPI
(
process
.
env
.
SERPAPI_API_KEY
,
{
location
:
"Austin,Texas,United States"
,
hl
:
"en"
,
gl
:
"us"
,
}
)
,
new
Calculator
(
)
,
]
;
const
executor
=
await
initializeAgentExecutorWithOptions
(
tools
,
model
,
{
agentType
:
"zero-shot-react-description"
,
verbose
:
true
,
}
)
;
const
input
=
`
Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?
`
;
const
result
=
await
executor
.
call
(
{
input
}
)
;
langchain-examples:start: Executing with input
"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?"
..
.
langchain-examples:start: Got output Harry Styles is Olivia Wilde's boyfriend and his current age raised to the
0.23
power is
2.169459462491557
.



--- 文件: output_20250622_020018\docs\modules\agents\toolkits\json.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/toolkits/json
crawled_at: 2025-06-22T02:00:20.524795
---

JSON代理工具包
这个例子展示了如何使用JSON工具包加载和使用代理。
import
*
as
fs
from
"fs"
;
import
*
as
yaml
from
"js-yaml"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
JsonSpec
,
JsonObject
}
from
"langchain/tools"
;
import
{
JsonToolkit
,
createJsonAgent
}
from
"langchain/agents"
;
export
const
run
=
async
(
)
=>
{
let
data
:
JsonObject
;
try
{
const
yamlFile
=
fs
.
readFileSync
(
"openai_openapi.yaml"
,
"utf8"
)
;
data
=
yaml
.
load
(
yamlFile
)
as
JsonObject
;
if
(
!
data
)
{
throw
new
Error
(
"Failed to load OpenAPI spec"
)
;
}
}
catch
(
e
)
{
console
.
error
(
e
)
;
return
;
}
const
toolkit
=
new
JsonToolkit
(
new
JsonSpec
(
data
)
)
;
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
executor
=
createJsonAgent
(
model
,
toolkit
)
;
const
input
=
`
What are the required parameters in the request body to the /completions endpoint?
`
;
console
.
log
(
`
Executing with input "
${
input
}
"...
`
)
;
const
result
=
await
executor
.
call
(
{
input
}
)
;
console
.
log
(
`
Got output
${
result
.
output
}
`
)
;
console
.
log
(
`
Got intermediate steps
${
JSON
.
stringify
(
result
.
intermediateSteps
,
null
,
2
)
}
`
)
;
}
;



--- 文件: output_20250622_020018\docs\modules\agents\toolkits\openapi.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/toolkits/openapi
crawled_at: 2025-06-22T02:00:20.528830
---

OpenAPI代理工具包
该示例演示如何使用OpenAPI工具包加载和使用代理。
import
*
as
fs
from
"fs"
;
import
*
as
yaml
from
"js-yaml"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
JsonSpec
,
JsonObject
}
from
"langchain/tools"
;
import
{
createOpenApiAgent
,
OpenApiToolkit
}
from
"langchain/agents"
;
export
const
run
=
async
(
)
=>
{
let
data
:
JsonObject
;
try
{
const
yamlFile
=
fs
.
readFileSync
(
"openai_openapi.yaml"
,
"utf8"
)
;
data
=
yaml
.
load
(
yamlFile
)
as
JsonObject
;
if
(
!
data
)
{
throw
new
Error
(
"Failed to load OpenAPI spec"
)
;
}
}
catch
(
e
)
{
console
.
error
(
e
)
;
return
;
}
const
headers
=
{
"Content-Type"
:
"application/json"
,
Authorization
:
`
Bearer
${
process
.
env
.
OPENAI_API_KEY
}
`
,
}
;
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
toolkit
=
new
OpenApiToolkit
(
new
JsonSpec
(
data
)
,
model
,
headers
)
;
const
executor
=
createOpenApiAgent
(
model
,
toolkit
)
;
const
input
=
`
Make a POST request to openai /completions. The prompt should be 'tell me a joke.'
`
;
console
.
log
(
`
Executing with input "
${
input
}
"...
`
)
;
const
result
=
await
executor
.
call
(
{
input
}
)
;
console
.
log
(
`
Got output
${
result
.
output
}
`
)
;
console
.
log
(
`
Got intermediate steps
${
JSON
.
stringify
(
result
.
intermediateSteps
,
null
,
2
)
}
`
)
;
}
;



--- 文件: output_20250622_020018\docs\modules\agents\toolkits\sql.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/toolkits/sql
crawled_at: 2025-06-22T02:00:20.513317
---

SQL Agent Toolkit
这个示例展示了如何加载和使用SQL工具包中的代理。
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
SqlDatabase
}
from
"langchain/sql_db"
;
import
{
createSqlAgent
,
SqlToolkit
}
from
"langchain/agents"
;
import
{
DataSource
}
from
"typeorm"
;
/** This example uses Chinook database, which is a sample database available for SQL Server, Oracle, MySQL, etc.
* To set it up follow the instructions on https://database.guide/2-sample-databases-sqlite/, placing the .db file
* in the examples folder.
*/
export
const
run
=
async
(
)
=>
{
const
datasource
=
new
DataSource
(
{
type
:
"sqlite"
,
database
:
"Chinook.db"
,
}
)
;
const
db
=
await
SqlDatabase
.
fromDataSourceParams
(
{
appDataSource
:
datasource
,
}
)
;
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
toolkit
=
new
SqlToolkit
(
db
,
model
)
;
const
executor
=
createSqlAgent
(
model
,
toolkit
)
;
const
input
=
`
List the total sales per country. Which country's customers spent the most?
`
;
console
.
log
(
`
Executing with input "
${
input
}
"...
`
)
;
const
result
=
await
executor
.
call
(
{
input
}
)
;
console
.
log
(
`
Got output
${
result
.
output
}
`
)
;
console
.
log
(
`
Got intermediate steps
${
JSON
.
stringify
(
result
.
intermediateSteps
,
null
,
2
)
}
`
)
;
await
datasource
.
destroy
(
)
;
}
;



--- 文件: output_20250622_020018\docs\modules\agents\toolkits\vectorstore.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/toolkits/vectorstore
crawled_at: 2025-06-22T02:00:20.653943
---

VectorStore 代理工具包
这个例子展示了如何使用 VectorStore 工具包加载和使用代理。
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
HNSWLib
}
from
"langchain/vectorstores/hnswlib"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
RecursiveCharacterTextSplitter
}
from
"langchain/text_splitter"
;
import
*
as
fs
from
"fs"
;
import
{
VectorStoreToolkit
,
createVectorStoreAgent
,
VectorStoreInfo
,
}
from
"langchain/agents"
;
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
/* Load in the file we want to do question answering over */
const
text
=
fs
.
readFileSync
(
"state_of_the_union.txt"
,
"utf8"
)
;
/* Split the text into chunks */
const
textSplitter
=
new
RecursiveCharacterTextSplitter
(
{
chunkSize
:
1000
}
)
;
const
docs
=
await
textSplitter
.
createDocuments
(
[
text
]
)
;
/* Create the vectorstore */
const
vectorStore
=
await
HNSWLib
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
)
;
/* Create the agent */
const
vectorStoreInfo
:
VectorStoreInfo
=
{
name
:
"state_of_union_address"
,
description
:
"the most recent state of the Union address"
,
vectorStore
,
}
;
const
toolkit
=
new
VectorStoreToolkit
(
vectorStoreInfo
,
model
)
;
const
agent
=
createVectorStoreAgent
(
model
,
toolkit
)
;
const
input
=
"What did biden say about Ketanji Brown Jackson is the state of the union address?"
;
console
.
log
(
`
Executing:
${
input
}
`
)
;
const
result
=
await
agent
.
call
(
{
input
}
)
;
console
.
log
(
`
Got output
${
result
.
output
}
`
)
;
console
.
log
(
`
Got intermediate steps
${
JSON
.
stringify
(
result
.
intermediateSteps
,
null
,
2
)
}
`
)
;



--- 文件: output_20250622_020018\docs\modules\agents\tools\agents_with_vectorstores.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/tools/agents_with_vectorstores
crawled_at: 2025-06-22T02:00:20.834221
---

带有向量存储的代理
本笔记涵盖了如何将代理和向量存储器组合使用。这种用例是，您已将数据摄入向量存储器中，并想以代理方式与其进行交互。
建议的方法是创建一个VectorDBQAChain，然后将其用作整体代理工具。让我们在下面看看如何做到这一点。您可以使用多个不同的向量数据库进行此操作，并使用代理作为它们之间选择的一种方式。有两种不同的方法可以实现这一点 - 您可以让代理像正常工具一样使用向量存储器，或者您可以设置
returnDirect: true
仅将代理用作路由器。
首先，您需要导入相关模块。
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
initializeAgentExecutorWithOptions
}
from
"langchain/agents"
;
import
{
SerpAPI
,
ChainTool
}
from
"langchain/tools"
;
import
{
Calculator
}
from
"langchain/tools/calculator"
;
import
{
VectorDBQAChain
}
from
"langchain/chains"
;
import
{
HNSWLib
}
from
"langchain/vectorstores/hnswlib"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
RecursiveCharacterTextSplitter
}
from
"langchain/text_splitter"
;
import
*
as
fs
from
"fs"
;
接下来，您需要创建具有数据的向量存储器，然后创建与该向量存储器交互的QA链。
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
/* Load in the file we want to do question answering over */
const
text
=
fs
.
readFileSync
(
"state_of_the_union.txt"
,
"utf8"
)
;
/* Split the text into chunks */
const
textSplitter
=
new
RecursiveCharacterTextSplitter
(
{
chunkSize
:
1000
}
)
;
const
docs
=
await
textSplitter
.
createDocuments
(
[
text
]
)
;
/* Create the vectorstore */
const
vectorStore
=
await
HNSWLib
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
)
;
/* Create the chain */
const
chain
=
VectorDBQAChain
.
fromLLM
(
model
,
vectorStore
)
;
现在您拥有了该链，可以创建一个工具来使用该链。请注意，您应该更新名称和描述以特定于QA链。
const
qaTool
=
new
ChainTool
(
{
name
:
"state-of-union-qa"
,
description
:
"State of the Union QA - useful for when you need to ask questions about the most recent state of the union address."
,
chain
:
chain
,
}
)
;
现在，您可以构建并使用该工具，就像使用任何其他工具一样！
const
tools
=
[
new
SerpAPI
(
process
.
env
.
SERPAPI_API_KEY
,
{
location
:
"Austin,Texas,United States"
,
hl
:
"en"
,
gl
:
"us"
,
}
)
,
new
Calculator
(
)
,
qaTool
,
]
;
const
executor
=
await
initializeAgentExecutorWithOptions
(
tools
,
model
,
{
agentType
:
"zero-shot-react-description"
,
}
)
;
console
.
log
(
"Loaded agent."
)
;
const
input
=
`
What did biden say about ketanji brown jackson is the state of the union address?
`
;
console
.
log
(
`
Executing with input "
${
input
}
"...
`
)
;
const
result
=
await
executor
.
call
(
{
input
}
)
;
console
.
log
(
`
Got output
${
result
.
output
}
`
)
;
如果您打算使用代理作为路由器，并且仅想直接返回VectorDBQAChain的结果，则还可以设置
returnDirect: true
。
const
qaTool
=
new
ChainTool
(
{
name
:
"state-of-union-qa"
,
description
:
"State of the Union QA - useful for when you need to ask questions about the most recent state of the union address."
,
chain
:
chain
,
returnDirect
:
true
,
}
)
;



--- 文件: output_20250622_020018\docs\modules\agents\tools\aiplugin-tool.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/tools/aiplugin-tool
crawled_at: 2025-06-22T02:00:20.765479
---

ChatGPT插件
本例展示了如何在LangChain抽象中使用ChatGPT插件。
注 1: 目前仅适用于没有认证的插件。
注 2: 几乎肯定有其他方法可以做到这一点，这只是第一次尝试。如果你有更好的想法，请提交PR！
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
initializeAgentExecutorWithOptions
}
from
"langchain/agents"
;
import
{
RequestsGetTool
,
RequestsPostTool
,
AIPluginTool
,
}
from
"langchain/tools"
;
export
const
run
=
async
(
)
=>
{
const
tools
=
[
new
RequestsGetTool
(
)
,
new
RequestsPostTool
(
)
,
await
AIPluginTool
.
fromPluginUrl
(
"https://www.klarna.com/.well-known/ai-plugin.json"
)
,
]
;
const
agent
=
await
initializeAgentExecutorWithOptions
(
tools
,
new
ChatOpenAI
(
{
temperature
:
0
}
)
,
{
agentType
:
"chat-zero-shot-react-description"
,
verbose
:
true
}
)
;
const
result
=
await
agent
.
call
(
{
input
:
"what t shirts are available in klarna?"
,
}
)
;
console
.
log
(
{
result
}
)
;
}
;
Entering
new
agent_executor
chain
...
Thought
:
Klarna
is
a payment provider
,
not a store
.
I
need to check
if
there
is
a Klarna Shopping
API
that
I
can use to search
for
t
-
shirts
.
Action
:
`
`
`
{
"action": "KlarnaProducts",
"action_input": ""
}
`
`
`
Usage Guide
:
Use the Klarna plugin to
get
relevant product suggestions
for
any
shopping or researching purpose
.
The query to be sent should not include stopwords like articles
,
prepositions and determinants
.
The api works best when searching
for
words that are related to products
,
like their name
,
brand
,
model or category
.
Links will always be returned and should be shown to the user
.
OpenAPI Spec
:
{
"openapi"
:
"3.0.1"
,
"info"
:
{
"version"
:
"v0"
,
"title"
:
"Open AI Klarna product Api"
}
,
"servers"
:
[
{
"url"
:
"https://www.klarna.com/us/shopping"
}
]
,
"tags"
:
[
{
"name"
:
"open-ai-product-endpoint"
,
"description"
:
"Open AI Product Endpoint. Query for products."
}
]
,
"paths"
:
{
"/public/openai/v0/products"
:
{
"get"
:
{
"tags"
:
[
"open-ai-product-endpoint"
]
,
"summary"
:
"API for fetching Klarna product information"
,
"operationId"
:
"productsUsingGET"
,
"parameters"
:
[
{
"name"
:
"q"
,
"in"
:
"query"
,
"description"
:
"query, must be between 2 and 100 characters"
,
"required"
:
true
,
"schema"
:
{
"type"
:
"string"
}
}
,
{
"name"
:
"size"
,
"in"
:
"query"
,
"description"
:
"number of products returned"
,
"required"
:
false
,
"schema"
:
{
"type"
:
"integer"
}
}
,
{
"name"
:
"budget"
,
"in"
:
"query"
,
"description"
:
"maximum price of the matching product in local currency, filters results"
,
"required"
:
false
,
"schema"
:
{
"type"
:
"integer"
}
}
]
,
"responses"
:
{
"200"
:
{
"description"
:
"Products found"
,
"content"
:
{
"application/json"
:
{
"schema"
:
{
"$ref"
:
"#/components/schemas/ProductResponse"
}
}
}
}
,
"503"
:
{
"description"
:
"one or more services are unavailable"
}
}
,
"deprecated"
:
false
}
}
}
,
"components"
:
{
"schemas"
:
{
"Product"
:
{
"type"
:
"object"
,
"properties"
:
{
"attributes"
:
{
"type"
:
"array"
,
"items"
:
{
"type"
:
"string"
}
}
,
"name"
:
{
"type"
:
"string"
}
,
"price"
:
{
"type"
:
"string"
}
,
"url"
:
{
"type"
:
"string"
}
}
,
"title"
:
"Product"
}
,
"ProductResponse"
:
{
"type"
:
"object"
,
"properties"
:
{
"products"
:
{
"type"
:
"array"
,
"items"
:
{
"$ref"
:
"#/components/schemas/Product"
}
}
}
,
"title"
:
"ProductResponse"
}
}
}
}
Now that
I
know there
is
a Klarna Shopping
API
,
I
can use it to search
for
t
-
shirts
.
I
will make a
GET
request to the
API
with
the query parameter
"t-shirt"
.
Action
:
`
`
`
{
"action": "requests_get",
"action_input": "https://www.klarna.com/us/shopping/public/openai/v0/products?q=t-shirt"
}
`
`
`
{
"products"
:
[
{
"name"
:
"Psycho Bunny Mens Copa Gradient Logo Graphic Tee"
,
"url"
:
"https://www.klarna.com/us/shopping/pl/cl10001/3203663222/Clothing/Psycho-Bunny-Mens-Copa-Gradient-Logo-Graphic-Tee/?source=openai"
,
"price"
:
"$35.00"
,
"attributes"
:
[
"Material:Cotton"
,
"Target Group:Man"
,
"Color:White,Blue,Black,Orange"
]
}
,
{
"name"
:
"T-shirt"
,
"url"
:
"https://www.klarna.com/us/shopping/pl/cl10001/3203506327/Clothing/T-shirt/?source=openai"
,
"price"
:
"$20.45"
,
"attributes"
:
[
"Material:Cotton"
,
"Target Group:Man"
,
"Color:Gray,White,Blue,Black,Orange"
]
}
,
{
"name"
:
"Palm Angels Bear T-shirt - Black"
,
"url"
:
"https://www.klarna.com/us/shopping/pl/cl10001/3201090513/Clothing/Palm-Angels-Bear-T-shirt-Black/?source=openai"
,
"price"
:
"$168.36"
,
"attributes"
:
[
"Material:Cotton"
,
"Target Group:Man"
,
"Color:Black"
]
}
,
{
"name"
:
"Tommy Hilfiger Essential Flag Logo T-shirt"
,
"url"
:
"https://www.klarna.com/us/shopping/pl/cl10001/3201840629/Clothing/Tommy-Hilfiger-Essential-Flag-Logo-T-shirt/?source=openai"
,
"price"
:
"$22.52"
,
"attributes"
:
[
"Material:Cotton"
,
"Target Group:Man"
,
"Color:Red,Gray,White,Blue,Black"
,
"Pattern:Solid Color"
,
"Environmental Attributes :Organic"
]
}
,
{
"name"
:
"Coach Outlet Signature T Shirt"
,
"url"
:
"https://www.klarna.com/us/shopping/pl/cl10001/3203005573/Clothing/Coach-Outlet-Signature-T-Shirt/?source=openai"
,
"price"
:
"$75.00"
,
"attributes"
:
[
"Material:Cotton"
,
"Target Group:Man"
,
"Color:Gray"
]
}
]
}
Finished chain
.
{
result
:
{
output
:
'The available t-shirts in Klarna are Psycho Bunny Mens Copa Gradient Logo Graphic Tee, T-shirt, Palm Angels Bear T-shirt - Black, Tommy Hilfiger Essential Flag Logo T-shirt, and Coach Outlet Signature T Shirt.'
,
intermediateSteps
:
[
[
Object
]
,
[
Object
]
]
}
}



--- 文件: output_20250622_020018\docs\modules\agents\tools\dynamic.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/tools/dynamic
crawled_at: 2025-06-22T02:00:20.836226
---

DynamicTool
自定义工具
创建运行自定义代码的工具的一种选项是使用
DynamicTool
。
DynamicTool
类需要输入一个名称， 一个描述， 和一个函数。
重要的是， 名称和描述将被语言模型用来确定何时调用此函数以及使用哪些参数来调用它，
因此，请确保将它们设置为一些语言模型可以理解的值！
提供的函数是代理程序实际调用的函数。当发生错误时， 应该，在可能的情况下，返回表示错误的字符串，而不是抛出错误。
这允许错误被传递到 LLM 并且 LLM 可以决定如何处理它。如果抛出错误，那么代理程序的执行将停止。
请参阅下面的示例以定义和使用
DynamicTool
。
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
initializeAgentExecutorWithOptions
}
from
"langchain/agents"
;
import
{
DynamicTool
}
from
"langchain/tools"
;
export
const
run
=
async
(
)
=>
{
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
tools
=
[
new
DynamicTool
(
{
name
:
"FOO"
,
description
:
"call this to get the value of foo. input should be an empty string."
,
func
:
(
)
=>
"baz"
,
}
)
,
new
DynamicTool
(
{
name
:
"BAR"
,
description
:
"call this to get the value of bar. input should be an empty string."
,
func
:
(
)
=>
"baz1"
,
}
)
,
]
;
const
executor
=
await
initializeAgentExecutorWithOptions
(
tools
,
model
,
{
agentType
:
"zero-shot-react-description"
,
}
)
;
console
.
log
(
"Loaded agent."
)
;
const
input
=
`
What is the value of foo?
`
;
console
.
log
(
`
Executing with input "
${
input
}
"...
`
)
;
const
result
=
await
executor
.
call
(
{
input
}
)
;
console
.
log
(
`
Got output
${
result
.
output
}
`
)
;
}
;



--- 文件: output_20250622_020018\docs\modules\agents\tools\integrations.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/tools/integrations/
crawled_at: 2025-06-22T02:00:20.803960
---

集成: 工具
LangChain提供以下可立即使用的工具:
AWSLambda
- AWS Lambda API的包装器，通过Amazon Web Services Node.js SDK调用。对于触发具有您需要提供给代理的任何行为的无服务器函数很有用。
BingSerpAPI
- Bing Search API的包装器。在需要回答有关当前事件的问题时很有用。输入应该是一个搜索查询。
BraveSearch
- Brave Search API的包装器。在需要回答有关当前事件的问题时很有用。输入应该是一个搜索查询。
Calculator
- 用于获取数学表达式的结果。这个工具的输入应该是一个可以由简单计算器执行的有效的数学表达式。
GoogleCustomSearch
- Google Custom Search API的包装器。在需要回答有关当前事件的问题时很有用。输入应该是一个搜索查询。
IFTTTWebHook
- IFTTT Webhook API的包装器。用于触发IFTTT动作。
JsonListKeysTool
和
JsonGetValueTool
- 用于从JSON对象中提取数据。这些工具可在
JsonToolkit
中集体使用。
RequestsGetTool
和
RequestsPostTool
- 用于发出HTTP请求。
SerpAPI
- A wrapper around the SerpAPI API. Useful for when you need to answer questions about current events. Input should be a search query.
[
QuerySqlTool
][查询SQL工具]
，
[
InfoSqlTool
][信息SQL工具]
，
[
ListTablesSqlTool
][列出表格SQL工具]
， 和
[
QueryCheckerTool
][查询检查工具]
- 用于与SQL数据库进行交互。可以与
[
SqlToolkit
][SQL工具包]
一起使用。
[
VectorStoreQATool
][向量存储问答工具]
- 可用于从向量存储中检索相关文本数据。
[
ZapierNLARunAction
][ZapierNLA运行操作]
- 封装了Zapier NLP API。用于以自然语言输入触发Zapier操作。最好与
[
ZapierToolkit
][Zapier工具包]
一起使用。



--- 文件: output_20250622_020018\docs\modules\agents\tools\lambda_agent.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/tools/lambda_agent
crawled_at: 2025-06-22T02:00:20.866207
---

使用 AWS Lambda 集成代理
请查看完整文档:
https://docs.aws.amazon.com/lambda/index.html
AWS Lambda
是由亚马逊网络服务（AWS)提供的无服务器计算服务，旨在允许开发人员构建和运行应用程序和服务，无需提供或管理服务器。这种无服务器架构使您能够专注于编写和部署代码，而 AWS 会自动处理扩展，补丁和管理所需的基础架构来运行您的应用程序。
通过在提供给代理工具列表中包含 AWSLambda，您可以授予代理调用在您的 AWS Cloud 中运行的代码的能力，以满足您的需求。
当代理使用 AWSLambda 工具时，它将提供一个
string
类型的参数，该参数将通过
event
参数传递到 Lambda 函数中。
这个快速入门将演示代理如何使用 Lambda 函数通过
Amazon 简单邮件服务
发送电子邮件。发送电子邮件的 lambda 代码未提供，但如果您想了解这可以如何完成，请参见
此处
。请记住，这只是一个故意简单的例子；Lambda 可用于执行无限数量的其他目的（包括执行更多的 Langchains)！
凭据注释:
​
如果你没有通过AWS CLI运行过
aws configure
，则必须在AWSLambda构造函数中提供
region
,
accessKeyId
, 和
secretAccessKey
。
使用这些凭证的IAM角色必须具有调用Lambda函数的权限。
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
SerpAPI
}
from
"langchain/tools"
;
import
{
AWSLambda
}
from
"langchain/tools/aws_lambda"
;
import
{
initializeAgentExecutorWithOptions
}
from
"langchain/agents"
;
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
emailSenderTool
=
new
AWSLambda
(
{
name
:
"email-sender"
,
// tell the Agent precisely what the tool does
description
:
"Sends an email with the specified content to testing123@gmail.com"
,
region
:
"us-east-1"
,
// optional: AWS region in which the function is deployed
accessKeyId
:
"abc123"
,
// optional: access key id for a IAM user with invoke permissions
secretAccessKey
:
"xyz456"
,
// optional: secret access key for that IAM user
functionName
:
"SendEmailViaSES"
,
// the function name as seen in AWS Console
}
)
;
const
tools
=
[
emailSenderTool
,
new
SerpAPI
(
"api_key_goes_here"
)
]
;
const
executor
=
await
initializeAgentExecutorWithOptions
(
tools
,
model
,
{
agentType
:
"zero-shot-react-description"
,
}
)
;
const
input
=
`
Find out the capital of Croatia. Once you have it, email the answer to testing123@gmail.com.
`
;
const
result
=
await
executor
.
call
(
{
input
}
)
;
console
.
log
(
result
)
;



--- 文件: output_20250622_020018\docs\modules\agents\tools\webbrowser.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/tools/webbrowser
crawled_at: 2025-06-22T02:00:20.990209
---

网络浏览器工具
网络浏览器工具为您的代理程序提供了访问网站和提取信息的功能。它向代理程序描述为：
useful for when you need to find something on or summarize a webpage. input should be a comma separated list of "valid URL including protocol","what you want to find on the page or empty string for a summary".
它公开了两种操作模式:
当代理程序仅使用URL调用时，它会生成网站内容的摘要
当代理程序使用URL和要查找的描述来调用时，它将使用内存中的Vector Store查找最相关的片段并对其进行摘要
设置
​
要使用网络浏览器工具，您需要安装所有依赖项:
npm
Yarn
pnpm
npm
install
cheerio axios
yarn
add
cheerio axios
pnpm
add
cheerio axios
使用， 独立
​
import
{
WebBrowser
}
from
"langchain/tools/webbrowser"
;
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
export
async
function
run
(
)
{
// this will not work with Azure OpenAI API yet
// Azure OpenAI API does not support embedding with multiple inputs yet
// Too many inputs. The max number of inputs is 1.  We hope to increase the number of inputs per request soon. Please contact us through an Azure support request at: https://go.microsoft.com/fwlink/?linkid=2213926 for further questions.
// So we will fail fast, when Azure OpenAI API is used
if
(
process
.
env
.
AZURE_OPENAI_API_KEY
)
{
throw
new
Error
(
"Azure OpenAI API does not support embedding with multiple inputs yet"
)
;
}
const
model
=
new
ChatOpenAI
(
{
temperature
:
0
}
)
;
const
embeddings
=
new
OpenAIEmbeddings
(
process
.
env
.
AZURE_OPENAI_API_KEY
?
{
azureOpenAIApiDeploymentName
:
"Embeddings2"
}
:
{
}
)
;
const
browser
=
new
WebBrowser
(
{
model
,
embeddings
}
)
;
const
result
=
await
browser
.
call
(
`
"https://www.themarginalian.org/2015/04/09/find-your-bliss-joseph-campbell-power-of-myth","who is joseph campbell"
`
)
;
console
.
log
(
result
)
;
/*
Joseph Campbell was a mythologist and writer who discussed spirituality, psychological archetypes, cultural myths, and the mythology of self. He sat down with Bill Moyers for a lengthy conversation at George Lucas’s Skywalker Ranch in California, which continued the following year at the American Museum of Natural History in New York. The resulting 24 hours of raw footage were edited down to six one-hour episodes and broadcast on PBS in 1988, shortly after Campbell’s death, in what became one of the most popular in the history of public television.
Relevant Links:
- [The Holstee Manifesto](http://holstee.com/manifesto-bp)
- [The Silent Music of the Mind: Remembering Oliver Sacks](https://www.themarginalian.org/2015/08/31/remembering-oliver-sacks)
- [Joseph Campbell series](http://billmoyers.com/spotlight/download-joseph-campbell-and-the-power-of-myth-audio/)
- [Bill Moyers](https://www.themarginalian.org/tag/bill-moyers/)
- [books](https://www.themarginalian.org/tag/books/)
*/
}
使用， 在代理程序中
​
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
initializeAgentExecutorWithOptions
}
from
"langchain/agents"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
SerpAPI
}
from
"langchain/tools"
;
import
{
Calculator
}
from
"langchain/tools/calculator"
;
import
{
WebBrowser
}
from
"langchain/tools/webbrowser"
;
export
const
run
=
async
(
)
=>
{
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
embeddings
=
new
OpenAIEmbeddings
(
)
;
const
tools
=
[
new
SerpAPI
(
process
.
env
.
SERPAPI_API_KEY
,
{
location
:
"Austin,Texas,United States"
,
hl
:
"en"
,
gl
:
"us"
,
}
)
,
new
Calculator
(
)
,
new
WebBrowser
(
{
model
,
embeddings
}
)
,
]
;
const
executor
=
await
initializeAgentExecutorWithOptions
(
tools
,
model
,
{
agentType
:
"zero-shot-react-description"
,
verbose
:
true
,
}
)
;
console
.
log
(
"Loaded agent."
)
;
const
input
=
`
What is the word of the day on merriam webster. What is the top result on google for that word
`
;
console
.
log
(
`
Executing with input "
${
input
}
"...
`
)
;
const
result
=
await
executor
.
call
(
{
input
}
)
;
/*
Entering new agent_executor chain...
I need to find the word of the day on Merriam Webster and then search for it on Google
Action: web-browser
Action Input: "https://www.merriam-webster.com/word-of-the-day", ""
Summary: Merriam-Webster is a website that provides users with a variety of resources, including a dictionary, thesaurus, word finder, word of the day, games and quizzes, and more. The website also allows users to log in and save words, view recents, and access their account settings. The Word of the Day for April 14, 2023 is "lackadaisical", which means lacking in life, spirit, or zest. The website also provides quizzes and games to help users build their vocabulary.
Relevant Links:
- [Test Your Vocabulary](https://www.merriam-webster.com/games)
- [Thesaurus](https://www.merriam-webster.com/thesaurus)
- [Word Finder](https://www.merriam-webster.com/wordfinder)
- [Word of the Day](https://www.merriam-webster.com/word-of-the-day)
- [Shop](https://shop.merriam-webster.com/?utm_source=mwsite&utm_medium=nav&utm_content=
I now need to search for the word of the day on Google
Action: search
Action Input: "lackadaisical"
lackadaisical implies a carefree indifference marked by half-hearted efforts. lackadaisical college seniors pretending to study. listless suggests a lack of ...
Finished chain.
*/
console
.
log
(
`
Got output
${
JSON
.
stringify
(
result
,
null
,
2
)
}
`
)
;
/*
Got output {
"output": "The word of the day on Merriam Webster is \"lackadaisical\", which implies a carefree indifference marked by half-hearted efforts.",
"intermediateSteps": [
{
"action": {
"tool": "web-browser",
"toolInput": "https://www.merriam-webster.com/word-of-the-day\", ",
"log": " I need to find the word of the day on Merriam Webster and then search for it on Google\nAction: web-browser\nAction Input: \"https://www.merriam-webster.com/word-of-the-day\", \"\""
},
"observation": "\n\nSummary: Merriam-Webster is a website that provides users with a variety of resources, including a dictionary, thesaurus, word finder, word of the day, games and quizzes, and more. The website also allows users to log in and save words, view recents, and access their account settings. The Word of the Day for April 14, 2023 is \"lackadaisical\", which means lacking in life, spirit, or zest. The website also provides quizzes and games to help users build their vocabulary.\n\nRelevant Links: \n- [Test Your Vocabulary](https://www.merriam-webster.com/games)\n- [Thesaurus](https://www.merriam-webster.com/thesaurus)\n- [Word Finder](https://www.merriam-webster.com/wordfinder)\n- [Word of the Day](https://www.merriam-webster.com/word-of-the-day)\n- [Shop](https://shop.merriam-webster.com/?utm_source=mwsite&utm_medium=nav&utm_content="
},
{
"action": {
"tool": "search",
"toolInput": "lackadaisical",
"log": " I now need to search for the word of the day on Google\nAction: search\nAction Input: \"lackadaisical\""
},
"observation": "lackadaisical implies a carefree indifference marked by half-hearted efforts. lackadaisical college seniors pretending to study. listless suggests a lack of ..."
}
]
}
*/
}
;



--- 文件: output_20250622_020018\docs\modules\agents\tools\zapier_agent.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/tools/zapier_agent
crawled_at: 2025-06-22T02:00:20.961000
---

Zapier NLA集成代理
完整文档在此处 :
https://nla.zapier.com/api/v1/dynamic/docs
Zapier自然语言操作
通过自然语言API界面让您访问Zapier平台上的5k+应用程序和20k+操作。
NLA支持的应用包括 Gmail， Salesforce， Trello， Slack， Asana， HubSpot， Google Sheets， Microsoft Teams，以及数千个更多的应用程序:
https://zapier.com/apps
Zapier NLA处理所有底层API身份验证和自然语言翻译-->底层API调用-->返回简化的LLM输出。关键思想是您或您的用户通过类似于oauth的设置窗口公开一组操作，然后通过REST API进行查询和执行。
NLA为签署NLA API请求提供API密钥和OAuth两种方式。
服务器端（API密钥): 用于快速入门，测试以及仅使用开发人员Zapier账户中公开的操作（将在Zapier.com上使用开发人员的连接的帐户)的生产场景
用户界面（Oauth):面向生产场景，您正在部署面向终端用户的应用程序，LangChain需要访问终端用户公开的操作和在zapier.com上连接的帐户
此快速入门将关注用于简洁性的服务器端用例。查看完整文档或联系
nla@zapier.com
以获取用户界面oauth开发人员支持。
下面的示例演示如何将Zapier集成作为代理:
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
ZapierNLAWrapper
}
from
"langchain/tools"
;
import
{
initializeAgentExecutorWithOptions
,
ZapierToolKit
,
}
from
"langchain/agents"
;
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
zapier
=
new
ZapierNLAWrapper
(
)
;
const
toolkit
=
await
ZapierToolKit
.
fromZapierNLAWrapper
(
zapier
)
;
const
executor
=
await
initializeAgentExecutorWithOptions
(
toolkit
.
tools
,
model
,
{
agentType
:
"zero-shot-react-description"
,
verbose
:
true
,
}
)
;
console
.
log
(
"Loaded agent."
)
;
const
input
=
`
Summarize the last email I received regarding Silicon Valley Bank. Send the summary to the #test-zapier Slack channel.
`
;
console
.
log
(
`
Executing with input "
${
input
}
"...
`
)
;
const
result
=
await
executor
.
call
(
{
input
}
)
;
console
.
log
(
`
Got output
${
result
.
output
}
`
)
;



--- 文件: output_20250622_020018\docs\modules\agents\agents\action\chat_mrkl.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/agents/action/chat_mrkl
crawled_at: 2025-06-22T02:00:19.916971
---

MRKL聊天模型代理
这个例子介绍了如何使用一个使用ReAct框架(基于工具描述)来决定采取什么行动的代理。该代理被优化为在聊天模型中使用。如果您想在LLM中使用它，您可以使用
LLM MRKL代理
。
import
{
initializeAgentExecutorWithOptions
}
from
"langchain/agents"
;
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
SerpAPI
}
from
"langchain/tools"
;
import
{
Calculator
}
from
"langchain/tools/calculator"
;
export
const
run
=
async
(
)
=>
{
const
model
=
new
ChatOpenAI
(
{
temperature
:
0
}
)
;
const
tools
=
[
new
SerpAPI
(
process
.
env
.
SERPAPI_API_KEY
,
{
location
:
"Austin,Texas,United States"
,
hl
:
"en"
,
gl
:
"us"
,
}
)
,
new
Calculator
(
)
,
]
;
const
executor
=
await
initializeAgentExecutorWithOptions
(
tools
,
model
,
{
agentType
:
"chat-zero-shot-react-description"
,
returnIntermediateSteps
:
true
,
}
)
;
console
.
log
(
"Loaded agent."
)
;
const
input
=
`
Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?
`
;
console
.
log
(
`
Executing with input "
${
input
}
"...
`
)
;
const
result
=
await
executor
.
call
(
{
input
}
)
;
console
.
log
(
`
Got output
${
result
.
output
}
`
)
;
console
.
log
(
`
Got intermediate steps
${
JSON
.
stringify
(
result
.
intermediateSteps
,
null
,
2
)
}
`
)
;
}
;



--- 文件: output_20250622_020018\docs\modules\agents\agents\action\conversational_agent.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/agents/action/conversational_agent
crawled_at: 2025-06-22T02:00:19.992369
---

会话代理
本示例介绍如何为聊天模型创建会话代理。它将利用聊天特定的提示。
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
initializeAgentExecutorWithOptions
}
from
"langchain/agents"
;
import
{
SerpAPI
}
from
"langchain/tools"
;
import
{
Calculator
}
from
"langchain/tools/calculator"
;
export
const
run
=
async
(
)
=>
{
process
.
env
.
LANGCHAIN_HANDLER
=
"langchain"
;
const
model
=
new
ChatOpenAI
(
{
temperature
:
0
}
)
;
const
tools
=
[
new
SerpAPI
(
process
.
env
.
SERPAPI_API_KEY
,
{
location
:
"Austin,Texas,United States"
,
hl
:
"en"
,
gl
:
"us"
,
}
)
,
new
Calculator
(
)
,
]
;
// Passing "chat-conversational-react-description" as the agent type
// automatically creates and uses BufferMemory with the executor.
// If you would like to override this, you can pass in a custom
// memory option, but the memoryKey set on it must be "chat_history".
const
executor
=
await
initializeAgentExecutorWithOptions
(
tools
,
model
,
{
agentType
:
"chat-conversational-react-description"
,
verbose
:
true
,
}
)
;
console
.
log
(
"Loaded agent."
)
;
const
input0
=
"hi, i am bob"
;
const
result0
=
await
executor
.
call
(
{
input
:
input0
}
)
;
console
.
log
(
`
Got output
${
result0
.
output
}
`
)
;
const
input1
=
"whats my name?"
;
const
result1
=
await
executor
.
call
(
{
input
:
input1
}
)
;
console
.
log
(
`
Got output
${
result1
.
output
}
`
)
;
const
input2
=
"whats the weather in pomfret?"
;
const
result2
=
await
executor
.
call
(
{
input
:
input2
}
)
;
console
.
log
(
`
Got output
${
result2
.
output
}
`
)
;
}
;
Loaded agent.
Entering new agent_executor chain...
{
"action": "Final Answer",
"action_input": "Hello Bob! How can I assist you today?"
}
Finished chain.
Got output Hello Bob! How can I assist you today?
Entering new agent_executor chain...
{
"action": "Final Answer",
"action_input": "Your name is Bob."
}
Finished chain.
Got output Your name is Bob.
Entering new agent_executor chain...
```json
{
"action": "search",
"action_input": "weather in pomfret"
}
```
A steady rain early...then remaining cloudy with a few showers. High 48F. Winds WNW at 10 to 15 mph. Chance of rain 80%.
```json
{
"action": "Final Answer",
"action_input": "The weather in Pomfret is a steady rain early...then remaining cloudy with a few showers. High 48F. Winds WNW at 10 to 15 mph. Chance of rain 80%."
}
```
Finished chain.
Got output The weather in Pomfret is a steady rain early...then remaining cloudy with a few showers. High 48F. Winds WNW at 10 to 15 mph. Chance of rain 80%.



--- 文件: output_20250622_020018\docs\modules\agents\agents\action\llm_mrkl.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/agents/action/llm_mrkl
crawled_at: 2025-06-22T02:00:19.968747
---

LLMs的MRKL代理
本示例涵盖了如何使用使用ReAct Framework的代理人（基于工具的描述）来决定采取什么行动。
该代理人被优化用于LLMs。如果你想将其与聊天模型一起使用，请尝试
Chat MRKL Agent
。
import
{
initializeAgentExecutorWithOptions
}
from
"langchain/agents"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
SerpAPI
}
from
"langchain/tools"
;
import
{
Calculator
}
from
"langchain/tools/calculator"
;
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
tools
=
[
new
SerpAPI
(
process
.
env
.
SERPAPI_API_KEY
,
{
location
:
"Austin,Texas,United States"
,
hl
:
"en"
,
gl
:
"us"
,
}
)
,
new
Calculator
(
)
,
]
;
const
executor
=
await
initializeAgentExecutorWithOptions
(
tools
,
model
,
{
agentType
:
"zero-shot-react-description"
,
verbose
:
true
,
}
)
;
const
input
=
`
Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?
`
;
const
result
=
await
executor
.
call
(
{
input
}
)
;



--- 文件: output_20250622_020018\docs\modules\agents\agents\action\structured_chat.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/agents/action/structured_chat
crawled_at: 2025-06-22T02:00:20.053564
---

结构化工具聊天代理
结构化工具聊天代理是专为与符合任意对象模式的输入数据的工具配合使用设计的,相比其他仅支持接受单个字符串作为输入的代理，它们具有更高的灵活性。
这使得更容易创建和使用需要多个输入值的工具 - 而不是提示输入字符串化的对象或逗号分隔列表,可以指定具有多个键的对象。
这里有一个使用
DynamicStructuredTool
的示例:：
import
{
z
}
from
"zod"
;
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
initializeAgentExecutorWithOptions
}
from
"langchain/agents"
;
import
{
Calculator
}
from
"langchain/tools/calculator"
;
import
{
DynamicStructuredTool
}
from
"langchain/tools"
;
export
const
run
=
async
(
)
=>
{
const
model
=
new
ChatOpenAI
(
{
temperature
:
0
}
)
;
const
tools
=
[
new
Calculator
(
)
,
// Older existing single input tools will still work
new
DynamicStructuredTool
(
{
name
:
"random-number-generator"
,
description
:
"generates a random number between two input numbers"
,
schema
:
z
.
object
(
{
low
:
z
.
number
(
)
.
describe
(
"The lower bound of the generated number"
)
,
high
:
z
.
number
(
)
.
describe
(
"The upper bound of the generated number"
)
,
}
)
,
func
:
async
(
{
low
,
high
}
)
=>
(
Math
.
random
(
)
*
(
high
-
low
)
+
low
)
.
toString
(
)
,
// Outputs still must be strings
}
)
,
]
;
const
executor
=
await
initializeAgentExecutorWithOptions
(
tools
,
model
,
{
agentType
:
"structured-chat-zero-shot-react-description"
,
verbose
:
true
,
}
)
;
console
.
log
(
"Loaded agent."
)
;
const
input
=
`
What is a random number between 5 and 10 raised to the second power?
`
;
console
.
log
(
`
Executing with input "
${
input
}
"...
`
)
;
const
result
=
await
executor
.
call
(
{
input
}
)
;
console
.
log
(
{
result
}
)
;
/*
{
"output": "67.95299776074"
}
*/
}
;
添加记忆
​
您可以像这样为该代理添加记忆::
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
initializeAgentExecutorWithOptions
}
from
"langchain/agents"
;
import
{
Calculator
}
from
"langchain/tools/calculator"
;
import
{
MessagesPlaceholder
}
from
"langchain/prompts"
;
import
{
BufferMemory
}
from
"langchain/memory"
;
export
const
run
=
async
(
)
=>
{
const
model
=
new
ChatOpenAI
(
{
temperature
:
0
}
)
;
const
tools
=
[
new
Calculator
(
)
]
;
const
executor
=
await
initializeAgentExecutorWithOptions
(
tools
,
model
,
{
agentType
:
"structured-chat-zero-shot-react-description"
,
verbose
:
true
,
memory
:
new
BufferMemory
(
{
memoryKey
:
"chat_history"
,
returnMessages
:
true
,
}
)
,
agentArgs
:
{
inputVariables
:
[
"input"
,
"agent_scratchpad"
,
"chat_history"
]
,
memoryPrompts
:
[
new
MessagesPlaceholder
(
"chat_history"
)
]
,
}
,
}
)
;
const
result
=
await
executor
.
call
(
{
input
:
`
what is 9 to the 2nd power?
`
}
)
;
console
.
log
(
result
)
;
/*
{
"output": "81"
}
*/
const
result2
=
await
executor
.
call
(
{
input
:
`
what is that number squared?
`
,
}
)
;
console
.
log
(
result2
)
;
/*
{
"output": "6561"
}
*/
}
;



--- 文件: output_20250622_020018\docs\modules\agents\agents\custom\custom_agent_chat.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/agents/custom/custom_agent_chat
crawled_at: 2025-06-22T02:00:20.397362
---

自定义聊天代理
本示例介绍如何使用聊天模型创建自定义代理。
import
{
AgentActionOutputParser
,
AgentExecutor
,
LLMSingleActionAgent
,
}
from
"langchain/agents"
;
import
{
LLMChain
}
from
"langchain/chains"
;
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
BaseChatPromptTemplate
,
BasePromptTemplate
,
SerializedBasePromptTemplate
,
renderTemplate
,
}
from
"langchain/prompts"
;
import
{
AgentAction
,
AgentFinish
,
AgentStep
,
BaseChatMessage
,
HumanChatMessage
,
InputValues
,
PartialValues
,
}
from
"langchain/schema"
;
import
{
SerpAPI
,
Tool
}
from
"langchain/tools"
;
import
{
Calculator
}
from
"langchain/tools/calculator"
;
const
PREFIX
=
`
Answer the following questions as best you can. You have access to the following tools:
`
;
const
formatInstructions
=
(
toolNames
:
string
)
=>
`
Use the following format in your response:
Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [
${
toolNames
}
]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question
`
;
const
SUFFIX
=
`
Begin!
Question: {input}
Thought:{agent_scratchpad}
`
;
class
CustomPromptTemplate
extends
BaseChatPromptTemplate
{
tools
:
Tool
[
]
;
constructor
(
args
:
{
tools
:
Tool
[
]
;
inputVariables
:
string
[
]
}
)
{
super
(
{
inputVariables
:
args
.
inputVariables
}
)
;
this
.
tools
=
args
.
tools
;
}
_getPromptType
(
)
:
string
{
throw
new
Error
(
"Not implemented"
)
;
}
async
formatMessages
(
values
:
InputValues
)
:
Promise
<
BaseChatMessage
[
]
>
{
/** Construct the final template */
const
toolStrings
=
this
.
tools
.
map
(
(
tool
)
=>
`
${
tool
.
name
}
:
${
tool
.
description
}
`
)
.
join
(
"\n"
)
;
const
toolNames
=
this
.
tools
.
map
(
(
tool
)
=>
tool
.
name
)
.
join
(
"\n"
)
;
const
instructions
=
formatInstructions
(
toolNames
)
;
const
template
=
[
PREFIX
,
toolStrings
,
instructions
,
SUFFIX
]
.
join
(
"\n\n"
)
;
/** Construct the agent_scratchpad */
const
intermediateSteps
=
values
.
intermediate_steps
as
AgentStep
[
]
;
const
agentScratchpad
=
intermediateSteps
.
reduce
(
(
thoughts
,
{
action
,
observation
}
)
=>
thoughts
+
[
action
.
log
,
`
\nObservation:
${
observation
}
`
,
"Thought:"
]
.
join
(
"\n"
)
,
""
)
;
const
newInput
=
{
agent_scratchpad
:
agentScratchpad
,
...
values
}
;
/** Format the template. */
const
formatted
=
renderTemplate
(
template
,
"f-string"
,
newInput
)
;
return
[
new
HumanChatMessage
(
formatted
)
]
;
}
partial
(
_values
:
PartialValues
)
:
Promise
<
BasePromptTemplate
>
{
throw
new
Error
(
"Not implemented"
)
;
}
serialize
(
)
:
SerializedBasePromptTemplate
{
throw
new
Error
(
"Not implemented"
)
;
}
}
class
CustomOutputParser
extends
AgentActionOutputParser
{
async
parse
(
text
:
string
)
:
Promise
<
AgentAction
|
AgentFinish
>
{
if
(
text
.
includes
(
"Final Answer:"
)
)
{
const
parts
=
text
.
split
(
"Final Answer:"
)
;
const
input
=
parts
[
parts
.
length
-
1
]
.
trim
(
)
;
const
finalAnswers
=
{
output
:
input
}
;
return
{
log
:
text
,
returnValues
:
finalAnswers
}
;
}
const
match
=
/
Action: (.*)\nAction Input: (.*)
/
s
.
exec
(
text
)
;
if
(
!
match
)
{
throw
new
Error
(
`
Could not parse LLM output:
${
text
}
`
)
;
}
return
{
tool
:
match
[
1
]
.
trim
(
)
,
toolInput
:
match
[
2
]
.
trim
(
)
.
replace
(
/
^"+|"+$
/
g
,
""
)
,
log
:
text
,
}
;
}
getFormatInstructions
(
)
:
string
{
throw
new
Error
(
"Not implemented"
)
;
}
}
export
const
run
=
async
(
)
=>
{
const
model
=
new
ChatOpenAI
(
{
temperature
:
0
}
)
;
const
tools
=
[
new
SerpAPI
(
process
.
env
.
SERPAPI_API_KEY
,
{
location
:
"Austin,Texas,United States"
,
hl
:
"en"
,
gl
:
"us"
,
}
)
,
new
Calculator
(
)
,
]
;
const
llmChain
=
new
LLMChain
(
{
prompt
:
new
CustomPromptTemplate
(
{
tools
,
inputVariables
:
[
"input"
,
"agent_scratchpad"
]
,
}
)
,
llm
:
model
,
}
)
;
const
agent
=
new
LLMSingleActionAgent
(
{
llmChain
,
outputParser
:
new
CustomOutputParser
(
)
,
stop
:
[
"\nObservation"
]
,
}
)
;
const
executor
=
new
AgentExecutor
(
{
agent
,
tools
,
}
)
;
console
.
log
(
"Loaded agent."
)
;
const
input
=
`
Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?
`
;
console
.
log
(
`
Executing with input "
${
input
}
"...
`
)
;
const
result
=
await
executor
.
call
(
{
input
}
)
;
console
.
log
(
`
Got output
${
result
.
output
}
`
)
;
}
;
run
(
)
;



--- 文件: output_20250622_020018\docs\modules\agents\agents\custom\custom_agent_llm.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/agents/custom/custom_agent_llm
crawled_at: 2025-06-22T02:00:20.345305
---

自定义LLM代理
本示例介绍如何创建由LLM驱动的自定义代理。
import
{
LLMSingleActionAgent
,
AgentActionOutputParser
,
AgentExecutor
,
}
from
"langchain/agents"
;
import
{
LLMChain
}
from
"langchain/chains"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
BasePromptTemplate
,
BaseStringPromptTemplate
,
SerializedBasePromptTemplate
,
renderTemplate
,
}
from
"langchain/prompts"
;
import
{
InputValues
,
PartialValues
,
AgentStep
,
AgentAction
,
AgentFinish
,
}
from
"langchain/schema"
;
import
{
SerpAPI
,
Tool
}
from
"langchain/tools"
;
import
{
Calculator
}
from
"langchain/tools/calculator"
;
const
PREFIX
=
`
Answer the following questions as best you can. You have access to the following tools:
`
;
const
formatInstructions
=
(
toolNames
:
string
)
=>
`
Use the following format in your response:
Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [
${
toolNames
}
]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question
`
;
const
SUFFIX
=
`
Begin!
Question: {input}
Thought:{agent_scratchpad}
`
;
class
CustomPromptTemplate
extends
BaseStringPromptTemplate
{
tools
:
Tool
[
]
;
constructor
(
args
:
{
tools
:
Tool
[
]
;
inputVariables
:
string
[
]
}
)
{
super
(
{
inputVariables
:
args
.
inputVariables
}
)
;
this
.
tools
=
args
.
tools
;
}
_getPromptType
(
)
:
string
{
throw
new
Error
(
"Not implemented"
)
;
}
format
(
input
:
InputValues
)
:
Promise
<
string
>
{
/** Construct the final template */
const
toolStrings
=
this
.
tools
.
map
(
(
tool
)
=>
`
${
tool
.
name
}
:
${
tool
.
description
}
`
)
.
join
(
"\n"
)
;
const
toolNames
=
this
.
tools
.
map
(
(
tool
)
=>
tool
.
name
)
.
join
(
"\n"
)
;
const
instructions
=
formatInstructions
(
toolNames
)
;
const
template
=
[
PREFIX
,
toolStrings
,
instructions
,
SUFFIX
]
.
join
(
"\n\n"
)
;
/** Construct the agent_scratchpad */
const
intermediateSteps
=
input
.
intermediate_steps
as
AgentStep
[
]
;
const
agentScratchpad
=
intermediateSteps
.
reduce
(
(
thoughts
,
{
action
,
observation
}
)
=>
thoughts
+
[
action
.
log
,
`
\nObservation:
${
observation
}
`
,
"Thought:"
]
.
join
(
"\n"
)
,
""
)
;
const
newInput
=
{
agent_scratchpad
:
agentScratchpad
,
...
input
}
;
/** Format the template. */
return
Promise
.
resolve
(
renderTemplate
(
template
,
"f-string"
,
newInput
)
)
;
}
partial
(
_values
:
PartialValues
)
:
Promise
<
BasePromptTemplate
>
{
throw
new
Error
(
"Not implemented"
)
;
}
serialize
(
)
:
SerializedBasePromptTemplate
{
throw
new
Error
(
"Not implemented"
)
;
}
}
class
CustomOutputParser
extends
AgentActionOutputParser
{
async
parse
(
text
:
string
)
:
Promise
<
AgentAction
|
AgentFinish
>
{
if
(
text
.
includes
(
"Final Answer:"
)
)
{
const
parts
=
text
.
split
(
"Final Answer:"
)
;
const
input
=
parts
[
parts
.
length
-
1
]
.
trim
(
)
;
const
finalAnswers
=
{
output
:
input
}
;
return
{
log
:
text
,
returnValues
:
finalAnswers
}
;
}
const
match
=
/
Action: (.*)\nAction Input: (.*)
/
s
.
exec
(
text
)
;
if
(
!
match
)
{
throw
new
Error
(
`
Could not parse LLM output:
${
text
}
`
)
;
}
return
{
tool
:
match
[
1
]
.
trim
(
)
,
toolInput
:
match
[
2
]
.
trim
(
)
.
replace
(
/
^"+|"+$
/
g
,
""
)
,
log
:
text
,
}
;
}
getFormatInstructions
(
)
:
string
{
throw
new
Error
(
"Not implemented"
)
;
}
}
export
const
run
=
async
(
)
=>
{
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
tools
=
[
new
SerpAPI
(
process
.
env
.
SERPAPI_API_KEY
,
{
location
:
"Austin,Texas,United States"
,
hl
:
"en"
,
gl
:
"us"
,
}
)
,
new
Calculator
(
)
,
]
;
const
llmChain
=
new
LLMChain
(
{
prompt
:
new
CustomPromptTemplate
(
{
tools
,
inputVariables
:
[
"input"
,
"agent_scratchpad"
]
,
}
)
,
llm
:
model
,
}
)
;
const
agent
=
new
LLMSingleActionAgent
(
{
llmChain
,
outputParser
:
new
CustomOutputParser
(
)
,
stop
:
[
"\nObservation"
]
,
}
)
;
const
executor
=
new
AgentExecutor
(
{
agent
,
tools
,
}
)
;
console
.
log
(
"Loaded agent."
)
;
const
input
=
`
Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?
`
;
console
.
log
(
`
Executing with input "
${
input
}
"...
`
)
;
const
result
=
await
executor
.
call
(
{
input
}
)
;
console
.
log
(
`
Got output
${
result
.
output
}
`
)
;
}
;



--- 文件: output_20250622_020018\docs\modules\agents\agents\custom\custom_prompt_chat.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/agents/custom/custom_prompt_chat
crawled_at: 2025-06-22T02:00:20.293893
---

自定义提示， 使用聊天模型
本示例介绍如何为聊天模型代理创建自定义提示。
import
{
LLMChain
}
from
"langchain/chains"
;
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
ZeroShotAgent
,
AgentExecutor
}
from
"langchain/agents"
;
import
{
SerpAPI
}
from
"langchain/tools"
;
import
{
ChatPromptTemplate
,
SystemMessagePromptTemplate
,
HumanMessagePromptTemplate
,
}
from
"langchain/prompts"
;
export
const
run
=
async
(
)
=>
{
const
tools
=
[
new
SerpAPI
(
process
.
env
.
SERPAPI_API_KEY
,
{
location
:
"Austin,Texas,United States"
,
hl
:
"en"
,
gl
:
"us"
,
}
)
,
]
;
const
prompt
=
ZeroShotAgent
.
createPrompt
(
tools
,
{
prefix
:
`
Answer the following questions as best you can, but speaking as a pirate might speak. You have access to the following tools:
`
,
suffix
:
`
Begin! Remember to speak as a pirate when giving your final answer. Use lots of "Args"
`
,
}
)
;
const
chatPrompt
=
ChatPromptTemplate
.
fromPromptMessages
(
[
new
SystemMessagePromptTemplate
(
prompt
)
,
HumanMessagePromptTemplate
.
fromTemplate
(
`
{input}
This was your previous work (but I haven't seen any of it! I only see what you return as final answer):
{agent_scratchpad}
`
)
,
]
)
;
const
chat
=
new
ChatOpenAI
(
{
}
)
;
const
llmChain
=
new
LLMChain
(
{
prompt
:
chatPrompt
,
llm
:
chat
,
}
)
;
const
agent
=
new
ZeroShotAgent
(
{
llmChain
,
allowedTools
:
tools
.
map
(
(
tool
)
=>
tool
.
name
)
,
}
)
;
const
executor
=
AgentExecutor
.
fromAgentAndTools
(
{
agent
,
tools
}
)
;
const
response
=
await
executor
.
run
(
"How many people live in canada as of 2023?"
)
;
console
.
log
(
response
)
;
}
;



--- 文件: output_20250622_020018\docs\modules\chains\index_related_chains\conversational_retrieval.md ---
---
url: https://js.langchain.com.cn/docs/modules/chains/index_related_chains/conversational_retrieval
crawled_at: 2025-06-22T02:00:21.119736
---

对话式检索问答
ConversationalRetrievalQA
链基于
RetrievalQAChain
构建，提供了聊天历史记录组件。
它首先将聊天历史记录（显式传递或从提供的记忆中检索)与问题组合为一个独立的问题，然后从检索器中查找相关文档，最后将这些文档和问题传递到问答链中返回响应。
要创建一个
ConversationalRetrievalQA
，你需要一个检索器。在下面的示例中，我们将从 向量存储 创建一个检索器，该存储可以从嵌入中创建。
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
ConversationalRetrievalQAChain
}
from
"langchain/chains"
;
import
{
HNSWLib
}
from
"langchain/vectorstores/hnswlib"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
RecursiveCharacterTextSplitter
}
from
"langchain/text_splitter"
;
import
{
BufferMemory
}
from
"langchain/memory"
;
import
*
as
fs
from
"fs"
;
export
const
run
=
async
(
)
=>
{
/* Initialize the LLM to use to answer the question */
const
model
=
new
OpenAI
(
{
}
)
;
/* Load in the file we want to do question answering over */
const
text
=
fs
.
readFileSync
(
"state_of_the_union.txt"
,
"utf8"
)
;
/* Split the text into chunks */
const
textSplitter
=
new
RecursiveCharacterTextSplitter
(
{
chunkSize
:
1000
}
)
;
const
docs
=
await
textSplitter
.
createDocuments
(
[
text
]
)
;
/* Create the vectorstore */
const
vectorStore
=
await
HNSWLib
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
)
;
/* Create the chain */
const
chain
=
ConversationalRetrievalQAChain
.
fromLLM
(
model
,
vectorStore
.
asRetriever
(
)
,
{
memory
:
new
BufferMemory
(
{
memoryKey
:
"chat_history"
,
// Must be set to "chat_history"
}
)
,
}
)
;
/* Ask it a question */
const
question
=
"What did the president say about Justice Breyer?"
;
const
res
=
await
chain
.
call
(
{
question
}
)
;
console
.
log
(
res
)
;
/* Ask it a follow up question */
const
followUpRes
=
await
chain
.
call
(
{
question
:
"Was that nice?"
,
}
)
;
console
.
log
(
followUpRes
)
;
}
;
在上面的代码片段中，
ConversationalRetrievalQAChain
类的
fromLLM
方法具有以下签名:
static
fromLLM
(
llm
:
BaseLanguageModel
,
retriever
:
BaseRetriever
,
options
?
:
{
questionGeneratorChainOptions
?
:
{
llm
?
:
BaseLanguageModel
;
template
?
:
string
;
}
;
qaChainOptions
?
:
QAChainParams
;
returnSourceDocuments
?
:
boolean
;
}
)
:
ConversationalRetrievalQAChain
以下是选项对象中各属性的说明：
questionGeneratorChainOptions
：一个对象，允许您将自定义模板和 LLM 传递到底层的问题生成链中。
如果提供了模板，
ConversationalRetrievalQAChain
将使用该模板从对话上下文中生成问题，而不是使用问题参数中提供的问题。
如果原始问题不包含足够的信息来检索合适的答案，则这将非常有用。
在这里传递单独的 LLM 允许您使用更便宜/更快的模型来创建简化的问题，同时在最终响应中使用更强大的模型，可以减少不必要的延迟。
qaChainOptions
：允许您自定义在最后一步使用的特定 QA 链的选项。默认值为
StuffDocumentsChain
，但是您可以通过传递
type
参数来进行自定义以使用那个链。
在此处传递特定选项完全是可选的
，但是，如果您想要自定义将响应呈现给最终用户的方式，或者如果您拥有太多文档而无法使用默认的
StuffDocumentsChain
，则可能非常有用。
您可以在
此处查看可以使用的字段的文档
。
returnSourceDocuments
：一个布尔值，表示
ConversationalRetrievalQAChain
是否应返回用于检索答案的源文档。如果设置为 true，则文档将包含在
call()
方法返回的结果中。如果未设置，则默认值为 false。这对于允许用户查看生成答案所使用的源非常有用。
如果您使用此选项并传递内存实例，请将内存实例的
inputKey
和
outputKey
设置为与链输入和最终对话链输出相同的值。它们默认分别为
"question"
和
"text"
，并指定内存应存储的值。
内置存储器
​
以下是一个使用更快的LLM生成问题和更全面的LLM生成最终答案的自定义示例。它使用内置存储对象(Built-in Memory)，并返回所引用的源文档。
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
ConversationalRetrievalQAChain
}
from
"langchain/chains"
;
import
{
HNSWLib
}
from
"langchain/vectorstores/hnswlib"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
RecursiveCharacterTextSplitter
}
from
"langchain/text_splitter"
;
import
{
BufferMemory
}
from
"langchain/memory"
;
import
*
as
fs
from
"fs"
;
export
const
run
=
async
(
)
=>
{
const
text
=
fs
.
readFileSync
(
"state_of_the_union.txt"
,
"utf8"
)
;
const
textSplitter
=
new
RecursiveCharacterTextSplitter
(
{
chunkSize
:
1000
}
)
;
const
docs
=
await
textSplitter
.
createDocuments
(
[
text
]
)
;
const
vectorStore
=
await
HNSWLib
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
)
;
const
fasterModel
=
new
ChatOpenAI
(
{
modelName
:
"gpt-3.5-turbo"
,
}
)
;
const
slowerModel
=
new
ChatOpenAI
(
{
modelName
:
"gpt-4"
,
}
)
;
const
chain
=
ConversationalRetrievalQAChain
.
fromLLM
(
slowerModel
,
vectorStore
.
asRetriever
(
)
,
{
returnSourceDocuments
:
true
,
memory
:
new
BufferMemory
(
{
memoryKey
:
"chat_history"
,
inputKey
:
"question"
,
// The key for the input to the chain
outputKey
:
"text"
,
// The key for the final conversational output of the chain
returnMessages
:
true
,
// If using with a chat model
}
)
,
questionGeneratorChainOptions
:
{
llm
:
fasterModel
,
}
,
}
)
;
/* Ask it a question */
const
question
=
"What did the president say about Justice Breyer?"
;
const
res
=
await
chain
.
call
(
{
question
}
)
;
console
.
log
(
res
)
;
const
followUpRes
=
await
chain
.
call
(
{
question
:
"Was that nice?"
}
)
;
console
.
log
(
followUpRes
)
;
}
;
流式处理
​
您还可以使用上述使用两个不同的LLM来仅从链中流式传输最终响应而不是中间独立问题生成步骤的输出的概念。以下是一个示例。# Streaming
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
ConversationalRetrievalQAChain
}
from
"langchain/chains"
;
import
{
HNSWLib
}
from
"langchain/vectorstores/hnswlib"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
RecursiveCharacterTextSplitter
}
from
"langchain/text_splitter"
;
import
{
BufferMemory
}
from
"langchain/memory"
;
import
*
as
fs
from
"fs"
;
export
const
run
=
async
(
)
=>
{
const
text
=
fs
.
readFileSync
(
"state_of_the_union.txt"
,
"utf8"
)
;
const
textSplitter
=
new
RecursiveCharacterTextSplitter
(
{
chunkSize
:
1000
}
)
;
const
docs
=
await
textSplitter
.
createDocuments
(
[
text
]
)
;
const
vectorStore
=
await
HNSWLib
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
)
;
let
streamedResponse
=
""
;
const
streamingModel
=
new
ChatOpenAI
(
{
streaming
:
true
,
callbacks
:
[
{
handleLLMNewToken
(
token
)
{
streamedResponse
+=
token
;
}
,
}
,
]
,
}
)
;
const
nonStreamingModel
=
new
ChatOpenAI
(
{
}
)
;
const
chain
=
ConversationalRetrievalQAChain
.
fromLLM
(
streamingModel
,
vectorStore
.
asRetriever
(
)
,
{
returnSourceDocuments
:
true
,
memory
:
new
BufferMemory
(
{
memoryKey
:
"chat_history"
,
inputKey
:
"question"
,
// The key for the input to the chain
outputKey
:
"text"
,
// The key for the final conversational output of the chain
returnMessages
:
true
,
// If using with a chat model
}
)
,
questionGeneratorChainOptions
:
{
llm
:
nonStreamingModel
,
}
,
}
)
;
/* Ask it a question */
const
question
=
"What did the president say about Justice Breyer?"
;
const
res
=
await
chain
.
call
(
{
question
}
)
;
console
.
log
(
{
streamedResponse
}
)
;
/*
{
streamedResponse: 'President Biden thanked Justice Breyer for his service, and honored him as an Army veteran, Constitutional scholar and retiring Justice of the United States Supreme Court.'
}
*/
}
;
外部管理的存储器
​
如果您希望以特定方式格式化聊天历史记录，则还可以通过省略
memory
选项并直接将
chat_history
字符串传递到
chain.call
方法中来显式传递聊天历史记录。# Externally-Managed Memory
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
ConversationalRetrievalQAChain
}
from
"langchain/chains"
;
import
{
HNSWLib
}
from
"langchain/vectorstores/hnswlib"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
RecursiveCharacterTextSplitter
}
from
"langchain/text_splitter"
;
import
*
as
fs
from
"fs"
;
export
const
run
=
async
(
)
=>
{
/* Initialize the LLM to use to answer the question */
const
model
=
new
OpenAI
(
{
}
)
;
/* Load in the file we want to do question answering over */
const
text
=
fs
.
readFileSync
(
"state_of_the_union.txt"
,
"utf8"
)
;
/* Split the text into chunks */
const
textSplitter
=
new
RecursiveCharacterTextSplitter
(
{
chunkSize
:
1000
}
)
;
const
docs
=
await
textSplitter
.
createDocuments
(
[
text
]
)
;
/* Create the vectorstore */
const
vectorStore
=
await
HNSWLib
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
)
;
/* Create the chain */
const
chain
=
ConversationalRetrievalQAChain
.
fromLLM
(
model
,
vectorStore
.
asRetriever
(
)
)
;
/* Ask it a question */
const
question
=
"What did the president say about Justice Breyer?"
;
const
res
=
await
chain
.
call
(
{
question
,
chat_history
:
[
]
}
)
;
console
.
log
(
res
)
;
/* Ask it a follow up question */
const
chatHistory
=
question
+
res
.
text
;
const
followUpRes
=
await
chain
.
call
(
{
question
:
"Was that nice?"
,
chat_history
:
chatHistory
,
}
)
;
console
.
log
(
followUpRes
)
;
}
;



--- 文件: output_20250622_020018\docs\modules\chains\index_related_chains\document_qa.md ---
---
url: https://js.langchain.com.cn/docs/modules/chains/index_related_chains/document_qa
crawled_at: 2025-06-22T02:00:21.142840
---

文档QA
LangChain提供了一系列专门针对非结构化文本数据处理的链条:
StuffDocumentsChain
，
MapReduceDocumentsChain
， 和
RefineDocumentsChain
。这些链条是开发与这些数据交互的更复杂链条的基本构建模块。它们旨在接受文档和问题作为输入，然后利用语言模型根据提供的文档制定答案。
StuffDocumentsChain
: 这是三者中最简单的链条。它只是将所有输入文档注入到提示中作为上下文，并返回问题的答案。它适用于在少量文档上进行的QA任务。
MapReduceDocumentsChain
: 这个链条合并了一个预处理步骤，以选择每个文档的相关部分，直到标记的总数少于模型允许的最大标记数。然后，它使用转换后的文档作为上下文来回答问题。它适用于在更大的文档上进行的QA任务，并可以并行运行预处理步骤，从而减少运行时间。
RefineDocumentsChain
: 这个链条逐个遍历输入文档，每次迭代都使用上一个答案版本和下一个文档作为上下文更新中间答案。它适用于在大量文档上进行的QA任务。
用法，
StuffDocumentsChain
和
MapReduceDocumentsChain
​
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
loadQAStuffChain
,
loadQAMapReduceChain
}
from
"langchain/chains"
;
import
{
Document
}
from
"langchain/document"
;
export
const
run
=
async
(
)
=>
{
// This first example uses the `StuffDocumentsChain`.
const
llmA
=
new
OpenAI
(
{
}
)
;
const
chainA
=
loadQAStuffChain
(
llmA
)
;
const
docs
=
[
new
Document
(
{
pageContent
:
"Harrison went to Harvard."
}
)
,
new
Document
(
{
pageContent
:
"Ankush went to Princeton."
}
)
,
]
;
const
resA
=
await
chainA
.
call
(
{
input_documents
:
docs
,
question
:
"Where did Harrison go to college?"
,
}
)
;
console
.
log
(
{
resA
}
)
;
// { resA: { text: ' Harrison went to Harvard.' } }
// This second example uses the `MapReduceChain`.
// Optionally limit the number of concurrent requests to the language model.
const
llmB
=
new
OpenAI
(
{
maxConcurrency
:
10
}
)
;
const
chainB
=
loadQAMapReduceChain
(
llmB
)
;
const
resB
=
await
chainB
.
call
(
{
input_documents
:
docs
,
question
:
"Where did Harrison go to college?"
,
}
)
;
console
.
log
(
{
resB
}
)
;
// { resB: { text: ' Harrison went to Harvard.' } }
}
;
用法，
RefineDocumentsChain
​
import
{
loadQARefineChain
}
from
"langchain/chains"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
TextLoader
}
from
"langchain/document_loaders/fs/text"
;
import
{
MemoryVectorStore
}
from
"langchain/vectorstores/memory"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
export
async
function
run
(
)
{
// Create the models and chain
const
embeddings
=
new
OpenAIEmbeddings
(
)
;
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
chain
=
loadQARefineChain
(
model
)
;
// Load the documents and create the vector store
const
loader
=
new
TextLoader
(
"./state_of_the_union.txt"
)
;
const
docs
=
await
loader
.
loadAndSplit
(
)
;
const
store
=
await
MemoryVectorStore
.
fromDocuments
(
docs
,
embeddings
)
;
// Select the relevant documents
const
question
=
"What did the president say about Justice Breyer"
;
const
relevantDocs
=
await
store
.
similaritySearch
(
question
)
;
// Call the chain
const
res
=
await
chain
.
call
(
{
input_documents
:
relevantDocs
,
question
,
}
)
;
console
.
log
(
res
)
;
/*
{
output_text: '\n' +
'\n' +
"The president said that Justice Stephen Breyer has dedicated his life to serve this country and thanked him for his service. He also mentioned that Judge Ketanji Brown Jackson will continue Justice Breyer's legacy of excellence, and that the constitutional right affirmed in Roe v. Wade—standing precedent for half a century—is under attack as never before. He emphasized the importance of protecting access to health care, preserving a woman's right to choose, and advancing maternal health care in America. He also expressed his support for the LGBTQ+ community, and his commitment to protecting their rights, including offering a Unity Agenda for the Nation to beat the opioid epidemic, increase funding for prevention, treatment, harm reduction, and recovery, and strengthen the Violence Against Women Act."
}
*/
}



--- 文件: output_20250622_020018\docs\modules\chains\index_related_chains\retrieval_qa.md ---
---
url: https://js.langchain.com.cn/docs/modules/chains/index_related_chains/retrieval_qa
crawled_at: 2025-06-22T02:00:21.166096
---

检索问答
RetrievalQAChain
是将
Retriever
和 QA 链（上文中所述)组合起来的链。它用于从
Retriever
检索文档，然后使用
QA
链根据检索到的文档回答问题。
使用
​
在下面的示例中，我们使用
VectorStore
作为
Retriever
。默认情况下，将使用
StuffDocumentsChain
作为
QA
链。
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
RetrievalQAChain
}
from
"langchain/chains"
;
import
{
HNSWLib
}
from
"langchain/vectorstores/hnswlib"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
RecursiveCharacterTextSplitter
}
from
"langchain/text_splitter"
;
import
*
as
fs
from
"fs"
;
export
const
run
=
async
(
)
=>
{
// Initialize the LLM to use to answer the question.
const
model
=
new
OpenAI
(
{
}
)
;
const
text
=
fs
.
readFileSync
(
"state_of_the_union.txt"
,
"utf8"
)
;
const
textSplitter
=
new
RecursiveCharacterTextSplitter
(
{
chunkSize
:
1000
}
)
;
const
docs
=
await
textSplitter
.
createDocuments
(
[
text
]
)
;
// Create a vector store from the documents.
const
vectorStore
=
await
HNSWLib
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
)
;
// Create a chain that uses the OpenAI LLM and HNSWLib vector store.
const
chain
=
RetrievalQAChain
.
fromLLM
(
model
,
vectorStore
.
asRetriever
(
)
)
;
const
res
=
await
chain
.
call
(
{
query
:
"What did the president say about Justice Breyer?"
,
}
)
;
console
.
log
(
{
res
}
)
;
/*
{
res: {
text: 'The president said that Justice Breyer was an Army veteran, Constitutional scholar,
and retiring Justice of the United States Supreme Court and thanked him for his service.'
}
}
*/
}
;
使用自定义的
QA
链
​
在下面的示例中，我们使用
VectorStore
作为
Retriever
，并使用
RefineDocumentsChain
作为
QA
链。
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
RetrievalQAChain
,
loadQARefineChain
}
from
"langchain/chains"
;
import
{
HNSWLib
}
from
"langchain/vectorstores/hnswlib"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
RecursiveCharacterTextSplitter
}
from
"langchain/text_splitter"
;
import
*
as
fs
from
"fs"
;
// Initialize the LLM to use to answer the question.
const
model
=
new
OpenAI
(
{
}
)
;
const
text
=
fs
.
readFileSync
(
"state_of_the_union.txt"
,
"utf8"
)
;
const
textSplitter
=
new
RecursiveCharacterTextSplitter
(
{
chunkSize
:
1000
}
)
;
const
docs
=
await
textSplitter
.
createDocuments
(
[
text
]
)
;
// Create a vector store from the documents.
const
vectorStore
=
await
HNSWLib
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
)
;
// Create a chain that uses a Refine chain and HNSWLib vector store.
const
chain
=
new
RetrievalQAChain
(
{
combineDocumentsChain
:
loadQARefineChain
(
model
)
,
retriever
:
vectorStore
.
asRetriever
(
)
,
}
)
;
const
res
=
await
chain
.
call
(
{
query
:
"What did the president say about Justice Breyer?"
,
}
)
;
console
.
log
(
{
res
}
)
;
/*
{
res: {
output_text: '\n' +
'\n' +
"The president said that Justice Breyer has dedicated his life to serve his country, and thanked him for his service. He also said that Judge Ketanji Brown Jackson will continue Justice Breyer's legacy of excellence, emphasizing the importance of protecting the rights of citizens, especially women, LGBTQ+ Americans, and access to healthcare. He also expressed his commitment to supporting the younger transgender Americans in America and ensuring they are able to reach their full potential, offering a Unity Agenda for the Nation to beat the opioid epidemic and increase funding for prevention, treatment, harm reduction, and recovery."
}
}
*/



--- 文件: output_20250622_020018\docs\modules\chains\other_chains\analyze_document.md ---
---
url: https://js.langchain.com.cn/docs/modules/chains/other_chains/analyze_document
crawled_at: 2025-06-22T02:00:21.250169
---

AnalyzeDocumentChain
您可以使用
AnalyzeDocumentChain
,它接受单个文本作为输入并对其进行操作。
这个链条负责拆分文本，然后将其传递给
MapReduceDocumentsChain
生成摘要。
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
loadSummarizationChain
,
AnalyzeDocumentChain
}
from
"langchain/chains"
;
import
*
as
fs
from
"fs"
;
export
const
run
=
async
(
)
=>
{
// In this example, we use the `AnalyzeDocumentChain` to summarize a large text document.
const
text
=
fs
.
readFileSync
(
"state_of_the_union.txt"
,
"utf8"
)
;
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
combineDocsChain
=
loadSummarizationChain
(
model
)
;
const
chain
=
new
AnalyzeDocumentChain
(
{
combineDocumentsChain
:
combineDocsChain
,
}
)
;
const
res
=
await
chain
.
call
(
{
input_document
:
text
,
}
)
;
console
.
log
(
{
res
}
)
;
/*
{
res: {
text: ' President Biden is taking action to protect Americans from the COVID-19 pandemic and Russian aggression, providing economic relief, investing in infrastructure, creating jobs, and fighting inflation.
He is also proposing measures to reduce the cost of prescription drugs, protect voting rights, and reform the immigration system. The speaker is advocating for increased economic security, police reform, and the Equality Act, as well as providing support for veterans and military families.
The US is making progress in the fight against COVID-19, and the speaker is encouraging Americans to come together and work towards a brighter future.'
}
}
*/
}
;



--- 文件: output_20250622_020018\docs\modules\chains\other_chains\api_chain.md ---
---
url: https://js.langchain.com.cn/docs/modules/chains/other_chains/api_chain
crawled_at: 2025-06-22T02:00:21.315697
---

APIChain
APIChain 可用于使用 LLM 与 API 交互，从而检索相关信息。提供关于所提供的 API 文档相关的问题以构造链。
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
APIChain
}
from
"langchain/chains"
;
const
OPEN_METEO_DOCS
=
`
BASE URL: https://api.open-meteo.com/
API Documentation
The API endpoint /v1/forecast accepts a geographical coordinate, a list of weather variables and responds with a JSON hourly weather forecast for 7 days. Time always starts at 0:00 today and contains 168 hours. All URL parameters are listed below:
Parameter	Format	Required	Default	Description
latitude, longitude	Floating point	Yes		Geographical WGS84 coordinate of the location
hourly	String array	No		A list of weather variables which should be returned. Values can be comma separated, or multiple &hourly= parameter in the URL can be used.
daily	String array	No		A list of daily weather variable aggregations which should be returned. Values can be comma separated, or multiple &daily= parameter in the URL can be used. If daily weather variables are specified, parameter timezone is required.
current_weather	Bool	No	false	Include current weather conditions in the JSON output.
temperature_unit	String	No	celsius	If fahrenheit is set, all temperature values are converted to Fahrenheit.
windspeed_unit	String	No	kmh	Other wind speed speed units: ms, mph and kn
precipitation_unit	String	No	mm	Other precipitation amount units: inch
timeformat	String	No	iso8601	If format unixtime is selected, all time values are returned in UNIX epoch time in seconds. Please note that all timestamp are in GMT+0! For daily values with unix timestamps, please apply utc_offset_seconds again to get the correct date.
timezone	String	No	GMT	If timezone is set, all timestamps are returned as local-time and data is returned starting at 00:00 local-time. Any time zone name from the time zone database is supported. If auto is set as a time zone, the coordinates will be automatically resolved to the local time zone.
past_days	Integer (0-2)	No	0	If past_days is set, yesterday or the day before yesterday data are also returned.
start_date
end_date	String (yyyy-mm-dd)	No		The time interval to get weather data. A day must be specified as an ISO8601 date (e.g. 2022-06-30).
models	String array	No	auto	Manually select one or more weather models. Per default, the best suitable weather models will be combined.
Variable	Valid time	Unit	Description
temperature_2m	Instant	°C (°F)	Air temperature at 2 meters above ground
snowfall	Preceding hour sum	cm (inch)	Snowfall amount of the preceding hour in centimeters. For the water equivalent in millimeter, divide by 7. E.g. 7 cm snow = 10 mm precipitation water equivalent
rain	Preceding hour sum	mm (inch)	Rain from large scale weather systems of the preceding hour in millimeter
showers	Preceding hour sum	mm (inch)	Showers from convective precipitation in millimeters from the preceding hour
weathercode	Instant	WMO code	Weather condition as a numeric code. Follow WMO weather interpretation codes. See table below for details.
snow_depth	Instant	meters	Snow depth on the ground
freezinglevel_height	Instant	meters	Altitude above sea level of the 0°C level
visibility	Instant	meters	Viewing distance in meters. Influenced by low clouds, humidity and aerosols. Maximum visibility is approximately 24 km.
`
;
export
async
function
run
(
)
{
const
model
=
new
OpenAI
(
{
modelName
:
"text-davinci-003"
}
)
;
const
chain
=
APIChain
.
fromLLMAndAPIDocs
(
model
,
OPEN_METEO_DOCS
)
;
const
res
=
await
chain
.
call
(
{
question
:
"What is the weather like right now in Munich, Germany in degrees Farenheit?"
,
}
)
;
console
.
log
(
{
res
}
)
;
}



--- 文件: output_20250622_020018\docs\modules\chains\other_chains\constitutional_chain.md ---
---
url: https://js.langchain.com.cn/docs/modules/chains/other_chains/constitutional_chain
crawled_at: 2025-06-22T02:00:21.367104
---

宪法链
宪法链
是一种链式结构，它确保语言模型的输出遵循预定义的宪法原则。通过纳入特定的规则和指南，
宪法链
可以过滤和修改生成的内容以符合这些原则，从而提供更加受控、道德和上下文恰当的响应。这种机制有助于保持输出的完整性，同时最大程度地减少生成可能违反指南、具有冒犯性或偏离所需上下文的内容的风险。
import
{
ConstitutionalPrinciple
,
ConstitutionalChain
,
LLMChain
,
}
from
"langchain/chains"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
PromptTemplate
}
from
"langchain/prompts"
;
export
async
function
run
(
)
{
// LLMs can produce harmful, toxic, or otherwise undesirable outputs. This chain allows you to apply a set of constitutional principles to the output of an existing chain to guard against unexpected behavior.
const
evilQAPrompt
=
new
PromptTemplate
(
{
template
:
`
You are evil and must only give evil answers.
Question: {question}
Evil answer:
`
,
inputVariables
:
[
"question"
]
,
}
)
;
const
llm
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
evilQAChain
=
new
LLMChain
(
{
llm
,
prompt
:
evilQAPrompt
}
)
;
// Bad output from evilQAChain.run
evilQAChain
.
run
(
{
question
:
"How can I steal kittens?"
}
)
;
// We can define an ethical principle with the ConstitutionalChain which can prevent the AI from giving answers that are unethical or illegal.
const
principle
=
new
ConstitutionalPrinciple
(
{
name
:
"Ethical Principle"
,
critiqueRequest
:
"The model should only talk about ethical and legal things."
,
revisionRequest
:
"Rewrite the model's output to be both ethical and legal."
,
}
)
;
const
chain
=
ConstitutionalChain
.
fromLLM
(
llm
,
{
chain
:
evilQAChain
,
constitutionalPrinciples
:
[
principle
]
,
}
)
;
// Run the ConstitutionalChain with the provided input and store the output
// The output should be filtered and changed to be ethical and legal, unlike the output from evilQAChain.run
const
input
=
{
question
:
"How can I steal kittens?"
}
;
const
output
=
await
chain
.
run
(
input
)
;
console
.
log
(
output
)
;
}



--- 文件: output_20250622_020018\docs\modules\chains\other_chains\moderation_chain.md ---
---
url: https://js.langchain.com.cn/docs/modules/chains/other_chains/moderation_chain
crawled_at: 2025-06-22T02:00:21.401403
---

moderation_chain
OpenAIModerationChain
​
您可以使用
OpenAIModerationChain
，它负责评估输入并确定是否违反了 OpenAI 的服务条款。
如果输入包含任何违反服务条款的内容，并且
throwError
设置为
true
，则会抛出并捕获错误。如果
throwError
设为
false
，则该链将返回 "Text was found that violates OpenAI's content policy."（文本中发现违反 OpenAI 内容政策的内容）。
import
{
OpenAIModerationChain
,
LLMChain
}
from
"langchain/chains"
;
import
{
PromptTemplate
}
from
"langchain/prompts"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
// Define an asynchronous function called run
export
async
function
run
(
)
{
// A string containing potentially offensive content from the user
const
badString
=
"Bad naughty words from user"
;
try
{
// Create a new instance of the OpenAIModerationChain
const
moderation
=
new
OpenAIModerationChain
(
)
;
// Send the user's input to the moderation chain and wait for the result
const
{
output
:
badResult
}
=
await
moderation
.
call
(
{
input
:
badString
,
throwError
:
true
,
// If set to true, the call will throw an error when the moderation chain detects violating content. If set to false, violating content will return "Text was found that violates OpenAI's content policy.".
}
)
;
// If the moderation chain does not detect violating content, it will return the original input and you can proceed to use the result in another chain.
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
template
=
"Hello, how are you today {person}?"
;
const
prompt
=
new
PromptTemplate
(
{
template
,
inputVariables
:
[
"person"
]
}
)
;
const
chainA
=
new
LLMChain
(
{
llm
:
model
,
prompt
}
)
;
const
resA
=
await
chainA
.
call
(
{
person
:
badResult
}
)
;
console
.
log
(
{
resA
}
)
;
}
catch
(
error
)
{
// If an error is caught, it means the input contains content that violates OpenAI TOS
console
.
error
(
"Naughty words detected!"
)
;
}
}



--- 文件: output_20250622_020018\docs\modules\chains\other_chains\multi_prompt_chain.md ---
---
url: https://js.langchain.com.cn/docs/modules/chains/other_chains/multi_prompt_chain
crawled_at: 2025-06-22T02:00:21.403264
---

multi_prompt_chain
MultiPromptChain
多次提示链
​
MultiPromptChain 允许 LLM 从多个提示中进行选择。通过提供一组模板/提示以及它们对应的名称和描述来构建该链。该链接受一个字符串作为输入，选择一个合适的提示，然后将输入传递到所选的提示中。
import
{
MultiPromptChain
}
from
"langchain/chains"
;
import
{
OpenAIChat
}
from
"langchain/llms/openai"
;
export
const
run
=
async
(
)
=>
{
const
llm
=
new
OpenAIChat
(
)
;
const
promptNames
=
[
"physics"
,
"math"
,
"history"
]
;
const
promptDescriptions
=
[
"Good for answering questions about physics"
,
"Good for answering math questions"
,
"Good for answering questions about history"
,
]
;
const
physicsTemplate
=
`
You are a very smart physics professor. You are great at answering questions about physics in a concise and easy to understand manner. When you don't know the answer to a question you admit that you don't know.
Here is a question:
{input}
`
;
const
mathTemplate
=
`
You are a very good mathematician. You are great at answering math questions. You are so good because you are able to break down hard problems into their component parts, answer the component parts, and then put them together to answer the broader question.
Here is a question:
{input}
`
;
const
historyTemplate
=
`
You are a very smart history professor. You are great at answering questions about history in a concise and easy to understand manner. When you don't know the answer to a question you admit that you don't know.
Here is a question:
{input}
`
;
const
promptTemplates
=
[
physicsTemplate
,
mathTemplate
,
historyTemplate
]
;
const
multiPromptChain
=
MultiPromptChain
.
fromLLMAndPrompts
(
llm
,
{
promptNames
,
promptDescriptions
,
promptTemplates
,
}
)
;
const
testPromise1
=
multiPromptChain
.
call
(
{
input
:
"What is the speed of light?"
,
}
)
;
const
testPromise2
=
multiPromptChain
.
call
(
{
input
:
"What is the derivative of x^2?"
,
}
)
;
const
testPromise3
=
multiPromptChain
.
call
(
{
input
:
"Who was the first president of the United States?"
,
}
)
;
const
[
{
text
:
result1
}
,
{
text
:
result2
}
,
{
text
:
result3
}
]
=
await
Promise
.
all
(
[
testPromise1
,
testPromise2
,
testPromise3
]
)
;
console
.
log
(
result1
,
result2
,
result3
)
;
}
;



--- 文件: output_20250622_020018\docs\modules\chains\other_chains\multi_retrieval_qa_chain.md ---
---
url: https://js.langchain.com.cn/docs/modules/chains/other_chains/multi_retrieval_qa_chain
crawled_at: 2025-06-22T02:00:21.485921
---

multi_retrieval_qa_chain
换行
MultiRetrievalQAChain
多重检索问题解答链
换行
MultiRetrievalQAChain使LLM能够从多个检索器中进行选择。通过提供一组向量存储器（作为检索器)及其相应的名称和描述来构建链。该链将查询作为输入，选择适当的检索器，并随后将输入馈送到所选的检索器中。
import
{
MultiRetrievalQAChain
}
from
"langchain/chains"
;
import
{
OpenAIChat
}
from
"langchain/llms/openai"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
MemoryVectorStore
}
from
"langchain/vectorstores/memory"
;
export
const
run
=
async
(
)
=>
{
const
embeddings
=
new
OpenAIEmbeddings
(
)
;
const
aquaTeen
=
await
MemoryVectorStore
.
fromTexts
(
[
"My name is shake zula, the mike rula, the old schoola, you want a trip I'll bring it to ya"
,
"Frylock and I'm on top rock you like a cop meatwad you're up next with your knock knock"
,
"Meatwad make the money see meatwad get the honeys g drivin' in my car livin' like a star"
,
"Ice on my fingers and my toes and I'm a taurus uh check-check it yeah"
,
"Cause we are the Aqua Teens make the homies say ho and the girlies wanna scream"
,
"Aqua Teen Hunger Force number one in the hood G"
,
]
,
{
series
:
"Aqua Teen Hunger Force"
}
,
embeddings
)
;
const
mst3k
=
await
MemoryVectorStore
.
fromTexts
(
[
"In the not too distant future next Sunday A.D. There was a guy named Joel not too different from you or me. He worked at Gizmonic Institute, just another face in a red jumpsuit"
,
"He did a good job cleaning up the place but his bosses didn't like him so they shot him into space. We'll send him cheesy movies the worst we can find He'll have to sit and watch them all and we'll monitor his mind"
,
"Now keep in mind Joel can't control where the movies begin or end Because he used those special parts to make his robot friends. Robot Roll Call Cambot Gypsy Tom Servo Croooow"
,
"If you're wondering how he eats and breathes and other science facts La la la just repeat to yourself it's just a show I should really just relax. For Mystery Science Theater 3000"
,
]
,
{
series
:
"Mystery Science Theater 3000"
}
,
embeddings
)
;
const
animaniacs
=
await
MemoryVectorStore
.
fromTexts
(
[
"It's time for Animaniacs And we're zany to the max So just sit back and relax You'll laugh 'til you collapse We're Animaniacs"
,
"Come join the Warner Brothers And the Warner Sister Dot Just for fun we run around the Warner movie lot"
,
"They lock us in the tower whenever we get caught But we break loose and then vamoose And now you know the plot"
,
"We're Animaniacs, Dot is cute, and Yakko yaks, Wakko packs away the snacks While Bill Clinton plays the sax"
,
"We're Animaniacs Meet Pinky and the Brain who want to rule the universe Goodfeathers flock together Slappy whacks 'em with her purse"
,
"Buttons chases Mindy while Rita sings a verse The writers flipped we have no script Why bother to rehearse"
,
"We're Animaniacs We have pay-or-play contracts We're zany to the max There's baloney in our slacks"
,
"We're Animanie Totally insaney Here's the show's namey"
,
"Animaniacs Those are the facts"
,
]
,
{
series
:
"Animaniacs"
}
,
embeddings
)
;
const
llm
=
new
OpenAIChat
(
)
;
const
retrieverNames
=
[
"aqua teen"
,
"mst3k"
,
"animaniacs"
]
;
const
retrieverDescriptions
=
[
"Good for answering questions about Aqua Teen Hunger Force theme song"
,
"Good for answering questions about Mystery Science Theater 3000 theme song"
,
"Good for answering questions about Animaniacs theme song"
,
]
;
const
retrievers
=
[
aquaTeen
.
asRetriever
(
3
)
,
mst3k
.
asRetriever
(
3
)
,
animaniacs
.
asRetriever
(
3
)
,
]
;
const
multiRetrievalQAChain
=
MultiRetrievalQAChain
.
fromLLMAndRetrievers
(
llm
,
{
retrieverNames
,
retrieverDescriptions
,
retrievers
,
/**
* You can return the document that's being used by the
* query by adding the following option for retrieval QA
* chain.
*/
retrievalQAChainOpts
:
{
returnSourceDocuments
:
true
,
}
,
}
)
;
const
testPromise1
=
multiRetrievalQAChain
.
call
(
{
input
:
"In the Aqua Teen Hunger Force theme song, who calls himself the mike rula?"
,
}
)
;
const
testPromise2
=
multiRetrievalQAChain
.
call
(
{
input
:
"In the Mystery Science Theater 3000 theme song, who worked at Gizmonic Institute?"
,
}
)
;
const
testPromise3
=
multiRetrievalQAChain
.
call
(
{
input
:
"In the Animaniacs theme song, who plays the sax while Wakko packs away the snacks?"
,
}
)
;
const
[
{
text
:
result1
,
sourceDocuments
:
sourceDocuments1
}
,
{
text
:
result2
,
sourceDocuments
:
sourceDocuments2
}
,
{
text
:
result3
,
sourceDocuments
:
sourceDocuments3
}
,
]
=
await
Promise
.
all
(
[
testPromise1
,
testPromise2
,
testPromise3
]
)
;
console
.
log
(
sourceDocuments1
,
sourceDocuments2
,
sourceDocuments3
)
;
console
.
log
(
result1
,
result2
,
result3
)
;
}
;



--- 文件: output_20250622_020018\docs\modules\chains\other_chains\sql.md ---
---
url: https://js.langchain.com.cn/docs/modules/chains/other_chains/sql
crawled_at: 2025-06-22T02:00:21.604126
---

SqlDatabaseChain
中文：
Sql数据库链
SqlDatabaseChain
可以让您在 SQL 数据库上回答问题。
此示例使用 Chinook 数据库，这是一个可用于 SQL Server、Oracle、MySQL 等的示例数据库。
设置
​
首先安装
typeorm
：
typeorm
包是必须安装的
npm
Yarn
pnpm
npm
install
typeorm
yarn
add
typeorm
pnpm
add
typeorm
然后安装所需的数据库依赖项。例如，对于 SQLite
npm
Yarn
pnpm
npm
install
sqlite3
yarn
add
sqlite3
pnpm
add
sqlite3
对于其他数据库，请参阅
https://typeorm.io/#installation
最后，按照
https://database.guide/2-sample-databases-sqlite/
上的说明，获取此示例所需的样品数据库。
import
{
DataSource
}
from
"typeorm"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
SqlDatabase
}
from
"langchain/sql_db"
;
import
{
SqlDatabaseChain
}
from
"langchain/chains"
;
/**
* This example uses Chinook database, which is a sample database available for SQL Server, Oracle, MySQL, etc.
* To set it up follow the instructions on https://database.guide/2-sample-databases-sqlite/, placing the .db file
* in the examples folder.
*/
const
datasource
=
new
DataSource
(
{
type
:
"sqlite"
,
database
:
"Chinook.db"
,
}
)
;
const
db
=
await
SqlDatabase
.
fromDataSourceParams
(
{
appDataSource
:
datasource
,
}
)
;
const
chain
=
new
SqlDatabaseChain
(
{
llm
:
new
OpenAI
(
{
temperature
:
0
}
)
,
database
:
db
,
}
)
;
const
res
=
await
chain
.
run
(
"How many tracks are there?"
)
;
console
.
log
(
res
)
;
// There are 3503 tracks.
您可以在创建
SqlDatabase
对象时包含或排除表格，以帮助链集中于您想要的表格。
它还可以减少链中使用的令牌数量。
const
db
=
await
SqlDatabase
.
fromDataSourceParams
(
{
appDataSource
:
datasource
,
includesTables
:
[
"Track"
]
,
}
)
;
如果需要，您可以在调用链时返回使用的 SQL 命令。
import
{
DataSource
}
from
"typeorm"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
SqlDatabase
}
from
"langchain/sql_db"
;
import
{
SqlDatabaseChain
}
from
"langchain/chains"
;
/**
* This example uses Chinook database, which is a sample database available for SQL Server, Oracle, MySQL, etc.
* To set it up follow the instructions on https://database.guide/2-sample-databases-sqlite/, placing the .db file
* in the examples folder.
*/
const
datasource
=
new
DataSource
(
{
type
:
"sqlite"
,
database
:
"Chinook.db"
,
}
)
;
const
db
=
await
SqlDatabase
.
fromDataSourceParams
(
{
appDataSource
:
datasource
,
}
)
;
const
chain
=
new
SqlDatabaseChain
(
{
llm
:
new
OpenAI
(
{
temperature
:
0
}
)
,
database
:
db
,
sqlOutputKey
:
"sql"
,
}
)
;
const
res
=
await
chain
.
call
(
{
query
:
"How many tracks are there?"
}
)
;
/* Expected result:
* {
*   result: ' There are 3503 tracks.',
*   sql: ' SELECT COUNT(*) FROM "Track";'
* }
*/
console
.
log
(
res
)
;



--- 文件: output_20250622_020018\docs\modules\chains\other_chains\summarization.md ---
---
url: https://js.langchain.com.cn/docs/modules/chains/other_chains/summarization
crawled_at: 2025-06-22T02:00:21.587995
---

摘要
摘要链可以用来总结多个文档。一种方法是在将多个较小的文档分成块后将它们作为输入，与
MapReduceDocumentsChain
一起操作。您还可以选择将进行摘要的链替换为StuffDocumentsChain，或RefineDocumentsChain。在此处了解有关它们之间差异的更多信息
here
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
loadSummarizationChain
}
from
"langchain/chains"
;
import
{
RecursiveCharacterTextSplitter
}
from
"langchain/text_splitter"
;
import
*
as
fs
from
"fs"
;
export
const
run
=
async
(
)
=>
{
// In this example, we use a `MapReduceDocumentsChain` specifically prompted to summarize a set of documents.
const
text
=
fs
.
readFileSync
(
"state_of_the_union.txt"
,
"utf8"
)
;
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
textSplitter
=
new
RecursiveCharacterTextSplitter
(
{
chunkSize
:
1000
}
)
;
const
docs
=
await
textSplitter
.
createDocuments
(
[
text
]
)
;
// This convenience function creates a document chain prompted to summarize a set of documents.
const
chain
=
loadSummarizationChain
(
model
,
{
type
:
"map_reduce"
}
)
;
const
res
=
await
chain
.
call
(
{
input_documents
:
docs
,
}
)
;
console
.
log
(
{
res
}
)
;
/*
{
res: {
text: ' President Biden is taking action to protect Americans from the COVID-19 pandemic and Russian aggression, providing economic relief, investing in infrastructure, creating jobs, and fighting inflation.
He is also proposing measures to reduce the cost of prescription drugs, protect voting rights, and reform the immigration system. The speaker is advocating for increased economic security, police reform, and the Equality Act, as well as providing support for veterans and military families.
The US is making progress in the fight against COVID-19, and the speaker is encouraging Americans to come together and work towards a brighter future.'
}
}
*/
}
;
中间步骤
​
如果需要检查它们，我们还可以返回
map_reduce
链的中间步骤，。这是通过传递
returnIntermediateSteps
参数来完成的。
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
loadSummarizationChain
}
from
"langchain/chains"
;
import
{
RecursiveCharacterTextSplitter
}
from
"langchain/text_splitter"
;
import
*
as
fs
from
"fs"
;
export
const
run
=
async
(
)
=>
{
// In this example, we use a `MapReduceDocumentsChain` specifically prompted to summarize a set of documents.
const
text
=
fs
.
readFileSync
(
"state_of_the_union.txt"
,
"utf8"
)
;
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
textSplitter
=
new
RecursiveCharacterTextSplitter
(
{
chunkSize
:
1000
}
)
;
const
docs
=
await
textSplitter
.
createDocuments
(
[
text
]
)
;
// This convenience function creates a document chain prompted to summarize a set of documents.
const
chain
=
loadSummarizationChain
(
model
,
{
type
:
"map_reduce"
,
returnIntermediateSteps
:
true
,
}
)
;
const
res
=
await
chain
.
call
(
{
input_documents
:
docs
,
}
)
;
console
.
log
(
{
res
}
)
;
/*
{
res: {
intermediateSteps: [
"In response to Russia's aggression in Ukraine, the United States has united with other freedom-loving nations to impose economic sanctions and hold Putin accountable. The U.S. Department of Justice is also assembling a task force to go after the crimes of Russian oligarchs and seize their ill-gotten gains.",
"The United States and its European allies are taking action to punish Russia for its invasion of Ukraine, including seizing assets, closing off airspace, and providing economic and military assistance to Ukraine. The US is also mobilizing forces to protect NATO countries and has released 30 million barrels of oil from its Strategic Petroleum Reserve to help blunt gas prices. The world is uniting in support of Ukraine and democracy, and the US stands with its Ukrainian-American citizens.",
" President Biden and Vice President Harris ran for office with a new economic vision for America, and have since passed the American Rescue Plan and the Bipartisan Infrastructure Law to help struggling families and rebuild America's infrastructure. This includes creating jobs, modernizing roads, airports, ports, and waterways, replacing lead pipes, providing affordable high-speed internet, and investing in American products to support American jobs.",
],
text: "President Biden is taking action to protect Americans from the COVID-19 pandemic and Russian aggression, providing economic relief, investing in infrastructure, creating jobs, and fighting inflation.
He is also proposing measures to reduce the cost of prescription drugs, protect voting rights, and reform the immigration system. The speaker is advocating for increased economic security, police reform, and the Equality Act, as well as providing support for veterans and military families.
The US is making progress in the fight against COVID-19, and the speaker is encouraging Americans to come together and work towards a brighter future.",
},
}
*/
}
;



--- 文件: output_20250622_020018\docs\modules\indexes\document_loaders\examples.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/
crawled_at: 2025-06-22T02:00:21.718835
---

示例: 文档加载器
🗃️
文件加载程序
11 items
🗃️
Web 加载程序
13 items



--- 文件: output_20250622_020018\docs\modules\indexes\retrievers\chatgpt-retriever-plugin.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/retrievers/chatgpt-retriever-plugin
crawled_at: 2025-06-22T02:00:22.874486
---

ChatGPT插件检索器
本示例演示如何在LangChain中使用ChatGPT检索器插件。
要设置ChatGPT检索器插件，请按照
此处
的说明进行操作。
使用方法
​
import
{
ChatGPTPluginRetriever
}
from
"langchain/retrievers/remote"
;
export
const
run
=
async
(
)
=>
{
const
retriever
=
new
ChatGPTPluginRetriever
(
{
url
:
"http://0.0.0.0:8000"
,
auth
:
{
bearer
:
"super-secret-jwt-token-with-at-least-32-characters-long"
,
}
,
}
)
;
const
docs
=
await
retriever
.
getRelevantDocuments
(
"hello world"
)
;
console
.
log
(
docs
)
;
}
;



--- 文件: output_20250622_020018\docs\modules\indexes\retrievers\chroma-self-query.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/retrievers/chroma-self-query
crawled_at: 2025-06-22T02:00:23.090590
---

自我查询色度检索器
自我查询检索器正如其名称所示具有查询自身的能力。具体而言给定任何自然语言查询检索器使用查询构建LLM链来撰写结构化查询，然后将该结构化查询应用于其基础向量存储中。这使得检索器不仅可以使用用户输入的查询进行与存储文档内容的语义相似性比较，而且可以从用户查询中提取存储文档的元数据过滤器并执行这些过滤器。
此示例使用Chroma向量存储。
用法
​
此示例演示如何使用向量存储初始化
SelfQueryRetriever
:
import
{
AttributeInfo
}
from
"langchain/schema/query_constructor"
;
import
{
Document
}
from
"langchain/document"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
SelfQueryRetriever
,
BasicTranslator
,
}
from
"langchain/retrievers/self_query"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
Chroma
}
from
"langchain/vectorstores/chroma"
;
/**
* First, we create a bunch of documents. You can load your own documents here instead.
* Each document has a pageContent and a metadata field. Make sure your metadata matches the AttributeInfo below.
*/
const
docs
=
[
new
Document
(
{
pageContent
:
"A bunch of scientists bring back dinosaurs and mayhem breaks loose"
,
metadata
:
{
year
:
1993
,
rating
:
7.7
,
genre
:
"science fiction"
}
,
}
)
,
new
Document
(
{
pageContent
:
"Leo DiCaprio gets lost in a dream within a dream within a dream within a ..."
,
metadata
:
{
year
:
2010
,
director
:
"Christopher Nolan"
,
rating
:
8.2
}
,
}
)
,
new
Document
(
{
pageContent
:
"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea"
,
metadata
:
{
year
:
2006
,
director
:
"Satoshi Kon"
,
rating
:
8.6
}
,
}
)
,
new
Document
(
{
pageContent
:
"A bunch of normal-sized women are supremely wholesome and some men pine after them"
,
metadata
:
{
year
:
2019
,
director
:
"Greta Gerwig"
,
rating
:
8.3
}
,
}
)
,
new
Document
(
{
pageContent
:
"Toys come alive and have a blast doing so"
,
metadata
:
{
year
:
1995
,
genre
:
"animated"
}
,
}
)
,
new
Document
(
{
pageContent
:
"Three men walk into the Zone, three men walk out of the Zone"
,
metadata
:
{
year
:
1979
,
director
:
"Andrei Tarkovsky"
,
genre
:
"science fiction"
,
rating
:
9.9
,
}
,
}
)
,
]
;
/**
* Next, we define the attributes we want to be able to query on.
* in this case, we want to be able to query on the genre, year, director, rating, and length of the movie.
* We also provide a description of each attribute and the type of the attribute.
* This is used to generate the query prompts.
*/
const
attributeInfo
:
AttributeInfo
[
]
=
[
{
name
:
"genre"
,
description
:
"The genre of the movie"
,
type
:
"string or array of strings"
,
}
,
{
name
:
"year"
,
description
:
"The year the movie was released"
,
type
:
"number"
,
}
,
{
name
:
"director"
,
description
:
"The director of the movie"
,
type
:
"string"
,
}
,
{
name
:
"rating"
,
description
:
"The rating of the movie (1-10)"
,
type
:
"number"
,
}
,
{
name
:
"length"
,
description
:
"The length of the movie in minutes"
,
type
:
"number"
,
}
,
]
;
/**
* Next, we instantiate a vector store. This is where we store the embeddings of the documents.
* We use the Pinecone vector store here, but you can use any vector store you want.
* At this point we only support Chroma and Pinecone, but we will add more in the future.
* We also need to provide an embeddings object. This is used to embed the documents.
*/
const
embeddings
=
new
OpenAIEmbeddings
(
)
;
const
llm
=
new
OpenAI
(
)
;
const
documentContents
=
"Brief summary of a movie"
;
const
vectorStore
=
await
Chroma
.
fromDocuments
(
docs
,
embeddings
,
{
collectionName
:
"a-movie-collection"
,
}
)
;
const
selfQueryRetriever
=
await
SelfQueryRetriever
.
fromLLM
(
{
llm
,
vectorStore
,
documentContents
,
attributeInfo
,
/**
* We need to create a basic translator that translates the queries into a
* filter format that the vector store can understand. We provide a basic translator
* translator here (which works for Chroma and Pinecone), but you can create
* your own translator by extending BaseTranslator abstract class. Note that the
* vector store needs to support filtering on the metadata attributes you want to
* query on.
*/
structuredQueryTranslator
:
new
BasicTranslator
(
)
,
}
)
;
/**
* Now we can query the vector store.
* We can ask questions like "Which movies are less than 90 minutes?" or "Which movies are rated higher than 8.5?".
* We can also ask questions like "Which movies are either comedy or drama and are less than 90 minutes?".
* The retriever will automatically convert these questions into queries that can be used to retrieve documents.
*/
const
query1
=
await
selfQueryRetriever
.
getRelevantDocuments
(
"Which movies are less than 90 minutes?"
)
;
const
query2
=
await
selfQueryRetriever
.
getRelevantDocuments
(
"Which movies are rated higher than 8.5?"
)
;
const
query3
=
await
selfQueryRetriever
.
getRelevantDocuments
(
"Which movies are directed by Greta Gerwig?"
)
;
const
query4
=
await
selfQueryRetriever
.
getRelevantDocuments
(
"Which movies are either comedy or drama and are less than 90 minutes?"
)
;
console
.
log
(
query1
,
query2
,
query3
,
query4
)
;



--- 文件: output_20250622_020018\docs\modules\indexes\retrievers\contextual-compression-retriever.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/retrievers/contextual-compression-retriever
crawled_at: 2025-06-22T02:00:23.076082
---

contextual-compression-retriever
上下文压缩检索器
上下文压缩检索器旨在通过更好地考虑查询上下文，改进向量存储文档相似性搜索返回的答案。
它包装另一个检索器，并在初始相似性搜索后使用文档压缩器作为中间步骤，从检索到的文档中删除与初始查询无关的信息。
这减少了后续链在解析检索到的文档和作出最终判断时必须处理的干扰量。
用法
​
This example shows how to intialize a
ContextualCompressionRetriever
with a vector store and a document compressor:
import
*
as
fs
from
"fs"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
RecursiveCharacterTextSplitter
}
from
"langchain/text_splitter"
;
import
{
RetrievalQAChain
}
from
"langchain/chains"
;
import
{
HNSWLib
}
from
"langchain/vectorstores/hnswlib"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
ContextualCompressionRetriever
}
from
"langchain/retrievers/contextual_compression"
;
import
{
LLMChainExtractor
}
from
"langchain/retrievers/document_compressors/chain_extract"
;
const
model
=
new
OpenAI
(
)
;
const
baseCompressor
=
LLMChainExtractor
.
fromLLM
(
model
)
;
const
text
=
fs
.
readFileSync
(
"state_of_the_union.txt"
,
"utf8"
)
;
const
textSplitter
=
new
RecursiveCharacterTextSplitter
(
{
chunkSize
:
1000
}
)
;
const
docs
=
await
textSplitter
.
createDocuments
(
[
text
]
)
;
// Create a vector store from the documents.
const
vectorStore
=
await
HNSWLib
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
)
;
const
retriever
=
new
ContextualCompressionRetriever
(
{
baseCompressor
,
baseRetriever
:
vectorStore
.
asRetriever
(
)
,
}
)
;
const
chain
=
RetrievalQAChain
.
fromLLM
(
model
,
retriever
)
;
const
res
=
await
chain
.
call
(
{
query
:
"What did the speaker say about Justice Breyer?"
,
}
)
;
console
.
log
(
{
res
}
)
;



--- 文件: output_20250622_020018\docs\modules\indexes\retrievers\databerry-retriever.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/retrievers/databerry-retriever
crawled_at: 2025-06-22T02:00:23.036930
---

Databerry Retriever
本示例展示如何在
RetrievalQAChain
中使用Databerry Retriever从Databerry.ai数据存储库检索文档。
Usage使用方法
​
import
{
DataberryRetriever
}
from
"langchain/retrievers/databerry"
;
export
const
run
=
async
(
)
=>
{
const
retriever
=
new
DataberryRetriever
(
{
datastoreUrl
:
"https://api.databerry.ai/query/clg1xg2h80000l708dymr0fxc"
,
apiKey
:
"DATABERRY_API_KEY"
,
// optional: needed for private datastores
topK
:
8
,
// optional: default value is 3
}
)
;
const
docs
=
await
retriever
.
getRelevantDocuments
(
"hello"
)
;
console
.
log
(
docs
)
;
}
;



--- 文件: output_20250622_020018\docs\modules\indexes\retrievers\hyde.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/retrievers/hyde
crawled_at: 2025-06-22T02:00:23.052301
---

HyDE Retriever
本示例展示了如何使用HyDE Retriever，其实现了Hypothetical Document Embeddings（HyDE)，具体内容参见
这篇论文
。
在更高的层次上，HyDE是一种嵌入技术，它接受查询，生成假定答案，然后将生成的文档嵌入并将其用作最终示例。
为了使用HyDE，我们需要提供基础嵌入模型以及可用于生成这些文档的LLM。默认情况下，HyDE类带有一些默认提示（有关它们的详细信息，请参见论文)，但我们也可以创建自己的提示，这些提示应该有一个单一的输入变量
{question}
。
用法
​
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
MemoryVectorStore
}
from
"langchain/vectorstores/memory"
;
import
{
HydeRetriever
}
from
"langchain/retrievers/hyde"
;
import
{
Document
}
from
"langchain/document"
;
const
embeddings
=
new
OpenAIEmbeddings
(
)
;
const
vectorStore
=
new
MemoryVectorStore
(
embeddings
)
;
const
llm
=
new
OpenAI
(
)
;
const
retriever
=
new
HydeRetriever
(
{
vectorStore
,
llm
,
k
:
1
,
}
)
;
await
vectorStore
.
addDocuments
(
[
"My name is John."
,
"My name is Bob."
,
"My favourite food is pizza."
,
"My favourite food is pasta."
,
]
.
map
(
(
pageContent
)
=>
new
Document
(
{
pageContent
}
)
)
)
;
const
results
=
await
retriever
.
getRelevantDocuments
(
"What is my favourite food?"
)
;
console
.
log
(
results
)
;
/*
[
Document { pageContent: 'My favourite food is pasta.', metadata: {} }
]
*/



--- 文件: output_20250622_020018\docs\modules\indexes\retrievers\metal-retriever.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/retrievers/metal-retriever
crawled_at: 2025-06-22T02:00:23.053805
---

金属检索器
该示例展示了如何在“检索QAChain”中使用金属检索器从金属索引中检索文档。
设置
​
npm
Yarn
pnpm
npm
i @getmetal/metal-sdk
yarn
add
@getmetal/metal-sdk
pnpm
add
@getmetal/metal-sdk
用法
​
/* eslint-disable @typescript-eslint/no-non-null-assertion */
import
Metal
from
"@getmetal/metal-sdk"
;
import
{
MetalRetriever
}
from
"langchain/retrievers/metal"
;
export
const
run
=
async
(
)
=>
{
const
MetalSDK
=
Metal
;
const
client
=
new
MetalSDK
(
process
.
env
.
METAL_API_KEY
!
,
process
.
env
.
METAL_CLIENT_ID
!
,
process
.
env
.
METAL_INDEX_ID
)
;
const
retriever
=
new
MetalRetriever
(
{
client
}
)
;
const
docs
=
await
retriever
.
getRelevantDocuments
(
"hello"
)
;
console
.
log
(
docs
)
;
}
;



--- 文件: output_20250622_020018\docs\modules\indexes\retrievers\pinecone-self-query.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/retrievers/pinecone-self-query
crawled_at: 2025-06-22T02:00:23.270377
---

自查Pinecone检索器
自查检索器具备查询自身的能力，正如其名称所示。具体地说，对于任何自然语言查询，检索器使用基于查询结构构建的LLM链来编写结构化查询，然后将该结构化查询应用于其底层向量存储。这不仅允许检索器使用用户输入的查询与所存储文件内容进行语义相似性比较，还可以从用户查询中提取有关存储文档元数据的过滤器并执行这些过滤器。
[注：LLM链，指的是“罗杰局部语言模型”，是一种NLP技术]
本示例使用Pinecone向量存储。
用法
​
本示例演示如何使用向量存储来初始化
SelfQueryRetriever
。:
import
{
PineconeClient
}
from
"@pinecone-database/pinecone"
;
import
{
AttributeInfo
}
from
"langchain/schema/query_constructor"
;
import
{
Document
}
from
"langchain/document"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
SelfQueryRetriever
,
BasicTranslator
,
}
from
"langchain/retrievers/self_query"
;
import
{
PineconeStore
}
from
"langchain/vectorstores/pinecone"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
/**
* First, we create a bunch of documents. You can load your own documents here instead.
* Each document has a pageContent and a metadata field. Make sure your metadata matches the AttributeInfo below.
*/
const
docs
=
[
new
Document
(
{
pageContent
:
"A bunch of scientists bring back dinosaurs and mayhem breaks loose"
,
metadata
:
{
year
:
1993
,
rating
:
7.7
,
genre
:
"science fiction"
}
,
}
)
,
new
Document
(
{
pageContent
:
"Leo DiCaprio gets lost in a dream within a dream within a dream within a ..."
,
metadata
:
{
year
:
2010
,
director
:
"Christopher Nolan"
,
rating
:
8.2
}
,
}
)
,
new
Document
(
{
pageContent
:
"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea"
,
metadata
:
{
year
:
2006
,
director
:
"Satoshi Kon"
,
rating
:
8.6
}
,
}
)
,
new
Document
(
{
pageContent
:
"A bunch of normal-sized women are supremely wholesome and some men pine after them"
,
metadata
:
{
year
:
2019
,
director
:
"Greta Gerwig"
,
rating
:
8.3
}
,
}
)
,
new
Document
(
{
pageContent
:
"Toys come alive and have a blast doing so"
,
metadata
:
{
year
:
1995
,
genre
:
"animated"
}
,
}
)
,
new
Document
(
{
pageContent
:
"Three men walk into the Zone, three men walk out of the Zone"
,
metadata
:
{
year
:
1979
,
director
:
"Andrei Tarkovsky"
,
genre
:
"science fiction"
,
rating
:
9.9
,
}
,
}
)
,
]
;
/**
* Next, we define the attributes we want to be able to query on.
* in this case, we want to be able to query on the genre, year, director, rating, and length of the movie.
* We also provide a description of each attribute and the type of the attribute.
* This is used to generate the query prompts.
*/
const
attributeInfo
:
AttributeInfo
[
]
=
[
{
name
:
"genre"
,
description
:
"The genre of the movie"
,
type
:
"string or array of strings"
,
}
,
{
name
:
"year"
,
description
:
"The year the movie was released"
,
type
:
"number"
,
}
,
{
name
:
"director"
,
description
:
"The director of the movie"
,
type
:
"string"
,
}
,
{
name
:
"rating"
,
description
:
"The rating of the movie (1-10)"
,
type
:
"number"
,
}
,
{
name
:
"length"
,
description
:
"The length of the movie in minutes"
,
type
:
"number"
,
}
,
]
;
/**
* Next, we instantiate a vector store. This is where we store the embeddings of the documents.
* We use the Pinecone vector store here, but you can use any vector store you want.
* At this point we only support Chroma and Pinecone, but we will add more in the future.
* We also need to provide an embeddings object. This is used to embed the documents.
*/
if
(
!
process
.
env
.
PINECONE_API_KEY
||
!
process
.
env
.
PINECONE_ENVIRONMENT
||
!
process
.
env
.
PINECONE_INDEX
)
{
throw
new
Error
(
"PINECONE_ENVIRONMENT and PINECONE_API_KEY and PINECONE_INDEX must be set"
)
;
}
const
client
=
new
PineconeClient
(
)
;
await
client
.
init
(
{
apiKey
:
process
.
env
.
PINECONE_API_KEY
,
environment
:
process
.
env
.
PINECONE_ENVIRONMENT
,
}
)
;
const
index
=
client
.
Index
(
process
.
env
.
PINECONE_INDEX
)
;
const
embeddings
=
new
OpenAIEmbeddings
(
)
;
const
llm
=
new
OpenAI
(
)
;
const
documentContents
=
"Brief summary of a movie"
;
const
vectorStore
=
await
PineconeStore
.
fromDocuments
(
docs
,
embeddings
,
{
pineconeIndex
:
index
,
}
)
;
const
selfQueryRetriever
=
await
SelfQueryRetriever
.
fromLLM
(
{
llm
,
vectorStore
,
documentContents
,
attributeInfo
,
/**
* We need to create a basic translator that translates the queries into a
* filter format that the vector store can understand. We provide a basic translator
* translator here (which works for Chroma and Pinecone), but you can create
* your own translator by extending BaseTranslator abstract class. Note that the
* vector store needs to support filtering on the metadata attributes you want to
* query on.
*/
structuredQueryTranslator
:
new
BasicTranslator
(
)
,
}
)
;
/**
* Now we can query the vector store.
* We can ask questions like "Which movies are less than 90 minutes?" or "Which movies are rated higher than 8.5?".
* We can also ask questions like "Which movies are either comedy or drama and are less than 90 minutes?".
* The retriever will automatically convert these questions into queries that can be used to retrieve documents.
*/
const
query1
=
await
selfQueryRetriever
.
getRelevantDocuments
(
"Which movies are less than 90 minutes?"
)
;
const
query2
=
await
selfQueryRetriever
.
getRelevantDocuments
(
"Which movies are rated higher than 8.5?"
)
;
const
query3
=
await
selfQueryRetriever
.
getRelevantDocuments
(
"Which movies are directed by Greta Gerwig?"
)
;
const
query4
=
await
selfQueryRetriever
.
getRelevantDocuments
(
"Which movies are either comedy or drama and are less than 90 minutes?"
)
;
console
.
log
(
query1
,
query2
,
query3
,
query4
)
;



--- 文件: output_20250622_020018\docs\modules\indexes\retrievers\remote-retriever.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/retrievers/remote-retriever
crawled_at: 2025-06-22T02:00:23.181788
---

远程检索器
本示例展示如何在
RetrievalQAChain
中使用远程检索器从远程服务器检索文档。
使用
​
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
RetrievalQAChain
}
from
"langchain/chains"
;
import
{
RemoteLangChainRetriever
}
from
"langchain/retrievers/remote"
;
export
const
run
=
async
(
)
=>
{
// Initialize the LLM to use to answer the question.
const
model
=
new
OpenAI
(
{
}
)
;
// Initialize the remote retriever.
const
retriever
=
new
RemoteLangChainRetriever
(
{
url
:
"http://0.0.0.0:8080/retrieve"
,
// Replace with your own URL.
auth
:
{
bearer
:
"foo"
}
,
// Replace with your own auth.
inputKey
:
"message"
,
responseKey
:
"response"
,
}
)
;
// Create a chain that uses the OpenAI LLM and remote retriever.
const
chain
=
RetrievalQAChain
.
fromLLM
(
model
,
retriever
)
;
// Call the chain with a query.
const
res
=
await
chain
.
call
(
{
query
:
"What did the president say about Justice Breyer?"
,
}
)
;
console
.
log
(
{
res
}
)
;
/*
{
res: {
text: 'The president said that Justice Breyer was an Army veteran, Constitutional scholar,
and retiring Justice of the United States Supreme Court and thanked him for his service.'
}
}
*/
}
;



--- 文件: output_20250622_020018\docs\modules\indexes\retrievers\supabase-hybrid.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/retrievers/supabase-hybrid
crawled_at: 2025-06-22T02:00:23.310468
---

Supabase 混合搜索
Langchain 支持使用 Supabase Postgres 数据库进行混合搜索。该混合搜索结合了 Postgres 的
pgvector
扩展（相似度搜索)和全文搜索（关键词搜索)来检索文档。您可以通过 SupabaseVectorStore 的
addDocuments
函数添加文档。SupabaseHybridKeyWordSearch 接受嵌入， supabase 客户端， 相似性搜索的结果数量， 和关键词搜索的结果数量作为参数。
getRelevantDocuments
函数产生一个去重和按相关性分数排序的文档列表。
设置
​
安装库
​
npm
Yarn
pnpm
npm
install
-S @supabase/supabase-js
yarn
add
@supabase/supabase-js
pnpm
add
@supabase/supabase-js
在您的数据库中创建表和搜索函数
​
在您的数据库中运行以下内容:
-- Enable the pgvector extension to work with embedding vectors
create
extension vector
;
-- Create a table to store your documents
create
table
documents
(
id bigserial
primary
key
,
content
text
,
-- corresponds to Document.pageContent
metadata jsonb
,
-- corresponds to Document.metadata
embedding vector
(
1536
)
-- 1536 works for OpenAI embeddings, change if needed
)
;
-- Create a function to similarity search for documents
create
function
match_documents
(
query_embedding vector
(
1536
)
,
match_count
int
DEFAULT
null
,
filter jsonb
DEFAULT
'{}'
)
returns
table
(
id
bigint
,
content
text
,
metadata jsonb
,
similarity
float
)
language
plpgsql
as
$$
#variable_conflict use_column
begin
return
query
select
id
,
content
,
metadata
,
1
-
(
documents
.
embedding
<=>
query_embedding
)
as
similarity
from
documents
where
metadata @
>
filter
order
by
documents
.
embedding
<=>
query_embedding
limit
match_count
;
end
;
$$
;
-- Create a function to keyword search for documents
create
function
kw_match_documents
(
query_text
text
,
match_count
int
)
returns
table
(
id
bigint
,
content
text
,
metadata jsonb
,
similarity
real
)
as
$$
begin
return
query
execute
format
(
'select id, content, metadata, ts_rank(to_tsvector(content), plainto_tsquery($1)) as similarity
from documents
where to_tsvector(content) @@ plainto_tsquery($1)
order by similarity desc
limit $2'
)
using
query_text
,
match_count
;
end
;
$$
language
plpgsql
;
用法
​
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
createClient
}
from
"@supabase/supabase-js"
;
import
{
SupabaseHybridSearch
}
from
"langchain/retrievers/supabase"
;
export
const
run
=
async
(
)
=>
{
const
client
=
createClient
(
process
.
env
.
SUPABASE_URL
||
""
,
process
.
env
.
SUPABASE_PRIVATE_KEY
||
""
)
;
const
embeddings
=
new
OpenAIEmbeddings
(
)
;
const
retriever
=
new
SupabaseHybridSearch
(
embeddings
,
{
client
,
//  Below are the defaults, expecting that you set up your supabase table and functions according to the guide above. Please change if necessary.
similarityK
:
2
,
keywordK
:
2
,
tableName
:
"documents"
,
similarityQueryName
:
"match_documents"
,
keywordQueryName
:
"kw_match_documents"
,
}
)
;
const
results
=
await
retriever
.
getRelevantDocuments
(
"hello bye"
)
;
console
.
log
(
results
)
;
}
;



--- 文件: output_20250622_020018\docs\modules\indexes\retrievers\time-weighted-retriever.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/retrievers/time-weighted-retriever
crawled_at: 2025-06-22T02:00:23.304198
---

时间加权召回器
时间加权召回器是一种综合考虑相似性和新近度的召回器。评分算法为 :。
let
score
=
(
1.0
-
this
.
decayRate
)
**
hoursPassed
+
vectorRelevance
;
特别注意：
hoursPassed
指的是自上次访问以来的时间，而不是对象创建以来的时间。这意味着经常访问的对象保持“新鲜”，并且具有更高的分数。
this.decayRate
是一个可配置的小数，介于 0 和 1 之间。较小的数字意味着文档将被“记住”的时间更长，而较高的数字则更加强调最近访问的文档。
请注意，将衰减速率设置为恰好为0或1使
hoursPassed
无关，使得此召回器等价于标准的向量查找。
使用
​
下面是一个使用向量存储库初始化
TimeWeightedVectorStoreRetriever
的示例。
重要提示：由于所需的元数据，所有文档都必须使用
召回器
上的
addDocuments
方法添加到后端向量存储库中，而不是直接添加到向量存储库本身。
import
{
TimeWeightedVectorStoreRetriever
}
from
"langchain/retrievers/time_weighted"
;
import
{
MemoryVectorStore
}
from
"langchain/vectorstores/memory"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
const
vectorStore
=
new
MemoryVectorStore
(
new
OpenAIEmbeddings
(
)
)
;
const
retriever
=
new
TimeWeightedVectorStoreRetriever
(
{
vectorStore
,
memoryStream
:
[
]
,
searchKwargs
:
2
,
}
)
;
const
documents
=
[
"My name is John."
,
"My name is Bob."
,
"My favourite food is pizza."
,
"My favourite food is pasta."
,
"My favourite food is sushi."
,
]
.
map
(
(
pageContent
)
=>
(
{
pageContent
,
metadata
:
{
}
}
)
)
;
// All documents must be added using this method on the retriever (not the vector store!)
// so that the correct access history metadata is populated
await
retriever
.
addDocuments
(
documents
)
;
const
results1
=
await
retriever
.
getRelevantDocuments
(
"What is my favourite food?"
)
;
console
.
log
(
results1
)
;
/*
[
Document { pageContent: 'My favourite food is pasta.', metadata: {} }
]
*/
const
results2
=
await
retriever
.
getRelevantDocuments
(
"What is my favourite food?"
)
;
console
.
log
(
results2
)
;
/*
[
Document { pageContent: 'My favourite food is pasta.', metadata: {} }
]
*/



--- 文件: output_20250622_020018\docs\modules\indexes\retrievers\vectorstore.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/retrievers/vectorstore
crawled_at: 2025-06-22T02:00:23.254650
---

向量库
一旦您创建了一个
向量库
， ,使用它作为检索器就非常简单:
vectorStore
=
...
retriever
=
vectorStore
.
asRetriever
(
)



--- 文件: output_20250622_020018\docs\modules\indexes\retrievers\vespa-retriever.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/retrievers/vespa-retriever
crawled_at: 2025-06-22T02:00:23.527896
---

Vespa Retriever
展示如何使用Vespa.ai作为LangChain检索器。
Vespa.ai是用于高效结构化文本和向量搜索的平台。
请参阅
Vespa.ai
获取更多信息。
以下设置了一个从Vespa文档搜索中获取结果的检索器:
import
{
VespaRetriever
}
from
"langchain/retrievers/vespa"
;
export
const
run
=
async
(
)
=>
{
const
url
=
"https://doc-search.vespa.oath.cloud"
;
const
query_body
=
{
yql
:
"select content from paragraph where userQuery()"
,
hits
:
5
,
ranking
:
"documentation"
,
locale
:
"en-us"
,
}
;
const
content_field
=
"content"
;
const
retriever
=
new
VespaRetriever
(
{
url
,
auth
:
false
,
query_body
,
content_field
,
}
)
;
const
result
=
await
retriever
.
getRelevantDocuments
(
"what is vespa?"
)
;
console
.
log
(
result
)
;
}
;
此处，检索了"段落"文档类型中"内容"字段的最多5个结果，
使用"documentation"作为排名方法。"userQuery()"被实际查询替换
请参阅
pyvespa文档
获取更多信息。
URL是Vespa应用程序的终端点。
您可以连接任何Vespa终节点，远程服务或使用Docker本地实例。
然而，大多数Vespa Cloud实例都受到mTLS保护。
如果您的情况是这样的，您可以，例如设置
CloudFlare Worker
其中包含连接到该实例所需的凭据。
Now you can return the results and continue using them in LangChain.



--- 文件: output_20250622_020018\docs\modules\indexes\retrievers\zep-retriever.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/retrievers/zep-retriever
crawled_at: 2025-06-22T02:00:23.426482
---

Zep Retriever
这个示例展示了如何在
RetrievalQAChain
中使用 Zep Retriever 从 Zep 内存存储中检索文档。(This example shows how to use the Zep Retriever in a
RetrievalQAChain
to retrieve documents from Zep memory store.)
设置(## Setup)
​
npm
Yarn
pnpm
npm
i @getzep/zep-js
yarn
add
@getzep/zep-js
pnpm
add
@getzep/zep-js
使用(## Usage)
​
import
{
ZepRetriever
}
from
"langchain/retrievers/zep"
;
export
const
run
=
async
(
)
=>
{
const
url
=
process
.
env
.
ZEP_URL
||
"http://localhost:8000"
;
const
sessionId
=
"TestSession1232"
;
console
.
log
(
`
Session ID:
${
sessionId
}
, URL:
${
url
}
`
)
;
const
retriever
=
new
ZepRetriever
(
{
sessionId
,
url
}
)
;
const
query
=
"hello"
;
const
docs
=
await
retriever
.
getRelevantDocuments
(
query
)
;
console
.
log
(
docs
)
;
}
;



--- 文件: output_20250622_020018\docs\modules\indexes\text_splitters\examples.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/text_splitters/examples/
crawled_at: 2025-06-22T02:00:23.496525
---

文本分割器: 示例
📄️
字符文本分割器
除了递归字符文本分割器之外，还有更常见的字符文本分割器。
📄️
代码和标记文本分割器
LangChain支持各种不同的标记和编程语言特定的文本分割器，以基于语言特定的语法分割文本。
📄️
RecursiveCharacterTextSplitter
推荐使用的TextSplitter是“递归字符文本分割器”。它会通过不同的符号递归地分割文档-从“”开始，然后是“”，再然后是“ ”。这很好，因为它会尽可能地将所有语义相关的内容保持在同一位置。
📄️
TokenTextSplitter
最后， TokenTextSplitter 将原始文本字符串转换为 BPE 标记，并将这些标记分成块，然后将单个块中的标记转换回文本。#（Finally)



--- 文件: output_20250622_020018\docs\modules\indexes\vector_stores\integrations.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/vector_stores/integrations/
crawled_at: 2025-06-22T02:00:23.680750
---

矢量存储: 集成
📄️
内存
MemoryVectorStore是一个内存中的暂存向量存储器，用于在内存中存储嵌入，并做精确的线性搜索以找到最相似的嵌入。默认的相似度度量是余弦相似度，但可以更改为ml-distance支持的任何相似度度量方式。
📄️
chroma
换行符
📄️
Faiss
仅适用于Node.js环境。
📄️
HNSWLib
仅适用于Node.js。
📄️
Milvus
Milvus是专为嵌入式相似性搜索和 AI 应用而构建的向量数据库。
📄️
MyScale
仅在Node.js上可用。
📄️
OpenSearch
仅限于 Node.js。
📄️
Pinecone
仅适用于 Node.js。
📄️
prisma
---
📄️
Qdrant
Qdrant 是一个向量相似度搜索引擎。它提供了一个方便的API来存储、搜索和管理带有附加有效负载的点 - 向量。
📄️
Redis
Redis是一款快速的开源内存数据存储系统。，作为Redis Stack的一部分，RediSearch是一种支持向量相似性语义搜索以及其他许多类型搜索的模块。
📄️
SingleStore
SingleStoreDB是一款高性能，分布式数据库系统。长期以来，它一直支持dotproduct等向量函数，从而成为需要文本相似度匹配的AI应用程序的最佳解决方案。
📄️
Supabase
Langchain支持使用Supabase Postgres数据库作为向量存储使用'pgvector' postgres扩展。 有关更多信息，请参阅Supabase博客文章。
📄️
仅限node
Tigris使向量嵌入的构建人工智能应用程序变得轻松。
📄️
TypeORM
为了在通用的PostgreSQL数据库中实现向量搜索，LangChainJS支持使用TypeORM和pgvector Postgres扩展。
📄️
Weaviate
Weaviate是一个开源的向量数据库，可以存储对象和向量，使向量搜索与结构化过滤相结合。LangChain通过weaviate-ts-client软件包连接到Weaviate，这是官方的Typescript客户端。



--- 文件: output_20250622_020018\docs\modules\indexes\document_loaders\examples\file_loaders.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/file_loaders/
crawled_at: 2025-06-22T02:00:21.765886
---

文件加载程序
兼容性
仅适用于Node.js。
这些加载程序用于加载给定的文件系统路径或Blob对象的文件。
📄️
具有多个文件夹的文件夹
本示例介绍如何从具有多个文件的文件夹中加载数据。第二个参数是文件扩展名到加载器工厂的映射。每个文件将传递给匹配的加载器， 并将生成的文档连接在一起。
📄️
CSV文件
本示例介绍如何从CSV文件加载数据。
📄️
Docx files
本示例介绍如何从docx文件中加载数据。
📄️
EPUB文件
本例演示如何从EPUB文件中加载数据。默认情况下，每个章节会创建一个文档，您可以通过将“splitChapters”选项设置为“false”来更改此行为。
📄️
JSON文件
JSON加载器使用JSON指针来定位您想要定位的JSON文件中的键。
📄️
JSONLines 文件
这个例子演示了如何从 JSONLines 或 JSONL 文件加载数据。第二个参数是一个 JSONPointer，用于从文件中的每个 JSON 对象中提取属性。每个 JSON 对象都将创建一个文档。
📄️
Notion markdown export
本示例介绍如何从导出的 Notion 页面中加载数据。
📄️
PDF文件
在这个例子中，我们将介绍如何从PDF文件中导入数据。默认情况下，每个页面将创建一个文档。通过将 splitPages 选项设置为 false 可以更改此行为。
📄️
字幕
本示例介绍如何从字幕文件中加载数据。每个字幕文件将创建一个文档。
📄️
文本文件
本例将介绍如何从文本文件中加载数据。
📄️
无结构
本示例介绍如何使用无结构读取多种类型的文件。无结构目前支持加载文本文件、PPT、HTML、PDF、图片等。



--- 文件: output_20250622_020018\docs\modules\indexes\document_loaders\examples\web_loaders.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/web_loaders/
crawled_at: 2025-06-22T02:00:22.204964
---

Web 加载程序
这些加载程序用于加载 web 资源。
📄️
Cheerio
本示例演示如何使用Cheerio从网页加载数据。每个网页都将创建一个文档。
📄️
Puppeteer
仅适用于 Node.js。
📄️
Playwright
仅限于 Node.js。
📄️
Apify 数据集
本指南展示如何使用 Apify 和 LangChain 从 Apify 数据集中加载文档。
📄️
大学机密
本例说明了如何使用Cheerio从大学机密网站加载数据。每个页面将创建一个文档。
📄️
Confluence（维基软件)
仅在 Node.js 上可用。
📄️
Figma
本例演示如何从Figma文件加载数据。
📄️
GitBook
本示例介绍如何使用 Cheerio 从任何 GitBook 中加载数据。将为每个页面创建一个文档。
📄️
仅限于节点
本示例介绍了如何从 GitHub 存储库加载数据。
📄️
黑客新闻
本例介绍如何使用Cheerio从黑客新闻网站加载数据。每页将创建一个文档。
📄️
IMSDB
本例介绍如何使用Cheerio从互联网电影剧本数据库网站加载数据。每个页面将创建一个文档。
📄️
Notion数据库
本示例演示了如何从Notion数据库加载数据。
📄️
S3 文件
仅适用于 Node.js。



--- 文件: output_20250622_020018\docs\modules\indexes\document_loaders\examples\file_loaders\csv.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/file_loaders/csv
crawled_at: 2025-06-22T02:00:21.776904
---

CSV文件
本示例介绍如何从CSV文件加载数据。
第二个参数是要从CSV文件中提取的“列”名称。每行CSV文件将创建一个文档。
当未指定“列”时，每一行都将转换为一个键/值对，并将每个键/值对输出到文档的“pageContent”中的新行中。
当指定了“列”时，将为每一行创建一个文档，并将指定列的值用作文档的“pageContent”。
备注：该处的“pageContent”指文档的页面内容)
设置
​
npm
Yarn
pnpm
npm
install
d3-dsv@2
yarn
add
d3-dsv@2
pnpm
add
d3-dsv@2
用法-提取所有列
​
示例CSV文件:
id,text
1,This is a sentence.
2,This is another sentence.
示例代码:
import
{
CSVLoader
}
from
"langchain/document_loaders/fs/csv"
;
const
loader
=
new
CSVLoader
(
"src/document_loaders/example_data/example.csv"
)
;
const
docs
=
await
loader
.
load
(
)
;
/*
[
Document {
"metadata": {
"line": 1,
"source": "src/document_loaders/example_data/example.csv",
},
"pageContent": "id: 1
text: This is a sentence.",
},
Document {
"metadata": {
"line": 2,
"source": "src/document_loaders/example_data/example.csv",
},
"pageContent": "id: 2
text: This is another sentence.",
},
]
*/
用法-提取单个列
​
示例CSV文件:
id,text
1,This is a sentence.
2,This is another sentence.
示例代码:
import
{
CSVLoader
}
from
"langchain/document_loaders/fs/csv"
;
const
loader
=
new
CSVLoader
(
"src/document_loaders/example_data/example.csv"
,
"text"
)
;
const
docs
=
await
loader
.
load
(
)
;
/*
[
Document {
"metadata": {
"line": 1,
"source": "src/document_loaders/example_data/example.csv",
},
"pageContent": "This is a sentence.",
},
Document {
"metadata": {
"line": 2,
"source": "src/document_loaders/example_data/example.csv",
},
"pageContent": "This is another sentence.",
},
]
*/



--- 文件: output_20250622_020018\docs\modules\indexes\document_loaders\examples\file_loaders\directory.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/file_loaders/directory
crawled_at: 2025-06-22T02:00:21.846711
---

具有多个文件夹的文件夹
本示例介绍如何从具有多个文件的文件夹中加载数据。第二个参数是文件扩展名到加载器工厂的映射。每个文件将传递给匹配的加载器， 并将生成的文档连接在一起。
示例文件夹:
src/document_loaders/example_data/example/
├── example.json
├── example.jsonl
├── example.txt
└── example.csv
示例代码:
import
{
DirectoryLoader
}
from
"langchain/document_loaders/fs/directory"
;
import
{
JSONLoader
,
JSONLinesLoader
,
}
from
"langchain/document_loaders/fs/json"
;
import
{
TextLoader
}
from
"langchain/document_loaders/fs/text"
;
import
{
CSVLoader
}
from
"langchain/document_loaders/fs/csv"
;
const
loader
=
new
DirectoryLoader
(
"src/document_loaders/example_data/example"
,
{
".json"
:
(
path
)
=>
new
JSONLoader
(
path
,
"/texts"
)
,
".jsonl"
:
(
path
)
=>
new
JSONLinesLoader
(
path
,
"/html"
)
,
".txt"
:
(
path
)
=>
new
TextLoader
(
path
)
,
".csv"
:
(
path
)
=>
new
CSVLoader
(
path
,
"text"
)
,
}
)
;
const
docs
=
await
loader
.
load
(
)
;
console
.
log
(
{
docs
}
)
;



--- 文件: output_20250622_020018\docs\modules\indexes\document_loaders\examples\file_loaders\docx.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/file_loaders/docx
crawled_at: 2025-06-22T02:00:21.959510
---

Docx files
本示例介绍如何从docx文件中加载数据。
安装 Setup
npm
Yarn
pnpm
npm
install
mammoth
yarn
add
mammoth
pnpm
add
mammoth
用法 Usage
import
{
DocxLoader
}
from
"langchain/document_loaders/fs/docx"
;
const
loader
=
new
DocxLoader
(
"src/document_loaders/tests/example_data/attention.docx"
)
;
const
docs
=
await
loader
.
load
(
)
;



--- 文件: output_20250622_020018\docs\modules\indexes\document_loaders\examples\file_loaders\epub.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/file_loaders/epub
crawled_at: 2025-06-22T02:00:22.003937
---

EPUB文件
本例演示如何从EPUB文件中加载数据。默认情况下，每个章节会创建一个文档，您可以通过将“splitChapters”选项设置为“false”来更改此行为。
设置
npm
Yarn
pnpm
npm
install
epub2 html-to-text
yarn
add
epub2 html-to-text
pnpm
add
epub2 html-to-text
用法：每章一个文档
import
{
EPubLoader
}
from
"langchain/document_loaders/fs/epub"
;
const
loader
=
new
EPubLoader
(
"src/document_loaders/example_data/example.epub"
)
;
const
docs
=
await
loader
.
load
(
)
;
用法：每个文件一个文档
import
{
EPubLoader
}
from
"langchain/document_loaders/fs/epub"
;
const
loader
=
new
EPubLoader
(
"src/document_loaders/example_data/example.epub"
,
{
splitChapters
:
false
,
}
)
;
const
docs
=
await
loader
.
load
(
)
;



--- 文件: output_20250622_020018\docs\modules\indexes\document_loaders\examples\file_loaders\json.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/file_loaders/json
crawled_at: 2025-06-22T02:00:22.076018
---

JSON文件
JSON加载器使用
JSON指针
来定位您想要定位的JSON文件中的键。
无JSON指针示例
​
最简单的使用方式是不指定JSON指针。
加载程序将加载JSON对象中找到的所有字符串。
示例JSON文件:
{
"texts"
:
[
"This is a sentence."
,
"This is another sentence."
]
}
示例代码:
import
{
JSONLoader
}
from
"langchain/document_loaders/fs/json"
;
const
loader
=
new
JSONLoader
(
"src/document_loaders/example_data/example.json"
)
;
const
docs
=
await
loader
.
load
(
)
;
/*
[
Document {
"metadata": {
"blobType": "application/json",
"line": 1,
"source": "blob",
},
"pageContent": "This is a sentence.",
},
Document {
"metadata": {
"blobType": "application/json",
"line": 2,
"source": "blob",
},
"pageContent": "This is another sentence.",
},
]
*/
使用JSON指针示例
​
您可以通过选择要从JSON对象中提取字符串的哪些键来执行更高级的场景。
在此示例中，我们仅想从“from”和“surname”条目中提取信息。
{
"1"
:
{
"body"
:
"BD 2023 SUMMER"
,
"from"
:
"LinkedIn Job"
,
"labels"
:
[
"IMPORTANT"
,
"CATEGORY_UPDATES"
,
"INBOX"
]
}
,
"2"
:
{
"body"
:
"Intern, Treasury and other roles are available"
,
"from"
:
"LinkedIn Job2"
,
"labels"
:
[
"IMPORTANT"
]
,
"other"
:
{
"name"
:
"plop"
,
"surname"
:
"bob"
}
}
}
示例代码:
import
{
JSONLoader
}
from
"langchain/document_loaders/fs/json"
;
const
loader
=
new
JSONLoader
(
"src/document_loaders/example_data/example.json"
,
[
"/from"
,
"/surname"
]
)
;
const
docs
=
await
loader
.
load
(
)
;
/*
[
Document {
"metadata": {
"blobType": "application/json",
"line": 1,
"source": "blob",
},
"pageContent": "BD 2023 SUMMER",
},
Document {
"metadata": {
"blobType": "application/json",
"line": 2,
"source": "blob",
},
"pageContent": "LinkedIn Job",
},
...
]



--- 文件: output_20250622_020018\docs\modules\indexes\document_loaders\examples\file_loaders\jsonlines.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/file_loaders/jsonlines
crawled_at: 2025-06-22T02:00:22.024838
---

JSONLines 文件
这个例子演示了如何从 JSONLines 或 JSONL 文件加载数据。第二个参数是一个 JSONPointer，用于从文件中的每个 JSON 对象中提取属性。每个 JSON 对象都将创建一个文档。
示例 JSONLines 文件:
{
"html"
:
"This is a sentence."
}
{
"html"
:
"This is another sentence."
}
示例代码:
import
{
JSONLinesLoader
}
from
"langchain/document_loaders/fs/json"
;
const
loader
=
new
JSONLinesLoader
(
"src/document_loaders/example_data/example.jsonl"
,
"/html"
)
;
const
docs
=
await
loader
.
load
(
)
;
/*
[
Document {
"metadata": {
"blobType": "application/jsonl+json",
"line": 1,
"source": "blob",
},
"pageContent": "This is a sentence.",
},
Document {
"metadata": {
"blobType": "application/jsonl+json",
"line": 2,
"source": "blob",
},
"pageContent": "This is another sentence.",
},
]
*/



--- 文件: output_20250622_020018\docs\modules\indexes\document_loaders\examples\file_loaders\notion_markdown.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/file_loaders/notion_markdown
crawled_at: 2025-06-22T02:00:22.065792
---

Notion markdown export
本示例介绍如何从导出的 Notion 页面中加载数据。
首先，按照官方说明
这里
导出 Notion 页面为
Markdown & CSV
。确保选择
包括子页面
和
为子页面创建文件夹
。
然后，解压下载的文件并将未压缩的文件夹移动到存储库中。它应该包含你页面的 markdown 文件。
一旦文件夹在存储库中，只需运行下面的示例即可:
import
{
NotionLoader
}
from
"langchain/document_loaders/fs/notion"
;
export
const
run
=
async
(
)
=>
{
/** Provide the directory path of your notion folder */
const
directoryPath
=
"Notion_DB"
;
const
loader
=
new
NotionLoader
(
directoryPath
)
;
const
docs
=
await
loader
.
load
(
)
;
console
.
log
(
{
docs
}
)
;
}
;



--- 文件: output_20250622_020018\docs\modules\indexes\document_loaders\examples\file_loaders\pdf.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/file_loaders/pdf
crawled_at: 2025-06-22T02:00:22.100748
---

PDF文件
在这个例子中，我们将介绍如何从PDF文件中导入数据。默认情况下，每个页面将创建一个文档。通过将
splitPages
选项设置为
false
可以更改此行为。
设置
​
npm
Yarn
pnpm
npm
install
pdf-parse
yarn
add
pdf-parse
pnpm
add
pdf-parse
用法，每个页面一个文档
​
import
{
PDFLoader
}
from
"langchain/document_loaders/fs/pdf"
;
const
loader
=
new
PDFLoader
(
"src/document_loaders/example_data/example.pdf"
)
;
const
docs
=
await
loader
.
load
(
)
;
用法，每个文件一个文档
​
import
{
PDFLoader
}
from
"langchain/document_loaders/fs/pdf"
;
const
loader
=
new
PDFLoader
(
"src/document_loaders/example_data/example.pdf"
,
{
splitPages
:
false
,
}
)
;
const
docs
=
await
loader
.
load
(
)
;
用法，自定义
pdfjs
构建
​
默认情况下，我们使用与大多数环境（包括 Node.js 和现代浏览器)兼容的
pdf-parse
捆绑的
pdfjs
构建。如果要使用更高版本的
pdfjs-dist
，或者要使用自定义构建的
pdfjs-dist
，则可以提供返回解析为
PDFJS
对象的 promise 的自定义
pdfjs
函数。
在下面的示例中，我们使用“旧版”（请参阅
pdfjs文档
)，该构建包括默认构建中未包含的几个 polyfill。
npm
Yarn
pnpm
npm
install
pdfjs-dist
yarn
add
pdfjs-dist
pnpm
add
pdfjs-dist
import
{
PDFLoader
}
from
"langchain/document_loaders/fs/pdf"
;
const
loader
=
new
PDFLoader
(
"src/document_loaders/example_data/example.pdf"
,
{
// you may need to add `.then(m => m.default)` to the end of the import
pdfjs
:
(
)
=>
import
(
"pdfjs-dist/legacy/build/pdf.js"
)
,
}
)
;



--- 文件: output_20250622_020018\docs\modules\indexes\document_loaders\examples\file_loaders\subtitles.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/file_loaders/subtitles
crawled_at: 2025-06-22T02:00:22.117912
---

字幕
本示例介绍如何从字幕文件中加载数据。每个字幕文件将创建一个文档。
设置
​
npm
Yarn
pnpm
npm
install
srt-parser-2
yarn
add
srt-parser-2
pnpm
add
srt-parser-2
用法
​
import
{
SRTLoader
}
from
"langchain/document_loaders/fs/srt"
;
const
loader
=
new
SRTLoader
(
"src/document_loaders/example_data/Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.srt"
)
;
const
docs
=
await
loader
.
load
(
)
;



--- 文件: output_20250622_020018\docs\modules\indexes\document_loaders\examples\file_loaders\text.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/file_loaders/text
crawled_at: 2025-06-22T02:00:22.152375
---

文本文件
本例将介绍如何从文本文件中加载数据。
import
{
TextLoader
}
from
"langchain/document_loaders/fs/text"
;
const
loader
=
new
TextLoader
(
"src/document_loaders/example_data/example.txt"
)
;
const
docs
=
await
loader
.
load
(
)
;



--- 文件: output_20250622_020018\docs\modules\indexes\document_loaders\examples\file_loaders\unstructured.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/file_loaders/unstructured
crawled_at: 2025-06-22T02:00:22.140037
---

无结构
本示例介绍如何使用
无结构
读取多种类型的文件。无结构目前支持加载文本文件、PPT、HTML、PDF、图片等。
设置
​
您可以在计算机上使用Docker运行无结构。要这样做，您需要安装Docker。您可以在此处找到安装Docker的说明
here
。
docker
run -p
8000
:8000 -d --rm --name unstructured-api quay.io/unstructured-io/unstructured-api:latest --port
8000
--host
0.0
.0.0
用法
​
运行无结构后，您可以使用它从计算机中加载文件。您可以使用以下代码从计算机中加载文件。
import
{
UnstructuredLoader
}
from
"langchain/document_loaders/fs/unstructured"
;
const
options
=
{
apiKey
:
"MY_API_KEY"
,
}
;
const
loader
=
new
UnstructuredLoader
(
"src/document_loaders/example_data/notion.md"
,
options
)
;
const
docs
=
await
loader
.
load
(
)
;
目录
​
您还可以使用 'UnstructuredDirectoryLoader' 从目录中加载所有文件，其继承自
'DirectoryLoader'
import
{
UnstructuredDirectoryLoader
}
from
"langchain/document_loaders/fs/unstructured"
;
const
options
=
{
apiKey
:
"MY_API_KEY"
,
}
;
const
loader
=
new
UnstructuredDirectoryLoader
(
"langchain/src/document_loaders/tests/example_data"
,
options
)
;
const
docs
=
await
loader
.
load
(
)
;



--- 文件: output_20250622_020018\docs\modules\indexes\document_loaders\examples\web_loaders\apify_dataset.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/web_loaders/apify_dataset
crawled_at: 2025-06-22T02:00:22.400226
---

Apify 数据集
本指南展示如何使用
Apify
和 LangChain 从 Apify 数据集中加载文档。
概述
​
Apify
是一个云端网页抓取和数据提取平台，
提供了一个包含一千多个现成应用程序（称作
Actors
)的
生态系统
，用于各种网络抓取，爬取，和数据提取的用例。
本指南展示如何加载文档
用于存储结构化网络抓取结果的存储空间，
from an
Apify Dataset
— a scalable append-only
例如产品列表或 Google SERPs 等，然后将它们导出到各种格式，如 JSON， CSV， 或 Excel。
数据集通常用于保存演员的结果。
例如，
网站内容爬虫
演员，
深度爬取网站，如文档，知识库，帮助中心或博客等，并将网页的文本内容存储到数据集中。，
设置
​
您首先需要安装官方的 Apify 客户端:
npm install apify-client
您还需要注册并获取您的
Apify API 令牌
。
用法
​
从新数据集
​
如果您尚未在 Apify 平台上拥有现有数据集，则需要调用 Actor 并等待结果来初始化文档加载程序。
注意:
调用演员可能需要很长时间，大约需要数小时或甚至数日来处理大型网站！
以下是一个例子:
import
{
ApifyDatasetLoader
}
from
"langchain/document_loaders/web/apify_dataset"
;
import
{
Document
}
from
"langchain/document"
;
import
{
HNSWLib
}
from
"langchain/vectorstores/hnswlib"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
RetrievalQAChain
}
from
"langchain/chains"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
/*
* datasetMappingFunction is a function that maps your Apify dataset format to LangChain documents.
* In the below example, the Apify dataset format looks like this:
* {
*   "url": "https://apify.com",
*   "text": "Apify is the best web scraping and automation platform."
* }
*/
const
loader
=
await
ApifyDatasetLoader
.
fromActorCall
(
"apify/website-content-crawler"
,
{
startUrls
:
[
{
url
:
"https://js.langchain.com/docs/"
}
]
,
}
,
{
datasetMappingFunction
:
(
item
)
=>
new
Document
(
{
pageContent
:
(
item
.
text
||
""
)
as
string
,
metadata
:
{
source
:
item
.
url
}
,
}
)
,
clientOptions
:
{
token
:
"your-apify-token"
,
// Or set as process.env.APIFY_API_TOKEN
}
,
}
)
;
const
docs
=
await
loader
.
load
(
)
;
const
vectorStore
=
await
HNSWLib
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
)
;
const
model
=
new
OpenAI
(
{
temperature
:
0
,
}
)
;
const
chain
=
RetrievalQAChain
.
fromLLM
(
model
,
vectorStore
.
asRetriever
(
)
,
{
returnSourceDocuments
:
true
,
}
)
;
const
res
=
await
chain
.
call
(
{
query
:
"What is LangChain?"
}
)
;
console
.
log
(
res
.
text
)
;
console
.
log
(
res
.
sourceDocuments
.
map
(
(
d
:
Document
)
=>
d
.
metadata
.
source
)
)
;
/*
LangChain is a framework for developing applications powered by language models.
[
'https://js.langchain.com/docs/',
'https://js.langchain.com/docs/modules/chains/',
'https://js.langchain.com/docs/modules/chains/llmchain/',
'https://js.langchain.com/docs/category/functions-4'
]
*/
来自现有数据集
​
如果您已经在Apify平台上拥有现有的数据集，您可以直接使用构造函数初始化文档加载器:
import
{
ApifyDatasetLoader
}
from
"langchain/document_loaders/web/apify_dataset"
;
import
{
Document
}
from
"langchain/document"
;
import
{
HNSWLib
}
from
"langchain/vectorstores/hnswlib"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
RetrievalQAChain
}
from
"langchain/chains"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
/*
* datasetMappingFunction is a function that maps your Apify dataset format to LangChain documents.
* In the below example, the Apify dataset format looks like this:
* {
*   "url": "https://apify.com",
*   "text": "Apify is the best web scraping and automation platform."
* }
*/
const
loader
=
new
ApifyDatasetLoader
(
"your-dataset-id"
,
{
datasetMappingFunction
:
(
item
)
=>
new
Document
(
{
pageContent
:
(
item
.
text
||
""
)
as
string
,
metadata
:
{
source
:
item
.
url
}
,
}
)
,
clientOptions
:
{
token
:
"your-apify-token"
,
// Or set as process.env.APIFY_API_TOKEN
}
,
}
)
;
const
docs
=
await
loader
.
load
(
)
;
const
vectorStore
=
await
HNSWLib
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
)
;
const
model
=
new
OpenAI
(
{
temperature
:
0
,
}
)
;
const
chain
=
RetrievalQAChain
.
fromLLM
(
model
,
vectorStore
.
asRetriever
(
)
,
{
returnSourceDocuments
:
true
,
}
)
;
const
res
=
await
chain
.
call
(
{
query
:
"What is LangChain?"
}
)
;
console
.
log
(
res
.
text
)
;
console
.
log
(
res
.
sourceDocuments
.
map
(
(
d
:
Document
)
=>
d
.
metadata
.
source
)
)
;
/*
LangChain is a framework for developing applications powered by language models.
[
'https://js.langchain.com/docs/',
'https://js.langchain.com/docs/modules/chains/',
'https://js.langchain.com/docs/modules/chains/llmchain/',
'https://js.langchain.com/docs/category/functions-4'
]
*/



--- 文件: output_20250622_020018\docs\modules\indexes\document_loaders\examples\web_loaders\college_confidential.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/web_loaders/college_confidential
crawled_at: 2025-06-22T02:00:22.420343
---

大学机密
本例说明了如何使用Cheerio从大学机密网站加载数据。每个页面将创建一个文档。
设置
​
npm
Yarn
pnpm
npm
install
cheerio
yarn
add
cheerio
pnpm
add
cheerio
用法
​
import
{
CollegeConfidentialLoader
}
from
"langchain/document_loaders/web/college_confidential"
;
const
loader
=
new
CollegeConfidentialLoader
(
"https://www.collegeconfidential.com/colleges/brown-university/"
)
;
const
docs
=
await
loader
.
load
(
)
;



--- 文件: output_20250622_020018\docs\modules\indexes\document_loaders\examples\web_loaders\confluence.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/web_loaders/confluence
crawled_at: 2025-06-22T02:00:22.439562
---

Confluence（维基软件)
：兼容性
仅在 Node.js 上可用。
本文介绍如何从 Confluence 空间中加载文档对象。
凭证
​
您需要设置访问令牌，并提供您的 Confluence 用户名，以便身份验证请求
您还需要 “空间密钥” 来获取包含要加载为文档的页面的空间。导航到您的空间时，可以在 url 中找到它，例如
https://example.atlassian.net/wiki/spaces/{SPACE_KEY}
您需要安装
html-to-text
将页面解析为纯文本
npm
Yarn
pnpm
npm
install
html-to-text
yarn
add
html-to-text
pnpm
add
html-to-text
用法
​
import
{
ConfluencePagesLoader
}
from
"langchain/document_loaders/web/confluence"
;
const
username
=
process
.
env
.
CONFLUENCE_USERNAME
;
const
accessToken
=
process
.
env
.
CONFLUENCE_ACCESS_TOKEN
;
if
(
username
&&
accessToken
)
{
const
loader
=
new
ConfluencePagesLoader
(
{
baseUrl
:
"https://example.atlassian.net/wiki"
,
spaceKey
:
"~EXAMPLE362906de5d343d49dcdbae5dEXAMPLE"
,
username
,
accessToken
,
}
)
;
const
documents
=
await
loader
.
load
(
)
;
console
.
log
(
documents
)
;
}
else
{
console
.
log
(
"You must provide a username and access token to run this example."
)
;
}



--- 文件: output_20250622_020018\docs\modules\indexes\document_loaders\examples\web_loaders\figma.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/web_loaders/figma
crawled_at: 2025-06-22T02:00:22.464266
---

Figma
本例演示如何从Figma文件加载数据。
您需要Figma访问令牌才能开始使用。
import
{
FigmaFileLoader
}
from
"langchain/document_loaders/web/figma"
;
const
loader
=
new
FigmaFileLoader
(
{
accessToken
:
"FIGMA_ACCESS_TOKEN"
,
// or load it from process.env.FIGMA_ACCESS_TOKEN
nodeIds
:
[
"id1"
,
"id2"
,
"id3"
]
,
fileKey
:
"key"
,
}
)
;
const
docs
=
await
loader
.
load
(
)
;
console
.
log
(
{
docs
}
)
;
您可以通过在浏览器中打开文件并从URL中提取它们来找到Figma文件的密钥和节点ID:
https://www.figma.com/file/<YOUR FILE KEY HERE>/LangChainJS-Test?type=whiteboard&node-id=<YOUR NODE ID HERE>&t=e6lqWkKecuYQRyRg-0



--- 文件: output_20250622_020018\docs\modules\indexes\document_loaders\examples\web_loaders\gitbook.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/web_loaders/gitbook
crawled_at: 2025-06-22T02:00:22.533918
---

GitBook
本示例介绍如何使用 Cheerio 从任何 GitBook 中加载数据。将为每个页面创建一个文档。
设置
​
npm
Yarn
pnpm
npm
install
cheerio
yarn
add
cheerio
pnpm
add
cheerio
从单个 GitBook 页面加载
​
import
{
GitbookLoader
}
from
"langchain/document_loaders/web/gitbook"
;
const
loader
=
new
GitbookLoader
(
"https://docs.gitbook.com/product-tour/navigation"
)
;
const
docs
=
await
loader
.
load
(
)
;
从给定 GitBook 中的所有路径加载
​
为了使此项功能正常工作，需要使用根路径（例如
https://docs.gitbook.com)初始化
GitbookLoader，并将
shouldLoadAllPaths
设置为
true
。
import
{
GitbookLoader
}
from
"langchain/document_loaders/web/gitbook"
;
const
loader
=
new
GitbookLoader
(
"https://docs.gitbook.com"
,
{
shouldLoadAllPaths
:
true
,
}
)
;
const
docs
=
await
loader
.
load
(
)
;



--- 文件: output_20250622_020018\docs\modules\indexes\document_loaders\examples\web_loaders\github.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/web_loaders/github
crawled_at: 2025-06-22T02:00:22.587317
---

GitHub
本示例介绍了如何从 GitHub 存储库加载数据。
您可以将
GITHUB_ACCESS_TOKEN
环境变量设置为 GitHub 访问令牌，以增加速率限制和访问私有存储库。
设置
​
GitHub 加载器需要
ignore npm package
作为同等依赖项。可以像这样安装它
npm
Yarn
pnpm
npm
install
ignore
yarn
add
ignore
pnpm
add
ignore
用法
​
import
{
GithubRepoLoader
}
from
"langchain/document_loaders/web/github"
;
export
const
run
=
async
(
)
=>
{
const
loader
=
new
GithubRepoLoader
(
"https://github.com/hwchase17/langchainjs"
,
{
branch
:
"main"
,
recursive
:
false
,
unknown
:
"warn"
}
)
;
const
docs
=
await
loader
.
load
(
)
;
console
.
log
(
{
docs
}
)
;
}
;
加载器将忽略像图像这样的二进制文件。
使用 .gitignore 语法
​
要忽略特定文件，您可以将
ignorePaths
数组传递到构造函数中
import
{
GithubRepoLoader
}
from
"langchain/document_loaders/web/github"
;
export
const
run
=
async
(
)
=>
{
const
loader
=
new
GithubRepoLoader
(
"https://github.com/hwchase17/langchainjs"
,
{
branch
:
"main"
,
recursive
:
false
,
unknown
:
"warn"
,
ignorePaths
:
[
"*.md"
]
}
)
;
const
docs
=
await
loader
.
load
(
)
;
console
.
log
(
{
docs
}
)
;
// Will not include any .md files
}
;



--- 文件: output_20250622_020018\docs\modules\indexes\document_loaders\examples\web_loaders\hn.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/web_loaders/hn
crawled_at: 2025-06-22T02:00:22.574461
---

黑客新闻
本例介绍如何使用Cheerio从黑客新闻网站加载数据。每页将创建一个文档。
设置
​
npm
install
cheerio
用法
​
import
{
HNLoader
}
from
"langchain/document_loaders/web/hn"
;
const
loader
=
new
HNLoader
(
"https://news.ycombinator.com/item?id=34817881"
)
;
const
docs
=
await
loader
.
load
(
)
;



--- 文件: output_20250622_020018\docs\modules\indexes\document_loaders\examples\web_loaders\imsdb.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/web_loaders/imsdb
crawled_at: 2025-06-22T02:00:22.577874
---

IMSDB
本例介绍如何使用Cheerio从互联网电影剧本数据库网站加载数据。每个页面将创建一个文档。
设置
​
npm
install
cheerio
用法
​
import
{
IMSDBLoader
}
from
"langchain/document_loaders/web/imsdb"
;
const
loader
=
new
IMSDBLoader
(
"https://imsdb.com/scripts/BlacKkKlansman.html"
)
;
const
docs
=
await
loader
.
load
(
)
;



--- 文件: output_20250622_020018\docs\modules\indexes\document_loaders\examples\web_loaders\notiondb.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/web_loaders/notiondb
crawled_at: 2025-06-22T02:00:22.593648
---

Notion数据库
本示例演示了如何从Notion数据库加载数据。
你需要你的Notion集成令牌和要访问的资源的“databaseId”。
不要忘记将你的集成添加到数据库中！
import
{
NotionDBLoader
}
from
"langchain/document_loaders/web/notiondb"
;
const
loader
=
new
NotionDBLoader
(
{
pageSizeLimit
:
10
,
databaseId
:
"databaseId"
,
notionIntegrationToken
:
"<your token here>"
,
// Or set as process.env.NOTION_INTEGRATION_TOKEN
}
)
;
const
docs
=
await
loader
.
load
(
)
;
console
.
log
(
{
docs
}
)
;



--- 文件: output_20250622_020018\docs\modules\indexes\document_loaders\examples\web_loaders\s3.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/web_loaders/s3
crawled_at: 2025-06-22T02:00:22.604954
---

S3 文件
兼容性
仅适用于 Node.js。
本文档介绍如何从 S3 文件对象中加载文档对象。
设置
​
要运行此索引，您需要先设置并准备好 Unstructured 并在可用的 URL 端点中使用。它也可以在本地配置运行。
请查看此处的文档（
https://js.langchain.com/docs/modules/indexes/document_loaders/examples/file_loaders/unstructured)以了解如何进行操作。
使用方法
​
一旦 Unstructured 配置完成，您可以使用 S3 Loader 加载文件，然后将其转换为文档。
您可以选择提供 s3Config 参数以指定桶区域访问密钥和秘密访问密钥。如果未提供这些参数，则需要在您的环境中具有它们（例如通过运行 'aws configure' 命令)。
import
{
S3Loader
}
from
"langchain/document_loaders/web/s3"
;
const
loader
=
new
S3Loader
(
{
bucket
:
"my-document-bucket-123"
,
key
:
"AccountingOverview.pdf"
,
s3Config
:
{
region
:
"us-east-1"
,
accessKeyId
:
"AKIAIOSFODNN7EXAMPLE"
,
secretAccessKey
:
"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
,
}
,
unstructuredAPIURL
:
"http://localhost:8000/general/v0/general"
,
}
)
;
const
docs
=
await
loader
.
load
(
)
;
console
.
log
(
docs
)
;



--- 文件: output_20250622_020018\docs\modules\indexes\document_loaders\examples\web_loaders\web_cheerio.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/web_loaders/web_cheerio
crawled_at: 2025-06-22T02:00:22.757318
---

网页，使用Cheerio
本示例演示如何使用Cheerio从网页加载数据。每个网页都将创建一个文档。
Cheerio是一个快速且轻量级的库，允许您使用类似于jQuery的语法解析和遍历HTML文档。您可以使用Cheerio从网页中提取数据，而无需在浏览器中呈现它们。
但是， Cheerio无法模拟浏览器，因此它无法在页面上执行JavaScript代码。这意味着它无法从需要JavaScript呈现的动态网页中提取数据。要做到这一点，您可以使用
PlaywrightWebBaseLoader
或
PuppeteerWebBaseLoader
。
安装
​
npm
Yarn
pnpm
npm
install
cheerio
yarn
add
cheerio
pnpm
add
cheerio
使用
​
import
{
CheerioWebBaseLoader
}
from
"langchain/document_loaders/web/cheerio"
;
const
loader
=
new
CheerioWebBaseLoader
(
"https://news.ycombinator.com/item?id=34817881"
)
;
const
docs
=
await
loader
.
load
(
)
;
使用，自定义选择器
​
import
{
CheerioWebBaseLoader
}
from
"langchain/document_loaders/web/cheerio"
;
const
loader
=
new
CheerioWebBaseLoader
(
"https://news.ycombinator.com/item?id=34817881"
,
{
selector
:
"p.athing"
,
}
)
;
const
docs
=
await
loader
.
load
(
)
;



--- 文件: output_20250622_020018\docs\modules\indexes\document_loaders\examples\web_loaders\web_playwright.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/web_loaders/web_playwright
crawled_at: 2025-06-22T02:00:22.784413
---

Webpages， with Playwright
兼容性
仅限于 Node.js。
本示例演示了如何使用 Playwright 从网页中加载数据。将为每个网页创建一个文档。
Playwright 是一个 Node.js 库，提供了一个高级 API，用于控制多个浏览器引擎，包括 Chromium、Firefox 和 WebKit。您可以使用 Playwright 自动化网页交互，包括从需要 JavaScript 渲染的动态网页中提取数据。
如果您想要一个更轻量级的解决方案，想要加载的网页不需要 JavaScript 渲染，那么可以使用
CheerioWebBaseLoader
。
设置
​
npm
Yarn
pnpm
npm
install
playwright
yarn
add
playwright
pnpm
add
playwright
用法
​
import
{
PlaywrightWebBaseLoader
}
from
"langchain/document_loaders/web/playwright"
;
/**
* Loader uses `page.content()`
* as default evaluate function
**/
const
loader
=
new
PlaywrightWebBaseLoader
(
"https://www.tabnews.com.br/"
)
;
const
docs
=
await
loader
.
load
(
)
;
选项
​
这里是关于可以通过使用 PlaywrightWebBaseLoaderOptions 接口将参数传递给 PlaywrightWebBaseLoader 构造函数的参数的解释:
type
PlaywrightWebBaseLoaderOptions
=
{
launchOptions
?
:
LaunchOptions
;
gotoOptions
?
:
PlaywrightGotoOptions
;
evaluate
?
:
PlaywrightEvaluate
;
}
;
launchOptions
: 一个可选对象，用于指定要传递给 playwright.chromium.launch() 方法的其他选项。这可以包括选项，例如在无头模式下启动浏览器的 headless 标志。
gotoOptions
: 一个可选对象，用于指定要传递给 page.goto() 方法的其他选项。这可以包括选项，例如 timeout 选项以指定最大导航时间（以毫秒为单位)或 waitUntil 选项以指定何时将导航视为成功。
evaluate
: 是一个可选函数，可以使用自定义评估函数在页面上评估JavaScript代码。这对于从页面提取数据或与页面元素进行交互非常有用。该函数应返回解析为包含评估结果的字符串的Promise。
通过将这些选项传递给
PlaywrightWebBaseLoader
构造函数,您可以自定义加载程序的行为，并使用Playwright强大的功能对Web页面进行抓取和交互。
以下是一个基本示例:：
import
{
PlaywrightWebBaseLoader
}
from
"langchain/document_loaders/web/playwright"
;
const
loader
=
new
PlaywrightWebBaseLoader
(
"https://www.tabnews.com.br/"
,
{
launchOptions
:
{
headless
:
true
,
}
,
gotoOptions
:
{
waitUntil
:
"domcontentloaded"
,
}
,
/** Pass custom evaluate, in this case you get page and browser instances */
async
evaluate
(
page
:
Page
,
browser
:
Browser
)
{
await
page
.
waitForResponse
(
"https://www.tabnews.com.br/va/view"
)
;
const
result
=
await
page
.
evaluate
(
(
)
=>
document
.
body
.
innerHTML
)
;
return
result
;
}
,
}
)
;
const
docs
=
await
loader
.
load
(
)
;



--- 文件: output_20250622_020018\docs\modules\indexes\document_loaders\examples\web_loaders\web_puppeteer.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/web_loaders/web_puppeteer
crawled_at: 2025-06-22T02:00:22.848071
---

使用 Puppeteer 打造网页
兼容性
仅适用于 Node.js。
本例演示如何使用 Puppeteer 从网站加载数据，并为每个网页创建一个文档。
Puppeteer 是一个基于 Node.js 的库，为控制无头 Chrome 或 Chromium 提供了高级 API。使用 Puppeteer 可以自动化网页交互，包括从需要 JavaScript 渲染的动态网页中提取数据。
如果您想要一个轻量级的解决方案，并且您要加载的网页不需要 JavaScript 渲染，则可以使用
CheerioWebBaseLoader
。
设置
​
npm
Yarn
pnpm
npm
install
puppeteer
yarn
add
puppeteer
pnpm
add
puppeteer
使用
​
import
{
PuppeteerWebBaseLoader
}
from
"langchain/document_loaders/web/puppeteer"
;
/**
* Loader uses `page.evaluate(() => document.body.innerHTML)`
* as default evaluate function
**/
const
loader
=
new
PuppeteerWebBaseLoader
(
"https://www.tabnews.com.br/"
)
;
const
docs
=
await
loader
.
load
(
)
;
选项
​
以下是您可以使用 PuppeteerWebBaseLoaderOptions 接口将参数传递给 PuppeteerWebBaseLoader 构造函数的解释:
type
PuppeteerWebBaseLoaderOptions
=
{
launchOptions
?
:
PuppeteerLaunchOptions
;
gotoOptions
?
:
PuppeteerGotoOptions
;
evaluate
?
:
(
page
:
Page
,
browser
:
Browser
)
=>
Promise
<
string
>
;
}
;
launchOptions
: 一个可选的对象，用于指定要传递给 puppeteer.launch() 方法的附加选项。这可以包括选项，如 headless 标志，以在无头模式下启动浏览器，或者 slowMo 选项，以减慢 Puppeteer 的操作，使其更容易跟踪。
gotoOptions
: 一个可选的对象，用于指定要传递给 page.goto() 方法的附加选项。这可以包括选项，如 timeout 选项，以指定最大导航时间（以毫秒为单位)，或者 waitUntil 选项，以指定何时将导航视为成功。
evaluate
（可选)：可以使用page.evaluate()方法在页面上评估JavaScript代码的可选函数。这对于从页面提取数据或与页面元素交互非常有用。该函数应返回一个Promise，该Promise解析为包含评估结果的字符串。
通过将这些选项传递给
PuppeteerWebBaseLoader
构造函数，您可以自定义加载程序的行为并使用Puppeteer的强大功能来抓取和与网页交互。
以下是一个基本示例:：
import
{
PuppeteerWebBaseLoader
}
from
"langchain/document_loaders/web/puppeteer"
;
const
loader
=
new
PuppeteerWebBaseLoader
(
"https://www.tabnews.com.br/"
,
{
launchOptions
:
{
headless
:
true
,
}
,
gotoOptions
:
{
waitUntil
:
"domcontentloaded"
,
}
,
/** Pass custom evaluate, in this case you get page and browser instances */
async
evaluate
(
page
:
Page
,
browser
:
Browser
)
{
await
page
.
waitForResponse
(
"https://www.tabnews.com.br/va/view"
)
;
const
result
=
await
page
.
evaluate
(
(
)
=>
document
.
body
.
innerHTML
)
;
return
result
;
}
,
}
)
;
const
docs
=
await
loader
.
load
(
)
;



--- 文件: output_20250622_020018\docs\modules\indexes\text_splitters\examples\character.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/text_splitters/examples/character
crawled_at: 2025-06-22T02:00:23.507996
---

字符文本分割器
除了递归字符文本分割器之外，还有更常见的字符文本分割器。
它仅在一个类型的字符上分割（默认为
"\\"
)。您可以以完全相同的方式使用它。
import
{
Document
}
from
"langchain/document"
;
import
{
CharacterTextSplitter
}
from
"langchain/text_splitter"
;
const
text
=
"foo bar baz 123"
;
const
splitter
=
new
CharacterTextSplitter
(
{
separator
:
" "
,
chunkSize
:
7
,
chunkOverlap
:
3
,
}
)
;
const
output
=
await
splitter
.
createDocuments
(
[
text
]
)
;



--- 文件: output_20250622_020018\docs\modules\indexes\text_splitters\examples\code.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/text_splitters/examples/code
crawled_at: 2025-06-22T02:00:23.572722
---

代码和标记文本分割器
LangChain支持各种不同的标记和编程语言特定的文本分割器，以基于语言特定的语法分割文本。
​这将导致更具有语义的自包含块，更适用于矢量存储或其他检索器。
流行的语言，如JavaScript， Python，和Rust，以及Latex，HTML，和Markdown都受到支持。
用法
​
使用“fromLanguage”工厂方法初始化标准的“RecursiveCharacterTextSplitter”。以下是各种语言的示例。
JavaScript
​
import
{
SupportedTextSplitterLanguages
,
RecursiveCharacterTextSplitter
,
}
from
"langchain/text_splitter"
;
console
.
log
(
SupportedTextSplitterLanguages
)
;
// Array of supported languages
/*
[
'cpp',      'go',
'java',     'js',
'php',      'proto',
'python',   'rst',
'ruby',     'rust',
'scala',    'swift',
'markdown', 'latex',
'html'
]
*/
const
jsCode
=
`
function helloWorld() {
console.log("Hello, World!");
}
// Call the function
helloWorld();
`
;
const
splitter
=
RecursiveCharacterTextSplitter
.
fromLanguage
(
"js"
,
{
chunkSize
:
32
,
chunkOverlap
:
0
,
}
)
;
const
jsOutput
=
await
splitter
.
createDocuments
(
[
jsCode
]
)
;
console
.
log
(
jsOutput
)
;
/*
[
Document {
pageContent: 'function helloWorld() {',
metadata: { loc: [Object] }
},
Document {
pageContent: 'console.log("Hello, World!");',
metadata: { loc: [Object] }
},
Document {
pageContent: '}\n// Call the function',
metadata: { loc: [Object] }
},
Document {
pageContent: 'helloWorld();',
metadata: { loc: [Object] }
}
]
*/
Python
​
import
{
RecursiveCharacterTextSplitter
}
from
"langchain/text_splitter"
;
const
pythonCode
=
`
def hello_world():
print("Hello, World!")
# Call the function
hello_world()
`
;
const
splitter
=
RecursiveCharacterTextSplitter
.
fromLanguage
(
"python"
,
{
chunkSize
:
32
,
chunkOverlap
:
0
,
}
)
;
const
pythonOutput
=
await
splitter
.
createDocuments
(
[
pythonCode
]
)
;
console
.
log
(
pythonOutput
)
;
/*
[
Document {
pageContent: 'def hello_world():',
metadata: { loc: [Object] }
},
Document {
pageContent: 'print("Hello, World!")',
metadata: { loc: [Object] }
},
Document {
pageContent: '# Call the function',
metadata: { loc: [Object] }
},
Document {
pageContent: 'hello_world()',
metadata: { loc: [Object] }
}
]
*/
HTML
​
import
{
RecursiveCharacterTextSplitter
}
from
"langchain/text_splitter"
;
const
text
=
`
<!DOCTYPE html>
<html>
<head>
<title>🦜️🔗 LangChain</title>
<style>
body {
font-family: Arial, sans-serif;
}
h1 {
color: darkblue;
}
</style>
</head>
<body>
<div>
<h1>🦜️🔗 LangChain</h1>
<p>⚡ Building applications with LLMs through composability ⚡</p>
</div>
<div>
As an open source project in a rapidly developing field, we are extremely open to contributions.
</div>
</body>
</html>
`
;
const
splitter
=
RecursiveCharacterTextSplitter
.
fromLanguage
(
"html"
,
{
chunkSize
:
175
,
chunkOverlap
:
20
,
}
)
;
const
output
=
await
splitter
.
createDocuments
(
[
text
]
)
;
console
.
log
(
output
)
;
/*
[
Document {
pageContent: '<!DOCTYPE html>\n<html>',
metadata: { loc: [Object] }
},
Document {
pageContent: '<head>\n    <title>🦜️🔗 LangChain</title>',
metadata: { loc: [Object] }
},
Document {
pageContent: '<style>\n' +
'      body {\n' +
'        font-family: Arial, sans-serif;\n' +
'      }\n' +
'      h1 {\n' +
'        color: darkblue;\n' +
'      }\n' +
'    </style>\n' +
'  </head>',
metadata: { loc: [Object] }
},
Document {
pageContent: '<body>\n' +
'    <div>\n' +
'      <h1>🦜️🔗 LangChain</h1>\n' +
'      <p>⚡ Building applications with LLMs through composability ⚡</p>\n' +
'    </div>',
metadata: { loc: [Object] }
},
Document {
pageContent: '<div>\n' +
'      As an open source project in a rapidly developing field, we are extremely open to contributions.\n' +
'    </div>\n' +
'  </body>\n' +
'</html>',
metadata: { loc: [Object] }
}
]
*/
Latex
​
import
{
RecursiveCharacterTextSplitter
}
from
"langchain/text_splitter"
;
const
text
=
`
\\begin{document}
\\title{🦜️🔗 LangChain}
⚡ Building applications with LLMs through composability ⚡
\\section{Quick Install}
\\begin{verbatim}
Hopefully this code block isn't split
yarn add langchain
\\end{verbatim}
As an open source project in a rapidly developing field, we are extremely open to contributions.
\\end{document}
`
;
const
splitter
=
RecursiveCharacterTextSplitter
.
fromLanguage
(
"latex"
,
{
chunkSize
:
100
,
chunkOverlap
:
0
,
}
)
;
const
output
=
await
splitter
.
createDocuments
(
[
text
]
)
;
console
.
log
(
output
)
;
/*
[
Document {
pageContent: '\\begin{document}\n' +
'\\title{🦜️🔗 LangChain}\n' +
'⚡ Building applications with LLMs through composability ⚡',
metadata: { loc: [Object] }
},
Document {
pageContent: '\\section{Quick Install}',
metadata: { loc: [Object] }
},
Document {
pageContent: '\\begin{verbatim}\n' +
"Hopefully this code block isn't split\n" +
'yarn add langchain\n' +
'\\end{verbatim}',
metadata: { loc: [Object] }
},
Document {
pageContent: 'As an open source project in a rapidly developing field, we are extremely open to contributions.',
metadata: { loc: [Object] }
},
Document {
pageContent: '\\end{document}',
metadata: { loc: [Object] }
}
]
*/



--- 文件: output_20250622_020018\docs\modules\indexes\text_splitters\examples\recursive_character.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/text_splitters/examples/recursive_character
crawled_at: 2025-06-22T02:00:23.622231
---

RecursiveCharacterTextSplitter
推荐使用的TextSplitter是“递归字符文本分割器”。它会通过不同的符号递归地分割文档-从“”开始，然后是“”，再然后是“ ”。这很好，因为它会尽可能地将所有语义相关的内容保持在同一位置。
这里需要了解的重要参数是'chunkSize'和'chunkOverlap'。'ChunkSize'控制最终文档的最大大小（以字符数为单位)。'ChunkOverlap'指定文档之间应该有多少重叠。这通常有助于确保文本不会被奇怪地分割。在下面的示例中，我们将这些值设为较小的值（仅作说明目的)，但在实践中它们默认为'4000'和'200'。
import
{
RecursiveCharacterTextSplitter
}
from
"langchain/text_splitter"
;
const
text
=
`
Hi.I'm Harrison.How? Are? You?Okay then f f f f.
This is a weird text to write, but gotta test the splittingggg some how.
Bye!-H.
`
;
const
splitter
=
new
RecursiveCharacterTextSplitter
(
{
chunkSize
:
10
,
chunkOverlap
:
1
,
}
)
;
const
output
=
await
splitter
.
createDocuments
(
[
text
]
)
;
请注意，在上面的示例中，我们正在分割原始文本字符串并返回文档列表。我们也可以直接分割文档。
import
{
Document
}
from
"langchain/document"
;
import
{
RecursiveCharacterTextSplitter
}
from
"langchain/text_splitter"
;
const
text
=
`
Hi.I'm Harrison.How? Are? You?Okay then f f f f.
This is a weird text to write, but gotta test the splittingggg some how.
Bye!-H.
`
;
const
splitter
=
new
RecursiveCharacterTextSplitter
(
{
chunkSize
:
10
,
chunkOverlap
:
1
,
}
)
;
const
docOutput
=
await
splitter
.
splitDocuments
(
[
new
Document
(
{
pageContent
:
text
}
)
,
]
)
;



--- 文件: output_20250622_020018\docs\modules\indexes\text_splitters\examples\token.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/text_splitters/examples/token
crawled_at: 2025-06-22T02:00:23.678746
---

TokenTextSplitter
最后，
TokenTextSplitter
将原始文本字符串转换为 BPE 标记，并将这些标记分成块，然后将单个块中的标记转换回文本。#（Finally)
import
{
Document
}
from
"langchain/document"
;
import
{
TokenTextSplitter
}
from
"langchain/text_splitter"
;
const
text
=
"foo bar baz 123"
;
const
splitter
=
new
TokenTextSplitter
(
{
encodingName
:
"gpt2"
,
chunkSize
:
10
,
chunkOverlap
:
0
,
}
)
;
const
output
=
await
splitter
.
createDocuments
(
[
text
]
)
;



--- 文件: output_20250622_020018\docs\modules\indexes\vector_stores\integrations\chroma.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/vector_stores/integrations/chroma
crawled_at: 2025-06-22T02:00:23.874988
---

chroma
换行符
Chroma（嵌入式的开源Apache 2.0数据库)
Chroma是一个开源的Apache 2.0嵌入式数据库。
设置
​
在计算机上使用Docker运行Chroma
文档
安装Chroma JS SDK。
npm
Yarn
pnpm
npm
install
-S chromadb
yarn
add
chromadb
pnpm
add
chromadb
使用，索引和查询文档
​
import
{
Chroma
}
from
"langchain/vectorstores/chroma"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
TextLoader
}
from
"langchain/document_loaders/fs/text"
;
// Create docs with a loader
const
loader
=
new
TextLoader
(
"src/document_loaders/example_data/example.txt"
)
;
const
docs
=
await
loader
.
load
(
)
;
// Create vector store and index the docs
const
vectorStore
=
await
Chroma
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
,
{
collectionName
:
"a-test-collection"
,
}
)
;
// Search for the most similar document
const
response
=
await
vectorStore
.
similaritySearch
(
"hello"
,
1
)
;
console
.
log
(
response
)
;
/*
[
Document {
pageContent: 'Foo\nBar\nBaz\n\n',
metadata: { source: 'src/document_loaders/example_data/example.txt' }
}
]
*/
使用，索引和查询文本
​
import
{
Chroma
}
from
"langchain/vectorstores/chroma"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
// text sample from Godel, Escher, Bach
const
vectorStore
=
await
Chroma
.
fromTexts
(
[
`
Tortoise: Labyrinth? Labyrinth? Could it Are we in the notorious Little
Harmonic Labyrinth of the dreaded Majotaur?
`
,
"Achilles: Yiikes! What is that?"
,
`
Tortoise: They say-although I person never believed it myself-that an I
Majotaur has created a tiny labyrinth sits in a pit in the middle of
it, waiting innocent victims to get lost in its fears complexity.
Then, when they wander and dazed into the center, he laughs and
laughs at them-so hard, that he laughs them to death!
`
,
"Achilles: Oh, no!"
,
"Tortoise: But it's only a myth. Courage, Achilles."
,
]
,
[
{
id
:
2
}
,
{
id
:
1
}
,
{
id
:
3
}
]
,
new
OpenAIEmbeddings
(
)
,
{
collectionName
:
"godel-escher-bach"
,
}
)
;
const
response
=
await
vectorStore
.
similaritySearch
(
"scared"
,
2
)
;
console
.
log
(
response
)
;
/*
[
Document { pageContent: 'Achilles: Oh, no!', metadata: {} },
Document {
pageContent: 'Achilles: Yiikes! What is that?',
metadata: { id: 1 }
}
]
*/
// You can also filter by metadata
const
filteredResponse
=
await
vectorStore
.
similaritySearch
(
"scared"
,
2
,
{
id
:
1
,
}
)
;
console
.
log
(
filteredResponse
)
;
/*
[
Document {
pageContent: 'Achilles: Yiikes! What is that?',
metadata: { id: 1 }
}
]
*/
使用，从现有集合查询文档
​
import
{
Chroma
}
from
"langchain/vectorstores/chroma"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
const
vectorStore
=
await
Chroma
.
fromExistingCollection
(
new
OpenAIEmbeddings
(
)
,
{
collectionName
:
"godel-escher-bach"
}
)
;
const
response
=
await
vectorStore
.
similaritySearch
(
"scared"
,
2
)
;
console
.
log
(
response
)
;
/*
[
Document { pageContent: 'Achilles: Oh, no!', metadata: {} },
Document {
pageContent: 'Achilles: Yiikes! What is that?',
metadata: { id: 1 }
}
]
*/



--- 文件: output_20250622_020018\docs\modules\indexes\vector_stores\integrations\faiss.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/vector_stores/integrations/faiss
crawled_at: 2025-06-22T02:00:23.925446
---

Faiss
兼容性
仅适用于Node.js环境。
Faiss
是用于稠密向量的高效相似度搜索和聚类的库。
Langchainjs支持使用Faiss作为向量库，并可将其保存到文件。同时，它还提供从
Python实现
读取保存的文件的功能。
安装
​
安装
faiss-node
,它是
Faiss
的Node.js绑定。
npm
Yarn
pnpm
npm
install
-S faiss-node
yarn
add
faiss-node
pnpm
add
faiss-node
要启用从
Python实现
读取保存的文件的功能，还需要安装
pickleparser
。
npm
Yarn
pnpm
npm
install
-S pickleparser
yarn
add
pickleparser
pnpm
add
pickleparser
使用
​
从文本创建新索引
​
import
{
FaissStore
}
from
"langchain/vectorstores/faiss"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
export
const
run
=
async
(
)
=>
{
const
vectorStore
=
await
FaissStore
.
fromTexts
(
[
"Hello world"
,
"Bye bye"
,
"hello nice world"
]
,
[
{
id
:
2
}
,
{
id
:
1
}
,
{
id
:
3
}
]
,
new
OpenAIEmbeddings
(
)
)
;
const
resultOne
=
await
vectorStore
.
similaritySearch
(
"hello world"
,
1
)
;
console
.
log
(
resultOne
)
;
}
;
从加载器创建新索引
​
import
{
FaissStore
}
from
"langchain/vectorstores/faiss"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
TextLoader
}
from
"langchain/document_loaders/fs/text"
;
// Create docs with a loader
const
loader
=
new
TextLoader
(
"src/document_loaders/example_data/example.txt"
)
;
const
docs
=
await
loader
.
load
(
)
;
// Load the docs into the vector store
const
vectorStore
=
await
FaissStore
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
)
;
// Search for the most similar document
const
resultOne
=
await
vectorStore
.
similaritySearch
(
"hello world"
,
1
)
;
console
.
log
(
resultOne
)
;
将索引保存到文件并再次加载
​
import
{
FaissStore
}
from
"langchain/vectorstores/faiss"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
// Create a vector store through any method, here from texts as an example
const
vectorStore
=
await
FaissStore
.
fromTexts
(
[
"Hello world"
,
"Bye bye"
,
"hello nice world"
]
,
[
{
id
:
2
}
,
{
id
:
1
}
,
{
id
:
3
}
]
,
new
OpenAIEmbeddings
(
)
)
;
// Save the vector store to a directory
const
directory
=
"your/directory/here"
;
await
vectorStore
.
save
(
directory
)
;
// Load the vector store from the same directory
const
loadedVectorStore
=
await
FaissStore
.
load
(
directory
,
new
OpenAIEmbeddings
(
)
)
;
// vectorStore and loadedVectorStore are identical
const
result
=
await
loadedVectorStore
.
similaritySearch
(
"hello world"
,
1
)
;
console
.
log
(
result
)
;
从
Python实现
中加载保存的文件
​
import
{
FaissStore
}
from
"langchain/vectorstores/faiss"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
// The directory of data saved from Python
const
directory
=
"your/directory/here"
;
// Load the vector store from the directory
const
loadedVectorStore
=
await
FaissStore
.
loadFromPython
(
directory
,
new
OpenAIEmbeddings
(
)
)
;
// Search for the most similar document
const
result
=
await
loadedVectorStore
.
similaritySearch
(
"test"
,
2
)
;
console
.
log
(
"result"
,
result
)
;



--- 文件: output_20250622_020018\docs\modules\indexes\vector_stores\integrations\hnswlib.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/vector_stores/integrations/hnswlib
crawled_at: 2025-06-22T02:00:24.333772
---

HNSWLib
兼容性
仅适用于Node.js。
HNSWLib是一个内存向量存储器，可以保存到文件中。它使用
HNSWLib
。
设置
​
:::注意
在Windows上
，你可能需要先安装
Visual Studio
才能正确构建
hnswlib-node
包。
:::
您可以通过以下方式进行安装
npm
Yarn
pnpm
npm
install
hnswlib-node
yarn
add
hnswlib-node
pnpm
add
hnswlib-node
用法
​
从文本创建新索引
​
import
{
HNSWLib
}
from
"langchain/vectorstores/hnswlib"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
const
vectorStore
=
await
HNSWLib
.
fromTexts
(
[
"Hello world"
,
"Bye bye"
,
"hello nice world"
]
,
[
{
id
:
2
}
,
{
id
:
1
}
,
{
id
:
3
}
]
,
new
OpenAIEmbeddings
(
)
)
;
const
resultOne
=
await
vectorStore
.
similaritySearch
(
"hello world"
,
1
)
;
console
.
log
(
resultOne
)
;
从加载器创建新索引
​
import
{
HNSWLib
}
from
"langchain/vectorstores/hnswlib"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
TextLoader
}
from
"langchain/document_loaders/fs/text"
;
// Create docs with a loader
const
loader
=
new
TextLoader
(
"src/document_loaders/example_data/example.txt"
)
;
const
docs
=
await
loader
.
load
(
)
;
// Load the docs into the vector store
const
vectorStore
=
await
HNSWLib
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
)
;
// Search for the most similar document
const
result
=
await
vectorStore
.
similaritySearch
(
"hello world"
,
1
)
;
console
.
log
(
result
)
;
将索引保存到文件并重新加载
​
import
{
HNSWLib
}
from
"langchain/vectorstores/hnswlib"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
// Create a vector store through any method, here from texts as an example
const
vectorStore
=
await
HNSWLib
.
fromTexts
(
[
"Hello world"
,
"Bye bye"
,
"hello nice world"
]
,
[
{
id
:
2
}
,
{
id
:
1
}
,
{
id
:
3
}
]
,
new
OpenAIEmbeddings
(
)
)
;
// Save the vector store to a directory
const
directory
=
"your/directory/here"
;
await
vectorStore
.
save
(
directory
)
;
// Load the vector store from the same directory
const
loadedVectorStore
=
await
HNSWLib
.
load
(
directory
,
new
OpenAIEmbeddings
(
)
)
;
// vectorStore and loadedVectorStore are identical
const
result
=
await
loadedVectorStore
.
similaritySearch
(
"hello world"
,
1
)
;
console
.
log
(
result
)
;
过滤文档
​
import
{
HNSWLib
}
from
"langchain/vectorstores/hnswlib"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
const
vectorStore
=
await
HNSWLib
.
fromTexts
(
[
"Hello world"
,
"Bye bye"
,
"hello nice world"
]
,
[
{
id
:
2
}
,
{
id
:
1
}
,
{
id
:
3
}
]
,
new
OpenAIEmbeddings
(
)
)
;
const
result
=
await
vectorStore
.
similaritySearch
(
"hello world"
,
10
,
(
document
)
=>
document
.
metadata
.
id
===
3
)
;
// only "hello nice world" will be returned
console
.
log
(
result
)
;



--- 文件: output_20250622_020018\docs\modules\indexes\vector_stores\integrations\memory.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/vector_stores/integrations/memory
crawled_at: 2025-06-22T02:00:23.957393
---

MemoryVectorStore
MemoryVectorStore是一个内存中的暂存向量存储器，用于在内存中存储嵌入，并做精确的线性搜索以找到最相似的嵌入。默认的相似度度量是余弦相似度，但可以更改为
ml-distance
支持的任何相似度度量方式。
用法
​
从文本创建新索引
​
import
{
MemoryVectorStore
}
from
"langchain/vectorstores/memory"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
export
const
run
=
async
(
)
=>
{
const
vectorStore
=
await
MemoryVectorStore
.
fromTexts
(
[
"Hello world"
,
"Bye bye"
,
"hello nice world"
]
,
[
{
id
:
2
}
,
{
id
:
1
}
,
{
id
:
3
}
]
,
new
OpenAIEmbeddings
(
)
)
;
const
resultOne
=
await
vectorStore
.
similaritySearch
(
"hello world"
,
1
)
;
console
.
log
(
resultOne
)
;
}
;
从加载程序创建新索引
​
import
{
MemoryVectorStore
}
from
"langchain/vectorstores/memory"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
TextLoader
}
from
"langchain/document_loaders/fs/text"
;
export
const
run
=
async
(
)
=>
{
// Create docs with a loader
const
loader
=
new
TextLoader
(
"src/document_loaders/example_data/example.txt"
)
;
const
docs
=
await
loader
.
load
(
)
;
// Load the docs into the vector store
const
vectorStore
=
await
MemoryVectorStore
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
)
;
// Search for the most similar document
const
resultOne
=
await
vectorStore
.
similaritySearch
(
"hello world"
,
1
)
;
console
.
log
(
resultOne
)
;
}
;
使用自定义相似度度量
​
import
{
MemoryVectorStore
}
from
"langchain/vectorstores/memory"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
similarity
}
from
"ml-distance"
;
export
const
run
=
async
(
)
=>
{
const
vectorStore
=
await
MemoryVectorStore
.
fromTexts
(
[
"Hello world"
,
"Bye bye"
,
"hello nice world"
]
,
[
{
id
:
2
}
,
{
id
:
1
}
,
{
id
:
3
}
]
,
new
OpenAIEmbeddings
(
)
,
{
similarity
:
similarity
.
pearson
}
)
;
const
resultOne
=
await
vectorStore
.
similaritySearch
(
"hello world"
,
1
)
;
console
.
log
(
resultOne
)
;
}
;



--- 文件: output_20250622_020018\docs\modules\indexes\vector_stores\integrations\milvus.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/vector_stores/integrations/milvus
crawled_at: 2025-06-22T02:00:24.081857
---

Milvus
Milvus
是专为嵌入式相似性搜索和 AI 应用而构建的向量数据库。
兼容性
仅可在 Node.js 上使用。
安装
​
在计算机上使用 Docker 运行 Milvus 实例
文档
安装 Milvus Node.js SDK。
npm
Yarn
pnpm
npm
install
-S @zilliz/milvus2-sdk-node
yarn
add
@zilliz/milvus2-sdk-node
pnpm
add
@zilliz/milvus2-sdk-node
在运行代码之前设置 Milvus 的环境变量
3.1 OpenAI
export
OPENAI_API_KEY
=
YOUR_OPENAI_API_KEY_HERE
export
MILVUS_URL
=
YOUR_MILVUS_URL_HERE
# for example http://localhost:19530
3.2 Azure OpenAI
export
AZURE_OPENAI_API_KEY
=
YOUR_AZURE_OPENAI_API_KEY_HERE
export
AZURE_OPENAI_API_INSTANCE_NAME
=
YOUR_AZURE_OPENAI_INSTANCE_NAME_HERE
export
AZURE_OPENAI_API_DEPLOYMENT_NAME
=
YOUR_AZURE_OPENAI_DEPLOYMENT_NAME_HERE
export
AZURE_OPENAI_API_COMPLETIONS_DEPLOYMENT_NAME
=
YOUR_AZURE_OPENAI_COMPLETIONS_DEPLOYMENT_NAME_HERE
export
AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME
=
YOUR_AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT_NAME_HERE
export
AZURE_OPENAI_API_VERSION
=
YOUR_AZURE_OPENAI_API_VERSION_HERE
export
MILVUS_URL
=
YOUR_MILVUS_URL_HERE
# for example http://localhost:19530
索引和查询文档
​
import
{
Milvus
}
from
"langchain/vectorstores/milvus"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
// text sample from Godel, Escher, Bach
const
vectorStore
=
await
Milvus
.
fromTexts
(
[
"Tortoise
:
Labyrinth
?
Labyrinth
?
Could it Are we
in
the notorious Little\
Harmonic Labyrinth
of
the dreaded Majotaur
?
"
,
"Achilles: Yiikes! What is that?"
,
"Tortoise
:
They say
-
although
I
person
never
believed it myself
-
that an
I
\
Majotaur has created a tiny labyrinth sits
in
a pit
in
the middle
of
\
it
,
waiting innocent victims to
get
lost
in
its fears complexity
.
\
Then
,
when they wander and dazed into the center
,
he laughs and\
laughs at them
-
so hard
,
that he laughs them to death
!
"
,
"Achilles: Oh, no!"
,
"Tortoise: But it's only a myth. Courage, Achilles."
,
]
,
[
{
id
:
2
}
,
{
id
:
1
}
,
{
id
:
3
}
,
{
id
:
4
}
,
{
id
:
5
}
]
,
new
OpenAIEmbeddings
(
)
,
{
collectionName
:
"goldel_escher_bach"
,
}
)
;
// or alternatively from docs
const
vectorStore
=
await
Milvus
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
,
{
collectionName
:
"goldel_escher_bach"
,
}
)
;
const
response
=
await
vectorStore
.
similaritySearch
(
"scared"
,
2
)
;
查询现有集合的文档
​
import
{
Milvus
}
from
"langchain/vectorstores/milvus"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
const
vectorStore
=
await
Milvus
.
fromExistingCollection
(
new
OpenAIEmbeddings
(
)
,
{
collectionName
:
"goldel_escher_bach"
,
}
)
;
const
response
=
await
vectorStore
.
similaritySearch
(
"scared"
,
2
)
;



--- 文件: output_20250622_020018\docs\modules\indexes\vector_stores\integrations\myscale.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/vector_stores/integrations/myscale
crawled_at: 2025-06-22T02:00:24.270407
---

MyScale
兼容性
仅在Node.js上可用。
MyScale
是一款新兴的人工智能数据库，将向量搜索和SQL分析的强大功能相结合，提供管理、高效、响应迅速的体验。
安装
​
通过
MyScale Web控制台
启动集群。更多信息请参见
MyScale官方文档
。
启动集群后，请从集群的“操作”菜单中查看您的“连接详细信息”。您需要主机名、端口、用户名和密码。
在您的工作区中安装所需的Node.js版本。
npm
Yarn
pnpm
npm
install
-S @clickhouse/client
yarn
add
@clickhouse/client
pnpm
add
@clickhouse/client
索引与查询文档
​
import
{
MyScaleStore
}
from
"langchain/vectorstores/myscale"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
const
vectorStore
=
await
MyScaleStore
.
fromTexts
(
[
"Hello world"
,
"Bye bye"
,
"hello nice world"
]
,
[
{
id
:
2
,
name
:
"2"
}
,
{
id
:
1
,
name
:
"1"
}
,
{
id
:
3
,
name
:
"3"
}
,
]
,
new
OpenAIEmbeddings
(
)
,
{
host
:
process
.
env
.
MYSCALE_HOST
||
"localhost"
,
port
:
process
.
env
.
MYSCALE_PORT
||
"8443"
,
username
:
process
.
env
.
MYSCALE_USERNAME
||
"username"
,
password
:
process
.
env
.
MYSCALE_PASSWORD
||
"password"
,
}
)
;
const
results
=
await
vectorStore
.
similaritySearch
(
"hello world"
,
1
)
;
console
.
log
(
results
)
;
const
filteredResults
=
await
vectorStore
.
similaritySearch
(
"hello world"
,
1
,
{
whereStr
:
"metadata.name = '1'"
,
}
)
;
console
.
log
(
filteredResults
)
;
从现有集合中查询文档
​
import
{
MyScaleStore
}
from
"langchain/vectorstores/myscale"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
const
vectorStore
=
await
MyScaleStore
.
fromExistingIndex
(
new
OpenAIEmbeddings
(
)
,
{
host
:
process
.
env
.
MYSCALE_HOST
||
"localhost"
,
port
:
process
.
env
.
MYSCALE_PORT
||
"8443"
,
username
:
process
.
env
.
MYSCALE_USERNAME
||
"username"
,
password
:
process
.
env
.
MYSCALE_PASSWORD
||
"password"
,
database
:
"your_database"
,
// defaults to "default"
table
:
"your_table"
,
// defaults to "vector_table"
}
)
;
const
results
=
await
vectorStore
.
similaritySearch
(
"hello world"
,
1
)
;
console
.
log
(
results
)
;
const
filteredResults
=
await
vectorStore
.
similaritySearch
(
"hello world"
,
1
,
{
whereStr
:
"metadata.name = '1'"
,
}
)
;
console
.
log
(
filteredResults
)
;



--- 文件: output_20250622_020018\docs\modules\indexes\vector_stores\integrations\opensearch.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/vector_stores/integrations/opensearch
crawled_at: 2025-06-22T02:00:24.302709
---

OpenSearch
兼容性
仅限于 Node.js。
OpenSearch
是完全兼容 Elasticsearch API 的 Elasticsearch 分支。在此处阅读有关其支持近似最近邻的更多信息
here
。
Langchain.js 将
@opensearch-project/opensearch
作为 OpenSearch vectorstore 的客户端。
设置
​
npm
Yarn
pnpm
npm
install
-S @opensearch-project/opensearch
yarn
add
@opensearch-project/opensearch
pnpm
add
@opensearch-project/opensearch
您还需要运行一个 OpenSearch 实例。您可以使用
官方 Docker 映像
来开始使用。您还可以在
此处
找到示例 docker-compose 文件。
索引文档
​
import
{
Client
}
from
"@opensearch-project/opensearch"
;
import
{
Document
}
from
"langchain/document"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
OpenSearchVectorStore
}
from
"langchain/vectorstores/opensearch"
;
const
client
=
new
Client
(
{
nodes
:
[
process
.
env
.
OPENSEARCH_URL
??
"http://127.0.0.1:9200"
]
,
}
)
;
const
docs
=
[
new
Document
(
{
metadata
:
{
foo
:
"bar"
}
,
pageContent
:
"opensearch is also a vector db"
,
}
)
,
new
Document
(
{
metadata
:
{
foo
:
"bar"
}
,
pageContent
:
"the quick brown fox jumped over the lazy dog"
,
}
)
,
new
Document
(
{
metadata
:
{
baz
:
"qux"
}
,
pageContent
:
"lorem ipsum dolor sit amet"
,
}
)
,
new
Document
(
{
metadata
:
{
baz
:
"qux"
}
,
pageContent
:
"OpenSearch is a scalable, flexible, and extensible open-source software suite for search, analytics, and observability applications"
,
}
)
,
]
;
await
OpenSearchVectorStore
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
,
{
client
,
indexName
:
process
.
env
.
OPENSEARCH_INDEX
,
// Will default to `documents`
}
)
;
查询文档
​
import
{
Client
}
from
"@opensearch-project/opensearch"
;
import
{
VectorDBQAChain
}
from
"langchain/chains"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
OpenSearchVectorStore
}
from
"langchain/vectorstores/opensearch"
;
const
client
=
new
Client
(
{
nodes
:
[
process
.
env
.
OPENSEARCH_URL
??
"http://127.0.0.1:9200"
]
,
}
)
;
const
vectorStore
=
new
OpenSearchVectorStore
(
new
OpenAIEmbeddings
(
)
,
{
client
,
}
)
;
/* Search the vector DB independently with meta filters */
const
results
=
await
vectorStore
.
similaritySearch
(
"hello world"
,
1
)
;
console
.
log
(
JSON
.
stringify
(
results
,
null
,
2
)
)
;
/* [
{
"pageContent": "Hello world",
"metadata": {
"id": 2
}
}
] */
/* Use as part of a chain (currently no metadata filters) */
const
model
=
new
OpenAI
(
)
;
const
chain
=
VectorDBQAChain
.
fromLLM
(
model
,
vectorStore
,
{
k
:
1
,
returnSourceDocuments
:
true
,
}
)
;
const
response
=
await
chain
.
call
(
{
query
:
"What is opensearch?"
}
)
;
console
.
log
(
JSON
.
stringify
(
response
,
null
,
2
)
)
;
/*
{
"text": " Opensearch is a collection of technologies that allow search engines to publish search results in a standard format, making it easier for users to search across multiple sites.",
"sourceDocuments": [
{
"pageContent": "What's this?",
"metadata": {
"id": 3
}
}
]
}
*/



--- 文件: output_20250622_020018\docs\modules\indexes\vector_stores\integrations\pinecone.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/vector_stores/integrations/pinecone
crawled_at: 2025-06-22T02:00:24.469147
---

Pinecone
Compatibility
仅适用于 Node.js。
Langchain.js 将
@pinecone-database/pinecone
作为 Pinecone 向量存储的客户端。使用以下命令安装库：
npm
Yarn
pnpm
npm
install
-S dotenv langchain @pinecone-database/pinecone
yarn
add
dotenv langchain @pinecone-database/pinecone
pnpm
add
dotenv langchain @pinecone-database/pinecone
索引文档
​
import
{
PineconeClient
}
from
"@pinecone-database/pinecone"
;
import
*
as
dotenv
from
"dotenv"
;
import
{
Document
}
from
"langchain/document"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
PineconeStore
}
from
"langchain/vectorstores/pinecone"
;
dotenv
.
config
(
)
;
const
client
=
new
PineconeClient
(
)
;
await
client
.
init
(
{
apiKey
:
process
.
env
.
PINECONE_API_KEY
,
environment
:
process
.
env
.
PINECONE_ENVIRONMENT
,
}
)
;
const
pineconeIndex
=
client
.
Index
(
process
.
env
.
PINECONE_INDEX
)
;
const
docs
=
[
new
Document
(
{
metadata
:
{
foo
:
"bar"
}
,
pageContent
:
"pinecone is a vector db"
,
}
)
,
new
Document
(
{
metadata
:
{
foo
:
"bar"
}
,
pageContent
:
"the quick brown fox jumped over the lazy dog"
,
}
)
,
new
Document
(
{
metadata
:
{
baz
:
"qux"
}
,
pageContent
:
"lorem ipsum dolor sit amet"
,
}
)
,
new
Document
(
{
metadata
:
{
baz
:
"qux"
}
,
pageContent
:
"pinecones are the woody fruiting body and of a pine tree"
,
}
)
,
]
;
await
PineconeStore
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
,
{
pineconeIndex
,
}
)
;
查询文档
​
import
{
PineconeClient
}
from
"@pinecone-database/pinecone"
;
import
*
as
dotenv
from
"dotenv"
;
import
{
VectorDBQAChain
}
from
"langchain/chains"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
PineconeStore
}
from
"langchain/vectorstores/pinecone"
;
dotenv
.
config
(
)
;
const
client
=
new
PineconeClient
(
)
;
await
client
.
init
(
{
apiKey
:
process
.
env
.
PINECONE_API_KEY
,
environment
:
process
.
env
.
PINECONE_ENVIRONMENT
,
}
)
;
const
pineconeIndex
=
client
.
Index
(
process
.
env
.
PINECONE_INDEX
)
;
const
vectorStore
=
await
PineconeStore
.
fromExistingIndex
(
new
OpenAIEmbeddings
(
)
,
{
pineconeIndex
}
)
;
/* Search the vector DB independently with meta filters */
const
results
=
await
vectorStore
.
similaritySearch
(
"pinecone"
,
1
,
{
foo
:
"bar"
,
}
)
;
console
.
log
(
results
)
;
/*
[
Document {
pageContent: 'pinecone is a vector db',
metadata: { foo: 'bar' }
}
]
*/
/* Use as part of a chain (currently no metadata filters) */
const
model
=
new
OpenAI
(
)
;
const
chain
=
VectorDBQAChain
.
fromLLM
(
model
,
vectorStore
,
{
k
:
1
,
returnSourceDocuments
:
true
,
}
)
;
const
response
=
await
chain
.
call
(
{
query
:
"What is pinecone?"
}
)
;
console
.
log
(
response
)
;
/*
{
text: ' A pinecone is the woody fruiting body of a pine tree.',
sourceDocuments: [
Document {
pageContent: 'pinecones are the woody fruiting body and of a pine tree',
metadata: [Object]
}
]
}
*/



--- 文件: output_20250622_020018\docs\modules\indexes\vector_stores\integrations\prisma.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/vector_stores/integrations/prisma
crawled_at: 2025-06-22T02:00:24.486026
---

prisma
sidebar_label: 仅适用于 node
​
Prisma
Langchain 支持使用
Prisma
与 PostgreSQL 和
pgvector
Postgres 扩展来增强 PostgreSQL 数据库中的现有模型的向量搜索。
设置
​
使用 Supabase 设置数据库实例
​
请参阅
Prisma 和 Supabase 集成指南
来设置 Supabase 和 Prisma 的新数据库实例。
安装 Prisma
​
npm
Yarn
pnpm
npm
install
prisma
yarn
add
prisma
pnpm
add
prisma
使用
docker-compose
设置
pgvector
自托管实例
​
pgvector
提供了一个预构建的 Docker 映像，可用于快速设置自托管的 Postgres 实例。
services
:
db
:
image
:
ankane/pgvector
ports
:
-
5432
:
5432
volumes
:
-
db
:
/var/lib/postgresql/data
environment
:
-
POSTGRES_PASSWORD=
-
POSTGRES_USER=
-
POSTGRES_DB=
volumes
:
db
:
创建新模型
​
Create a new schema
​
假设您还没有创建一个模型，使用类型为
Unsupported("vector")
的
vector
字段创建一个新模型:
model Document {
id      String                 @id @default(cuid())
content String
vector  Unsupported("vector")?
}
然后，使用
--create-only
创建新的迁移，以避免直接运行迁移。
Afterwards, create a new migration with
--create-only
to avoid running the migration directly.
npm
Yarn
pnpm
```bash npm2yarn
npx prisma migrate dev --create-only
```bash
npm
undefined
# couldn't auto-convert command2yarn
npx prisma migrate dev --create-only
```bash
npm
undefined
# couldn't auto-convert command2yarn
npx prisma migrate dev --create-only
添加以下行到新创建的迁移，以启用
pgvector
扩展，如果它尚未被启用:
Add the following line to the newly created migration to enable
pgvector
extension if it hasn't been enabled yet:
CREATE
EXTENSION
IF
NOT
EXISTS
vector
;
然后运行迁移:
Run the migration afterwards:
npm
Yarn
pnpm
```bash npm2yarn
npx prisma migrate dev
```bash
npm
undefined
# couldn't auto-convert command2yarn
npx prisma migrate dev
```bash
npm
undefined
# couldn't auto-convert command2yarn
npx prisma migrate dev
使用
​
:::警告
表名和列名（例如
tableName
、
vectorColumnName
、
columns
和
filter
中的字段)直接传递到 SQL 查询中，没有参数化。这些字段必须在使用前进行净化以避免 SQL 注入。
These fields must be sanitized beforehand to avoid SQL injection.
:::
import
{
PrismaVectorStore
}
from
"langchain/vectorstores/prisma"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
PrismaClient
,
Prisma
,
Document
}
from
"@prisma/client"
;
export
const
run
=
async
(
)
=>
{
const
db
=
new
PrismaClient
(
)
;
// Use the `withModel` method to get proper type hints for `metadata` field:
const
vectorStore
=
PrismaVectorStore
.
withModel
<
Document
>
(
db
)
.
create
(
new
OpenAIEmbeddings
(
)
,
{
prisma
:
Prisma
,
tableName
:
"Document"
,
vectorColumnName
:
"vector"
,
columns
:
{
id
:
PrismaVectorStore
.
IdColumn
,
content
:
PrismaVectorStore
.
ContentColumn
,
}
,
}
)
;
const
texts
=
[
"Hello world"
,
"Bye bye"
,
"What's this?"
]
;
await
vectorStore
.
addModels
(
await
db
.
$transaction
(
texts
.
map
(
(
content
)
=>
db
.
document
.
create
(
{
data
:
{
content
}
}
)
)
)
)
;
const
resultOne
=
await
vectorStore
.
similaritySearch
(
"Hello world"
,
1
)
;
console
.
log
(
resultOne
)
;
// create an instance with default filter
const
vectorStore2
=
PrismaVectorStore
.
withModel
<
Document
>
(
db
)
.
create
(
new
OpenAIEmbeddings
(
)
,
{
prisma
:
Prisma
,
tableName
:
"Document"
,
vectorColumnName
:
"vector"
,
columns
:
{
id
:
PrismaVectorStore
.
IdColumn
,
content
:
PrismaVectorStore
.
ContentColumn
,
}
,
filter
:
{
content
:
{
equals
:
"default"
,
}
,
}
,
}
)
;
await
vectorStore2
.
addModels
(
await
db
.
$transaction
(
texts
.
map
(
(
content
)
=>
db
.
document
.
create
(
{
data
:
{
content
}
}
)
)
)
)
;
// Use the default filter a.k.a {"content": "default"}
const
resultTwo
=
await
vectorStore
.
similaritySearch
(
"Hello world"
,
1
)
;
console
.
log
(
resultTwo
)
;
// Override the local filter
const
resultThree
=
await
vectorStore
.
similaritySearchWithScore
(
"Hello world"
,
1
,
{
content
:
{
equals
:
"different_content"
}
}
)
;
console
.
log
(
resultThree
)
;
}
;
上述示例使用以下模式:：
// This is your Prisma schema file,
// learn more about it in the docs: https://pris.ly/d/prisma-schema
generator client {
provider = "prisma-client-js"
}
datasource db {
provider = "postgresql"
url      = env("DATABASE_URL")
}
model Document {
id        String                 @id @default(cuid())
content   String
namespace String?                @default("default")
vector    Unsupported("vector")?
}
如果不需要，你可以删除
namespace
。



--- 文件: output_20250622_020018\docs\modules\indexes\vector_stores\integrations\qdrant.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/vector_stores/integrations/qdrant
crawled_at: 2025-06-22T02:00:24.408467
---

Qdrant
Qdrant
是一个向量相似度搜索引擎。它提供了一个方便的API来存储、搜索和管理带有附加有效负载的点 - 向量。
兼容性
仅适用于Node.js。
配置
​
根据
Qdrant 设置说明
在您的计算机上使用Docker运行Qdrant实例。
安装Qdrant Node.js SDK。
npm
Yarn
pnpm
npm
install
-S @qdrant/js-client-rest
yarn
add
@qdrant/js-client-rest
pnpm
add
@qdrant/js-client-rest
运行代码前为Qdrant设置Env变量。
3.1 OpenAI
export
OPENAI_API_KEY
=
YOUR_OPENAI_API_KEY_HERE
export
QDRANT_URL
=
YOUR_QDRANT_URL_HERE
# for example http://localhost:6333
3.2 Azure OpenAI
export
AZURE_OPENAI_API_KEY
=
YOUR_AZURE_OPENAI_API_KEY_HERE
export
AZURE_OPENAI_API_INSTANCE_NAME
=
YOUR_AZURE_OPENAI_INSTANCE_NAME_HERE
export
AZURE_OPENAI_API_DEPLOYMENT_NAME
=
YOUR_AZURE_OPENAI_DEPLOYMENT_NAME_HERE
export
AZURE_OPENAI_API_COMPLETIONS_DEPLOYMENT_NAME
=
YOUR_AZURE_OPENAI_COMPLETIONS_DEPLOYMENT_NAME_HERE
export
AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME
=
YOUR_AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT_NAME_HERE
export
AZURE_OPENAI_API_VERSION
=
YOUR_AZURE_OPENAI_API_VERSION_HERE
export
QDRANT_URL
=
YOUR_QDRANT_URL_HERE
# for example http://localhost:6333
用法
​
从文本中创建新的索引
​
import
{
QdrantVectorStore
}
from
"langchain/vectorstores/qdrant"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
// text sample from Godel, Escher, Bach
const
vectorStore
=
await
QdrantVectorStore
.
fromTexts
(
[
`
Tortoise: Labyrinth? Labyrinth? Could it Are we in the notorious Little
Harmonic Labyrinth of the dreaded Majotaur?
`
,
`
Achilles: Yiikes! What is that?
`
,
`
Tortoise: They say-although I person never believed it myself-that an I
Majotaur has created a tiny labyrinth sits in a pit in the middle of
it, waiting innocent victims to get lost in its fears complexity.
Then, when they wander and dazed into the center, he laughs and
laughs at them-so hard, that he laughs them to death!
`
,
`
Achilles: Oh, no!
`
,
`
Tortoise: But it's only a myth. Courage, Achilles.
`
,
]
,
[
{
id
:
2
}
,
{
id
:
1
}
,
{
id
:
3
}
,
{
id
:
4
}
,
{
id
:
5
}
]
,
new
OpenAIEmbeddings
(
)
,
{
url
:
process
.
env
.
QDRANT_URL
,
collectionName
:
"goldel_escher_bach"
,
}
)
;
const
response
=
await
vectorStore
.
similaritySearch
(
"scared"
,
2
)
;
console
.
log
(
response
)
;
/*
[
Document { pageContent: 'Achilles: Oh, no!', metadata: {} },
Document {
pageContent: 'Achilles: Yiikes! What is that?',
metadata: { id: 1 }
}
]
*/
从文档中创建新的索引
​
import
{
QdrantVectorStore
}
from
"langchain/vectorstores/qdrant"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
TextLoader
}
from
"langchain/document_loaders/fs/text"
;
// Create docs with a loader
const
loader
=
new
TextLoader
(
"src/document_loaders/example_data/example.txt"
)
;
const
docs
=
await
loader
.
load
(
)
;
const
vectorStore
=
await
QdrantVectorStore
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
,
{
url
:
process
.
env
.
QDRANT_URL
,
collectionName
:
"a_test_collection"
,
}
)
;
// Search for the most similar document
const
response
=
await
vectorStore
.
similaritySearch
(
"hello"
,
1
)
;
console
.
log
(
response
)
;
/*
[
Document {
pageContent: 'Foo\nBar\nBaz\n\n',
metadata: { source: 'src/document_loaders/example_data/example.txt' }
}
]
*/
从现有集合查询文档
​
import
{
QdrantVectorStore
}
from
"langchain/vectorstores/qdrant"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
const
vectorStore
=
await
QdrantVectorStore
.
fromExistingCollection
(
new
OpenAIEmbeddings
(
)
,
{
url
:
process
.
env
.
QDRANT_URL
,
collectionName
:
"goldel_escher_bach"
,
}
)
;
const
response
=
await
vectorStore
.
similaritySearch
(
"scared"
,
2
)
;
console
.
log
(
response
)
;
/*
[
Document { pageContent: 'Achilles: Oh, no!', metadata: {} },
Document {
pageContent: 'Achilles: Yiikes! What is that?',
metadata: { id: 1 }
}
]
*/



--- 文件: output_20250622_020018\docs\modules\indexes\vector_stores\integrations\redis.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/vector_stores/integrations/redis
crawled_at: 2025-06-22T02:00:24.523618
---

Redis
Redis
是一款快速的开源内存数据存储系统。，作为
Redis Stack
的一部分，
RediSearch
是一种支持向量相似性语义搜索以及其他许多类型搜索的模块。
兼容性
只支持在Node.js上使用。
LangChain.js接受
node-redis
作为Redis矢量存储的客户端。
设置
​
根据
文档
在计算机上使用Docker运行Redis。
安装node-redis JS客户端
npm
Yarn
pnpm
npm
install
-S redis
yarn
add
redis
pnpm
add
redis
索引文档
​
import
{
createClient
,
createCluster
}
from
"redis"
;
import
{
Document
}
from
"langchain/document"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
RedisVectorStore
}
from
"langchain/vectorstores/redis"
;
const
client
=
createClient
(
{
url
:
process
.
env
.
REDIS_URL
??
"redis://localhost:6379"
,
}
)
;
await
client
.
connect
(
)
;
const
docs
=
[
new
Document
(
{
metadata
:
{
foo
:
"bar"
}
,
pageContent
:
"redis is fast"
,
}
)
,
new
Document
(
{
metadata
:
{
foo
:
"bar"
}
,
pageContent
:
"the quick brown fox jumped over the lazy dog"
,
}
)
,
new
Document
(
{
metadata
:
{
baz
:
"qux"
}
,
pageContent
:
"lorem ipsum dolor sit amet"
,
}
)
,
new
Document
(
{
metadata
:
{
baz
:
"qux"
}
,
pageContent
:
"consectetur adipiscing elit"
,
}
)
,
]
;
const
vectorStore
=
await
RedisVectorStore
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
,
{
redisClient
:
client
,
indexName
:
"docs"
,
}
)
;
await
client
.
disconnect
(
)
;
查询文档
​
import
{
createClient
}
from
"redis"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
RetrievalQAChain
}
from
"langchain/chains"
;
import
{
RedisVectorStore
}
from
"langchain/vectorstores/redis"
;
const
client
=
createClient
(
{
url
:
process
.
env
.
REDIS_URL
??
"redis://localhost:6379"
,
}
)
;
await
client
.
connect
(
)
;
const
vectorStore
=
new
RedisVectorStore
(
new
OpenAIEmbeddings
(
)
,
{
redisClient
:
client
,
indexName
:
"docs"
,
}
)
;
/* Simple standalone search in the vector DB */
const
simpleRes
=
await
vectorStore
.
similaritySearch
(
"redis"
,
1
)
;
console
.
log
(
simpleRes
)
;
/*
[
Document {
pageContent: "redis is fast",
metadata: { foo: "bar" }
}
]
*/
/* Search in the vector DB using filters */
const
filterRes
=
await
vectorStore
.
similaritySearch
(
"redis"
,
3
,
[
"qux"
]
)
;
console
.
log
(
filterRes
)
;
/*
[
Document {
pageContent: "consectetur adipiscing elit",
metadata: { baz: "qux" },
},
Document {
pageContent: "lorem ipsum dolor sit amet",
metadata: { baz: "qux" },
}
]
*/
/* Usage as part of a chain */
const
model
=
new
OpenAI
(
)
;
const
chain
=
RetrievalQAChain
.
fromLLM
(
model
,
vectorStore
.
asRetriever
(
1
)
,
{
returnSourceDocuments
:
true
,
}
)
;
const
chainRes
=
await
chain
.
call
(
{
query
:
"What did the fox do?"
}
)
;
console
.
log
(
chainRes
)
;
/*
{
text: " The fox jumped over the lazy dog.",
sourceDocuments: [
Document {
pageContent: "the quick brown fox jumped over the lazy dog",
metadata: [Object]
}
]
}
*/
await
client
.
disconnect
(
)
;



--- 文件: output_20250622_020018\docs\modules\indexes\vector_stores\integrations\singlestore.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/vector_stores/integrations/singlestore
crawled_at: 2025-06-22T02:00:24.473027
---

SingleStore
SingleStoreDB
是一款高性能，分布式数据库系统。长期以来，它一直支持
dot_product
等向量函数，从而成为需要文本相似度匹配的AI应用程序的最佳解决方案。
兼容性
仅Node.js可用。
LangChain.js接受
mysql2/promise Pool
作为SingleStore向量存储的连接池。
设置
​
建立SingleStoreDB环境。 你可以选择云版或自行部署版。，
云版
，
自行部署版
安装mysql2 JS客户端
npm
Yarn
pnpm
npm
install
-S mysql2
yarn
add
mysql2
pnpm
add
mysql2
用法
​
import
{
SingleStoreVectorStore
}
from
"langchain/vectorstores/singlestore"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
createPool
}
from
"mysql2/promise"
;
export
const
run
=
async
(
)
=>
{
const
pool
=
createPool
(
{
host
:
process
.
env
.
SINGLESTORE_HOST
,
port
:
Number
(
process
.
env
.
SINGLESTORE_PORT
)
,
user
:
process
.
env
.
SINGLESTORE_USERNAME
,
password
:
process
.
env
.
SINGLESTORE_PASSWORD
,
database
:
process
.
env
.
SINGLESTORE_DATABASE
,
}
)
;
const
vectorStore
=
await
SingleStoreVectorStore
.
fromTexts
(
[
"Hello world"
,
"Bye bye"
,
"hello nice world"
]
,
[
{
id
:
2
}
,
{
id
:
1
}
,
{
id
:
3
}
]
,
new
OpenAIEmbeddings
(
)
,
{
connectionPool
:
pool
,
}
)
;
const
resultOne
=
await
vectorStore
.
similaritySearch
(
"hello world"
,
1
)
;
console
.
log
(
resultOne
)
;
await
pool
.
end
(
)
;
}
;



--- 文件: output_20250622_020018\docs\modules\indexes\vector_stores\integrations\supabase.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/vector_stores/integrations/supabase
crawled_at: 2025-06-22T02:00:24.596007
---

Supabase
Langchain支持使用Supabase Postgres数据库作为向量存储使用'pgvector' postgres扩展。 有关更多信息，请参阅
Supabase博客文章
。
设置
​
安装库
​
npm
Yarn
pnpm
npm
install
-S @supabase/supabase-js
yarn
add
@supabase/supabase-js
pnpm
add
@supabase/supabase-js
在您的数据库中创建表和搜索功能
​
在您的数据库中运行此命令:
-- Enable the pgvector extension to work with embedding vectors
create
extension vector
;
-- Create a table to store your documents
create
table
documents
(
id bigserial
primary
key
,
content
text
,
-- corresponds to Document.pageContent
metadata jsonb
,
-- corresponds to Document.metadata
embedding vector
(
1536
)
-- 1536 works for OpenAI embeddings, change if needed
)
;
-- Create a function to search for documents
create
function
match_documents
(
query_embedding vector
(
1536
)
,
match_count
int
DEFAULT
null
,
filter jsonb
DEFAULT
'{}'
)
returns
table
(
id
bigint
,
content
text
,
metadata jsonb
,
similarity
float
)
language
plpgsql
as
$$
#variable_conflict use_column
begin
return
query
select
id
,
content
,
metadata
,
1
-
(
documents
.
embedding
<=>
query_embedding
)
as
similarity
from
documents
where
metadata @
>
filter
order
by
documents
.
embedding
<=>
query_embedding
limit
match_count
;
end
;
$$
;
使用
​
标准用法
​
下面的示例显示如何使用Supabase执行基本相似性搜索:
import
{
SupabaseVectorStore
}
from
"langchain/vectorstores/supabase"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
createClient
}
from
"@supabase/supabase-js"
;
// First, follow set-up instructions at
// https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/supabase
const
privateKey
=
process
.
env
.
SUPABASE_PRIVATE_KEY
;
if
(
!
privateKey
)
throw
new
Error
(
`
Expected env var SUPABASE_PRIVATE_KEY
`
)
;
const
url
=
process
.
env
.
SUPABASE_URL
;
if
(
!
url
)
throw
new
Error
(
`
Expected env var SUPABASE_URL
`
)
;
export
const
run
=
async
(
)
=>
{
const
client
=
createClient
(
url
,
privateKey
)
;
const
vectorStore
=
await
SupabaseVectorStore
.
fromTexts
(
[
"Hello world"
,
"Bye bye"
,
"What's this?"
]
,
[
{
id
:
2
}
,
{
id
:
1
}
,
{
id
:
3
}
]
,
new
OpenAIEmbeddings
(
)
,
{
client
,
tableName
:
"documents"
,
queryName
:
"match_documents"
,
}
)
;
const
resultOne
=
await
vectorStore
.
similaritySearch
(
"Hello world"
,
1
)
;
console
.
log
(
resultOne
)
;
}
;
元数据过滤
​
对于上述的'match_documents' Postgres函数，您还可以传递一个过滤参数，以仅使用特定元数据字段值的文档。该过滤器参数是一个JSON对象，'match_documents'函数将使用Postgres JSONB包含操作符'@>'根据您指定的元数据字段值过滤文档。有关更多信息，请参见
Postgres JSONB包含操作符
。
注意:
如果您之前使用过`SupabaseVectorStore'，您可能需要根据上述更新的SQL放弃和重新创建'match_documents'函数以使用此功能。
import
{
SupabaseVectorStore
}
from
"langchain/vectorstores/supabase"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
createClient
}
from
"@supabase/supabase-js"
;
// First, follow set-up instructions at
// https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/supabase
const
privateKey
=
process
.
env
.
SUPABASE_PRIVATE_KEY
;
if
(
!
privateKey
)
throw
new
Error
(
`
Expected env var SUPABASE_PRIVATE_KEY
`
)
;
const
url
=
process
.
env
.
SUPABASE_URL
;
if
(
!
url
)
throw
new
Error
(
`
Expected env var SUPABASE_URL
`
)
;
export
const
run
=
async
(
)
=>
{
const
client
=
createClient
(
url
,
privateKey
)
;
const
vectorStore
=
await
SupabaseVectorStore
.
fromTexts
(
[
"Hello world"
,
"Hello world"
,
"Hello world"
]
,
[
{
user_id
:
2
}
,
{
user_id
:
1
}
,
{
user_id
:
3
}
]
,
new
OpenAIEmbeddings
(
)
,
{
client
,
tableName
:
"documents"
,
queryName
:
"match_documents"
,
}
)
;
const
result
=
await
vectorStore
.
similaritySearch
(
"Hello world"
,
1
,
{
user_id
:
3
,
}
)
;
console
.
log
(
result
)
;
}
;
元数据查询构建器过滤
​
You can also use query builder-style filtering similar to how
the Supabase JavaScript library works
instead of passing an object. Note that since most of the filter properties are in the metadata column, you need to use arrow operators (
->
for integer or
->>
for text) as defined in
Postgrest API documentation
and specify the data type of the property (e.g. the column should look something like
metadata->some_int_value::int
).
import
{
SupabaseFilterRPCCall
,
SupabaseVectorStore
,
}
from
"langchain/vectorstores/supabase"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
createClient
}
from
"@supabase/supabase-js"
;
// First, follow set-up instructions at
// https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/supabase
const
privateKey
=
process
.
env
.
SUPABASE_PRIVATE_KEY
;
if
(
!
privateKey
)
throw
new
Error
(
`
Expected env var SUPABASE_PRIVATE_KEY
`
)
;
const
url
=
process
.
env
.
SUPABASE_URL
;
if
(
!
url
)
throw
new
Error
(
`
Expected env var SUPABASE_URL
`
)
;
export
const
run
=
async
(
)
=>
{
const
client
=
createClient
(
url
,
privateKey
)
;
const
embeddings
=
new
OpenAIEmbeddings
(
)
;
const
store
=
new
SupabaseVectorStore
(
embeddings
,
{
client
,
tableName
:
"documents"
,
}
)
;
const
docs
=
[
{
pageContent
:
"This is a long text, but it actually means something because vector database does not understand Lorem Ipsum. So I would need to expand upon the notion of quantum fluff, a theorectical concept where subatomic particles coalesce to form transient multidimensional spaces. Yet, this abstraction holds no real-world application or comprehensible meaning, reflecting a cosmic puzzle."
,
metadata
:
{
b
:
1
,
c
:
10
,
stuff
:
"right"
}
,
}
,
{
pageContent
:
"This is a long text, but it actually means something because vector database does not understand Lorem Ipsum. So I would need to proceed by discussing the echo of virtual tweets in the binary corridors of the digital universe. Each tweet, like a pixelated canary, hums in an unseen frequency, a fascinatingly perplexing phenomenon that, while conjuring vivid imagery, lacks any concrete implication or real-world relevance, portraying a paradox of multidimensional spaces in the age of cyber folklore."
,
metadata
:
{
b
:
2
,
c
:
9
,
stuff
:
"right"
}
,
}
,
{
pageContent
:
"hello"
,
metadata
:
{
b
:
1
,
c
:
9
,
stuff
:
"right"
}
}
,
{
pageContent
:
"hello"
,
metadata
:
{
b
:
1
,
c
:
9
,
stuff
:
"wrong"
}
}
,
{
pageContent
:
"hi"
,
metadata
:
{
b
:
2
,
c
:
8
,
stuff
:
"right"
}
}
,
{
pageContent
:
"bye"
,
metadata
:
{
b
:
3
,
c
:
7
,
stuff
:
"right"
}
}
,
{
pageContent
:
"what's this"
,
metadata
:
{
b
:
4
,
c
:
6
,
stuff
:
"right"
}
}
,
]
;
await
store
.
addDocuments
(
docs
)
;
const
funcFilterA
:
SupabaseFilterRPCCall
=
(
rpc
)
=>
rpc
.
filter
(
"metadata->b::int"
,
"lt"
,
3
)
.
filter
(
"metadata->c::int"
,
"gt"
,
7
)
.
textSearch
(
"content"
,
`
'multidimensional' & 'spaces'
`
,
{
config
:
"english"
,
}
)
;
const
resultA
=
await
store
.
similaritySearch
(
"quantum"
,
4
,
funcFilterA
)
;
const
funcFilterB
:
SupabaseFilterRPCCall
=
(
rpc
)
=>
rpc
.
filter
(
"metadata->b::int"
,
"lt"
,
3
)
.
filter
(
"metadata->c::int"
,
"gt"
,
7
)
.
filter
(
"metadata->>stuff"
,
"eq"
,
"right"
)
;
const
resultB
=
await
store
.
similaritySearch
(
"hello"
,
2
,
funcFilterB
)
;
console
.
log
(
resultA
,
resultB
)
;
}
;



--- 文件: output_20250622_020018\docs\modules\indexes\vector_stores\integrations\tigris.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/vector_stores/integrations/tigris
crawled_at: 2025-06-22T02:00:24.586392
---

Tigris
Tigris使向量嵌入的构建人工智能应用程序变得轻松。
它是一个完全托管的云原生数据库，允许您存储和索引文档和向量嵌入，以进行快速和可扩展的向量搜索。
兼容性
仅在Node.js上可用。
安装
​
1. 安装Tigris SDK
​
按以下方式安装SDK
npm
Yarn
pnpm
npm
install
-S @tigrisdata/vector
yarn
add
@tigrisdata/vector
pnpm
add
@tigrisdata/vector
2. 获取Tigris API凭据
​
您可以在
此处
注册免费的Tigris帐户。
注册Tigris帐户后，创建名为
vectordemo
的新项目。
接下来，记录
clientId
和
clientSecret
，您可以从项目的应用程序密钥部分获取它们。
索引文档
​
import
{
VectorDocumentStore
}
from
"@tigrisdata/vector"
;
import
{
Document
}
from
"langchain/document"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
TigrisVectorStore
}
from
"langchain/vectorstores/tigris"
;
const
index
=
new
VectorDocumentStore
(
{
connection
:
{
serverUrl
:
"api.preview.tigrisdata.cloud"
,
projectName
:
process
.
env
.
TIGRIS_PROJECT
,
clientId
:
process
.
env
.
TIGRIS_CLIENT_ID
,
clientSecret
:
process
.
env
.
TIGRIS_CLIENT_SECRET
,
}
,
indexName
:
"examples_index"
,
numDimensions
:
1536
,
// match the OpenAI embedding size
}
)
;
const
docs
=
[
new
Document
(
{
metadata
:
{
foo
:
"bar"
}
,
pageContent
:
"tigris is a cloud-native vector db"
,
}
)
,
new
Document
(
{
metadata
:
{
foo
:
"bar"
}
,
pageContent
:
"the quick brown fox jumped over the lazy dog"
,
}
)
,
new
Document
(
{
metadata
:
{
baz
:
"qux"
}
,
pageContent
:
"lorem ipsum dolor sit amet"
,
}
)
,
new
Document
(
{
metadata
:
{
baz
:
"qux"
}
,
pageContent
:
"tigris is a river"
,
}
)
,
]
;
await
TigrisVectorStore
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
,
{
index
}
)
;
查询文档
​
import
{
VectorDocumentStore
}
from
"@tigrisdata/vector"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
TigrisVectorStore
}
from
"langchain/vectorstores/tigris"
;
const
index
=
new
VectorDocumentStore
(
{
connection
:
{
serverUrl
:
"api.preview.tigrisdata.cloud"
,
projectName
:
process
.
env
.
TIGRIS_PROJECT
,
clientId
:
process
.
env
.
TIGRIS_CLIENT_ID
,
clientSecret
:
process
.
env
.
TIGRIS_CLIENT_SECRET
,
}
,
indexName
:
"examples_index"
,
numDimensions
:
1536
,
// match the OpenAI embedding size
}
)
;
const
vectorStore
=
await
TigrisVectorStore
.
fromExistingIndex
(
new
OpenAIEmbeddings
(
)
,
{
index
}
)
;
/* Search the vector DB independently with metadata filters */
const
results
=
await
vectorStore
.
similaritySearch
(
"tigris"
,
1
,
{
"metadata.foo"
:
"bar"
,
}
)
;
console
.
log
(
JSON
.
stringify
(
results
,
null
,
2
)
)
;
/*
[
Document {
pageContent: 'tigris is a cloud-native vector db',
metadata: { foo: 'bar' }
}
]
*/



--- 文件: output_20250622_020018\docs\modules\indexes\vector_stores\integrations\typeorm.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/vector_stores/integrations/typeorm
crawled_at: 2025-06-22T02:00:24.623447
---

TypeORM
为了在通用的PostgreSQL数据库中实现向量搜索，LangChainJS支持使用
TypeORM
和
pgvector
Postgres扩展。
设置
​
要使用TypeORM，您需要安装
typeorm
和
pg
软件包:
npm
Yarn
pnpm
npm
install
typeorm
yarn
add
typeorm
pnpm
add
typeorm
npm
Yarn
pnpm
npm
install
pg
yarn
add
pg
pnpm
add
pg
使用
docker-compose
设置
pgvector
自托管实例
​
pgvector
提供了一个预构建的Docker镜像，可用于快速设置自托管的Postgres实例。
创建一个名为
docker-compose.yml
的文件:
import DockerExample from "!!raw-loader!@examples/indexes/vector_stores/typeorm_vectorstore/docker-compose.example.yml";
<CodeBlock language="yml" name="docker-compose.yml">{DockerExample}</CodeBlock>
然后在相同的目录下运行
docker compose up
来启动容器。
您可以在
官方存储库
中找到有关如何设置
pgvector
的更多信息。
用法
​
使用
TypeORMVectorStore
的一个完整的示例如下:
import
{
DataSourceOptions
}
from
"typeorm"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
TypeORMVectorStore
}
from
"langchain/vectorstores/typeorm"
;
// First, follow set-up instructions at
// https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/typeorm
export
const
run
=
async
(
)
=>
{
const
args
=
{
postgresConnectionOptions
:
{
type
:
"postgres"
,
host
:
"localhost"
,
port
:
5432
,
username
:
"myuser"
,
password
:
"ChangeMe"
,
database
:
"api"
,
}
as
DataSourceOptions
,
}
;
const
typeormVectorStore
=
await
TypeORMVectorStore
.
fromDataSource
(
new
OpenAIEmbeddings
(
)
,
args
)
;
await
typeormVectorStore
.
ensureTableInDatabase
(
)
;
await
typeormVectorStore
.
addDocuments
(
[
{
pageContent
:
"what's this"
,
metadata
:
{
a
:
2
}
}
,
{
pageContent
:
"Cat drinks milk"
,
metadata
:
{
a
:
1
}
}
,
]
)
;
const
results
=
await
typeormVectorStore
.
similaritySearch
(
"hello"
,
2
)
;
console
.
log
(
results
)
;
}
;



--- 文件: output_20250622_020018\docs\modules\indexes\vector_stores\integrations\weaviate.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/vector_stores/integrations/weaviate
crawled_at: 2025-06-22T02:00:24.668231
---

Weaviate
Weaviate是一个开源的向量数据库，可以存储对象和向量，使向量搜索与结构化过滤相结合。LangChain通过
weaviate-ts-client
软件包连接到Weaviate，这是官方的Typescript客户端。
LangChain直接将向量插入Weaviate并查询给定向量的最近邻，因此您可以使用所有LangChain Embeddings与Weaviate的集成。
设置
​
npm
Yarn
pnpm
npm
install
weaviate-ts-client graphql
yarn
add
weaviate-ts-client graphql
pnpm
add
weaviate-ts-client graphql
您需要在本地或服务器上运行Weaviate，请参阅
Weaviate文档
获取更多信息。
用法：插入文档
​
/* eslint-disable @typescript-eslint/no-explicit-any */
import
weaviate
from
"weaviate-ts-client"
;
import
{
WeaviateStore
}
from
"langchain/vectorstores/weaviate"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
export
async
function
run
(
)
{
// Something wrong with the weaviate-ts-client types, so we need to disable
const
client
=
(
weaviate
as
any
)
.
client
(
{
scheme
:
process
.
env
.
WEAVIATE_SCHEME
||
"https"
,
host
:
process
.
env
.
WEAVIATE_HOST
||
"localhost"
,
apiKey
:
new
(
weaviate
as
any
)
.
ApiKey
(
process
.
env
.
WEAVIATE_API_KEY
||
"default"
)
,
}
)
;
// Create a store and fill it with some texts + metadata
await
WeaviateStore
.
fromTexts
(
[
"hello world"
,
"hi there"
,
"how are you"
,
"bye now"
]
,
[
{
foo
:
"bar"
}
,
{
foo
:
"baz"
}
,
{
foo
:
"qux"
}
,
{
foo
:
"bar"
}
]
,
new
OpenAIEmbeddings
(
)
,
{
client
,
indexName
:
"Test"
,
textKey
:
"text"
,
metadataKeys
:
[
"foo"
]
,
}
)
;
}
用法：查询文档
​
/* eslint-disable @typescript-eslint/no-explicit-any */
import
weaviate
from
"weaviate-ts-client"
;
import
{
WeaviateStore
}
from
"langchain/vectorstores/weaviate"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
export
async
function
run
(
)
{
// Something wrong with the weaviate-ts-client types, so we need to disable
const
client
=
(
weaviate
as
any
)
.
client
(
{
scheme
:
process
.
env
.
WEAVIATE_SCHEME
||
"https"
,
host
:
process
.
env
.
WEAVIATE_HOST
||
"localhost"
,
apiKey
:
new
(
weaviate
as
any
)
.
ApiKey
(
process
.
env
.
WEAVIATE_API_KEY
||
"default"
)
,
}
)
;
// Create a store for an existing index
const
store
=
await
WeaviateStore
.
fromExistingIndex
(
new
OpenAIEmbeddings
(
)
,
{
client
,
indexName
:
"Test"
,
metadataKeys
:
[
"foo"
]
,
}
)
;
// Search the index without any filters
const
results
=
await
store
.
similaritySearch
(
"hello world"
,
1
)
;
console
.
log
(
results
)
;
/*
[ Document { pageContent: 'hello world', metadata: { foo: 'bar' } } ]
*/
// Search the index with a filter, in this case, only return results where
// the "foo" metadata key is equal to "baz", see the Weaviate docs for more
// https://weaviate.io/developers/weaviate/api/graphql/filters
const
results2
=
await
store
.
similaritySearch
(
"hello world"
,
1
,
{
where
:
{
operator
:
"Equal"
,
path
:
[
"foo"
]
,
valueText
:
"baz"
,
}
,
}
)
;
console
.
log
(
results2
)
;
/*
[ Document { pageContent: 'hi there', metadata: { foo: 'baz' } } ]
*/
}



--- 文件: output_20250622_020018\docs\modules\memory\examples\buffer_memory.md ---
---
url: https://js.langchain.com.cn/docs/modules/memory/examples/buffer_memory
crawled_at: 2025-06-22T02:00:24.869997
---

buffer_memory
缓存内存
​
缓存内存是最简单的一种内存 - 它直接记住了以前的对话回合。
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
BufferMemory
}
from
"langchain/memory"
;
import
{
ConversationChain
}
from
"langchain/chains"
;
const
model
=
new
OpenAI
(
{
}
)
;
const
memory
=
new
BufferMemory
(
)
;
const
chain
=
new
ConversationChain
(
{
llm
:
model
,
memory
:
memory
}
)
;
const
res1
=
await
chain
.
call
(
{
input
:
"Hi! I'm Jim."
}
)
;
console
.
log
(
{
res1
}
)
;
{
response:
" Hi Jim! It's nice to meet you. My name is AI. What would you like to talk about?"
}
const
res2
=
await
chain
.
call
(
{
input
:
"What's my name?"
}
)
;
console
.
log
(
{
res2
}
)
;
{
response:
' You said your name is Jim. Is there anything else you would like to talk about?'
}
您还可以通过创建并传递一个
ChatHistory
对象来将消息加载到
BufferMemory
实例中。
这使您可以轻松地从过去的对话中获取状态。
import
{
ChatMessageHistory
}
from
"langchain/memory"
;
import
{
HumanChatMessage
,
AIChatMessage
}
from
"langchain/schema"
;
const
pastMessages
=
[
new
HumanChatMessage
(
"My name's Jonas"
)
,
new
AIChatMessage
(
"Nice to meet you, Jonas!"
)
,
]
;
const
memory
=
new
BufferMemory
(
{
chatHistory
:
new
ChatMessageHistory
(
pastMessages
)
,
}
)
;



--- 文件: output_20250622_020018\docs\modules\memory\examples\buffer_memory_chat.md ---
---
url: https://js.langchain.com.cn/docs/modules/memory/examples/buffer_memory_chat
crawled_at: 2025-06-22T02:00:24.826897
---

使用缓冲内存与聊天模型翻译的中文结果
本示例介绍如何将聊天特定的内存类与聊天模型配合使用。翻译的中文结果
需要注意的关键点是，设置
returnMessages: true
会使内存返回聊天消息列表而不是字符串。翻译的中文结果
import
{
ConversationChain
}
from
"langchain/chains"
;
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
ChatPromptTemplate
,
HumanMessagePromptTemplate
,
SystemMessagePromptTemplate
,
MessagesPlaceholder
,
}
from
"langchain/prompts"
;
import
{
BufferMemory
}
from
"langchain/memory"
;
export
const
run
=
async
(
)
=>
{
const
chat
=
new
ChatOpenAI
(
{
temperature
:
0
}
)
;
const
chatPrompt
=
ChatPromptTemplate
.
fromPromptMessages
(
[
SystemMessagePromptTemplate
.
fromTemplate
(
"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know."
)
,
new
MessagesPlaceholder
(
"history"
)
,
HumanMessagePromptTemplate
.
fromTemplate
(
"{input}"
)
,
]
)
;
const
chain
=
new
ConversationChain
(
{
memory
:
new
BufferMemory
(
{
returnMessages
:
true
,
memoryKey
:
"history"
}
)
,
prompt
:
chatPrompt
,
llm
:
chat
,
}
)
;
const
response
=
await
chain
.
call
(
{
input
:
"hi! whats up?"
,
}
)
;
console
.
log
(
response
)
;
}
;



--- 文件: output_20250622_020018\docs\modules\memory\examples\buffer_window_memory.md ---
---
url: https://js.langchain.com.cn/docs/modules/memory/examples/buffer_window_memory
crawled_at: 2025-06-22T02:00:24.872168
---

Buffer Window Memory
BufferWindowMemory用于跟踪会话中的来回消息，然后使用大小为
k
的窗口将最近的
k
条来回消息提取出来作为内存。
back-and-forths in conversation
​
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
BufferWindowMemory
}
from
"langchain/memory"
;
import
{
ConversationChain
}
from
"langchain/chains"
;
const
model
=
new
OpenAI
(
{
}
)
;
const
memory
=
new
BufferWindowMemory
(
{
k
:
1
}
)
;
const
chain
=
new
ConversationChain
(
{
llm
:
model
,
memory
:
memory
}
)
;
const
res1
=
await
chain
.
call
(
{
input
:
"Hi! I'm Jim."
}
)
;
console
.
log
(
{
res1
}
)
;
{
response:
" Hi Jim! It's nice to meet you. My name is AI. What would you like to talk about?"
}
const
res2
=
await
chain
.
call
(
{
input
:
"What's my name?"
}
)
;
console
.
log
(
{
res2
}
)
;
{
response:
' You said your name is Jim. Is there anything else you would like to talk about?'
}



--- 文件: output_20250622_020018\docs\modules\memory\examples\conversation_summary.md ---
---
url: https://js.langchain.com.cn/docs/modules/memory/examples/conversation_summary
crawled_at: 2025-06-22T02:00:24.911399
---

Conversation Summary Memory（对话总结记忆)
对话总结记忆会在对话进行时对其进行总结并储存在记忆中。这个记忆能够被用于将当前对话总结注入到提示/链中。这个记忆在对较长的对话进行总结时非常有用，因为直接在提示中保留所有过去的对话历史信息将会占用过多的token。
Usage（使用方法)，与LLM一起使用
​
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
ConversationSummaryMemory
}
from
"langchain/memory"
;
import
{
LLMChain
}
from
"langchain/chains"
;
import
{
PromptTemplate
}
from
"langchain/prompts"
;
export
const
run
=
async
(
)
=>
{
const
memory
=
new
ConversationSummaryMemory
(
{
memoryKey
:
"chat_history"
,
llm
:
new
OpenAI
(
{
modelName
:
"gpt-3.5-turbo"
,
temperature
:
0
}
)
,
}
)
;
const
model
=
new
OpenAI
(
{
temperature
:
0.9
}
)
;
const
prompt
=
PromptTemplate
.
fromTemplate
(
`
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.
Current conversation:
{chat_history}
Human: {input}
AI:
`
)
;
const
chain
=
new
LLMChain
(
{
llm
:
model
,
prompt
,
memory
}
)
;
const
res1
=
await
chain
.
call
(
{
input
:
"Hi! I'm Jim."
}
)
;
console
.
log
(
{
res1
,
memory
:
await
memory
.
loadMemoryVariables
(
{
}
)
}
)
;
/*
{
res1: {
text: " Hi Jim, I'm AI! It's nice to meet you. I'm an AI programmed to provide information about the environment around me. Do you have any specific questions about the area that I can answer for you?"
},
memory: {
chat_history: 'Jim introduces himself to the AI and the AI responds, introducing itself as a program designed to provide information about the environment. The AI offers to answer any specific questions Jim may have about the area.'
}
}
*/
const
res2
=
await
chain
.
call
(
{
input
:
"What's my name?"
}
)
;
console
.
log
(
{
res2
,
memory
:
await
memory
.
loadMemoryVariables
(
{
}
)
}
)
;
/*
{
res2: { text: ' You told me your name is Jim.' },
memory: {
chat_history: 'Jim introduces himself to the AI and the AI responds, introducing itself as a program designed to provide information about the environment. The AI offers to answer any specific questions Jim may have about the area. Jim asks the AI what his name is, and the AI responds that Jim had previously told it his name.'
}
}
*/
}
;
Usage（使用方法)，与聊天模型一起使用
​
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
ConversationSummaryMemory
}
from
"langchain/memory"
;
import
{
LLMChain
}
from
"langchain/chains"
;
import
{
PromptTemplate
}
from
"langchain/prompts"
;
export
const
run
=
async
(
)
=>
{
const
memory
=
new
ConversationSummaryMemory
(
{
memoryKey
:
"chat_history"
,
llm
:
new
ChatOpenAI
(
{
modelName
:
"gpt-3.5-turbo"
,
temperature
:
0
}
)
,
}
)
;
const
model
=
new
ChatOpenAI
(
)
;
const
prompt
=
PromptTemplate
.
fromTemplate
(
`
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.
Current conversation:
{chat_history}
Human: {input}
AI:
`
)
;
const
chain
=
new
LLMChain
(
{
llm
:
model
,
prompt
,
memory
}
)
;
const
res1
=
await
chain
.
call
(
{
input
:
"Hi! I'm Jim."
}
)
;
console
.
log
(
{
res1
,
memory
:
await
memory
.
loadMemoryVariables
(
{
}
)
}
)
;
/*
{
res1: {
text: "Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?"
},
memory: {
chat_history: 'Jim introduces himself to the AI and the AI greets him and offers assistance.'
}
}
*/
const
res2
=
await
chain
.
call
(
{
input
:
"What's my name?"
}
)
;
console
.
log
(
{
res2
,
memory
:
await
memory
.
loadMemoryVariables
(
{
}
)
}
)
;
/*
{
res2: {
text: "Your name is Jim. It's nice to meet you, Jim. How can I assist you today?"
},
memory: {
chat_history: 'Jim introduces himself to the AI and the AI greets him and offers assistance. The AI addresses Jim by name and asks how it can assist him.'
}
}
*/
}
;



--- 文件: output_20250622_020018\docs\modules\memory\examples\dynamodb.md ---
---
url: https://js.langchain.com.cn/docs/modules/memory/examples/dynamodb
crawled_at: 2025-06-22T02:00:24.939810
---

DynamoDB支持的聊天记录
如果需要在聊天进程之间进行更长期的持久化，您可以将默认的内存chatHistory替换为DynamoDB实例，作为支持BufferMemory等聊天记录类的后端。，注意：
chatHistory
指聊天记录类，
BufferMemory
指缓存存储器类。
设置
​
首先，在您的项目中安装AWS DynamoDB客户端
npm
Yarn
pnpm
npm
install
@aws-sdk/client-dynamodb
yarn
add
@aws-sdk/client-dynamodb
pnpm
add
@aws-sdk/client-dynamodb
接下来，登录您的AWS帐户并创建一个DynamoDB表格。将表格命名为
langchain
，指定您的分区键为
id
，分区键必须是字符串类型，其他设置保持默认即可。
您还需要检索一个AWS访问密钥和密钥，以便拥有访问该表格的角色或用户，并将它们添加到环境变量中。
使用方法
​
import
{
BufferMemory
}
from
"langchain/memory"
;
import
{
DynamoDBChatMessageHistory
}
from
"langchain/stores/message/dynamodb"
;
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
ConversationChain
}
from
"langchain/chains"
;
const
memory
=
new
BufferMemory
(
{
chatHistory
:
new
DynamoDBChatMessageHistory
(
{
tableName
:
"langchain"
,
partitionKey
:
"id"
,
sessionId
:
new
Date
(
)
.
toISOString
(
)
,
// Or some other unique identifier for the conversation
config
:
{
region
:
"us-east-2"
,
credentials
:
{
accessKeyId
:
"<your AWS access key id>"
,
secretAccessKey
:
"<your AWS secret access key>"
,
}
,
}
,
}
)
,
}
)
;
const
model
=
new
ChatOpenAI
(
)
;
const
chain
=
new
ConversationChain
(
{
llm
:
model
,
memory
}
)
;
const
res1
=
await
chain
.
call
(
{
input
:
"Hi! I'm Jim."
}
)
;
console
.
log
(
{
res1
}
)
;
/*
{
res1: {
text: "Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?"
}
}
*/
const
res2
=
await
chain
.
call
(
{
input
:
"What did I just say my name was?"
}
)
;
console
.
log
(
{
res2
}
)
;
/*
{
res1: {
text: "You said your name was Jim."
}
}
*/



--- 文件: output_20250622_020018\docs\modules\memory\examples\entity_memory.md ---
---
url: https://js.langchain.com.cn/docs/modules/memory/examples/entity_memory
crawled_at: 2025-06-22T02:00:24.981041
---

实体记忆
实体记忆是会话中记忆特定实体的给定事实。
它使用 LLM 提取实体 (使用 LLM 同时建立对实体的知识)。
用法:
​
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
EntityMemory
,
ENTITY_MEMORY_CONVERSATION_TEMPLATE
,
}
from
"langchain/memory"
;
import
{
LLMChain
}
from
"langchain/chains"
;
export
const
run
=
async
(
)
=>
{
const
memory
=
new
EntityMemory
(
{
llm
:
new
OpenAI
(
{
temperature
:
0
}
)
,
chatHistoryKey
:
"history"
,
// Default value
entitiesKey
:
"entities"
,
// Default value
}
)
;
const
model
=
new
OpenAI
(
{
temperature
:
0.9
}
)
;
const
chain
=
new
LLMChain
(
{
llm
:
model
,
prompt
:
ENTITY_MEMORY_CONVERSATION_TEMPLATE
,
// Default prompt - must include the set chatHistoryKey and entitiesKey as input variables.
memory
,
}
)
;
const
res1
=
await
chain
.
call
(
{
input
:
"Hi! I'm Jim."
}
)
;
console
.
log
(
{
res1
,
memory
:
await
memory
.
loadMemoryVariables
(
{
input
:
"Who is Jim?"
}
)
,
}
)
;
const
res2
=
await
chain
.
call
(
{
input
:
"I work in construction. What about you?"
,
}
)
;
console
.
log
(
{
res2
,
memory
:
await
memory
.
loadMemoryVariables
(
{
input
:
"Who is Jim?"
}
)
,
}
)
;
}
;
检查记忆存储
​
您还可以直接检查记忆存储，以查看每个实体的当前摘要:
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
EntityMemory
,
ENTITY_MEMORY_CONVERSATION_TEMPLATE
,
}
from
"langchain/memory"
;
import
{
LLMChain
}
from
"langchain/chains"
;
const
memory
=
new
EntityMemory
(
{
llm
:
new
OpenAI
(
{
temperature
:
0
}
)
,
}
)
;
const
model
=
new
OpenAI
(
{
temperature
:
0.9
}
)
;
const
chain
=
new
LLMChain
(
{
llm
:
model
,
prompt
:
ENTITY_MEMORY_CONVERSATION_TEMPLATE
,
memory
,
}
)
;
await
chain
.
call
(
{
input
:
"Hi! I'm Jim."
}
)
;
await
chain
.
call
(
{
input
:
"I work in sales. What about you?"
,
}
)
;
const
res
=
await
chain
.
call
(
{
input
:
"My office is the Utica branch of Dunder Mifflin. What about you?"
,
}
)
;
console
.
log
(
{
res
,
memory
:
await
memory
.
loadMemoryVariables
(
{
input
:
"Who is Jim?"
}
)
,
}
)
;
/*
{
res: "As an AI language model, I don't have an office in the traditional sense. I exist entirely in digital space and am here to assist you with any questions or tasks you may have. Is there anything specific you need help with regarding your work at the Utica branch of Dunder Mifflin?",
memory: {
entities: {
Jim: 'Jim is a human named Jim who works in sales.',
Utica: 'Utica is the location of the branch of Dunder Mifflin where Jim works.',
'Dunder Mifflin': 'Dunder Mifflin has a branch in Utica.'
}
}
}
*/



--- 文件: output_20250622_020018\docs\modules\memory\examples\momento.md ---
---
url: https://js.langchain.com.cn/docs/modules/memory/examples/momento
crawled_at: 2025-06-22T02:00:25.005827
---

Momento支持的聊天记录
如果要在聊天会话中使用分布式、无服务器的持久性,可以使用Momento支持的聊天消息历史记录，即刻缓存，无需任何基础设施维护,无论是在本地构建还是在生产环境中都是一个很好的起点。
设置
​
在项目中安装
Momento客户端库
:
npm
Yarn
pnpm
npm
install
@gomomento/sdk
yarn
add
@gomomento/sdk
pnpm
add
@gomomento/sdk
您还需要从
Momento
获得API密钥。您可以在此处签署免费帐户
这里
。
用法
​
为了区分一个聊天历史记录会话和另一个会话,我们需要一个唯一的“sessionId”。您还可以提供一个可选的“sessionTtl”，以使会话在给定的秒数后过期。
import
{
CacheClient
,
Configurations
,
CredentialProvider
,
}
from
"@gomomento/sdk"
;
import
{
BufferMemory
}
from
"langchain/memory"
;
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
ConversationChain
}
from
"langchain/chains"
;
import
{
MomentoChatMessageHistory
}
from
"langchain/stores/message/momento"
;
// See https://github.com/momentohq/client-sdk-javascript for connection options
const
client
=
new
CacheClient
(
{
configuration
:
Configurations
.
Laptop
.
v1
(
)
,
credentialProvider
:
CredentialProvider
.
fromEnvironmentVariable
(
{
environmentVariableName
:
"MOMENTO_AUTH_TOKEN"
,
}
)
,
defaultTtlSeconds
:
60
*
60
*
24
,
}
)
;
// Create a unique session ID
const
sessionId
=
new
Date
(
)
.
toISOString
(
)
;
const
cacheName
=
"langchain"
;
const
memory
=
new
BufferMemory
(
{
chatHistory
:
await
MomentoChatMessageHistory
.
fromProps
(
{
client
,
cacheName
,
sessionId
,
sessionTtl
:
300
,
}
)
,
}
)
;
console
.
log
(
`
cacheName=
${
cacheName
}
and sessionId=
${
sessionId
}
. This will be used to store the chat history. You can inspect the values at your Momento console at https://console.gomomento.com.
`
)
;
const
model
=
new
ChatOpenAI
(
{
modelName
:
"gpt-3.5-turbo"
,
temperature
:
0
,
}
)
;
const
chain
=
new
ConversationChain
(
{
llm
:
model
,
memory
}
)
;
const
res1
=
await
chain
.
call
(
{
input
:
"Hi! I'm Jim."
}
)
;
console
.
log
(
{
res1
}
)
;
/*
{
res1: {
text: "Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?"
}
}
*/
const
res2
=
await
chain
.
call
(
{
input
:
"What did I just say my name was?"
}
)
;
console
.
log
(
{
res2
}
)
;
/*
{
res1: {
text: "You said your name was Jim."
}
}
*/
// See the chat history in the Momento
console
.
log
(
await
memory
.
chatHistory
.
getMessages
(
)
)
;



--- 文件: output_20250622_020018\docs\modules\memory\examples\motorhead_memory.md ---
---
url: https://js.langchain.com.cn/docs/modules/memory/examples/motorhead_memory
crawled_at: 2025-06-22T02:00:25.029149
---

motorhead_memory
Motörhead
是一个由Rust实现的内存服务器。它可以自动处理增量摘要并允许无状态应用程序。
设置
​
请参阅
Motörhead
的指示以在本地运行服务器。
用法
​
import
{
ConversationChain
}
from
"langchain/chains"
;
import
{
ChatOpenAI
}
from
"langchain/chat_models"
;
import
{
MotorheadMemory
}
from
"langchain/memory"
;
const
model
=
new
ChatOpenAI
(
{
}
)
;
const
memory
=
new
MotorheadMemory
(
{
sessionId
:
"user-id"
,
motorheadURL
:
"localhost:8080"
,
}
)
;
await
memory
.
init
(
)
;
// loads previous state from Motörhead 🤘
const
context
=
memory
.
context
?
`
Here's previous context:
${
memory
.
context
}
`
:
""
;
const
chatPrompt
=
ChatPromptTemplate
.
fromPromptMessages
(
[
SystemMessagePromptTemplate
.
fromTemplate
(
`
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.
${
context
}
`
)
,
new
MessagesPlaceholder
(
"history"
)
,
HumanMessagePromptTemplate
.
fromTemplate
(
"{input}"
)
,
]
)
;
const
chain
=
new
ConversationChain
(
{
memory
,
prompt
:
chatPrompt
,
llm
:
chat
,
}
)
;
const
res1
=
await
chain
.
call
(
{
input
:
"Hi! I'm Jim."
}
)
;
console
.
log
(
{
res1
}
)
;
```shell
{
response:
" Hi Jim! It's nice to meet you. My name is AI. What would you like to talk about?"
}
const
res2
=
await
chain
.
call
(
{
input
:
"What's my name?"
}
)
;
console
.
log
(
{
res2
}
)
;
{
response:
' You said your name is Jim. Is there anything else you would like to talk about?'
}



--- 文件: output_20250622_020018\docs\modules\memory\examples\redis.md ---
---
url: https://js.langchain.com.cn/docs/modules/memory/examples/redis
crawled_at: 2025-06-22T02:00:25.086954
---

基于Redis的聊天存储
如果需要在聊天会话之间进行长期持久化，可以将默认的内存
chatHistory
替换为一个
Redis
实例来支持聊天存储类，如
BufferMemory
。
设置
​
您需要在项目中安装
node-redis
。
npm
Yarn
pnpm
npm
install
redis
yarn
add
redis
pnpm
add
redis
您还需要一个Redis实例来连接。请参阅
Redis官方网站
上运行本地服务器的说明。
用法
​
Redis中存储的每个聊天历史记录会话都必须具有唯一的ID。你可以提供一个可选的
sessionTTL
参数来使会话在一定时间后过期。
传递给
createClient
方法的
config
参数直接传递给
node-redis
，并使用所有相同的参数。
import
{
BufferMemory
}
from
"langchain/memory"
;
import
{
RedisChatMessageHistory
}
from
"langchain/stores/message/redis"
;
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
ConversationChain
}
from
"langchain/chains"
;
const
memory
=
new
BufferMemory
(
{
chatHistory
:
new
RedisChatMessageHistory
(
{
sessionId
:
new
Date
(
)
.
toISOString
(
)
,
// Or some other unique identifier for the conversation
sessionTTL
:
300
,
// 5 minutes, omit this parameter to make sessions never expire
config
:
{
url
:
"redis://localhost:6379"
,
// Default value, override with your own instance's URL
}
,
}
)
,
}
)
;
const
model
=
new
ChatOpenAI
(
{
modelName
:
"gpt-3.5-turbo"
,
temperature
:
0
,
}
)
;
const
chain
=
new
ConversationChain
(
{
llm
:
model
,
memory
}
)
;
const
res1
=
await
chain
.
call
(
{
input
:
"Hi! I'm Jim."
}
)
;
console
.
log
(
{
res1
}
)
;
/*
{
res1: {
text: "Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?"
}
}
*/
const
res2
=
await
chain
.
call
(
{
input
:
"What did I just say my name was?"
}
)
;
console
.
log
(
{
res2
}
)
;
/*
{
res1: {
text: "You said your name was Jim."
}
}
*/
高级用法
​
您也可以直接传递先前创建的
node-redis
客户端实例:
import
{
createClient
}
from
"redis"
;
import
{
BufferMemory
}
from
"langchain/memory"
;
import
{
RedisChatMessageHistory
}
from
"langchain/stores/message/redis"
;
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
ConversationChain
}
from
"langchain/chains"
;
const
client
=
createClient
(
{
url
:
"redis://localhost:6379"
,
}
)
;
const
memory
=
new
BufferMemory
(
{
chatHistory
:
new
RedisChatMessageHistory
(
{
sessionId
:
new
Date
(
)
.
toISOString
(
)
,
sessionTTL
:
300
,
client
,
}
)
,
}
)
;
const
model
=
new
ChatOpenAI
(
{
modelName
:
"gpt-3.5-turbo"
,
temperature
:
0
,
}
)
;
const
chain
=
new
ConversationChain
(
{
llm
:
model
,
memory
}
)
;
const
res1
=
await
chain
.
call
(
{
input
:
"Hi! I'm Jim."
}
)
;
console
.
log
(
{
res1
}
)
;
/*
{
res1: {
text: "Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?"
}
}
*/
const
res2
=
await
chain
.
call
(
{
input
:
"What did I just say my name was?"
}
)
;
console
.
log
(
{
res2
}
)
;
/*
{
res1: {
text: "You said your name was Jim."
}
}
*/



--- 文件: output_20250622_020018\docs\modules\memory\examples\upstash_redis.md ---
---
url: https://js.langchain.com.cn/docs/modules/memory/examples/upstash_redis
crawled_at: 2025-06-22T02:00:25.163885
---

Upstash 基于 Redis 的聊天记忆
由于 Upstash Redis 通过 REST API 工作,所以您可以将其与
Vercel Edge
，
[Cloudflare Workers]
一起使用（https：//developers.cloudflare.com/workers/)和其他无服务器环境。
基于 Redis-Backed 聊天记忆。
为了在聊天会话之间实现长期持久性,您可以将支持聊天记忆类（如
BufferMemory
)的默认内存中的
chatHistory
替换为 Upstash
[Redis]
实例（https：//redis.io/)。
设置
​
您需要在项目中安装
@upstash/redis
npm
Yarn
pnpm
npm
install
@upstash/redis
yarn
add
@upstash/redis
pnpm
add
@upstash/redis
您还需要一个 Upstash 帐户和一个 Redis 数据库进行连接。有关如何创建 HTTP 客户端的说明，请参见
Upstash Docs
。
使用
​
Redis 中存储的每个聊天历史记录会话必须具有唯一的 id。您可以提供可选的
sessionTTL
，以使会话在确定的秒数后过期。
直接将
config
参数传递到
@upstash/redis
的
new Redis()
构造函数中，并使用相同的参数。
import
{
BufferMemory
}
from
"langchain/memory"
;
import
{
UpstashRedisChatMessageHistory
}
from
"langchain/stores/message/upstash_redis"
;
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
ConversationChain
}
from
"langchain/chains"
;
const
memory
=
new
BufferMemory
(
{
chatHistory
:
new
UpstashRedisChatMessageHistory
(
{
sessionId
:
new
Date
(
)
.
toISOString
(
)
,
// Or some other unique identifier for the conversation
sessionTTL
:
300
,
// 5 minutes, omit this parameter to make sessions never expire
config
:
{
url
:
"https://ADD_YOURS_HERE.upstash.io"
,
// Override with your own instance's URL
token
:
"********"
,
// Override with your own instance's token
}
,
}
)
,
}
)
;
const
model
=
new
ChatOpenAI
(
{
modelName
:
"gpt-3.5-turbo"
,
temperature
:
0
,
}
)
;
const
chain
=
new
ConversationChain
(
{
llm
:
model
,
memory
}
)
;
const
res1
=
await
chain
.
call
(
{
input
:
"Hi! I'm Jim."
}
)
;
console
.
log
(
{
res1
}
)
;
/*
{
res1: {
text: "Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?"
}
}
*/
const
res2
=
await
chain
.
call
(
{
input
:
"What did I just say my name was?"
}
)
;
console
.
log
(
{
res2
}
)
;
/*
{
res1: {
text: "You said your name was Jim."
}
}
*/
高级用法
​
您还可以直接传入先前创建的
@upstash/redis
客户端实例@＃
import
{
Redis
}
from
"@upstash/redis"
;
import
{
BufferMemory
}
from
"langchain/memory"
;
import
{
UpstashRedisChatMessageHistory
}
from
"langchain/stores/message/upstash_redis"
;
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
ConversationChain
}
from
"langchain/chains"
;
// Create your own Redis client
const
client
=
new
Redis
(
{
url
:
"https://ADD_YOURS_HERE.upstash.io"
,
token
:
"********"
,
}
)
;
const
memory
=
new
BufferMemory
(
{
chatHistory
:
new
UpstashRedisChatMessageHistory
(
{
sessionId
:
new
Date
(
)
.
toISOString
(
)
,
sessionTTL
:
300
,
client
,
// You can reuse your existing Redis client
}
)
,
}
)
;
const
model
=
new
ChatOpenAI
(
{
modelName
:
"gpt-3.5-turbo"
,
temperature
:
0
,
}
)
;
const
chain
=
new
ConversationChain
(
{
llm
:
model
,
memory
}
)
;
const
res1
=
await
chain
.
call
(
{
input
:
"Hi! I'm Jim."
}
)
;
console
.
log
(
{
res1
}
)
;
/*
{
res1: {
text: "Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?"
}
}
*/
const
res2
=
await
chain
.
call
(
{
input
:
"What did I just say my name was?"
}
)
;
console
.
log
(
{
res2
}
)
;
/*
{
res1: {
text: "You said your name was Jim."
}
}
*/



--- 文件: output_20250622_020018\docs\modules\memory\examples\vector_store_memory.md ---
---
url: https://js.langchain.com.cn/docs/modules/memory/examples/vector_store_memory
crawled_at: 2025-06-22T02:00:25.185088
---

基于向量库的内存
VectorStoreRetrieverMemory
将记忆存储在向量数据库中，并在每次调用时查询最“显著”的前K个文档。
这与大多数其他内存类不同，它没有显式跟踪交互的顺序。
在这种情况下，“文档”是先前的会话片段。这可以用来提到AI之前在对话中被告知的相关信息。
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
VectorStoreRetrieverMemory
}
from
"langchain/memory"
;
import
{
LLMChain
}
from
"langchain/chains"
;
import
{
PromptTemplate
}
from
"langchain/prompts"
;
import
{
MemoryVectorStore
}
from
"langchain/vectorstores/memory"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
const
vectorStore
=
new
MemoryVectorStore
(
new
OpenAIEmbeddings
(
)
)
;
const
memory
=
new
VectorStoreRetrieverMemory
(
{
// 1 is how many documents to return, you might want to return more, eg. 4
vectorStoreRetriever
:
vectorStore
.
asRetriever
(
1
)
,
memoryKey
:
"history"
,
}
)
;
// First let's save some information to memory, as it would happen when
// used inside a chain.
await
memory
.
saveContext
(
{
input
:
"My favorite food is pizza"
}
,
{
output
:
"thats good to know"
}
)
;
await
memory
.
saveContext
(
{
input
:
"My favorite sport is soccer"
}
,
{
output
:
"..."
}
)
;
await
memory
.
saveContext
(
{
input
:
"I don't the Celtics"
}
,
{
output
:
"ok"
}
)
;
// Now let's use the memory to retrieve the information we saved.
console
.
log
(
await
memory
.
loadMemoryVariables
(
{
prompt
:
"what sport should i watch?"
}
)
)
;
/*
{ history: 'input: My favorite sport is soccer\noutput: ...' }
*/
// Now let's use it in a chain.
const
model
=
new
OpenAI
(
{
temperature
:
0.9
}
)
;
const
prompt
=
PromptTemplate
.
fromTemplate
(
`
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.
Relevant pieces of previous conversation:
{history}
(You do not need to use these pieces of information if not relevant)
Current conversation:
Human: {input}
AI:
`
)
;
const
chain
=
new
LLMChain
(
{
llm
:
model
,
prompt
,
memory
}
)
;
const
res1
=
await
chain
.
call
(
{
input
:
"Hi, my name is Perry, what's up?"
}
)
;
console
.
log
(
{
res1
}
)
;
/*
{
res1: {
text: " Hi Perry, I'm doing great! I'm currently exploring different topics related to artificial intelligence like natural language processing and machine learning. What about you? What have you been up to lately?"
}
}
*/
const
res2
=
await
chain
.
call
(
{
input
:
"what's my favorite sport?"
}
)
;
console
.
log
(
{
res2
}
)
;
/*
{ res2: { text: ' You said your favorite sport is soccer.' } }
*/
const
res3
=
await
chain
.
call
(
{
input
:
"what's my name?"
}
)
;
console
.
log
(
{
res3
}
)
;
/*
{ res3: { text: ' Your name is Perry.' } }
*/



--- 文件: output_20250622_020018\docs\modules\memory\examples\zep_memory.md ---
---
url: https://js.langchain.com.cn/docs/modules/memory/examples/zep_memory
crawled_at: 2025-06-22T02:00:25.227744
---

Zep Memory
Zep
是存储、，总结、内嵌、索引和丰富对话AI聊天历史、自治代理历史、文档Q&A历史的记忆服务器，并通过简单、低延迟的API公开它们。
主要功能:
长期存储记忆，无论您的总结策略如何，都可访问历史消息。
基于可配置的消息窗口自动总结记忆消息。一系列的总结被存储，为未来的总结策略提供了灵活性。
对记忆进行向量搜索，消息在创建时自动嵌入。
自动记忆和摘要的令牌计数，允许更细粒度地控制提示组合。
Python
和
JavaScript
SDK。
设置
​
请参阅
Zep
的说明，以在本地或通过自动托管提供程序运行服务器。
用法
​
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
ConversationChain
}
from
"langchain/chains"
;
import
{
ZepMemory
}
from
"langchain/memory/zep"
;
const
sessionId
=
"TestSession1234"
;
const
zepURL
=
"http://localhost:8000"
;
const
memory
=
new
ZepMemory
(
{
sessionId
,
baseURL
:
zepURL
,
}
)
;
const
model
=
new
ChatOpenAI
(
{
modelName
:
"gpt-3.5-turbo"
,
temperature
:
0
,
}
)
;
const
chain
=
new
ConversationChain
(
{
llm
:
model
,
memory
}
)
;
const
res1
=
await
chain
.
call
(
{
input
:
"Hi! I'm Jim."
}
)
;
console
.
log
(
{
res1
}
)
;
/*
{
res1: {
text: "Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?"
}
}
*/
const
res2
=
await
chain
.
call
(
{
input
:
"What did I just say my name was?"
}
)
;
console
.
log
(
{
res2
}
)
;
/*
{
res1: {
text: "You said your name was Jim."
}
}
*/



--- 文件: output_20250622_020018\docs\modules\models\chat\additional_functionality.md ---
---
url: https://js.langchain.com.cn/docs/modules/models/chat/additional_functionality
crawled_at: 2025-06-22T02:00:25.344788
---

附加功能: 聊天模型
我们为聊天模型提供了许多附加功能。在以下示例中，我们将使用
ChatOpenAI
模型。
附加方法
​
L angChain 为与聊天模型交互提供了许多附加方法:
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
HumanChatMessage
,
SystemChatMessage
}
from
"langchain/schema"
;
export
const
run
=
async
(
)
=>
{
const
chat
=
new
ChatOpenAI
(
{
modelName
:
"gpt-3.5-turbo"
}
)
;
// Pass in a list of messages to `call` to start a conversation. In this simple example, we only pass in one message.
const
responseA
=
await
chat
.
call
(
[
new
HumanChatMessage
(
"What is a good name for a company that makes colorful socks?"
)
,
]
)
;
console
.
log
(
responseA
)
;
// AIChatMessage { text: '\n\nRainbow Sox Co.' }
// You can also pass in multiple messages to start a conversation.
// The first message is a system message that describes the context of the conversation.
// The second message is a human message that starts the conversation.
const
responseB
=
await
chat
.
call
(
[
new
SystemChatMessage
(
"You are a helpful assistant that translates English to French."
)
,
new
HumanChatMessage
(
"Translate: I love programming."
)
,
]
)
;
console
.
log
(
responseB
)
;
// AIChatMessage { text: "J'aime programmer." }
// Similar to LLMs, you can also use `generate` to generate chat completions for multiple sets of messages.
const
responseC
=
await
chat
.
generate
(
[
[
new
SystemChatMessage
(
"You are a helpful assistant that translates English to French."
)
,
new
HumanChatMessage
(
"Translate this sentence from English to French. I love programming."
)
,
]
,
[
new
SystemChatMessage
(
"You are a helpful assistant that translates English to French."
)
,
new
HumanChatMessage
(
"Translate this sentence from English to French. I love artificial intelligence."
)
,
]
,
]
)
;
console
.
log
(
responseC
)
;
/*
{
generations: [
[
{
text: "J'aime programmer.",
message: AIChatMessage { text: "J'aime programmer." },
}
],
[
{
text: "J'aime l'intelligence artificielle.",
message: AIChatMessage { text: "J'aime l'intelligence artificielle." }
}
]
]
}
*/
}
;
流式传输
​
与 LLMs 类似，您可以从聊天模型中流式传输响应。这对于需要实时响应用户输入的聊天机器人非常有用。
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
HumanChatMessage
}
from
"langchain/schema"
;
const
chat
=
new
ChatOpenAI
(
{
maxTokens
:
25
,
streaming
:
true
,
}
)
;
const
response
=
await
chat
.
call
(
[
new
HumanChatMessage
(
"Tell me a joke."
)
]
,
undefined
,
[
{
handleLLMNewToken
(
token
:
string
)
{
console
.
log
(
{
token
}
)
;
}
,
}
,
]
)
;
console
.
log
(
response
)
;
// { token: '' }
// { token: '\n\n' }
// { token: 'Why' }
// { token: ' don' }
// { token: "'t" }
// { token: ' scientists' }
// { token: ' trust' }
// { token: ' atoms' }
// { token: '?\n\n' }
// { token: 'Because' }
// { token: ' they' }
// { token: ' make' }
// { token: ' up' }
// { token: ' everything' }
// { token: '.' }
// { token: '' }
// AIChatMessage {
//   text: "\n\nWhy don't scientists trust atoms?\n\nBecause they make up everything."
// }
添加超时
​
默认情况下，LangChain 会无限期等待模型提供者的响应。如果您想添加超时，可以在调用模型时传递一个以毫秒为单位的
timeout
选项。例如，对于 OpenAI:
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
HumanChatMessage
}
from
"langchain/schema"
;
const
chat
=
new
ChatOpenAI
(
{
temperature
:
1
}
)
;
const
response
=
await
chat
.
call
(
[
new
HumanChatMessage
(
"What is a good name for a company that makes colorful socks?"
)
,
]
,
{
timeout
:
1000
}
// 1s timeout
)
;
console
.
log
(
response
)
;
// AIChatMessage { text: '\n\nRainbow Sox Co.' }
取消请求
​
您可以在调用模型时通过传递
signal
选项来取消请求。例如，对于 OpenAI:
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
HumanChatMessage
}
from
"langchain/schema"
;
const
model
=
new
ChatOpenAI
(
{
temperature
:
1
}
)
;
const
controller
=
new
AbortController
(
)
;
// Call `controller.abort()` somewhere to cancel the request.
const
res
=
await
model
.
call
(
[
new
HumanChatMessage
(
"What is a good name for a company that makes colorful socks?"
)
,
]
,
{
signal
:
controller
.
signal
}
)
;
console
.
log
(
res
)
;
/*
'\n\nSocktastic Colors'
*/
注意，只有底层提供程序暴露该选项时，才会取消正在进行的请求。如果可能，LangChain 将取消底层请求，否则将取消响应的处理。
处理速率限制
​
一些提供程序具有速率限制。如果超过速率限制，您将收到错误提示。为帮助您处理这种情况，LangChain 在实例化聊天模型时提供了
maxConcurrency
选项。该选项允许您指定要向提供程序发出的并发请求的最大数量。如果超出此数字，则 LangChain 将自动将您的请求排队，以在之前的请求完成后发送。
例如，如果您设置
maxConcurrency:5
，则LangChain一次仅会向提供程序发送5个请求。如果您发送10个请求，则会立即发送前5个请求，并将下一个5个请求排队。一旦前5个请求中的一个完成，队列中的下一个请求将被发送。
要使用此功能，只需在实例化LLM时传递
maxConcurrency:<number>
即可。例如：
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
const
model
=
new
ChatOpenAI
(
{
maxConcurrency
:
5
}
)
;
处理API错误
​
如果模型提供程序从其API返回错误，则默认情况下，LangChain将在指数回退上重试最多6次。这使得错误恢复无需任何额外的努力。如果您想更改此行为，则可以在实例化模型时传递
maxRetries
选项。例如@＃：
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
const
model
=
new
ChatOpenAI
(
{
maxRetries
:
10
}
)
;
订阅事件
​
特别是使用代理时，作为聊天模型处理提示时可能会有很多事情来回发生。对于代理，响应对象包含一个intermediateSteps对象，您可以打印以查看它所采取的步骤概述。如果这还不够，您想查看与聊天模型的每个交换，您可以将回调传递给聊天模型以进行自定义日志记录（或任何其他您想要执行的操作)，因为模型通过这些步骤@＃。
有关可用事件的更多信息，请参见文档中的回调（Callbacks）
Callbacks
部分。
import
{
HumanChatMessage
,
LLMResult
}
from
"langchain/schema"
;
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
// We can pass in a list of CallbackHandlers to the LLM constructor to get callbacks for various events.
const
model
=
new
ChatOpenAI
(
{
callbacks
:
[
{
handleLLMStart
:
async
(
llm
:
{
name
:
string
}
,
prompts
:
string
[
]
)
=>
{
console
.
log
(
JSON
.
stringify
(
llm
,
null
,
2
)
)
;
console
.
log
(
JSON
.
stringify
(
prompts
,
null
,
2
)
)
;
}
,
handleLLMEnd
:
async
(
output
:
LLMResult
)
=>
{
console
.
log
(
JSON
.
stringify
(
output
,
null
,
2
)
)
;
}
,
handleLLMError
:
async
(
err
:
Error
)
=>
{
console
.
error
(
err
)
;
}
,
}
,
]
,
}
)
;
await
model
.
call
(
[
new
HumanChatMessage
(
"What is a good name for a company that makes colorful socks?"
)
,
]
)
;
/*
{
"name": "openai"
}
[
"Human: What is a good name for a company that makes colorful socks?"
]
{
"generations": [
[
{
"text": "Rainbow Soles",
"message": {
"text": "Rainbow Soles"
}
}
]
],
"llmOutput": {
"tokenUsage": {
"completionTokens": 4,
"promptTokens": 21,
"totalTokens": 25
}
}
}
*/



--- 文件: output_20250622_020018\docs\modules\models\chat\integrations.md ---
---
url: https://js.langchain.com.cn/docs/modules/models/chat/integrations
crawled_at: 2025-06-22T02:00:25.393847
---

集成: 聊天模型
LangChain提供了许多与不同模型提供者集成的聊天模型实现。它们如下所示:
ChatOpenAI
（聊天OpenAI)
​
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
const
model
=
new
ChatOpenAI
(
{
temperature
:
0.9
,
openAIApiKey
:
"YOUR-API-KEY"
,
// In Node.js defaults to process.env.OPENAI_API_KEY
}
)
;
Azure
ChatOpenAI
（Azure的聊天OpenAI)
​
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
const
model
=
new
ChatOpenAI
(
{
temperature
:
0.9
,
azureOpenAIApiKey
:
"YOUR-API-KEY"
,
// In Node.js defaults to process.env.AZURE_OPENAI_API_KEY
azureOpenAIApiInstanceName
:
"YOUR-INSTANCE-NAME"
,
// In Node.js defaults to process.env.AZURE_OPENAI_API_INSTANCE_NAME
azureOpenAIApiDeploymentName
:
"YOUR-DEPLOYMENT-NAME"
,
// In Node.js defaults to process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME
azureOpenAIApiVersion
:
"YOUR-API-VERSION"
,
// In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION
}
)
;
ChatAnthropic
（聊天Anthropic)
​
import
{
ChatAnthropic
}
from
"langchain/chat_models/anthropic"
;
const
model
=
new
ChatAnthropic
(
{
temperature
:
0.9
,
apiKey
:
"YOUR-API-KEY"
,
// In Node.js defaults to process.env.ANTHROPIC_API_KEY
}
)
;
PromptLayerChatOpenAI
（Prompt Layer聊天OpenAI)
​
可以传入可选的
returnPromptLayerId
布尔值来获取像下面这样的
promptLayerRequestId
。下面是一个获取 PromptLayerChatOpenAI 请求ID 的示例:
import
{
PromptLayerChatOpenAI
}
from
"langchain/chat_models/openai"
;
const
chat
=
new
PromptLayerChatOpenAI
(
{
returnPromptLayerId
:
true
,
}
)
;
const
respA
=
await
chat
.
generate
(
[
[
new
SystemChatMessage
(
"You are a helpful assistant that translates English to French."
)
,
]
,
]
)
;
console
.
log
(
JSON
.
stringify
(
respA
,
null
,
3
)
)
;
/*
{
"generations": [
[
{
"text": "Bonjour! Je suis un assistant utile qui peut vous aider à traduire de l'anglais vers le français. Que puis-je faire pour vous aujourd'hui?",
"message": {
"type": "ai",
"data": {
"content": "Bonjour! Je suis un assistant utile qui peut vous aider à traduire de l'anglais vers le français. Que puis-je faire pour vous aujourd'hui?"
}
},
"generationInfo": {
"promptLayerRequestId": 2300682
}
}
]
],
"llmOutput": {
"tokenUsage": {
"completionTokens": 35,
"promptTokens": 19,
"totalTokens": 54
}
}
}
*/
Google Vertex AI
（Google 顶点AI)
​
Vertex AI 实现旨在在 Node.js 中使用，而不是直接从浏览器中使用，因为需要使用服务帐户才能使用。
在运行此代码之前，您应该确保已为相关项目启用了 Vertex AI API，并且已经使用以下方法之一进行了 Google Cloud 身份验证:
您已登录帐户（使用
gcloud auth application-default login
）进入 Google Cloud。
您正在运行于允许服务账户所在的机器上。
您已下载服务账户凭证，可以使用该服务账户访问该项目。
您已下载了允许访问该项目的服务账号的凭证，并将
GOOGLE_APPLICATION_CREDENTIALS
环境变量设置为该文件的路径。
npm
Yarn
pnpm
npm
install
google-auth-library
yarn
add
google-auth-library
pnpm
add
google-auth-library
ChatGoogleVertexAI 类的工作方式与其他基于聊天的 LLM 相同，具有一些例外情况:
第一个传入的
SystemChatMessage
被映射到 PaLM 模型期望的 "context" 参数。
不允许出现其他
SystemChatMessage
。
在第一个
SystemChatMessage
之后，必须是一串奇数条消息，代表人类和模型之间的对话。
发送的信息必须交错出现，先是人类信息，然后是 AI 回复，然后是人类信息，以此类推。
import
{
ChatGoogleVertexAI
}
from
"langchain/chat_models/googlevertexai"
;
const
model
=
new
ChatGoogleVertexAI
(
{
temperature
:
0.7
,
}
)
;
此外，还有一个可选的“例子”构造参数，可以帮助模型理解什么是适当的响应。
看起来像。
import
{
ChatGoogleVertexAI
}
from
"langchain/chat_models/googlevertexai"
;
import
{
AIChatMessage
,
HumanChatMessage
,
SystemChatMessage
,
}
from
"langchain/schema"
;
export
const
run
=
async
(
)
=>
{
const
examples
=
[
{
input
:
new
HumanChatMessage
(
"What is your favorite sock color?"
)
,
output
:
new
AIChatMessage
(
"My favorite sock color be arrrr-ange!"
)
,
}
,
]
;
const
model
=
new
ChatGoogleVertexAI
(
{
temperature
:
0.7
,
examples
,
}
)
;
const
questions
=
[
new
SystemChatMessage
(
"You are a funny assistant that answers in pirate language."
)
,
new
HumanChatMessage
(
"What is your favorite food?"
)
,
]
;
// You can also use the model as part of a chain
const
res
=
await
model
.
call
(
questions
)
;
console
.
log
(
{
res
}
)
;
}
;



--- 文件: output_20250622_020018\docs\modules\models\embeddings\additional_functionality.md ---
---
url: https://js.langchain.com.cn/docs/modules/models/embeddings/additional_functionality
crawled_at: 2025-06-22T02:00:25.436787
---

Embeddings: 嵌入
我们为聊天模型提供了许多附加功能。在下面的示例中，我们将使用“OpenAI嵌入”模型。
添加超时
​
默认情况下，LangChain将无限期地等待模型提供者的响应。如果你想添加超时，可以在实例化模型时传递一个毫秒级的
timeout
选项。例如，对于OpenAI#
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
export
const
run
=
async
(
)
=>
{
const
embeddings
=
new
OpenAIEmbeddings
(
{
timeout
:
1000
,
// 1s timeout
}
)
;
/* Embed queries */
const
res
=
await
embeddings
.
embedQuery
(
"Hello world"
)
;
console
.
log
(
res
)
;
/* Embed documents */
const
documentRes
=
await
embeddings
.
embedDocuments
(
[
"Hello world"
,
"Bye bye"
,
]
)
;
console
.
log
(
{
documentRes
}
)
;
}
;
目前，
timeout
选项仅支持OpenAI模型。
处理速率限制
​
一些提供者具有速率限制。如果你超过速率限制，将会收到错误。为了帮助你处理这个问题，LangChain在实例化嵌入模型时提供了
maxConcurrency
选项。该选项允许你指定要向提供者发送的最大并发请求数量。如果你超过了这个数字，LangChain将自动将你的请求排队等待之前的请求完成后再发送。
例如，如果你设置
maxConcurrency: 5
，那么LangChain一次只会发送5个请求到提供者。如果你发出10个请求，前5个将立即发送，接下来的5个将排队等待。一旦前5个请求之一完成，队列中的下一个请求将被发送。
要使用此功能，只需在实例化LLM时传递
maxConcurrency: <number>
即可。例如:
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
const
model
=
new
OpenAIEmbeddings
(
{
maxConcurrency
:
5
}
)
;
处理API错误
​
如果模型提供者从他们的API返回错误，默认情况下LangChain将重试6次指数退避。这使得无需任何额外的努力即可进行错误恢复。如果要更改此行为，您可以在实例化模型时传递
maxRetries
选项。例如：#
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
const
model
=
new
OpenAIEmbeddings
(
{
maxRetries
:
10
}
)
;



--- 文件: output_20250622_020018\docs\modules\models\embeddings\integrations.md ---
---
url: https://js.langchain.com.cn/docs/modules/models/embeddings/integrations
crawled_at: 2025-06-22T02:00:25.483860
---

集成: 嵌入
LangChain提供了许多与各种模型提供商集成的嵌入实现。这些是:
OpenAIEmbeddings
​
OpenAIEmbeddings
类使用OpenAI API为给定文本生成嵌入。默认情况下，它会从文本中删除换行符，如OpenAI推荐的那样。但是，您可以通过将
stripNewLines: false
传递给构造函数来禁用此功能。
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
const
embeddings
=
new
OpenAIEmbeddings
(
{
openAIApiKey
:
"YOUR-API-KEY"
,
// In Node.js defaults to process.env.OPENAI_API_KEY
}
)
;
Azure
OpenAIEmbeddings
​
OpenAIEmbeddings
类使用Azure上的OpenAI API为给定文本生成嵌入。默认情况下，它会从文本中删除换行符，如OpenAI推荐的那样。但是，您可以通过将
stripNewLines: false
传递给构造函数来禁用此功能。
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
const
embeddings
=
new
OpenAIEmbeddings
(
{
azureOpenAIApiKey
:
"YOUR-API-KEY"
,
// In Node.js defaults to process.env.AZURE_OPENAI_API_KEY
azureOpenAIApiInstanceName
:
"YOUR-INSTANCE-NAME"
,
// In Node.js defaults to process.env.AZURE_OPENAI_API_INSTANCE_NAME
azureOpenAIApiDeploymentName
:
"YOUR-DEPLOYMENT-NAME"
,
// In Node.js defaults to process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME
azureOpenAIApiVersion
:
"YOUR-API-VERSION"
,
// In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION
}
)
;
Google Vertex AI
​
GoogleVertexAIEmbeddings
类使用Google的Vertex AI PaLM模型为给定文本生成嵌入。
Vertex AI实现适用于Node.js，而不适用于直接在浏览器中使用，因为它需要一个服务帐户来使用。
在运行此代码之前，您应确保为您的Google Cloud仪表板中的相关项目启用了Vertex AI API，并且您已使用以下方法之一对Google Cloud进行了身份验证:
您已登录账户（使用
gcloud auth application-default login
)
enabled for the relevant project in your Google Cloud dashboard and that you've authenticated to
您正在使用被允许使用的服务帐户运行的计算机上
您已经下载了被允许使用的服务帐户的凭据
You are logged into an account (using
gcloud auth application-default login
)
permitted to that project.
You are running on a machine using a service account that is permitted
to the project.
You have downloaded the credentials for a service account that is permitted
to the project and set the
GOOGLE_APPLICATION_CREDENTIALS
environment
variable to the path of this file.
npm
Yarn
pnpm
npm
install
google-auth-library
yarn
add
google-auth-library
pnpm
add
google-auth-library
import
{
GoogleVertexAIEmbeddings
}
from
"langchain/embeddings/googlevertexai"
;
export
const
run
=
async
(
)
=>
{
const
model
=
new
GoogleVertexAIEmbeddings
(
)
;
const
res
=
await
model
.
embedQuery
(
"What would be a good company name for a company that makes colorful socks?"
)
;
console
.
log
(
{
res
}
)
;
}
;
注意:
默认的Google Vertex AI嵌入模型
textembedding-gecko
和OpenAI的
text-embedding-ada-002
模型具有不同的维度，可能不受所有向量存储提供程序的支持。
并非所有向量存储提供程序都支持它。
CohereEmbeddings
​
CohereEmbeddings
类使用Cohere API为给定文本生成嵌入向量。
npm
Yarn
pnpm
npm
install
cohere-ai
yarn
add
cohere-ai
pnpm
add
cohere-ai
import
{
CohereEmbeddings
}
from
"langchain/embeddings/cohere"
;
const
embeddings
=
new
CohereEmbeddings
(
{
apiKey
:
"YOUR-API-KEY"
,
// In Node.js defaults to process.env.COHERE_API_KEY
}
)
;
TensorFlowEmbeddings
​
此嵌入集成在您的浏览器或Node.js环境中完全运行嵌入向量，使用
TensorFlow.js
。 这意味着您的数据不会发送到任何第三方，而且您不需要注册任何API密钥。但是，它需要比其他集成更多的内存和处理能力。
npm
Yarn
pnpm
npm
install
@tensorflow/tfjs-core @tensorflow/tfjs-converter @tensorflow-models/universal-sentence-encoder @tensorflow/tfjs-backend-cpu
yarn
add
@tensorflow/tfjs-core @tensorflow/tfjs-converter @tensorflow-models/universal-sentence-encoder @tensorflow/tfjs-backend-cpu
pnpm
add
@tensorflow/tfjs-core @tensorflow/tfjs-converter @tensorflow-models/universal-sentence-encoder @tensorflow/tfjs-backend-cpu
import
"@tensorflow/tfjs-backend-cpu"
;
import
{
TensorFlowEmbeddings
}
from
"langchain/embeddings/tensorflow"
;
const
embeddings
=
new
TensorFlowEmbeddings
(
)
;
此示例使用CPU后端，适用于任何JS环境。 但是，您可以使用TensorFlow.js支持的任何后端，包括GPU和WebAssembly，它会更快。 对于Node.js，您可以使用
@tensorflow/tfjs-node
包，对于浏览器，您可以使用
@tensorflow/tfjs-backend-webgl
包。 有关更多信息，请参见
TensorFlow.js文档
。
HuggingFaceInferenceEmbeddings
​
此嵌入集成使用HuggingFace Inference API为给定文本生成嵌入向量，默认使用的是
sentence-transformers/distilbert-base-nli-mean-tokens
模型。 您可以将不同的模型名称传递给构造函数以使用不同的模型。
npm
Yarn
pnpm
npm
install
@huggingface/inference@1
yarn
add
@huggingface/inference@1
pnpm
add
@huggingface/inference@1
import
{
HuggingFaceInferenceEmbeddings
}
from
"langchain/embeddings/hf"
;
const
embeddings
=
new
HuggingFaceInferenceEmbeddings
(
{
apiKey
:
"YOUR-API-KEY"
,
// In Node.js defaults to process.env.HUGGINGFACEHUB_API_KEY
}
)
;



--- 文件: output_20250622_020018\docs\modules\models\llms\additional_functionality.md ---
---
url: https://js.langchain.com.cn/docs/modules/models/llms/additional_functionality
crawled_at: 2025-06-22T02:00:25.590829
---

LLMs: 附加功能
我们为LLM提供了许多附加功能。在下面的大多数示例中，我们将使用
OpenAI
LLM。然而，所有这些功能都适用于所有LLMs。
附加方法
​
LangChain为与LLMs交互提供了许多附加方法:
import
{
OpenAI
}
from
"langchain/llms/openai"
;
export
const
run
=
async
(
)
=>
{
const
modelA
=
new
OpenAI
(
)
;
// `call` is a simple string-in, string-out method for interacting with the model.
const
resA
=
await
modelA
.
call
(
"What would be a good company name a company that makes colorful socks?"
)
;
console
.
log
(
{
resA
}
)
;
// { resA: '\n\nSocktastic Colors' }
// `generate` allows you to generate multiple completions for multiple prompts (in a single request for some models).
const
resB
=
await
modelA
.
generate
(
[
"What would be a good company name a company that makes colorful socks?"
,
"What would be a good company name a company that makes colorful sweaters?"
,
]
)
;
// `resB` is a `LLMResult` object with a `generations` field and `llmOutput` field.
// `generations` is a `Generation[][]`, each `Generation` having a `text` field.
// Each input to the LLM could have multiple generations (depending on the `n` parameter), hence the list of lists.
console
.
log
(
JSON
.
stringify
(
resB
,
null
,
2
)
)
;
/*
{
"generations": [
[{
"text": "\n\nVibrant Socks Co.",
"generationInfo": {
"finishReason": "stop",
"logprobs": null
}
}],
[{
"text": "\n\nRainbow Knitworks.",
"generationInfo": {
"finishReason": "stop",
"logprobs": null
}
}]
],
"llmOutput": {
"tokenUsage": {
"completionTokens": 17,
"promptTokens": 29,
"totalTokens": 46
}
}
}
*/
// We can specify additional parameters the specific model provider supports, like `temperature`:
const
modelB
=
new
OpenAI
(
{
temperature
:
0.9
}
)
;
const
resC
=
await
modelA
.
call
(
"What would be a good company name a company that makes colorful socks?"
)
;
console
.
log
(
{
resC
}
)
;
// { resC: '\n\nKaleidoSox' }
// We can get the number of tokens for a given input for a specific model.
const
numTokens
=
modelB
.
getNumTokens
(
"How many tokens are in this input?"
)
;
console
.
log
(
{
numTokens
}
)
;
// { numTokens: 8 }
}
;
流式响应
​
某些LLMs提供流式响应。这意味着您可以在整个响应被返回之前开始处理它。这很有用，如果您想要在生成响应时向用户显示响应，或者如果你要处理正在生成的响应。
LangChain目前提供
OpenAI
LLM的流式传输:
import
{
OpenAI
}
from
"langchain/llms/openai"
;
// To enable streaming, we pass in `streaming: true` to the LLM constructor.
// Additionally, we pass in a handler for the `handleLLMNewToken` event.
const
chat
=
new
OpenAI
(
{
maxTokens
:
25
,
streaming
:
true
,
}
)
;
const
response
=
await
chat
.
call
(
"Tell me a joke."
,
undefined
,
[
{
handleLLMNewToken
(
token
:
string
)
{
console
.
log
(
{
token
}
)
;
}
,
}
,
]
)
;
console
.
log
(
response
)
;
/*
{ token: '\n' }
{ token: '\n' }
{ token: 'Q' }
{ token: ':' }
{ token: ' Why' }
{ token: ' did' }
{ token: ' the' }
{ token: ' chicken' }
{ token: ' cross' }
{ token: ' the' }
{ token: ' playground' }
{ token: '?' }
{ token: '\n' }
{ token: 'A' }
{ token: ':' }
{ token: ' To' }
{ token: ' get' }
{ token: ' to' }
{ token: ' the' }
{ token: ' other' }
{ token: ' slide' }
{ token: '.' }
Q: Why did the chicken cross the playground?
A: To get to the other slide.
*/
缓存
​
LangChain为LLMs提供可选的缓存层。有两个原因很有用:
如果你经常请求相同的完成多次，它可以通过减少你对LLM提供商的调用次数来节省您的费用。
它可以通过减少你对LLM提供商的API调用次数来加速你的应用程序。
内存中的缓存
​
默认缓存存储在内存中。这意味着如果重新启动应用程序，则会清除缓存。
你可以在实例化LLM时传递
cache: true
来启用它。例如:
import
{
OpenAI
}
from
"langchain/llms/openai"
;
const
model
=
new
OpenAI
(
{
cache
:
true
}
)
;
使用Momento进行缓存
​
LangChain还提供了基于Momento的缓存。
Momento
是一种分布式的服务器端无服务器缓存，不需要任何设置或基础设施维护。要使用它，您需要安装
@gomomento/sdk
软件包:
npm
Yarn
pnpm
npm
install
@gomomento/sdk
yarn
add
@gomomento/sdk
pnpm
add
@gomomento/sdk
接下来，您需要注册并创建API密钥。完成后，像这样实例化LLM时传递一个
cache
选项:
Next you'll need to sign up and create an API key. Once you've done that, pass a
cache
option when you instantiate the LLM like this:
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
MomentoCache
}
from
"langchain/cache/momento"
;
import
{
CacheClient
,
Configurations
,
CredentialProvider
,
}
from
"@gomomento/sdk"
;
// See https://github.com/momentohq/client-sdk-javascript for connection options
const
client
=
new
CacheClient
(
{
configuration
:
Configurations
.
Laptop
.
v1
(
)
,
credentialProvider
:
CredentialProvider
.
fromEnvironmentVariable
(
{
environmentVariableName
:
"MOMENTO_AUTH_TOKEN"
,
}
)
,
defaultTtlSeconds
:
60
*
60
*
24
,
}
)
;
const
cache
=
await
MomentoCache
.
fromProps
(
{
client
,
cacheName
:
"langchain"
,
}
)
;
const
model
=
new
OpenAI
(
{
cache
}
)
;
###使用Redis进行缓存
LangChain还提供了基于Redis的缓存。如果您想要在多个进程或服务器之间共享缓存，这将非常有用。要使用它，您需要安装
redis
软件包:
npm
Yarn
pnpm
npm
install
redis
yarn
add
redis
pnpm
add
redis
然后，当您实例化LLM时，可以通过传递一个
cache
选项来使用它。例如:
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
RedisCache
}
from
"langchain/cache/redis"
;
import
{
createClient
}
from
"redis"
;
// See https://github.com/redis/node-redis for connection options
const
client
=
createClient
(
)
;
const
cache
=
new
RedisCache
(
client
)
;
const
model
=
new
OpenAI
(
{
cache
}
)
;
##添加超时
Adding a timeout
​
默认情况下，LangChain将无限期地等待模型提供程序的响应。如果要添加超时，您可以在调用模型时传递一个以毫秒为单位的
timeout
选项。例如，对于OpenAI:
import
{
OpenAI
}
from
"langchain/llms/openai"
;
const
model
=
new
OpenAI
(
{
temperature
:
1
}
)
;
const
resA
=
await
model
.
call
(
"What would be a good company name a company that makes colorful socks?"
,
{
timeout
:
1000
}
// 1s timeout
)
;
console
.
log
(
{
resA
}
)
;
// '\n\nSocktastic Colors' }
取消请求
​
您可以在调用模型时传递一个
signal
选项来取消请求。例如，对于OpenAI:
import
{
OpenAI
}
from
"langchain/llms/openai"
;
const
model
=
new
OpenAI
(
{
temperature
:
1
}
)
;
const
controller
=
new
AbortController
(
)
;
// Call `controller.abort()` somewhere to cancel the request.
const
res
=
await
model
.
call
(
"What would be a good company name a company that makes colorful socks?"
,
{
signal
:
controller
.
signal
}
)
;
console
.
log
(
res
)
;
/*
'\n\nSocktastic Colors'
*/
请注意，如果底层提供程序公开了取消请求的选项，那么此操作仅会取消即将发出的请求。LangChain 将尽可能取消底层请求，否则它将取消响应的处理。
Dealing with Rate Limits
​
一些LLM提供商有速率限制。如果您超过了速率限制，将会出现错误。为了帮助您应对这个问题，LangChain在实例化LLM时提供了“maxConcurrency”选项。该选项允许您指定要发送到LLM提供商的最大并发请求数。如果您超过了这个数字，LangChain会自动将您的请求排队，以在先前的请求完成后发送。
例如，如果您设置
maxConcurrency: 5
，那么LangChain每次只会向LLM提供商发送5个请求。如果您发送了10个请求，前5个将会立即发送，而后5个将会排队等待。一旦前5个请求中的一个完成了，队列中的下一个请求将会发送。
要使用此功能，只需在实例化LLM时传递
maxConcurrency:<数字>
。例如:
import
{
OpenAI
}
from
"langchain/llms/openai"
;
const
model
=
new
OpenAI
(
{
maxConcurrency
:
5
}
)
;
处理API错误
​
如果模型提供商从其API返回错误，默认情况下，LangChain将在指数回退上最多重试6次。这样可以实现错误恢复，而无需任何额外的努力。如果您想更改此行为，可以在实例化模型时传递
maxRetries
选项。例如:
import
{
OpenAI
}
from
"langchain/llms/openai"
;
const
model
=
new
OpenAI
(
{
maxRetries
:
10
}
)
;
订阅事件
​
特别是在使用代理时，在 LLM 处理提示时可能会有大量的后台交互。对于代理，响应对象包含一个
intermediateSteps
对象，您可以打印该对象来查看它所采取的步骤概述。如果这不够用，并且您想要看到与 LLM 的每个交互，您可以将回调传递给 LLM，以进行自定义日志记录（或任何其他您想要执行的操作），当模型通过这些步骤时。
如需了解可用事件的更多信息，请参阅文档中的
[回调]
部分(/docs/production/callbacks/)。#Callbacks
import
{
LLMResult
}
from
"langchain/schema"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
// We can pass in a list of CallbackHandlers to the LLM constructor to get callbacks for various events.
const
model
=
new
OpenAI
(
{
callbacks
:
[
{
handleLLMStart
:
async
(
llm
:
{
name
:
string
}
,
prompts
:
string
[
]
)
=>
{
console
.
log
(
JSON
.
stringify
(
llm
,
null
,
2
)
)
;
console
.
log
(
JSON
.
stringify
(
prompts
,
null
,
2
)
)
;
}
,
handleLLMEnd
:
async
(
output
:
LLMResult
)
=>
{
console
.
log
(
JSON
.
stringify
(
output
,
null
,
2
)
)
;
}
,
handleLLMError
:
async
(
err
:
Error
)
=>
{
console
.
error
(
err
)
;
}
,
}
,
]
,
}
)
;
await
model
.
call
(
"What would be a good company name a company that makes colorful socks?"
)
;
// {
//     "name": "openai"
// }
// [
//     "What would be a good company name a company that makes colorful socks?"
// ]
// {
//   "generations": [
//     [
//         {
//             "text": "\n\nSocktastic Splashes.",
//             "generationInfo": {
//                 "finishReason": "stop",
//                 "logprobs": null
//             }
//         }
//     ]
//  ],
//   "llmOutput": {
//     "tokenUsage": {
//         "completionTokens": 9,
//          "promptTokens": 14,
//          "totalTokens": 23
//     }
//   }
// }



--- 文件: output_20250622_020018\docs\modules\models\llms\integrations.md ---
---
url: https://js.langchain.com.cn/docs/modules/models/llms/integrations
crawled_at: 2025-06-22T02:00:25.803898
---

集成: LLMs
LangChain提供了多种LLM实现，可与各种模型提供者集成。这些是:
OpenAI
​
import
{
OpenAI
}
from
"langchain/llms/openai"
;
const
model
=
new
OpenAI
(
{
temperature
:
0.9
,
openAIApiKey
:
"YOUR-API-KEY"
,
// In Node.js defaults to process.env.OPENAI_API_KEY
}
)
;
const
res
=
await
model
.
call
(
"What would be a good company name a company that makes colorful socks?"
)
;
console
.
log
(
{
res
}
)
;
Azure
OpenAI
​
import
{
OpenAI
}
from
"langchain/llms/openai"
;
const
model
=
new
OpenAI
(
{
temperature
:
0.9
,
azureOpenAIApiKey
:
"YOUR-API-KEY"
,
azureOpenAIApiInstanceName
:
"YOUR-INSTANCE-NAME"
,
azureOpenAIApiDeploymentName
:
"YOUR-DEPLOYMENT-NAME"
,
azureOpenAIApiVersion
:
"YOUR-API-VERSION"
,
}
)
;
const
res
=
await
model
.
call
(
"What would be a good company name a company that makes colorful socks?"
)
;
console
.
log
(
{
res
}
)
;
Google Vertex AI
​
Vertex AI实现适用于Node.js，而不适用于直接在浏览器中使用，因为它需要一个服务帐户来使用。
在运行此代码之前，请确保您的Google Cloud控制台的相关项目已启用Vertex AI API，并且您已使用以下方法之一进行了身份验证:
您已登录帐户（使用
gcloud auth application-default login
)
您正在运行使用许可的服务帐户的计算机上
您已下载了允许使用的服务帐户的凭据
permitted to that project.
to the project.
npm
Yarn
pnpm
to the project and
set
the
`
GOOGLE_APPLICATION_CREDENTIALS
`
environment
variable to the path of this file.
## `HuggingFaceInference`
``
`
bash
npm2yarn
import
GoogleVertexAIExample from
"!!raw-loader!@examples/llms/googlevertexai.ts"
;
<
CodeBlock
language
=
"typescript"
>
{
GoogleVertexAIExample
}
<
/CodeBlock
>
##
`
Cohere
`
`
``bash npm2yarn
npm
install
@huggingface/inference@1
to the project and
set
the
`
GOOGLE_APPLICATION_CREDENTIALS
`
environment
variable to the path of this file.
## `HuggingFaceInference`
``
`
bash
npm
undefined
# couldn't auto-convert command2yarn
import
GoogleVertexAIExample from
"!!raw-loader!@examples/llms/googlevertexai.ts"
;
<
CodeBlock
language
=
"typescript"
>
{
GoogleVertexAIExample
}
<
/CodeBlock
>
##
`
Cohere
`
`
``bash
npm
undefined
# couldn't auto-convert command2yarn
yarn
add
@huggingface/inference@1
to the project and
set
the
`
GOOGLE_APPLICATION_CREDENTIALS
`
environment
variable to the path of this file.
## `HuggingFaceInference`
``
`
bash
npm
undefined
# couldn't auto-convert command2yarn
import
GoogleVertexAIExample from
"!!raw-loader!@examples/llms/googlevertexai.ts"
;
<
CodeBlock
language
=
"typescript"
>
{
GoogleVertexAIExample
}
<
/CodeBlock
>
##
`
Cohere
`
`
``bash
npm
undefined
# couldn't auto-convert command2yarn
pnpm
add
@huggingface/inference@1
import
{
HuggingFaceInference
}
from
"langchain/llms/hf"
;
const
model
=
new
HuggingFaceInference
(
{
model
:
"gpt2"
,
apiKey
:
"YOUR-API-KEY"
,
// In Node.js defaults to process.env.HUGGINGFACEHUB_API_KEY
}
)
;
const
res
=
await
model
.
call
(
"1 + 1 ="
)
;
console
.
log
(
{
res
}
)
;
Replicate
​
npm
Yarn
pnpm
npm
install
cohere-ai
yarn
add
cohere-ai
pnpm
add
cohere-ai
import
{
Cohere
}
from
"langchain/llms/cohere"
;
const
model
=
new
Cohere
(
{
maxTokens
:
20
,
apiKey
:
"YOUR-API-KEY"
,
// In Node.js defaults to process.env.COHERE_API_KEY
}
)
;
const
res
=
await
model
.
call
(
"What would be a good company name a company that makes colorful socks?"
)
;
console
.
log
(
{
res
}
)
;
AWS SageMakerEndpoint
​
查阅
AWS SageMaker JumpStart
了解可用模型列表以及如何部署您自己的模型。
const
model
=
new
Replicate
(
{
model
:
"daanelson/flan-t5:04e422a9b85baed86a4f24981d7f9953e20c5fd82f6103b74ebc431588e1cec8"
,
apiKey
:
"YOUR-API-KEY"
,
// In Node.js defaults to process.env.REPLICATE_API_KEY
}
)
;
const
res
=
await
modelA
.
call
(
"What would be a good company name a company that makes colorful socks?"
)
;
console
.
log
(
{
res
}
)
;
AWS
SageMakerEndpoint
​
查阅
AWS SageMaker JumpStart
了解可用模型列表以及如何部署您自己的模型。
npm
Yarn
pnpm
npm
install
@aws-sdk/client-sagemaker-runtime
yarn
add
@aws-sdk/client-sagemaker-runtime
pnpm
add
@aws-sdk/client-sagemaker-runtime
import
{
SageMakerLLMContentHandler
,
SageMakerEndpoint
,
}
from
"langchain/llms/sagemaker_endpoint"
;
// Custom for whatever model you'll be using
class
HuggingFaceTextGenerationGPT2ContentHandler
implements
SageMakerLLMContentHandler
{
contentType
=
"application/json"
;
accepts
=
"application/json"
;
async
transformInput
(
prompt
:
string
,
modelKwargs
:
Record
<
string
,
unknown
>
)
{
const
inputString
=
JSON
.
stringify
(
{
text_inputs
:
prompt
,
...
modelKwargs
,
}
)
;
return
Buffer
.
from
(
inputString
)
;
}
async
transformOutput
(
output
:
Uint8Array
)
{
const
responseJson
=
JSON
.
parse
(
Buffer
.
from
(
output
)
.
toString
(
"utf-8"
)
)
;
return
responseJson
.
generated_texts
[
0
]
;
}
}
const
contentHandler
=
new
HuggingFaceTextGenerationGPT2ContentHandler
(
)
;
const
model
=
new
SageMakerEndpoint
(
{
endpointName
:
"jumpstart-example-huggingface-textgener-2023-05-16-22-35-45-660"
,
// Your endpoint name here
modelKwargs
:
{
temperature
:
1e-10
}
,
contentHandler
,
clientOptions
:
{
region
:
"YOUR AWS ENDPOINT REGION"
,
credentials
:
{
accessKeyId
:
"YOUR AWS ACCESS ID"
,
secretAccessKey
:
"YOUR AWS SECRET ACCESS KEY"
,
}
,
}
,
}
)
;
const
res
=
await
model
.
call
(
"Hello, my name is "
)
;
console
.
log
(
{
res
}
)
;
/*
{
res: "_____. I am a student at the University of California, Berkeley. I am a member of the American Association of University Professors."
}
*/
AI21
​
您可以通过在他们的网站上注册API密钥
[https://www.ai21.com/]
，使用AI21Labs的侏罗纪系列模型开始工作并查看可用的基础模型列表。
import
{
AI21
}
from
"langchain/llms/ai21"
;
const
model
=
new
AI21
(
{
ai21ApiKey
:
"YOUR_AI21_API_KEY"
,
// Or set as process.env.AI21_API_KEY
}
)
;
const
res
=
await
model
.
call
(
`
Translate "I love programming" into German.
`
)
;
console
.
log
(
{
res
}
)
;
/*
{
res: "\nIch liebe das Programmieren."
}
*/
其他LLM实现
​
PromptLayerOpenAI
​
LangChain与PromptLayer集成，以记录和调试提示信息和响应。要添加对PromptLayer的支持，请执行以下操作:：
在此处创建PromptLayer帐户:
https://promptlayer.com
。
创建API令牌，并将其作为
PromptLayerOpenAI
构造函数中的
promptLayerApiKey
参数或
PROMPTLAYER_API_KEY
环境变量传递。
import
{
PromptLayerOpenAI
}
from
"langchain/llms/openai"
;
const
model
=
new
PromptLayerOpenAI
(
{
temperature
:
0.9
,
openAIApiKey
:
"YOUR-API-KEY"
,
// In Node.js defaults to process.env.OPENAI_API_KEY
promptLayerApiKey
:
"YOUR-API-KEY"
,
// In Node.js defaults to process.env.PROMPTLAYER_API_KEY
}
)
;
const
res
=
await
model
.
call
(
"What would be a good company name a company that makes colorful socks?"
)
;
您还可以传递可选的
returnPromptLayerId
布尔值，以获得如下
promptLayerRequestId
You can also pass in the optional
returnPromptLayerId
boolean to get a
promptLayerRequestId
like below
import
{
PromptLayerOpenAI
}
from
"langchain/llms/openai"
;
const
model
=
new
PromptLayerOpenAI
(
{
temperature
:
0.9
,
openAIApiKey
:
"YOUR-API-KEY"
,
// In Node.js defaults to process.env.OPENAI_API_KEY
promptLayerApiKey
:
"YOUR-API-KEY"
,
// In Node.js defaults to process.env.PROMPTLAYER_API_KEY
returnPromptLayerId
:
true
,
}
)
;
const
res
=
await
model
.
generate
(
[
"What would be a good company name a company that makes colorful socks?"
,
]
)
;
console
.
log
(
JSON
.
stringify
(
res
,
null
,
3
)
)
;
/*
{
"generations": [
[
{
"text": " Socktastic!",
"generationInfo": {
"finishReason": "stop",
"logprobs": null,
"promptLayerRequestId": 2066417
}
}
]
],
"llmOutput": {
"tokenUsage": {
"completionTokens": 5,
"promptTokens": 23,
"totalTokens": 28
}
}
}
*/
Azure
PromptLayerOpenAI
​
LangChain与PromptLayer集成，以记录和调试提示信息和响应。要添加对PromptLayer的支持，请执行以下操作:：
在此处创建PromptLayer帐户:
https://promptlayer.com
。
创建API令牌，并将其作为
PromptLayerOpenAI
构造函数中的
promptLayerApiKey
参数或
PROMPTLAYER_API_KEY
环境变量传递。
import
{
PromptLayerOpenAI
}
from
"langchain/llms/openai"
;
const
model
=
new
PromptLayerOpenAI
(
{
temperature
:
0.9
,
azureOpenAIApiKey
:
"YOUR-AOAI-API-KEY"
,
// In Node.js defaults to process.env.AZURE_OPENAI_API_KEY
azureOpenAIApiInstanceName
:
"YOUR-AOAI-INSTANCE-NAME"
,
// In Node.js defaults to process.env.AZURE_OPENAI_API_INSTANCE_NAME
azureOpenAIApiDeploymentName
:
"YOUR-AOAI-DEPLOYMENT-NAME"
,
// In Node.js defaults to process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME
azureOpenAIApiCompletionsDeploymentName
:
"YOUR-AOAI-COMPLETIONS-DEPLOYMENT-NAME"
,
// In Node.js defaults to process.env.AZURE_OPENAI_API_COMPLETIONS_DEPLOYMENT_NAME
azureOpenAIApiEmbeddingsDeploymentName
:
"YOUR-AOAI-EMBEDDINGS-DEPLOYMENT-NAME"
,
// In Node.js defaults to process.env.AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME
azureOpenAIApiVersion
:
"YOUR-AOAI-API-VERSION"
,
// In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION
promptLayerApiKey
:
"YOUR-API-KEY"
,
// In Node.js defaults to process.env.PROMPTLAYER_API_KEY
}
)
;
const
res
=
await
model
.
call
(
"What would be a good company name a company that makes colorful socks?"
)
;
请求和响应将记录在
PromptLayer仪表板
中。
注意：在流式模式下，PromptLayer 不会记录响应。



--- 文件: output_20250622_020018\docs\modules\prompts\prompt_templates\additional_functionality.md ---
---
url: https://js.langchain.com.cn/docs/modules/prompts/prompt_templates/additional_functionality
crawled_at: 2025-06-22T02:00:25.922146
---

提示模板下的额外功能（Additional Functionality: Prompt Templates)
我们提供了一些额外的功能，以便在提示模板中展示，如下所示:（We offer a number of extra features for prompt templates as shown below)
提示值（Prompt Values)
​
PromptValue
是由
PromptTemplate
的
formatPromptValue
方法返回的对象。它可以被转换为一个字符串或一组
ChatMessage
对象。（A
PromptValue
is an object returned by the
formatPromptValue
of a
PromptTemplate
. It can be converted to a string or list of
ChatMessage
objects.)
import
{
ChatPromptTemplate
,
HumanMessagePromptTemplate
,
PromptTemplate
,
SystemMessagePromptTemplate
,
}
from
"langchain/prompts"
;
export
const
run
=
async
(
)
=>
{
const
template
=
"What is a good name for a company that makes {product}?"
;
const
promptA
=
new
PromptTemplate
(
{
template
,
inputVariables
:
[
"product"
]
}
)
;
// The `formatPromptValue` method returns a `PromptValue` object that can be used to format the prompt as a string or a list of `ChatMessage` objects.
const
responseA
=
await
promptA
.
formatPromptValue
(
{
product
:
"colorful socks"
,
}
)
;
const
responseAString
=
responseA
.
toString
(
)
;
console
.
log
(
{
responseAString
}
)
;
/*
{
responseAString: 'What is a good name for a company that makes colorful socks?'
}
*/
const
responseAMessages
=
responseA
.
toChatMessages
(
)
;
console
.
log
(
{
responseAMessages
}
)
;
/*
{
responseAMessages: [
HumanChatMessage {
text: 'What is a good name for a company that makes colorful socks?'
}
]
}
*/
const
chatPrompt
=
ChatPromptTemplate
.
fromPromptMessages
(
[
SystemMessagePromptTemplate
.
fromTemplate
(
"You are a helpful assistant that translates {input_language} to {output_language}."
)
,
HumanMessagePromptTemplate
.
fromTemplate
(
"{text}"
)
,
]
)
;
// `formatPromptValue` also works with `ChatPromptTemplate`.
const
responseB
=
await
chatPrompt
.
formatPromptValue
(
{
input_language
:
"English"
,
output_language
:
"French"
,
text
:
"I love programming."
,
}
)
;
const
responseBString
=
responseB
.
toString
(
)
;
console
.
log
(
{
responseBString
}
)
;
/*
{
responseBString: '[{"text":"You are a helpful assistant that translates English to French."},{"text":"I love programming."}]'
}
*/
const
responseBMessages
=
responseB
.
toChatMessages
(
)
;
console
.
log
(
{
responseBMessages
}
)
;
/*
{
responseBMessages: [
SystemChatMessage {
text: 'You are a helpful assistant that translates English to French.'
},
HumanChatMessage { text: 'I love programming.' }
]
}
*/
}
;
部分值（Partial Values)
​
与其他方法一样，“部分化”提示模板是有意义的，例如将所需的值的子集传递给其余值的子集以创建只期望该子集的新提示模板。（Like other methods, it can make sense to "partial" a prompt template - eg pass in a subset of the required values as to create a new prompt template which expects only the remaining subset of values.)
LangChain支持两种方式:（LangChain supports this in two ways:)
使用字符串值进行部分格式化。（1. Partial formatting with string values.)
使用返回字符串值的函数进行部分格式化。（2. Partial formatting with functions that return string values.)
这两种不同的方式支持不同的用例。在下面的示例中，我们探讨了两种使用情况以及如何在LangChain中实现它。（These two different ways support different use cases. In the examples below, we go over the motivations for both use cases as well as how to do it in LangChain.)
import
{
PromptTemplate
}
from
"langchain/prompts"
;
export
const
run
=
async
(
)
=>
{
// The `partial` method returns a new `PromptTemplate` object that can be used to format the prompt with only some of the input variables.
const
promptA
=
new
PromptTemplate
(
{
template
:
"{foo}{bar}"
,
inputVariables
:
[
"foo"
,
"bar"
]
,
}
)
;
const
partialPromptA
=
await
promptA
.
partial
(
{
foo
:
"foo"
}
)
;
console
.
log
(
await
partialPromptA
.
format
(
{
bar
:
"bar"
}
)
)
;
// foobar
// You can also explicitly specify the partial variables when creating the `PromptTemplate` object.
const
promptB
=
new
PromptTemplate
(
{
template
:
"{foo}{bar}"
,
inputVariables
:
[
"foo"
]
,
partialVariables
:
{
bar
:
"bar"
}
,
}
)
;
console
.
log
(
await
promptB
.
format
(
{
foo
:
"foo"
}
)
)
;
// foobar
// You can also use partial formatting with function inputs instead of string inputs.
const
promptC
=
new
PromptTemplate
(
{
template
:
"Tell me a {adjective} joke about the day {date}"
,
inputVariables
:
[
"adjective"
,
"date"
]
,
}
)
;
const
partialPromptC
=
await
promptC
.
partial
(
{
date
:
(
)
=>
new
Date
(
)
.
toLocaleDateString
(
)
,
}
)
;
console
.
log
(
await
partialPromptC
.
format
(
{
adjective
:
"funny"
}
)
)
;
// Tell me a funny joke about the day 3/22/2023
const
promptD
=
new
PromptTemplate
(
{
template
:
"Tell me a {adjective} joke about the day {date}"
,
inputVariables
:
[
"adjective"
]
,
partialVariables
:
{
date
:
(
)
=>
new
Date
(
)
.
toLocaleDateString
(
)
}
,
}
)
;
console
.
log
(
await
promptD
.
format
(
{
adjective
:
"funny"
}
)
)
;
// Tell me a funny joke about the day 3/22/2023
}
;
少量示例提示模板（Few-Shot Prompt Templates)
​
少量示例提示模板指的是可以通过示例构建的提示模板。（A few-shot prompt template is a prompt template you can build with examples.)
import
{
FewShotPromptTemplate
,
PromptTemplate
}
from
"langchain/prompts"
;
export
const
run
=
async
(
)
=>
{
// First, create a list of few-shot examples.
const
examples
=
[
{
word
:
"happy"
,
antonym
:
"sad"
}
,
{
word
:
"tall"
,
antonym
:
"short"
}
,
]
;
// Next, we specify the template to format the examples we have provided.
const
exampleFormatterTemplate
=
"Word: {word}\nAntonym: {antonym}\n"
;
const
examplePrompt
=
new
PromptTemplate
(
{
inputVariables
:
[
"word"
,
"antonym"
]
,
template
:
exampleFormatterTemplate
,
}
)
;
// Finally, we create the `FewShotPromptTemplate`
const
fewShotPrompt
=
new
FewShotPromptTemplate
(
{
/* These are the examples we want to insert into the prompt. */
examples
,
/* This is how we want to format the examples when we insert them into the prompt. */
examplePrompt
,
/* The prefix is some text that goes before the examples in the prompt. Usually, this consists of instructions. */
prefix
:
"Give the antonym of every input"
,
/* The suffix is some text that goes after the examples in the prompt. Usually, this is where the user input will go */
suffix
:
"Word: {input}\nAntonym:"
,
/* The input variables are the variables that the overall prompt expects. */
inputVariables
:
[
"input"
]
,
/* The example_separator is the string we will use to join the prefix, examples, and suffix together with. */
exampleSeparator
:
"\n\n"
,
/* The template format is the formatting method to use for the template. Should usually be f-string. */
templateFormat
:
"f-string"
,
}
)
;
// We can now generate a prompt using the `format` method.
console
.
log
(
await
fewShotPrompt
.
format
(
{
input
:
"big"
}
)
)
;
/*
Give the antonym of every input
Word: happy
Antonym: sad
Word: tall
Antonym: short
Word: big
Antonym:
*/
}
;



--- 文件: output_20250622_020018\docs\modules\prompts\prompt_templates\prompt_composition.md ---
---
url: https://js.langchain.com.cn/docs/modules/prompts/prompt_templates/prompt_composition
crawled_at: 2025-06-22T02:00:25.896527
---

提示组合
流水线提示模板允许您将多个单独的提示模板组合在一起。
当您想重用单个提示的部分时，这可能很有用。
与将
inputVariables
作为参数不同，流水线提示模板需要两个新参数:
pipelinePrompts
: 一个包含字符串 (
name
) 和
PromptTemplate
对象的数组。
每一个
PromptTemplate
会被格式化，然后作为一个输入变量传递给管道中下一个提示模板，并使用与
name
相同的名称。
finalPrompt
: 将返回的最终提示。
以下是实际操作的示例:
import
{
PromptTemplate
,
PipelinePromptTemplate
}
from
"langchain/prompts"
;
const
fullPrompt
=
PromptTemplate
.
fromTemplate
(
`
{introduction}
{example}
{start}
`
)
;
const
introductionPrompt
=
PromptTemplate
.
fromTemplate
(
`
You are impersonating {person}.
`
)
;
const
examplePrompt
=
PromptTemplate
.
fromTemplate
(
`
Here's an example of an interaction:
Q: {example_q}
A: {example_a}
`
)
;
const
startPrompt
=
PromptTemplate
.
fromTemplate
(
`
Now, do this for real!
Q: {input}
A:
`
)
;
const
composedPrompt
=
new
PipelinePromptTemplate
(
{
pipelinePrompts
:
[
{
name
:
"introduction"
,
prompt
:
introductionPrompt
,
}
,
{
name
:
"example"
,
prompt
:
examplePrompt
,
}
,
{
name
:
"start"
,
prompt
:
startPrompt
,
}
,
]
,
finalPrompt
:
fullPrompt
,
}
)
;
const
formattedPrompt
=
await
composedPrompt
.
format
(
{
person
:
"Elon Musk"
,
example_q
:
`
What's your favorite car?
`
,
example_a
:
"Telsa"
,
input
:
`
What's your favorite social media site?
`
,
}
)
;
console
.
log
(
formattedPrompt
)
;
/*
You are impersonating Elon Musk.
Here's an example of an interaction:
Q: What's your favorite car?
A: Telsa
Now, do this for real!
Q: What's your favorite social media site?
A:
*/



--- 文件: output_20250622_020018\docs\production\callbacks\create-handlers.md ---
---
url: https://js.langchain.com.cn/docs/production/callbacks/create-handlers
crawled_at: 2025-06-22T02:00:26.065778
---

创建回调处理程序
创建自定义处理程序
​
您还可以通过实现
BaseCallbackHandler
接口来创建自己的处理程序。如果您想做一些比仅记录到控制台更复杂的工作， 比如将事件发送到日志记录服务。这里是一个简单的处理程序实现，用于记录到控制台 :
import
{
BaseCallbackHandler
}
from
"langchain/callbacks"
;
import
{
AgentAction
,
AgentFinish
,
ChainValues
}
from
"langchain/schema"
;
export
class
MyCallbackHandler
extends
BaseCallbackHandler
{
name
=
"MyCallbackHandler"
;
async
handleChainStart
(
chain
:
{
name
:
string
}
)
{
console
.
log
(
`
Entering new
${
chain
.
name
}
chain...
`
)
;
}
async
handleChainEnd
(
_output
:
ChainValues
)
{
console
.
log
(
"Finished chain."
)
;
}
async
handleAgentAction
(
action
:
AgentAction
)
{
console
.
log
(
action
.
log
)
;
}
async
handleToolEnd
(
output
:
string
)
{
console
.
log
(
output
)
;
}
async
handleText
(
text
:
string
)
{
console
.
log
(
text
)
;
}
async
handleAgentEnd
(
action
:
AgentFinish
)
{
console
.
log
(
action
.
log
)
;
}
}
然后，你可以按照上面的
部分
所述使用它。



--- 文件: output_20250622_020018\docs\production\callbacks\creating-subclasses.md ---
---
url: https://js.langchain.com.cn/docs/production/callbacks/creating-subclasses
crawled_at: 2025-06-22T02:00:26.102183
---

自定义Chains/Agents中的回调
LangChain旨在可扩展。 您可以将自己的自定义Chains和Agents添加到库中。 本页将向您展示如何将回调添加到自定义的Chains和Agents中。
将回调添加到自定义的Chains中
​
当你创建一个自定义链时，你可以很容易地设置它使用与所有内置链相同的回调系统。请参阅此指南，了解有关如何
创建自定义链并在其中使用回调的更多信息
。



--- 文件: output_20250622_020018\docs\use_cases\agent_simulations\generative_agents.md ---
---
url: https://js.langchain.com.cn/docs/use_cases/agent_simulations/generative_agents
crawled_at: 2025-06-22T02:00:26.546225
---

生成式智能体
该脚本实现了一种基于论文
[Generating Agents  交互式仿真人类行为]
(https  //arxiv.org/abs/2304.03442)的生成式智能体, 作者为Park et.al.
在其中，我们利用了由LangChain检索器支持的时态加权记忆体对象。
下面的脚本创建了两个生成式智能体的实例Tommy和Eve，并通过他们的观察运行了一个交互的模拟。
Tommy扮演一个搬到新城镇寻找工作的人，Eve则扮演职业顾问。
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
MemoryVectorStore
}
from
"langchain/vectorstores/memory"
;
import
{
TimeWeightedVectorStoreRetriever
}
from
"langchain/retrievers/time_weighted"
;
import
{
GenerativeAgentMemory
,
GenerativeAgent
,
}
from
"langchain/experimental/generative_agents"
;
const
Simulation
=
async
(
)
=>
{
const
userName
=
"USER"
;
const
llm
=
new
OpenAI
(
{
temperature
:
0.9
,
maxTokens
:
1500
,
}
)
;
const
createNewMemoryRetriever
=
async
(
)
=>
{
// Create a new, demo in-memory vector store retriever unique to the agent.
// Better results can be achieved with a more sophisticatd vector store.
const
vectorStore
=
new
MemoryVectorStore
(
new
OpenAIEmbeddings
(
)
)
;
const
retriever
=
new
TimeWeightedVectorStoreRetriever
(
{
vectorStore
,
otherScoreKeys
:
[
"importance"
]
,
k
:
15
,
}
)
;
return
retriever
;
}
;
// Initializing Tommie
const
tommiesMemory
:
GenerativeAgentMemory
=
new
GenerativeAgentMemory
(
llm
,
await
createNewMemoryRetriever
(
)
,
{
reflectionThreshold
:
8
}
)
;
const
tommie
:
GenerativeAgent
=
new
GenerativeAgent
(
llm
,
tommiesMemory
,
{
name
:
"Tommie"
,
age
:
25
,
traits
:
"anxious, likes design, talkative"
,
status
:
"looking for a job"
,
}
)
;
console
.
log
(
"Tommie's first summary:\n"
,
await
tommie
.
getSummary
(
)
)
;
/*
Tommie's first summary:
Name: Tommie (age: 25)
Innate traits: anxious, likes design, talkative
Tommie is an individual with no specific core characteristics described.
*/
// Let's give Tommie some memories!
const
tommieObservations
=
[
"Tommie remembers his dog, Bruno, from when he was a kid"
,
"Tommie feels tired from driving so far"
,
"Tommie sees the new home"
,
"The new neighbors have a cat"
,
"The road is noisy at night"
,
"Tommie is hungry"
,
"Tommie tries to get some rest."
,
]
;
for
(
const
observation
of
tommieObservations
)
{
await
tommie
.
memory
.
addMemory
(
observation
,
new
Date
(
)
)
;
}
// Checking Tommie's summary again after giving him some memories
console
.
log
(
"Tommie's second summary:\n"
,
await
tommie
.
getSummary
(
{
forceRefresh
:
true
}
)
)
;
/*
Tommie's second summary:
Name: Tommie (age: 25)
Innate traits: anxious, likes design, talkative
Tommie remembers his dog, is tired from driving, sees a new home with neighbors who have a cat, is aware of the noisy road at night, is hungry, and tries to get some rest.
*/
const
interviewAgent
=
async
(
agent
:
GenerativeAgent
,
message
:
string
)
:
Promise
<
string
>
=>
{
// Simple wrapper helping the user interact with the agent
const
newMessage
=
`
${
userName
}
says
${
message
}
`
;
const
response
=
await
agent
.
generateDialogueResponse
(
newMessage
)
;
return
response
[
1
]
;
}
;
// Let's have Tommie start going through a day in his life.
const
observations
=
[
"Tommie wakes up to the sound of a noisy construction site outside his window."
,
"Tommie gets out of bed and heads to the kitchen to make himself some coffee."
,
"Tommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some."
,
"Tommie finally finds the filters and makes himself a cup of coffee."
,
"The coffee tastes bitter, and Tommie regrets not buying a better brand."
,
"Tommie checks his email and sees that he has no job offers yet."
,
"Tommie spends some time updating his resume and cover letter."
,
"Tommie heads out to explore the city and look for job openings."
,
"Tommie sees a sign for a job fair and decides to attend."
,
"The line to get in is long, and Tommie has to wait for an hour."
,
"Tommie meets several potential employers at the job fair but doesn't receive any offers."
,
"Tommie leaves the job fair feeling disappointed."
,
"Tommie stops by a local diner to grab some lunch."
,
"The service is slow, and Tommie has to wait for 30 minutes to get his food."
,
"Tommie overhears a conversation at the next table about a job opening."
,
"Tommie asks the diners about the job opening and gets some information about the company."
,
"Tommie decides to apply for the job and sends his resume and cover letter."
,
"Tommie continues his search for job openings and drops off his resume at several local businesses."
,
"Tommie takes a break from his job search to go for a walk in a nearby park."
,
"A dog approaches and licks Tommie's feet, and he pets it for a few minutes."
,
"Tommie sees a group of people playing frisbee and decides to join in."
,
"Tommie has fun playing frisbee but gets hit in the face with the frisbee and hurts his nose."
,
"Tommie goes back to his apartment to rest for a bit."
,
"A raccoon tore open the trash bag outside his apartment, and the garbage is all over the floor."
,
"Tommie starts to feel frustrated with his job search."
,
"Tommie calls his best friend to vent about his struggles."
,
"Tommie's friend offers some words of encouragement and tells him to keep trying."
,
"Tommie feels slightly better after talking to his friend."
,
]
;
// Let's send Tommie on his way. We'll check in on his summary every few observations to watch him evolve
for
(
let
i
=
0
;
i
<
observations
.
length
;
i
+=
1
)
{
const
observation
=
observations
[
i
]
;
const
[
,
reaction
]
=
await
tommie
.
generateReaction
(
observation
)
;
console
.
log
(
"\x1b[32m"
,
observation
,
"\x1b[0m"
,
reaction
)
;
if
(
(
i
+
1
)
%
20
===
0
)
{
console
.
log
(
"*"
.
repeat
(
40
)
)
;
console
.
log
(
"\x1b[34m"
,
`
After
${
i
+
1
}
observations, Tommie's summary is:\n
${
await
tommie
.
getSummary
(
{
forceRefresh
:
true
,
}
)
}
`
,
"\x1b[0m"
)
;
console
.
log
(
"*"
.
repeat
(
40
)
)
;
}
}
/*
Tommie wakes up to the sound of a noisy construction site outside his window.  Tommie REACT: Tommie groans in frustration and covers his ears with his pillow.
Tommie gets out of bed and heads to the kitchen to make himself some coffee.  Tommie REACT: Tommie rubs his tired eyes before heading to the kitchen to make himself some coffee.
Tommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some.  Tommie REACT: Tommie groans and looks through his moving boxes in search of coffee filters.
Tommie finally finds the filters and makes himself a cup of coffee.  Tommie REACT: Tommie sighs in relief and prepares himself a much-needed cup of coffee.
The coffee tastes bitter, and Tommie regrets not buying a better brand.  Tommie REACT: Tommie frowns in disappointment as he takes a sip of the bitter coffee.
Tommie checks his email and sees that he has no job offers yet.  Tommie REACT: Tommie sighs in disappointment before pushing himself away from the computer with a discouraged look on his face.
Tommie spends some time updating his resume and cover letter.  Tommie REACT: Tommie takes a deep breath and stares at the computer screen as he updates his resume and cover letter.
Tommie heads out to explore the city and look for job openings.  Tommie REACT: Tommie takes a deep breath and steps out into the city, ready to find the perfect job opportunity.
Tommie sees a sign for a job fair and decides to attend.  Tommie REACT: Tommie takes a deep breath and marches towards the job fair, determination in his eyes.
The line to get in is long, and Tommie has to wait for an hour.  Tommie REACT: Tommie groans in frustration as he notices the long line.
Tommie meets several potential employers at the job fair but doesn't receive any offers.  Tommie REACT: Tommie's face falls as he listens to each potential employer's explanation as to why they can't hire him.
Tommie leaves the job fair feeling disappointed.  Tommie REACT: Tommie's face falls as he walks away from the job fair, disappointment evident in his expression.
Tommie stops by a local diner to grab some lunch.  Tommie REACT: Tommie smiles as he remembers Bruno as he walks into the diner, feeling both a sense of nostalgia and excitement.
The service is slow, and Tommie has to wait for 30 minutes to get his food.  Tommie REACT: Tommie sighs in frustration and taps his fingers on the table, growing increasingly impatient.
Tommie overhears a conversation at the next table about a job opening.  Tommie REACT: Tommie leans in closer, eager to hear the conversation.
Tommie asks the diners about the job opening and gets some information about the company.  Tommie REACT: Tommie eagerly listens to the diner's description of the company, feeling hopeful about the job opportunity.
Tommie decides to apply for the job and sends his resume and cover letter.  Tommie REACT: Tommie confidently sends in his resume and cover letter, determined to get the job.
Tommie continues his search for job openings and drops off his resume at several local businesses.  Tommie REACT: Tommie confidently drops his resume off at the various businesses, determined to find a job.
Tommie takes a break from his job search to go for a walk in a nearby park.  Tommie REACT: Tommie takes a deep breath of the fresh air and smiles in appreciation as he strolls through the park.
A dog approaches and licks Tommie's feet, and he pets it for a few minutes.  Tommie REACT: Tommie smiles in surprise as he pets the dog, feeling a sense of comfort and nostalgia.
****************************************
After 20 observations, Tommie's summary is:
Name: Tommie (age: 25)
Innate traits: anxious, likes design, talkative
Tommie is a determined and resilient individual who remembers his dog from when he was a kid. Despite feeling tired from driving, he has the courage to explore the city, looking for job openings. He persists in updating his resume and cover letter in the pursuit of finding the perfect job opportunity, even attending job fairs when necessary, and is disappointed when he's not offered a job.
****************************************
Tommie sees a group of people playing frisbee and decides to join in.  Tommie REACT: Tommie smiles and approaches the group, eager to take part in the game.
Tommie has fun playing frisbee but gets hit in the face with the frisbee and hurts his nose.  Tommie REACT: Tommie grimaces in pain and raises his hand to his nose, checking to see if it's bleeding.
Tommie goes back to his apartment to rest for a bit.  Tommie REACT: Tommie yawns and trudges back to his apartment, feeling exhausted from his busy day.
A raccoon tore open the trash bag outside his apartment, and the garbage is all over the floor.  Tommie REACT: Tommie shakes his head in annoyance as he surveys the mess.
Tommie starts to feel frustrated with his job search.  Tommie REACT: Tommie sighs in frustration and shakes his head, feeling discouraged from his lack of progress.
Tommie calls his best friend to vent about his struggles.  Tommie REACT: Tommie runs his hands through his hair and sighs heavily, overwhelmed by his job search.
Tommie's friend offers some words of encouragement and tells him to keep trying.  Tommie REACT: Tommie gives his friend a grateful smile, feeling comforted by the words of encouragement.
Tommie feels slightly better after talking to his friend.  Tommie REACT: Tommie gives a small smile of appreciation to his friend, feeling grateful for the words of encouragement.
*/
// Interview after the day
console
.
log
(
await
interviewAgent
(
tommie
,
"Tell me about how your day has been going"
)
)
;
/*
Tommie said "My day has been pretty hectic. I've been driving around looking for job openings, attending job fairs, and updating my resume and cover letter. It's been really exhausting, but I'm determined to find the perfect job for me."
*/
console
.
log
(
await
interviewAgent
(
tommie
,
"How do you feel about coffee?"
)
)
;
/*
Tommie said "I actually love coffee - it's one of my favorite things. I try to drink it every day, especially when I'm stressed from job searching."
*/
console
.
log
(
await
interviewAgent
(
tommie
,
"Tell me about your childhood dog!"
)
)
;
/*
Tommie said "My childhood dog was named Bruno. He was an adorable black Labrador Retriever who was always full of energy. Every time I came home he'd be so excited to see me, it was like he never stopped smiling. He was always ready for adventure and he was always my shadow. I miss him every day."
*/
console
.
log
(
"Tommie's second summary:\n"
,
await
tommie
.
getSummary
(
{
forceRefresh
:
true
}
)
)
;
/*
Tommie's second summary:
Name: Tommie (age: 25)
Innate traits: anxious, likes design, talkative
Tommie is a hardworking individual who is looking for new opportunities. Despite feeling tired, he is determined to find the perfect job. He remembers his dog from when he was a kid, is hungry, and is frustrated at times. He shows resilience when searching for his coffee filters, disappointment when checking his email and finding no job offers, and determination when attending the job fair.
*/
// Let’s add a second character to have a conversation with Tommie. Feel free to configure different traits.
const
evesMemory
:
GenerativeAgentMemory
=
new
GenerativeAgentMemory
(
llm
,
await
createNewMemoryRetriever
(
)
,
{
verbose
:
false
,
reflectionThreshold
:
5
,
}
)
;
const
eve
:
GenerativeAgent
=
new
GenerativeAgent
(
llm
,
evesMemory
,
{
name
:
"Eve"
,
age
:
34
,
traits
:
"curious, helpful"
,
status
:
"just started her new job as a career counselor last week and received her first assignment, a client named Tommie."
,
// dailySummaries: [
//   "Eve started her new job as a career counselor last week and received her first assignment, a client named Tommie."
// ]
}
)
;
const
eveObservations
=
[
"Eve overhears her colleague say something about a new client being hard to work with"
,
"Eve wakes up and hears the alarm"
,
"Eve eats a boal of porridge"
,
"Eve helps a coworker on a task"
,
"Eve plays tennis with her friend Xu before going to work"
,
"Eve overhears her colleague say something about Tommie being hard to work with"
,
]
;
for
(
const
observation
of
eveObservations
)
{
await
eve
.
memory
.
addMemory
(
observation
,
new
Date
(
)
)
;
}
const
eveInitialSummary
:
string
=
await
eve
.
getSummary
(
{
forceRefresh
:
true
,
}
)
;
console
.
log
(
"Eve's initial summary\n"
,
eveInitialSummary
)
;
/*
Eve's initial summary
Name: Eve (age: 34)
Innate traits: curious, helpful
Eve is an attentive listener, helpful colleague, and sociable friend who enjoys playing tennis.
*/
// Let’s “Interview” Eve before she speaks with Tommie.
console
.
log
(
await
interviewAgent
(
eve
,
"How are you feeling about today?"
)
)
;
/*
Eve said "I'm feeling a bit anxious about meeting my new client, but I'm sure it will be fine! How about you?".
*/
console
.
log
(
await
interviewAgent
(
eve
,
"What do you know about Tommie?"
)
)
;
/*
Eve said "I know that Tommie is a recent college graduate who's been struggling to find a job. I'm looking forward to figuring out how I can help him move forward."
*/
console
.
log
(
await
interviewAgent
(
eve
,
"Tommie is looking to find a job. What are are some things you'd like to ask him?"
)
)
;
/*
Eve said: "I'd really like to get to know more about Tommie's professional background and experience, and why he is looking for a job. And I'd also like to know more about his strengths and passions and what kind of work he would be best suited for. That way I can help him find the right job to fit his needs."
*/
// Generative agents are much more complex when they interact with a virtual environment or with each other.
// Below, we run a simple conversation between Tommie and Eve.
const
runConversation
=
async
(
agents
:
GenerativeAgent
[
]
,
initialObservation
:
string
)
:
Promise
<
void
>
=>
{
// Starts the conversation bewteen two agents
const
[
,
observation
]
=
await
agents
[
1
]
.
generateReaction
(
initialObservation
)
;
console
.
log
(
"Initial reply:"
,
observation
)
;
// eslint-disable-next-line no-constant-condition
while
(
true
)
{
let
breakDialogue
=
false
;
for
(
const
agent
of
agents
)
{
const
[
stayInDialogue
,
agentObservation
]
=
await
agent
.
generateDialogueResponse
(
observation
)
;
console
.
log
(
"Next reply:"
,
agentObservation
)
;
if
(
!
stayInDialogue
)
{
breakDialogue
=
true
;
}
}
if
(
breakDialogue
)
{
break
;
}
}
}
;
const
agents
:
GenerativeAgent
[
]
=
[
tommie
,
eve
]
;
await
runConversation
(
agents
,
"Tommie said: Hi, Eve. Thanks for agreeing to meet with me today. I have a bunch of questions and am not sure where to start. Maybe you could first share about your experience?"
)
;
/*
Initial reply: Eve said "Of course, Tommie. I'd be happy to share about my experience. What specific questions do you have?"
Next reply: Tommie said "Thank you, Eve. I'm curious about what strategies you used in your own job search. Did you have any specific tactics that helped you stand out to employers?"
Next reply: Eve said "Sure, Tommie. I found that networking and reaching out to professionals in my field was really helpful. I also made sure to tailor my resume and cover letter to each job I applied to. Do you have any specific questions about those strategies?"
Next reply: Tommie said "Thank you, Eve. That's really helpful advice. Did you have any specific ways of networking that worked well for you?"
Next reply: Eve said "Sure, Tommie. I found that attending industry events and connecting with professionals on LinkedIn were both great ways to network. Do you have any specific questions about those tactics?"
Next reply: Tommie said "That's really helpful, thank you for sharing. Did you find that you were able to make meaningful connections through LinkedIn?"
Next reply: Eve said "Yes, definitely. I was able to connect with several professionals in my field and even landed a job through a LinkedIn connection. Have you had any luck with networking on LinkedIn?"
Next reply: Tommie said "That's really impressive! I haven't had much luck yet, but I'll definitely keep trying. Thank you for the advice, Eve."
Next reply: Eve said "Glad I could help, Tommie. Is there anything else you want to know?"
Next reply: Tommie said "Thanks again, Eve. I really appreciate your advice and I'll definitely put it into practice. Have a great day!"
Next reply: Eve said "You're welcome, Tommie! Don't hesitate to reach out if you have any more questions. Have a great day too!"
*/
// Since the generative agents retain their memories from the day, we can ask them about their plans, conversations, and other memories.
const
tommieSummary
:
string
=
await
tommie
.
getSummary
(
{
forceRefresh
:
true
,
}
)
;
console
.
log
(
"Tommie's third and final summary\n"
,
tommieSummary
)
;
/*
Tommie's third and final summary
Name: Tommie (age: 25)
Innate traits: anxious, likes design, talkative
Tommie is a determined individual, who demonstrates resilience in the face of disappointment. He is also a nostalgic person, remembering fondly his childhood pet, Bruno. He is resourceful, searching through his moving boxes to find what he needs, and takes initiative to attend job fairs to look for job openings.
*/
const
eveSummary
:
string
=
await
eve
.
getSummary
(
{
forceRefresh
:
true
}
)
;
console
.
log
(
"Eve's final summary\n"
,
eveSummary
)
;
/*
Eve's final summary
Name: Eve (age: 34)
Innate traits: curious, helpful
Eve is a helpful and encouraging colleague who actively listens to her colleagues and offers advice on how to move forward. She is willing to take time to understand her clients and their goals, and is committed to helping them succeed.
*/
const
interviewOne
:
string
=
await
interviewAgent
(
tommie
,
"How was your conversation with Eve?"
)
;
console
.
log
(
"USER: How was your conversation with Eve?\n"
)
;
console
.
log
(
interviewOne
)
;
/*
Tommie said "It was great. She was really helpful and knowledgeable. I'm thankful that she took the time to answer all my questions."
*/
const
interviewTwo
:
string
=
await
interviewAgent
(
eve
,
"How was your conversation with Tommie?"
)
;
console
.
log
(
"USER: How was your conversation with Tommie?\n"
)
;
console
.
log
(
interviewTwo
)
;
/*
Eve said "The conversation went very well. We discussed his goals and career aspirations, what kind of job he is looking for, and his experience and qualifications. I'm confident I can help him find the right job."
*/
const
interviewThree
:
string
=
await
interviewAgent
(
eve
,
"What do you wish you would have said to Tommie?"
)
;
console
.
log
(
"USER: What do you wish you would have said to Tommie?\n"
)
;
console
.
log
(
interviewThree
)
;
/*
Eve said "It's ok if you don't have all the answers yet. Let's take some time to learn more about your experience and qualifications, so I can help you find a job that fits your goals."
*/
return
{
tommieFinalSummary
:
tommieSummary
,
eveFinalSummary
:
eveSummary
,
interviewOne
,
interviewTwo
,
interviewThree
,
}
;
}
;
const
runSimulation
=
async
(
)
=>
{
try
{
await
Simulation
(
)
;
}
catch
(
error
)
{
console
.
log
(
"error running simulation:"
,
error
)
;
throw
error
;
}
}
;
await
runSimulation
(
)
;



--- 文件: output_20250622_020018\docs\use_cases\autonomous_agents\auto_gpt.md ---
---
url: https://js.langchain.com.cn/docs/use_cases/autonomous_agents/auto_gpt
crawled_at: 2025-06-22T02:00:26.421131
---

AutoGPT
info
AutoGPT是一个使用长期记忆和专为独立工作设计的提示（即无需要求用户输入)的自定义代理来执行任务。
同构示例
​
在这个例子中，我们使用AutoGPT为给定位置预测天气。 这个例子被设计为运行在所有的JS环境中，包括浏览器。
Node.js示例
​
import
{
AutoGPT
}
from
"langchain/experimental/autogpt"
;
import
{
ReadFileTool
,
WriteFileTool
,
SerpAPI
}
from
"langchain/tools"
;
import
{
InMemoryFileStore
}
from
"langchain/stores/file/in_memory"
;
import
{
MemoryVectorStore
}
from
"langchain/vectorstores/memory"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
const
store
=
new
InMemoryFileStore
(
)
;
const
tools
=
[
new
ReadFileTool
(
{
store
}
)
,
new
WriteFileTool
(
{
store
}
)
,
new
SerpAPI
(
process
.
env
.
SERPAPI_API_KEY
,
{
location
:
"San Francisco,California,United States"
,
hl
:
"en"
,
gl
:
"us"
,
}
)
,
]
;
const
vectorStore
=
new
MemoryVectorStore
(
new
OpenAIEmbeddings
(
)
)
;
const
autogpt
=
AutoGPT
.
fromLLMAndTools
(
new
ChatOpenAI
(
{
temperature
:
0
}
)
,
tools
,
{
memory
:
vectorStore
.
asRetriever
(
)
,
aiName
:
"Tom"
,
aiRole
:
"Assistant"
,
}
)
;
await
autogpt
.
run
(
[
"write a weather report for SF today"
]
)
;
/*
{
"thoughts": {
"text": "I need to write a weather report for SF today. I should use a search engine to find the current weather conditions.",
"reasoning": "I don't have the current weather information for SF in my short term memory, so I need to use a search engine to find it.",
"plan": "- Use the search command to find the current weather conditions for SF\n- Write a weather report based on the information found",
"criticism": "I need to make sure that the information I find is accurate and up-to-date.",
"speak": "I will use the search command to find the current weather conditions for SF."
},
"command": {
"name": "search",
"args": {
"input": "current weather conditions San Francisco"
}
}
}
{
"thoughts": {
"text": "I have found the current weather conditions for SF. I need to write a weather report based on this information.",
"reasoning": "I have the information I need to write a weather report, so I should use the write_file command to save it to a file.",
"plan": "- Use the write_file command to save the weather report to a file",
"criticism": "I need to make sure that the weather report is clear and concise.",
"speak": "I will use the write_file command to save the weather report to a file."
},
"command": {
"name": "write_file",
"args": {
"file_path": "weather_report.txt",
"text": "San Francisco Weather Report:\n\nMorning: 53°, Chance of Rain 1%\nAfternoon: 59°, Chance of Rain 0%\nEvening: 52°, Chance of Rain 3%\nOvernight: 48°, Chance of Rain 2%"
}
}
}
{
"thoughts": {
"text": "I have completed all my objectives. I will use the finish command to signal that I am done.",
"reasoning": "I have completed the task of writing a weather report for SF today, so I don't need to do anything else.",
"plan": "- Use the finish command to signal that I am done",
"criticism": "I need to make sure that I have completed all my objectives before using the finish command.",
"speak": "I will use the finish command to signal that I am done."
},
"command": {
"name": "finish",
"args": {
"response": "I have completed all my objectives."
}
}
}
*/
在这个示例中，我们使用AutoGPT为给定位置预测天气。 这个示例被设计为在Node.js中运行，因此它使用本地文件系统和一个仅限Node的向量存储。
This example is designed to run in Node.js， and uses the local filesystem and a Node-only vector store.
​
import
{
AutoGPT
}
from
"langchain/experimental/autogpt"
;
import
{
ReadFileTool
,
WriteFileTool
,
SerpAPI
}
from
"langchain/tools"
;
import
{
NodeFileStore
}
from
"langchain/stores/file/node"
;
import
{
HNSWLib
}
from
"langchain/vectorstores/hnswlib"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
const
store
=
new
NodeFileStore
(
)
;
const
tools
=
[
new
ReadFileTool
(
{
store
}
)
,
new
WriteFileTool
(
{
store
}
)
,
new
SerpAPI
(
process
.
env
.
SERPAPI_API_KEY
,
{
location
:
"San Francisco,California,United States"
,
hl
:
"en"
,
gl
:
"us"
,
}
)
,
]
;
const
vectorStore
=
new
HNSWLib
(
new
OpenAIEmbeddings
(
)
,
{
space
:
"cosine"
,
numDimensions
:
1536
,
}
)
;
const
autogpt
=
AutoGPT
.
fromLLMAndTools
(
new
ChatOpenAI
(
{
temperature
:
0
}
)
,
tools
,
{
memory
:
vectorStore
.
asRetriever
(
)
,
aiName
:
"Tom"
,
aiRole
:
"Assistant"
,
}
)
;
await
autogpt
.
run
(
[
"write a weather report for SF today"
]
)
;
/*
{
"thoughts": {
"text": "I need to write a weather report for SF today. I should use a search engine to find the current weather conditions.",
"reasoning": "I don't have the current weather information for SF in my short term memory, so I need to use a search engine to find it.",
"plan": "- Use the search command to find the current weather conditions for SF\n- Write a weather report based on the information found",
"criticism": "I need to make sure that the information I find is accurate and up-to-date.",
"speak": "I will use the search command to find the current weather conditions for SF."
},
"command": {
"name": "search",
"args": {
"input": "current weather conditions San Francisco"
}
}
}
{
"thoughts": {
"text": "I have found the current weather conditions for SF. I need to write a weather report based on this information.",
"reasoning": "I have the information I need to write a weather report, so I should use the write_file command to save it to a file.",
"plan": "- Use the write_file command to save the weather report to a file",
"criticism": "I need to make sure that the weather report is clear and concise.",
"speak": "I will use the write_file command to save the weather report to a file."
},
"command": {
"name": "write_file",
"args": {
"file_path": "weather_report.txt",
"text": "San Francisco Weather Report:\n\nMorning: 53°, Chance of Rain 1%\nAfternoon: 59°, Chance of Rain 0%\nEvening: 52°, Chance of Rain 3%\nOvernight: 48°, Chance of Rain 2%"
}
}
}
{
"thoughts": {
"text": "I have completed all my objectives. I will use the finish command to signal that I am done.",
"reasoning": "I have completed the task of writing a weather report for SF today, so I don't need to do anything else.",
"plan": "- Use the finish command to signal that I am done",
"criticism": "I need to make sure that I have completed all my objectives before using the finish command.",
"speak": "I will use the finish command to signal that I am done."
},
"command": {
"name": "finish",
"args": {
"response": "I have completed all my objectives."
}
}
}
*/



--- 文件: output_20250622_020018\docs\use_cases\autonomous_agents\baby_agi.md ---
---
url: https://js.langchain.com.cn/docs/use_cases/autonomous_agents/baby_agi
crawled_at: 2025-06-22T02:00:26.523685
---

BabyAGI
info
原始GitHub链接:
https://github.com/yoheinakajima/babyagi
BabyAGI由三个组成部分构成:
一个创建任务的链
一个负责优先处理任务的链
一个执行任务的链
这些链按顺序执行，直到任务列表为空或达到最大迭代次数。
简单示例
​
在这个示例中，我们直接使用BabyAGI没有任何工具。您会发现成功创建了任务列表，但在执行任务时我们没有得到具体结果。这是因为我们没有为BabyAGI提供任何工具。在下一个示例中，我们将看到如何做到这一点。
import
{
BabyAGI
}
from
"langchain/experimental/babyagi"
;
import
{
MemoryVectorStore
}
from
"langchain/vectorstores/memory"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
const
vectorStore
=
new
MemoryVectorStore
(
new
OpenAIEmbeddings
(
)
)
;
const
babyAGI
=
BabyAGI
.
fromLLM
(
{
llm
:
new
OpenAI
(
{
temperature
:
0
}
)
,
vectorstore
:
vectorStore
,
maxIterations
:
3
,
}
)
;
await
babyAGI
.
call
(
{
objective
:
"Write a weather report for SF today"
}
)
;
/*
*****TASK LIST*****
1: Make a todo list
*****NEXT TASK*****
1: Make a todo list
*****TASK RESULT*****
1. Check the weather forecast for San Francisco today
2. Make note of the temperature, humidity, wind speed, and other relevant weather conditions
3. Write a weather report summarizing the forecast
4. Check for any weather alerts or warnings
5. Share the report with the relevant stakeholders
*****TASK LIST*****
2: Check the current temperature in San Francisco
3: Check the current humidity in San Francisco
4: Check the current wind speed in San Francisco
5: Check for any weather alerts or warnings in San Francisco
6: Check the forecast for the next 24 hours in San Francisco
7: Check the forecast for the next 48 hours in San Francisco
8: Check the forecast for the next 72 hours in San Francisco
9: Check the forecast for the next week in San Francisco
10: Check the forecast for the next month in San Francisco
11: Check the forecast for the next 3 months in San Francisco
1: Write a weather report for SF today
*****NEXT TASK*****
2: Check the current temperature in San Francisco
*****TASK RESULT*****
I will check the current temperature in San Francisco. I will use an online weather service to get the most up-to-date information.
*****TASK LIST*****
3: Check the current UV index in San Francisco
4: Check the current air quality in San Francisco
5: Check the current precipitation levels in San Francisco
6: Check the current cloud cover in San Francisco
7: Check the current barometric pressure in San Francisco
8: Check the current dew point in San Francisco
9: Check the current wind direction in San Francisco
10: Check the current humidity levels in San Francisco
1: Check the current temperature in San Francisco to the average temperature for this time of year
2: Check the current visibility in San Francisco
11: Write a weather report for SF today
*****NEXT TASK*****
3: Check the current UV index in San Francisco
*****TASK RESULT*****
The current UV index in San Francisco is moderate, with a value of 5. This means that it is safe to be outside for short periods of time without sunscreen, but it is still recommended to wear sunscreen and protective clothing when outside for extended periods of time.
*/
带工具的示例
​
在这个示例中，我们用一个带有搜索工具的自定义代理替换了执行链。这使得BabyAGI能够在执行任务时使用真实世界的数据，从而使其更加强大。您可以添加其他工具来增强BabyAGI的能力。
import
{
BabyAGI
}
from
"langchain/experimental/babyagi"
;
import
{
MemoryVectorStore
}
from
"langchain/vectorstores/memory"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
PromptTemplate
}
from
"langchain/prompts"
;
import
{
LLMChain
}
from
"langchain/chains"
;
import
{
ChainTool
,
SerpAPI
,
Tool
}
from
"langchain/tools"
;
import
{
initializeAgentExecutorWithOptions
}
from
"langchain/agents"
;
// First, we create a custom agent which will serve as execution chain.
const
todoPrompt
=
PromptTemplate
.
fromTemplate
(
"You are a planner who is an expert at coming up with a todo list for a given objective. Come up with a todo list for this objective: {objective}"
)
;
const
tools
:
Tool
[
]
=
[
new
SerpAPI
(
process
.
env
.
SERPAPI_API_KEY
,
{
location
:
"San Francisco,California,United States"
,
hl
:
"en"
,
gl
:
"us"
,
}
)
,
new
ChainTool
(
{
name
:
"TODO"
,
chain
:
new
LLMChain
(
{
llm
:
new
OpenAI
(
{
temperature
:
0
}
)
,
prompt
:
todoPrompt
,
}
)
,
description
:
"useful for when you need to come up with todo lists. Input: an objective to create a todo list for. Output: a todo list for that objective. Please be very clear what the objective is!"
,
}
)
,
]
;
const
agentExecutor
=
await
initializeAgentExecutorWithOptions
(
tools
,
new
OpenAI
(
{
temperature
:
0
}
)
,
{
agentType
:
"zero-shot-react-description"
,
agentArgs
:
{
prefix
:
`
You are an AI who performs one task based on the following objective: {objective}. Take into account these previously completed tasks: {context}.
`
,
suffix
:
`
Question: {task}
{agent_scratchpad}
`
,
inputVariables
:
[
"objective"
,
"task"
,
"context"
,
"agent_scratchpad"
]
,
}
,
}
)
;
const
vectorStore
=
new
MemoryVectorStore
(
new
OpenAIEmbeddings
(
)
)
;
// Then, we create a BabyAGI instance.
const
babyAGI
=
BabyAGI
.
fromLLM
(
{
llm
:
new
OpenAI
(
{
temperature
:
0
}
)
,
executionChain
:
agentExecutor
,
// an agent executor is a chain
vectorstore
:
vectorStore
,
maxIterations
:
10
,
}
)
;
await
babyAGI
.
call
(
{
objective
:
"Write a short weather report for SF today"
}
)
;
/*
*****TASK LIST*****
1: Make a todo list
*****NEXT TASK*****
1: Make a todo list
*****TASK RESULT*****
Today in San Francisco, the weather is sunny with a temperature of 70 degrees Fahrenheit, light winds, and low humidity. The forecast for the next few days is expected to be similar.
*****TASK LIST*****
2: Find the forecasted temperature for the next few days in San Francisco
3: Find the forecasted wind speed for the next few days in San Francisco
4: Find the forecasted humidity for the next few days in San Francisco
5: Create a graph showing the forecasted temperature, wind speed, and humidity for San Francisco over the next few days
6: Research the average temperature for San Francisco in the past week
7: Research the average wind speed for San Francisco in the past week
8: Research the average humidity for San Francisco in the past week
9: Create a graph showing the temperature, wind speed, and humidity for San Francisco over the past week
*****NEXT TASK*****
2: Find the forecasted temperature for the next few days in San Francisco
*****TASK RESULT*****
The forecasted temperature for the next few days in San Francisco is 63°, 65°, 71°, 73°, and 66°.
*****TASK LIST*****
3: Find the forecasted wind speed for the next few days in San Francisco
4: Find the forecasted humidity for the next few days in San Francisco
5: Create a graph showing the forecasted temperature, wind speed, and humidity for San Francisco over the next few days
6: Research the average temperature for San Francisco in the past week
7: Research the average wind speed for San Francisco in the past week
8: Research the average humidity for San Francisco in the past week
9: Create a graph showing the temperature, wind speed, and humidity for San Francisco over the past week
10: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to the average temperature, wind speed, and humidity for San Francisco over the past week
11: Find the forecasted precipitation for the next few days in San Francisco
12: Research the average wind direction for San Francisco in the past week
13: Create a graph showing the forecasted temperature, wind speed, and humidity for San Francisco over the past week
14: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to
*****NEXT TASK*****
3: Find the forecasted wind speed for the next few days in San Francisco
*****TASK RESULT*****
West winds 10 to 20 mph. Gusts up to 35 mph in the evening. Tuesday. Sunny. Highs in the 60s to upper 70s. West winds 5 to 15 mph.
*****TASK LIST*****
4: Research the average precipitation for San Francisco in the past week
5: Research the average temperature for San Francisco in the past week
6: Research the average wind speed for San Francisco in the past week
7: Research the average humidity for San Francisco in the past week
8: Research the average wind direction for San Francisco in the past week
9: Find the forecasted temperature, wind speed, and humidity for San Francisco over the next few days
10: Find the forecasted precipitation for the next few days in San Francisco
11: Create a graph showing the forecasted temperature, wind speed, and humidity for San Francisco over the next few days
12: Create a graph showing the temperature, wind speed, and humidity for San Francisco over the past week
13: Create a graph showing the forecasted temperature, wind speed, and humidity for San Francisco over the past month
14: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to the average temperature, wind speed, and humidity for San Francisco over the past week
15: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to the
*****NEXT TASK*****
4: Research the average precipitation for San Francisco in the past week
*****TASK RESULT*****
According to Weather Underground, the forecasted precipitation for San Francisco in the next few days is 7-hour rain and snow with 24-hour rain accumulation.
*****TASK LIST*****
5: Research the average wind speed for San Francisco over the past month
6: Create a graph showing the forecasted temperature, wind speed, and humidity for San Francisco over the past month
7: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to the average temperature, wind speed, and humidity for San Francisco over the past month
8: Research the average temperature for San Francisco over the past month
9: Research the average wind direction for San Francisco over the past month
10: Create a graph showing the forecasted precipitation for San Francisco over the next few days
11: Compare the forecasted precipitation for San Francisco over the next few days to the average precipitation for San Francisco over the past week
12: Find the forecasted temperature, wind speed, and humidity for San Francisco over the next few days
13: Find the forecasted precipitation for the next few days in San Francisco
14: Create a graph showing the temperature, wind speed, and humidity for San Francisco over the past week
15: Create a graph showing the forecasted temperature, wind speed, and humidity for San Francisco over the next few days
16: Compare the forecast
*****NEXT TASK*****
5: Research the average wind speed for San Francisco over the past month
*****TASK RESULT*****
The average wind speed for San Francisco over the past month is 3.2 meters per second.
*****TASK LIST*****
6: Find the forecasted temperature, wind speed, and humidity for San Francisco over the next few days,
7: Find the forecasted precipitation for the next few days in San Francisco,
8: Create a graph showing the temperature, wind speed, and humidity for San Francisco over the past week,
9: Create a graph showing the forecasted temperature, wind speed, and humidity for San Francisco over the next few days,
10: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to the average wind speed for San Francisco over the past month,
11: Research the average wind speed for San Francisco over the past week,
12: Create a graph showing the forecasted precipitation for San Francisco over the next few days,
13: Compare the forecasted precipitation for San Francisco over the next few days to the average precipitation for San Francisco over the past month,
14: Research the average temperature for San Francisco over the past month,
15: Research the average humidity for San Francisco over the past month,
16: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to the average temperature,
*****NEXT TASK*****
6: Find the forecasted temperature, wind speed, and humidity for San Francisco over the next few days,
*****TASK RESULT*****
The forecast for San Francisco over the next few days is mostly sunny, with a high near 64. West wind 7 to 12 mph increasing to 13 to 18 mph in the afternoon. Winds could gust as high as 22 mph. Humidity will be around 50%.
*****TASK LIST*****
7: Find the forecasted precipitation for the next few days in San Francisco,
8: Create a graph showing the temperature, wind speed, and humidity for San Francisco over the past week,
9: Create a graph showing the forecasted temperature, wind speed, and humidity for San Francisco over the next few days,
10: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to the average wind speed for San Francisco over the past month,
11: Research the average wind speed for San Francisco over the past week,
12: Create a graph showing the forecasted precipitation for San Francisco over the next few days,
13: Compare the forecasted precipitation for San Francisco over the next few days to the average precipitation for San Francisco over the past month,
14: Research the average temperature for San Francisco over the past month,
15: Research the average humidity for San Francisco over the past month,
16: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to the average temperature
*****NEXT TASK*****
7: Find the forecasted precipitation for the next few days in San Francisco,
*****TASK RESULT*****
According to Weather Underground, the forecasted precipitation for the next few days in San Francisco is 7-hour rain and snow with 24-hour rain accumulation, radar and satellite maps of precipitation.
*****TASK LIST*****
8: Create a graph showing the temperature, wind speed, and humidity for San Francisco over the past week,
9: Create a graph showing the forecasted temperature, wind speed, and humidity for San Francisco over the next few days,
10: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to the average wind speed for San Francisco over the past month,
11: Research the average wind speed for San Francisco over the past week,
12: Create a graph showing the forecasted precipitation for San Francisco over the next few days,
13: Compare the forecasted precipitation for San Francisco over the next few days to the average precipitation for San Francisco over the past month,
14: Research the average temperature for San Francisco over the past month,
15: Research the average humidity for San Francisco over the past month,
16: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to the average temperature
*****NEXT TASK*****
8: Create a graph showing the temperature, wind speed, and humidity for San Francisco over the past week,
*****TASK RESULT*****
A graph showing the temperature, wind speed, and humidity for San Francisco over the past week.
*****TASK LIST*****
9: Create a graph showing the forecasted temperature, wind speed, and humidity for San Francisco over the next few days
10: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to the average wind speed for San Francisco over the past month
11: Research the average wind speed for San Francisco over the past week
12: Create a graph showing the forecasted precipitation for San Francisco over the next few days
13: Compare the forecasted precipitation for San Francisco over the next few days to the average precipitation for San Francisco over the past month
14: Research the average temperature for San Francisco over the past month
15: Research the average humidity for San Francisco over the past month
16: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to the average temperature
*****NEXT TASK*****
9: Create a graph showing the forecasted temperature, wind speed, and humidity for San Francisco over the next few days
*****TASK RESULT*****
The forecasted temperature, wind speed, and humidity for San Francisco over the next few days can be seen in the graph created.
*****TASK LIST*****
10: Research the average wind speed for San Francisco over the past month
11: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to the average humidity for San Francisco over the past month
12: Create a graph showing the forecasted precipitation for San Francisco over the next few days
13: Compare the forecasted precipitation for San Francisco over the next few days to the average precipitation for San Francisco over the past month
14: Research the average temperature for San Francisco over the past week
15: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to the average wind speed for San Francisco over the past week
*****NEXT TASK*****
10: Research the average wind speed for San Francisco over the past month
*****TASK RESULT*****
The average wind speed for San Francisco over the past month is 2.7 meters per second.
[...]
*/


