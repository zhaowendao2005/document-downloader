--- æ–‡ä»¶: output_20250622_020018\docs.md ---
---
url: https://js.langchain.com.cn/docs/
crawled_at: 2025-06-22T02:00:19.480556
---

LangChain JS/TS ä¸­æ–‡æ–‡æ¡£
LangChainä¸­æ–‡ç½‘ 500é¡µè¶…è¯¦ç»†ä¸­æ–‡æ–‡æ¡£æ•™ç¨‹ï¼ŒåŠ©åŠ›LLM/chatGPTåº”ç”¨å¼€å‘
â€‹
LangChainæ˜¯ä¸€ç§åŸºäºè¯­è¨€æ¨¡å‹çš„åº”ç”¨ç¨‹åºå¼€å‘æ¡†æ¶ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œæœ€å¼ºå¤§å’Œä¸åŒçš„åº”ç”¨ç¨‹åºä¸ä»…ä¼šé€šè¿‡APIè°ƒç”¨è¯­è¨€æ¨¡å‹ï¼Œè¿˜ä¼š:
äº†è§£æ•°æ®
: è¿æ¥è¯­è¨€æ¨¡å‹ä¸å…¶ä»–æ•°æ®æº
æœ‰ä»£ç†
: å…è®¸è¯­è¨€æ¨¡å‹ä¸å…¶ç¯å¢ƒäº¤äº’
LangChainæ¡†æ¶æ˜¯æ ¹æ®ä¸Šè¿°åŸåˆ™è®¾è®¡çš„ã€‚
å…¥é—¨
â€‹
æŸ¥çœ‹ä¸‹é¢çš„æŒ‡å—ï¼Œäº†è§£å¦‚ä½•ä½¿ç”¨LangChainåˆ›å»ºè¯­è¨€æ¨¡å‹åº”ç”¨ç¨‹åºã€‚
å¿«é€Ÿå…¥é—¨
ä½¿ç”¨LLMs
å¿«é€Ÿå…¥é—¨
ä½¿ç”¨èŠå¤©æ¨¡å‹
ç»„ä»¶
â€‹
LangChainæä¾›æ”¯æŒçš„å‡ ä¸ªä¸»è¦æ¨¡å—ã€‚é’ˆå¯¹æ¯ä¸ªæ¨¡å—ï¼Œæˆ‘ä»¬æä¾›ä¸€äº›ç¤ºä¾‹ï¼Œä»¥ä¾¿å¼€å§‹å¹¶ç†Ÿæ‚‰ä¸€äº›æ¦‚å¿µã€‚æ¯ä¸ªç¤ºä¾‹éƒ½é“¾æ¥åˆ°ä½¿ç”¨çš„æ¨¡å—çš„APIæ–‡æ¡£ã€‚
è¿™äº›æ¨¡å—æŒ‰é€’å¢çš„å¤æ‚æ€§æ’åˆ—#
æ¨¡å¼
: è¿™åŒ…æ‹¬åœ¨æ•´ä¸ªåº“ä¸­ä½¿ç”¨çš„æ¥å£å’ŒåŸºç±»ã€‚
æ¨¡å‹
: è¿™åŒ…æ‹¬ä¸å„ç§LLMï¼ŒèŠå¤©æ¨¡å‹å’ŒåµŒå…¥æ¨¡å‹çš„é›†æˆã€‚
æç¤ºç¬¦
: è¿™åŒ…æ‹¬æç¤ºç¬¦æ¨¡æ¿å’Œä¸æç¤ºç¬¦ä¸€èµ·ä½¿ç”¨çš„åŠŸèƒ½ï¼Œä¾‹å¦‚è¾“å‡ºè§£æå™¨å’Œç¤ºä¾‹é€‰æ‹©å™¨ã€‚
ç´¢å¼•
: è¿™åŒ…æ‹¬ç”¨äºå¤„ç†æ‚¨è‡ªå·±çš„æ•°æ®çš„æ¨¡å¼å’ŒåŠŸèƒ½ï¼Œå¹¶ä½¿å…¶å‡†å¤‡å¥½ä¸è¯­è¨€æ¨¡å‹äº¤äº’ï¼ˆåŒ…æ‹¬æ–‡æ¡£åŠ è½½ç¨‹åºã€å‘é‡å­˜å‚¨å™¨ã€æ–‡æœ¬åˆ†å‰²å™¨å’Œæ£€ç´¢å™¨)ã€‚
Memory
: å†…å­˜æ˜¯æŒ‡é“¾/ä»£ç†è°ƒç”¨ä¹‹é—´ä¿å­˜çŠ¶æ€çš„æ¦‚å¿µã€‚LangChainæä¾›æ ‡å‡†çš„å†…å­˜æ¥å£ï¼Œä¸€ä¸ªå†…å­˜å®ç°çš„é›†åˆï¼Œä»¥åŠä½¿ç”¨å†…å­˜çš„é“¾/ä»£ç†çš„ç¤ºä¾‹ã€‚
Chains
: é“¾ä¸ä»…ä»…åªæ˜¯å•ä¸ªLLMè°ƒç”¨ï¼Œå®ƒæ˜¯ä¸€ç³»åˆ—è°ƒç”¨ï¼ˆä¸è®ºæ˜¯å¯¹LLMè¿˜æ˜¯å…¶ä»–å·¥å…·)ã€‚LangChainæä¾›æ ‡å‡†çš„é“¾æ¥å£ï¼Œä¸å…¶ä»–å·¥å…·è¿›è¡Œå¤šæ ·åŒ–çš„é›†æˆï¼Œå¹¶æä¾›å¸¸è§åº”ç”¨çš„ç«¯åˆ°ç«¯é“¾ã€‚
Agents
: ä»£ç†æ¶‰åŠLLMå¯¹è¦é‡‡å–çš„è¡ŒåŠ¨åšå‡ºå†³ç­–ï¼Œé‡‡å–è¡ŒåŠ¨ï¼Œçœ‹åˆ°è§‚å¯Ÿç»“æœï¼Œç„¶åé‡å¤è¯¥è¿‡ç¨‹ç›´åˆ°å®Œæˆã€‚LangChainæä¾›ä»£ç†æ ‡å‡†æ¥å£ï¼Œæä¾›ä¸€ç³»åˆ—ä»£ç†ä¾›é€‰æ‹©ä»¥åŠç«¯åˆ°ç«¯ä»£ç†ç¤ºä¾‹ã€‚
API å‚è€ƒ
â€‹
è¿™é‡Œ
å¯ä»¥æ‰¾åˆ°LangChainæ‰€æœ‰æ¨¡å—çš„APIå‚è€ƒä»¥åŠæ‰€æœ‰å¯¼å‡ºç±»å’Œå‡½æ•°çš„å®Œæ•´æ–‡æ¡£ã€‚
ç”Ÿäº§
â€‹
å½“æ‚¨ä»åŸå‹åˆ¶ä½œè¿‡æ¸¡åˆ°ç”Ÿäº§è¿‡ç¨‹æ—¶ï¼Œæˆ‘ä»¬æ­£åœ¨å¼€å‘èµ„æºæ¥å¸®åŠ©æ‚¨å®Œæˆæ­¤è¿‡ç¨‹ã€‚
These including:
éƒ¨ç½²
: å…³äºå¦‚ä½•å°†åº”ç”¨ç¨‹åºéƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒçš„èµ„æºã€‚
äº‹ä»¶/å›è°ƒ
: LangChainæ¨¡å—å…¬å¼€çš„äº‹ä»¶èµ„æºã€‚
è·Ÿè¸ª
: æœ‰å…³å¦‚ä½•ä½¿ç”¨è·Ÿè¸ªè®°å½•å’Œè°ƒè¯•åº”ç”¨ç¨‹åºçš„èµ„æºã€‚
é™„åŠ èµ„æº
â€‹
ä»¥ä¸‹æ˜¯æˆ‘ä»¬è®¤ä¸ºåœ¨å¼€å‘åº”ç”¨ç¨‹åºæ—¶å¯èƒ½æœ‰ç”¨çš„å…¶ä»–èµ„æºæ”¶é›†ï¼
LangChainHub
: LangChainHub æ˜¯ä¸€ä¸ªåˆ†äº«å’Œæ¢ç´¢å…¶ä»–è¯­è¨€é“¾å’Œä»£ç†çš„åœ°æ–¹ã€‚
Discord
: åŠ å…¥æˆ‘ä»¬çš„ Discordï¼Œè®¨è®ºå…³äº LangChain çš„ä¸€åˆ‡ï¼
ç”Ÿäº§æ”¯æŒ
: å½“æ‚¨å°† LangChain ç§»å…¥ç”Ÿäº§ç¯å¢ƒæ—¶ï¼Œæˆ‘ä»¬å¾ˆä¹æ„æä¾›æ›´å…¨é¢çš„æ”¯æŒã€‚è¯·å¡«å†™æ­¤è¡¨å•ï¼Œæˆ‘ä»¬å°†ä¸ºæ‚¨è®¾ç½®ä¸“ç”¨çš„æ”¯æŒ Slack é¢‘é“ã€‚



--- æ–‡ä»¶: output_20250622_020018\docs\ecosystem\databerry.md ---
---
url: https://js.langchain.com.cn/docs/ecosystem/databerry
crawled_at: 2025-06-22T02:00:19.507719
---

æ•°æ®è“
æœ¬é¡µé¢ä»‹ç»å¦‚ä½•åœ¨LangChainä¸­ä½¿ç”¨
Databerry
ã€‚
ä»€ä¹ˆæ˜¯æ•°æ®è“ï¼Ÿ
â€‹
æ•°æ®è“æ˜¯ä¸€ä¸ª
å¼€æº
çš„æ–‡æ¡£æ£€ç´¢å¹³å°ï¼Œå¸®åŠ©è¿æ¥æ‚¨çš„ä¸ªäººæ•°æ®å’Œå¤§å‹è¯­è¨€æ¨¡å‹ã€‚
å¿«é€Ÿå¼€å§‹
â€‹
ä»LangChainä¸­æ£€ç´¢å­˜å‚¨åœ¨æ•°æ®è“ä¸­çš„æ–‡æ¡£éå¸¸å®¹æ˜“ï¼
import
{
DataberryRetriever
}
from
"langchain/retrievers/databerry"
;
const
retriever
=
new
DataberryRetriever
(
{
datastoreUrl
:
"https://api.databerry.ai/query/clg1xg2h80000l708dymr0fxc"
,
apiKey
:
"DATABERRY_API_KEY"
,
// optional: needed for private datastores
topK
:
8
,
// optional: default value is 3
}
)
;
// Create a chain that uses the OpenAI LLM and Databerry retriever.
const
chain
=
RetrievalQAChain
.
fromLLM
(
model
,
retriever
)
;
// Call the chain with a query.
const
res
=
await
chain
.
call
(
{
query
:
"What's Databerry?"
,
}
)
;
console
.
log
(
{
res
}
)
;
/*
{
res: {
text: 'Databerry provides a user-friendly solution to quickly setup a semantic search system over your personal data without any technical knowledge.'
}
}
*/



--- æ–‡ä»¶: output_20250622_020018\docs\ecosystem\helicone.md ---
---
url: https://js.langchain.com.cn/docs/ecosystem/helicone
crawled_at: 2025-06-22T02:00:19.612630
---

Helicone
æœ¬é¡µä»‹ç»å¦‚ä½•åœ¨LangChainä¸­ä½¿ç”¨
Helicone
ã€‚
Helicone æ˜¯ä»€ä¹ˆï¼Ÿ
â€‹
Heliconeæ˜¯ä¸€ä¸ªå¼€æºçš„è§‚æµ‹å¹³å°ï¼Œä»£ç†æ‚¨çš„OpenAIæµé‡ï¼Œå¹¶ä¸ºæ‚¨æä¾›æœ‰å…³æ‚¨çš„èŠ±è´¹ã€å»¶è¿Ÿå’Œä½¿ç”¨æƒ…å†µçš„å…³é”®è§è§£ã€‚
å¿«é€Ÿå…¥é—¨
â€‹
åœ¨æ‚¨çš„LangChainç¯å¢ƒä¸­ï¼Œæ‚¨åªéœ€æ·»åŠ ä»¥ä¸‹å‚æ•°ã€‚
const
model
=
new
OpenAI
(
{
}
,
{
basePath
:
"https://oai.hconeai.com/v1"
,
}
)
;
const
res
=
await
model
.
call
(
"What is a helicone?"
)
;
ç°åœ¨ï¼Œå‰å¾€
helicone.ai
åˆ›å»ºæ‚¨çš„å¸æˆ·ï¼Œå¹¶åœ¨æˆ‘ä»¬çš„ä»ªè¡¨æ¿ä¸­æ·»åŠ OpenAI APIå¯†é’¥ä»¥æŸ¥çœ‹æ—¥å¿—ã€‚
å¦‚ä½•å¯ç”¨Heliconeç¼“å­˜
â€‹
const
model
=
new
OpenAI
(
{
}
,
{
basePath
:
"https://oai.hconeai.com/v1"
,
baseOptions
:
{
headers
:
{
"Helicone-Cache-Enabled"
:
"true"
,
}
,
}
,
}
)
;
const
res
=
await
model
.
call
(
"What is a helicone?"
)
;
Heliconeç¼“å­˜æ–‡æ¡£
å¦‚ä½•ä½¿ç”¨Heliconeè‡ªå®šä¹‰å±æ€§
â€‹
const
model
=
new
OpenAI
(
{
}
,
{
basePath
:
"https://oai.hconeai.com/v1"
,
baseOptions
:
{
headers
:
{
"Helicone-Property-Session"
:
"24"
,
"Helicone-Property-Conversation"
:
"support_issue_2"
,
"Helicone-Property-App"
:
"mobile"
,
}
,
}
,
}
)
;
const
res
=
await
model
.
call
(
"What is a helicone?"
)
;
Helicone property docs



--- æ–‡ä»¶: output_20250622_020018\docs\ecosystem\unstructured.md ---
---
url: https://js.langchain.com.cn/docs/ecosystem/unstructured
crawled_at: 2025-06-22T02:00:19.634627
---

éç»“æ„åŒ–æ•°æ®
æœ¬é¡µé¢ä»‹ç»å¦‚ä½•åœ¨LangChainä¸­ä½¿ç”¨
éç»“æ„åŒ–æ•°æ®
ã€‚
ä»€ä¹ˆæ˜¯éç»“æ„åŒ–æ•°æ®ï¼Ÿ
â€‹
éç»“æ„åŒ–æ˜¯ä¸€ä¸ª
å¼€æº
PythonåŒ…ï¼Œç”¨äºä»åŸå§‹æ–‡æ¡£ä¸­æå–æ–‡æœ¬ä»¥ç”¨äºæœºå™¨å­¦ä¹ åº”ç”¨ã€‚ç›®å‰æ”¯æŒåˆ†åŒºWordæ–‡æ¡£ï¼ˆ.docæˆ–.docxæ ¼å¼)ï¼Œå¹»ç¯ç‰‡ï¼ˆ.pptæˆ–.pptxæ ¼å¼)ï¼Œ Pdf ï¼Œ htmlæ–‡ä»¶ï¼Œå›¾åƒï¼Œç”µå­é‚®ä»¶ï¼ˆ.emlæˆ–.msgæ ¼å¼)ï¼Œç”µå­ä¹¦ï¼Œ markdownï¼Œå’Œçº¯æ–‡æœ¬æ–‡ä»¶ã€‚
unstructured
æ˜¯ä¸€ä¸ªPythonåŒ…ï¼Œä¸èƒ½ç›´æ¥ä¸TS / JSä¸€èµ·ä½¿ç”¨ï¼Œä½†æ˜¯Unstructuredè¿˜ç»´æŠ¤ä¸€ä¸ª
REST API
ä»¥æ”¯æŒä½¿ç”¨å…¶ä»–ç¼–ç¨‹è¯­è¨€ç¼–å†™çš„é¢„å¤„ç†æµæ°´çº¿ã€‚æ‰˜ç®¡çš„Unstructured APIçš„ç«¯ç‚¹ä¸º
https://api.unstructured.io/general/v0/general
ï¼Œæˆ–è€…æ‚¨å¯ä»¥ä½¿ç”¨
æ­¤å¤„
æ‰¾åˆ°çš„è¯´æ˜åœ¨æœ¬åœ°è¿è¡ŒæœåŠ¡ã€‚
ç›®å‰ï¼ˆæˆªè‡³2023å¹´4æœˆ26æ—¥)ï¼Œ Unstructured APIä¸éœ€è¦APIå¯†é’¥ã€‚å°†æ¥ï¼ŒAPIå°†éœ€è¦APIå¯†é’¥ã€‚
Unstructuredæ–‡æ¡£é¡µé¢
ä¼šåŒ…æ‹¬æœ‰å…³å¦‚ä½•è·å–APIå¯†é’¥çš„è¯´æ˜ï¼ˆä¸€æ—¦å¯ç”¨)ã€‚
å¿«é€Ÿå¼€å§‹
â€‹
æ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç åœ¨
langchain
ä¸­ä½¿ç”¨éç»“æ„åŒ–æ•°æ®ã€‚
å°†æ–‡ä»¶åæ›¿æ¢ä¸ºè¦å¤„ç†çš„æ–‡ä»¶ã€‚
å¦‚æœæ‚¨æ­£åœ¨æœ¬åœ°è¿è¡Œå®¹å™¨ï¼Œåˆ™å°†urlåˆ‡æ¢ä¸º
http://127.0.0.1:8000/general/v0/general
ã€‚
æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹
APIæ–‡æ¡£é¡µé¢
ã€‚
import
{
UnstructuredLoader
}
from
"langchain/document_loaders/fs/unstructured"
;
const
options
=
{
apiKey
:
"MY_API_KEY"
,
}
;
const
loader
=
new
UnstructuredLoader
(
"src/document_loaders/example_data/notion.md"
,
options
)
;
const
docs
=
await
loader
.
load
(
)
;
ç›®å½•
â€‹
æ‚¨è¿˜å¯ä»¥ä½¿ç”¨
UnstructuredDirectoryLoader
ä»ç›®å½•ä¸­åŠ è½½æ‰€æœ‰æ–‡ä»¶ï¼Œè¯¥ç±»ç»§æ‰¿è‡ª
DirectoryLoader
:
import
{
UnstructuredDirectoryLoader
}
from
"langchain/document_loaders/fs/unstructured"
;
const
options
=
{
apiKey
:
"MY_API_KEY"
,
}
;
const
loader
=
new
UnstructuredDirectoryLoader
(
"langchain/src/document_loaders/tests/example_data"
,
options
)
;
const
docs
=
await
loader
.
load
(
)
;
ç›®å‰ï¼Œ
UnstructuredLoader
æ”¯æŒä»¥ä¸‹æ–‡æ¡£ç±»å‹:
çº¯æ–‡æœ¬æ–‡ä»¶ï¼ˆ
.txt
/
.text
)
PDFï¼ˆ
.pdf
)
Wordæ–‡æ¡£ï¼ˆ
.doc
/
.docx
)
PowerPointsï¼ˆ
.ppt
/
.pptx
)
å›¾åƒæ–‡ä»¶ï¼ˆ
.jpg
/
.jpeg
)
ç”µå­é‚®ä»¶ï¼ˆ
.eml
/
.msg
)
HTMLï¼ˆ
.html
)
Markdownæ–‡ä»¶ï¼ˆ
.md
)
UnstructuredLoader
çš„è¾“å‡ºå°†æ˜¯ä¸€ä¸ªç±»ä¼¼ä»¥ä¸‹å†…å®¹çš„
Document
å¯¹è±¡æ•°ç»„:
[
Document
{
pageContent
:
`
Decoder: The decoder is also composed of a stack of N = 6
identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a
third sub-layer, wh
ich performs multi-head attention over the output of the encoder stack. Similar to the encoder, we
employ residual connections around each of the sub-layers, followed by layer normalization. We also
modify the self
-attention sub-layer in the decoder stack to prevent positions from attending to subsequent
positions. This masking, combined with fact that the output embeddings are offset by one position,
ensures that the predic
tions for position i can depend only on the known outputs at positions less than i.
`
,
metadata
:
{
page_number
:
3
,
filename
:
'1706.03762.pdf'
,
category
:
'NarrativeText'
}
}
,
Document
{
pageContent
:
'3.2 Attention'
,
metadata
:
{
page_number
:
3
,
filename
:
'1706.03762.pdf'
,
category
:
'Title'
}
]



--- æ–‡ä»¶: output_20250622_020018\docs\getting-started\guide-chat.md ---
---
url: https://js.langchain.com.cn/docs/getting-started/guide-chat
crawled_at: 2025-06-22T02:00:19.817403
---

å¿«é€Ÿå…¥é—¨ï¼Œ ä½¿ç”¨èŠå¤©æ¨¡å‹
èŠå¤©æ¨¡å‹æ˜¯ä¸€ç§è¯­è¨€æ¨¡å‹çš„å˜ä½“ã€‚
è™½ç„¶èŠå¤©æ¨¡å‹åœ¨å¹•åä½¿ç”¨è¯­è¨€æ¨¡å‹ï¼Œ ä½†å®ƒä»¬å…¬å¼€çš„æ¥å£æœ‰äº›ä¸åŒã€‚
å®ƒä»¬ä¸æ˜¯æš´éœ²ä¸€ä¸ª"è¾“å…¥æ–‡æœ¬ï¼Œè¾“å‡ºæ–‡æœ¬"çš„APIï¼Œè€Œæ˜¯æš´éœ²ä¸€ä¸ª"èŠå¤©æ¶ˆæ¯"åˆ°è¾“å…¥å’Œè¾“å‡ºçš„æ¥å£ã€‚
èŠå¤©æ¨¡å‹APIç›¸å½“æ–°ï¼Œ å› æ­¤æˆ‘ä»¬ä»åœ¨æ‰¾å‡ºæ­£ç¡®çš„æŠ½è±¡ã€‚
å®‰è£…å’Œè®¾ç½®
â€‹
è¦å¼€å§‹ä½¿ç”¨ï¼Œè¯·æŒ‰ç…§
å®‰è£…è¯´æ˜
å®‰è£…LangChainã€‚
å…¥é—¨æŒ‡å—
â€‹
æœ¬èŠ‚ä»‹ç»å¦‚ä½•ä½¿ç”¨èŠå¤©æ¨¡å‹å…¥é—¨ã€‚æ¥å£åŸºäºæ¶ˆæ¯è€Œä¸æ˜¯åŸå§‹æ–‡æœ¬ã€‚
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
HumanChatMessage
,
SystemChatMessage
}
from
"langchain/schema"
;
const
chat
=
new
ChatOpenAI
(
{
temperature
:
0
}
)
;
åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨å­˜å‚¨åœ¨ç¯å¢ƒå˜é‡
OPENAI_API_KEY
æˆ–
AZURE_OPENAI_API_KEY
ä¸­çš„APIå¯†é’¥åˆ›å»ºèŠå¤©æ¨¡å‹ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†è°ƒç”¨æ­¤èŠå¤©æ¨¡å‹ã€‚
â“˜
æ³¨æ„ï¼Œå¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯Azure OpenAIï¼Œè¯·ç¡®ä¿è¿˜è®¾ç½®äº†ç¯å¢ƒå˜é‡
AZURE_OPENAI_API_INSTANCE_NAME
ï¼Œ
AZURE_OPENAI_API_DEPLOYMENT_NAME
å’Œ
AZURE_OPENAI_API_VERSION
ã€‚
èŠå¤©æ¨¡å‹: æ¶ˆæ¯ä½œä¸ºè¾“å…¥ï¼Œ æ¶ˆæ¯ä½œä¸ºè¾“å‡º
â€‹
é€šè¿‡å°†ä¸€ä¸ªæˆ–å¤šä¸ªæ¶ˆæ¯ä¼ é€’ç»™èŠå¤©æ¨¡å‹ï¼Œå¯ä»¥è·å–èŠå¤©å®Œæˆã€‚å“åº”ä¹Ÿå°†æ˜¯ä¸€æ¡æ¶ˆæ¯ã€‚LangChainå½“å‰æ”¯æŒçš„æ¶ˆæ¯ç±»å‹ä¸º
AIChatMessage
ï¼Œ
HumanChatMessage
ï¼Œ
SystemChatMessage
ï¼Œå’Œé€šç”¨
ChatMessage
-- ChatMessageé‡‡ç”¨ä»»æ„è§’è‰²å‚æ•°ï¼Œåœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä¸ä¼šä½¿ç”¨ã€‚å¤§å¤šæ•°æ—¶å€™ï¼Œæ‚¨åªéœ€è¦å¤„ç†
HumanChatMessage
ï¼Œ
AIChatMessage
å’Œ
SystemChatMessage
ã€‚
const
response
=
await
chat
.
call
(
[
new
HumanChatMessage
(
"Translate this sentence from English to French. I love programming."
)
,
]
)
;
console
.
log
(
response
)
;
AIChatMessage { text: "J'aime programmer." }
å¤šæ¡æ¶ˆæ¯
â€‹
OpenAIçš„åœ¨çº¿èŠå¤©æ¨¡å‹ï¼ˆç›®å‰åŒ…æ‹¬
gpt-3.5-turbo
å’Œ
gpt-4
ä»¥åŠAzure OpenAIçš„
gpt-4-32k
)æ”¯æŒå¤šæ¡æ¶ˆæ¯ä½œä¸ºè¾“å…¥ã€‚è¯·å‚è§
[è¿™é‡Œ]
(
https://platform.openai.com/docs/guides/chat/chat
-vs-completions)äº†è§£æ›´å¤šä¿¡æ¯ã€‚ä»¥ä¸‹æ˜¯å‘èŠå¤©æ¨¡å‹å‘é€ç³»ç»Ÿæ¶ˆæ¯å’Œç”¨æˆ·æ¶ˆæ¯çš„ç¤ºä¾‹:
â“˜
æ³¨æ„ï¼Œå¦‚æœæ‚¨ä½¿ç”¨Azure OpenAIï¼Œè¯·ç¡®ä¿æ›´æ”¹éƒ¨ç½²åç§°ä»¥ä½¿ç”¨æ‚¨é€‰æ‹©çš„æ¨¡å‹çš„éƒ¨ç½²ã€‚
const
responseB
=
await
chat
.
call
(
[
new
SystemChatMessage
(
"You are a helpful assistant that translates English to French."
)
,
new
HumanChatMessage
(
"Translate: I love programming."
)
,
]
)
;
console
.
log
(
responseB
)
;
AIChatMessage { text: "J'aime programmer." }
å¤šæ¡å®Œæˆ
â€‹
æ‚¨å¯ä»¥è¿›ä¸€æ­¥ç”Ÿæˆå¤šä¸ªæ¶ˆæ¯é›†çš„å®Œæˆï¼Œä½¿ç”¨generateã€‚è¿™å°†è¿”å›å…·æœ‰é¢å¤–æ¶ˆæ¯å‚æ•°çš„LLMResultã€‚
const
responseC
=
await
chat
.
generate
(
[
[
new
SystemChatMessage
(
"You are a helpful assistant that translates English to French."
)
,
new
HumanChatMessage
(
"Translate this sentence from English to French. I love programming."
)
,
]
,
[
new
SystemChatMessage
(
"You are a helpful assistant that translates English to French."
)
,
new
HumanChatMessage
(
"Translate this sentence from English to French. I love artificial intelligence."
)
,
]
,
]
)
;
console
.
log
(
responseC
)
;
{
generations: [
[
{
text: "J'aime programmer.",
message: AIChatMessage { text: "J'aime programmer." },
}
],
[
{
text: "J'aime l'intelligence artificielle.",
message: AIChatMessage { text: "J'aime l'intelligence artificielle." }
}
]
]
}
èŠå¤©æç¤ºæ¨¡æ¿: ç®¡ç†èŠå¤©æ¨¡å‹çš„æç¤º
â€‹
æ‚¨å¯ä»¥é€šè¿‡ä½¿ç”¨
MessagePromptTemplate
æ¥åˆ©ç”¨æ¨¡æ¿ã€‚æ‚¨å¯ä»¥ä»ä¸€ä¸ªæˆ–å¤šä¸ª
MessagePromptTemplate
æ„å»º
ChatPromptTemplate
ã€‚æ‚¨å¯ä»¥ä½¿ç”¨
ChatPromptTemplate
çš„
formatPromptValue
- è¿™å°†è¿”å›ä¸€ä¸ª
PromptValue
æ‚¨å¯ä»¥å°†
PromptValue
è½¬æ¢ä¸ºå­—ç¬¦ä¸²æˆ–æ¶ˆæ¯å¯¹è±¡ï¼Œå…·ä½“å–å†³äºæ‚¨æ˜¯å¦æƒ³å°†æ ¼å¼åŒ–å€¼ç”¨ä½œllmæˆ–èŠå¤©æ¨¡å‹çš„è¾“å…¥ã€‚
ç»§ç»­ä¸Šä¸€ä¸ªç¤ºä¾‹:
import {
SystemMessagePromptTemplate,
HumanMessagePromptTemplate,
ChatPromptTemplate,
} from "langchain/prompts";
```typescript
é¦–å…ˆï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå¯é‡å¤ä½¿ç”¨çš„æ¨¡æ¿:
const translationPrompt = ChatPromptTemplate.fromPromptMessages([
SystemMessagePromptTemplate.fromTemplate(
"You are a helpful assistant that translates {input_language} to {output_language}."
),
HumanMessagePromptTemplate.fromTemplate("{text}"),
]);
ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆå“åº”:
const
responseA
=
await
chat
.
generatePrompt
(
[
await
translationPrompt
.
formatPromptValue
(
{
input_language
:
"English"
,
output_language
:
"French"
,
text
:
"I love programming."
,
}
)
,
]
)
;
console
.
log
(
responseA
)
;
{
generations: [
[
{
text: "J'aime programmer.",
message: AIChatMessage { text: "J'aime programmer." }
}
]
]
}
æ¨¡å‹+æç¤º=LLMChain
â€‹
è¿™ç§è¦æ±‚ç”¨æˆ·å®Œæˆæ ¼å¼åŒ–æç¤ºçš„æ¨¡å¼éå¸¸å¸¸è§ï¼Œå› æ­¤æˆ‘ä»¬ä»‹ç»äº†ä¸‹ä¸€ä¸ªè°œé¢˜: LLMChain
const
chain
=
new
LLMChain
(
{
prompt
:
translationPrompt
,
llm
:
chat
,
}
)
;
ç„¶åæ‚¨å¯ä»¥è°ƒç”¨é“¾:
const
responseB
=
await
chain
.
call
(
{
input_language
:
"English"
,
output_language
:
"French"
,
text
:
"I love programming."
,
}
)
;
console
.
log
(
responseB
)
;
{ text: "J'aime programmer." }
ä»£ç†: æ ¹æ®ç”¨æˆ·è¾“å…¥åŠ¨æ€è¿è¡Œé“¾
â€‹
æœ€åï¼Œæˆ‘ä»¬ä»‹ç»äº†å·¥å…·å’Œä»£ç†ï¼Œå®ƒä»¬é€šè¿‡å…¶ä»–èƒ½åŠ›æ‰©å±•äº†æ¨¡å‹ï¼Œä¾‹å¦‚æœç´¢æˆ–è®¡ç®—å™¨ã€‚
å·¥å…·æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒæ¥å—ä¸€ä¸ªå­—ç¬¦ä¸²ï¼ˆä¾‹å¦‚æœç´¢æŸ¥è¯¢)å¹¶è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²ï¼ˆä¾‹å¦‚æœç´¢ç»“æœ)ã€‚å®ƒä»¬è¿˜æœ‰ä¸€ä¸ªåç§°å’Œæè¿°ï¼Œç”±èŠå¤©æ¨¡å‹ç”¨äºè¯†åˆ«åº”è¯¥è°ƒç”¨å“ªä¸ªå·¥å…·ã€‚
class
Tool
{
name
:
string
;
description
:
string
;
call
(
arg
:
string
)
:
Promise
<
string
>
;
}
ä»£ç†æ˜¯å¯¹ä»£ç†æç¤ºé“¾ï¼ˆä¾‹å¦‚MRKL)çš„æ— çŠ¶æ€åŒ…è£…å™¨ï¼Œå®ƒè´Ÿè´£å°†å·¥å…·æŒ‰æ ¼å¼æ”¾å…¥æç¤ºä¸­ï¼Œå¹¶è§£æä»èŠå¤©æ¨¡å‹è·å–çš„å“åº”ã€‚
interface
AgentStep
{
action
:
AgentAction
;
observation
:
string
;
}
interface
AgentAction
{
tool
:
string
;
// Tool.name
toolInput
:
string
;
// Tool.call argument
}
interface
AgentFinish
{
returnValues
:
object
;
}
class
Agent
{
plan
(
steps
:
AgentStep
[
]
,
inputs
:
object
)
:
Promise
<
AgentAction
|
AgentFinish
>
;
}
è¦ä½¿ä»£ç†æ›´å¼ºå¤§ï¼Œæˆ‘ä»¬éœ€è¦ä½¿å®ƒä»¬è¿­ä»£ï¼Œå³å¤šæ¬¡è°ƒç”¨æ¨¡å‹ï¼Œç›´åˆ°å®ƒä»¬åˆ°è¾¾æœ€ç»ˆç­”æ¡ˆã€‚è¿™æ˜¯AgentExecutorçš„å·¥ä½œã€‚
class
AgentExecutor
{
// a simplified implementation
run
(
inputs
:
object
)
{
const
steps
=
[
]
;
while
(
true
)
{
const
step
=
await
this
.
agent
.
plan
(
steps
,
inputs
)
;
if
(
step
instanceof
AgentFinish
)
{
return
step
.
returnValues
;
}
steps
.
push
(
step
)
;
}
}
}
æœ€åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨AgentExecutorè¿è¡Œä»£ç†:
// Define the list of tools the agent can use
const
tools
=
[
new
SerpAPI
(
process
.
env
.
SERPAPI_API_KEY
,
{
location
:
"Austin,Texas,United States"
,
hl
:
"en"
,
gl
:
"us"
,
}
)
,
]
;
// Create the agent from the chat model and the tools
const
agent
=
ChatAgent
.
fromLLMAndTools
(
new
ChatOpenAI
(
)
,
tools
)
;
// Create an executor, which calls to the agent until an answer is found
const
executor
=
AgentExecutor
.
fromAgentAndTools
(
{
agent
,
tools
}
)
;
const
responseG
=
await
executor
.
run
(
"How many people live in canada as of 2023?"
)
;
console
.
log
(
responseG
)
;
38,626,704.
å†…å­˜: å‘é“¾å’Œä»£ç†æ·»åŠ çŠ¶æ€
â€‹
æ‚¨è¿˜å¯ä»¥ä½¿ç”¨é“¾æ¥å­˜å‚¨çŠ¶æ€ã€‚è¿™å¯¹äºèŠå¤©æœºå™¨äººç­‰åº”ç”¨ç¨‹åºéå¸¸æœ‰ç”¨ï¼Œå› ä¸ºæ‚¨éœ€è¦è·Ÿè¸ªä¼šè¯å†å²è®°å½•ã€‚MessagesPlaceholderæ˜¯ä¸€ä¸ªç‰¹æ®Šçš„æç¤ºæ¨¡æ¿ï¼Œæ¯æ¬¡è°ƒç”¨å°†æ›¿æ¢ä¸ºä¼ é€’çš„æ¶ˆæ¯ã€‚
const
chatPrompt
=
ChatPromptTemplate
.
fromPromptMessages
(
[
SystemMessagePromptTemplate
.
fromTemplate
(
"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know."
)
,
new
MessagesPlaceholder
(
"history"
)
,
HumanMessagePromptTemplate
.
fromTemplate
(
"{input}"
)
,
]
)
;
const
chain
=
new
ConversationChain
(
{
memory
:
new
BufferMemory
(
{
returnMessages
:
true
,
memoryKey
:
"history"
}
)
,
prompt
:
chatPrompt
,
llm
:
chat
,
}
)
;
è¯¥é“¾å°†å†…éƒ¨ç´¯åŠ å‘é€åˆ°æ¨¡å‹å’Œæ¥æ”¶çš„è¾“å‡ºæ¶ˆæ¯ã€‚ç„¶åï¼Œåœ¨ä¸‹ä¸€æ¬¡è°ƒç”¨æ—¶ï¼Œå®ƒå°†æŠŠæ¶ˆæ¯æ³¨å…¥æç¤ºç¬¦ä¸­ã€‚æ‰€ä»¥ä½ å¯ä»¥å¤šæ¬¡è°ƒç”¨è¯¥é“¾ï¼Œå¹¶ä¸”å®ƒä¼šè®°ä½å…ˆå‰çš„æ¶ˆæ¯ã€‚(The chain will internally accumulate the messages sent to the model and the ones received as output. Then it will inject the messages into the prompt on the next call. So you can call the chain a few times and it remembers previous messages.)
const
responseH
=
await
chain
.
call
(
{
input
:
"hi from London, how are you doing today"
,
}
)
;
console
.
log
(
responseH
)
;
{
response: "Hello! As an AI language model, I don't have feelings, but I'm functioning properly and ready to assist you with any questions or tasks you may have. How can I help you today?"
}
const
responseI
=
await
chain
.
call
(
{
input
:
"Do you know where I am?"
,
}
)
;
console
.
log
(
responseI
)
;
{
response: "Yes, you mentioned that you are from London. However, as an AI language model, I don't have access to your current location unless you provide me with that information."
}
æµå¼å¤„ç†
â€‹
æ‚¨è¿˜å¯ä»¥ä½¿ç”¨æµå¼ API è·å–æŒ‰ç…§ç”Ÿæˆé¡ºåºè¿”å›çš„å•è¯ã€‚è¿™å¯¹äºèŠå¤©æœºå™¨äººç­‰æƒ…å†µå¾ˆæœ‰ç”¨ï¼Œå› ä¸ºæ‚¨å¸Œæœ›åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”¨æˆ·å±•ç¤ºæ­£åœ¨ç”Ÿæˆçš„å†…å®¹ã€‚è¯·æ³¨æ„ï¼Œåœ¨å¯ç”¨æµå¼å¤„ç†æ—¶ï¼Œ:OpenAI ç›®å‰ä¸æ”¯æŒâ€œtokenUsageâ€æŠ¥å‘Šã€‚
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
HumanChatMessage
}
from
"langchain/schema"
;
const
chat
=
new
ChatOpenAI
(
{
streaming
:
true
,
callbacks
:
[
{
handleLLMNewToken
(
token
:
string
)
{
process
.
stdout
.
write
(
token
)
;
}
,
}
,
]
,
}
)
;
await
chat
.
call
(
[
new
HumanChatMessage
(
"Write me a song about sparkling water."
)
,
]
)
;
/*
Verse 1:
Bubbles rise, crisp and clear
Refreshing taste that brings us cheer
Sparkling water, so light and pure
Quenches our thirst, it's always secure
Chorus:
Sparkling water, oh how we love
Its fizzy bubbles and grace above
It's the perfect drink, anytime, anyplace
Refreshing as it gives us a taste
Verse 2:
From morning brunch to evening feast
It's the perfect drink for a treat
A sip of it brings a smile so bright
Our thirst is quenched in just one sip so light
...
*/



--- æ–‡ä»¶: output_20250622_020018\docs\getting-started\guide-llm.md ---
---
url: https://js.langchain.com.cn/docs/getting-started/guide-llm
crawled_at: 2025-06-22T02:00:19.843831
---

å¿«é€Ÿå…¥é—¨ï¼Œ ä½¿ç”¨LLMs
æœ¬æ•™ç¨‹å°†å¿«é€Ÿä»‹ç»å¦‚ä½•ä½¿ç”¨LangChainæ„å»ºç«¯åˆ°ç«¯è¯­è¨€æ¨¡å‹åº”ç”¨ç¨‹åºã€‚
å®‰è£…å’Œè®¾ç½®
â€‹
è¦å¼€å§‹ï¼Œè¯·æŒ‰ç…§
å®‰è£…è¯´æ˜
å®‰è£…LangChainã€‚
é€‰æ‹©LLM
â€‹
ä½¿ç”¨LangChainé€šå¸¸éœ€è¦ä¸ä¸€ä¸ªæˆ–å¤šä¸ªæ¨¡å‹æä¾›ç¨‹åºï¼Œæ•°æ®å­˜å‚¨ï¼ŒAPIç­‰è¿›è¡Œé›†æˆã€‚
å¯¹äºè¿™ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨OpenAIçš„APIï¼Œå› æ­¤ä¸éœ€è¦å…¶ä»–è®¾ç½®ã€‚
æ„å»ºè¯­è¨€æ¨¡å‹åº”ç”¨ç¨‹åº
â€‹
ç°åœ¨æˆ‘ä»¬å·²ç»å®‰è£…äº†LangChainï¼Œå¯ä»¥å¼€å§‹æ„å»ºæˆ‘ä»¬çš„è¯­è¨€æ¨¡å‹åº”ç”¨ç¨‹åºã€‚
LangChainæä¾›äº†è®¸å¤šæ¨¡å—ï¼Œå¯ç”¨äºæ„å»ºè¯­è¨€æ¨¡å‹åº”ç”¨ç¨‹åºã€‚å¯ä»¥å°†æ¨¡å—ç»“åˆèµ·æ¥åˆ›å»ºæ›´å¤æ‚çš„åº”ç”¨ç¨‹åºï¼Œä¹Ÿå¯ä»¥å•ç‹¬ç”¨äºç®€å•çš„åº”ç”¨ç¨‹åºã€‚
LLM:ä»è¯­è¨€æ¨¡å‹è·å–é¢„æµ‹
â€‹
LangChainçš„æœ€åŸºæœ¬æ„å»ºæ¨¡å—æ˜¯é’ˆå¯¹æŸäº›è¾“å…¥è°ƒç”¨LLMã€‚ è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹æ¥ä»‹ç»å¦‚ä½•æ‰§è¡Œæ­¤æ“ä½œã€‚ ä¸ºæ­¤ï¼Œå‡è®¾æˆ‘ä»¬æ­£åœ¨æ„å»ºä¸€é¡¹æœåŠ¡ï¼Œè¯¥æœåŠ¡æ ¹æ®å…¬å¸ç”Ÿäº§çš„äº§å“ç”Ÿæˆå…¬å¸åç§°ã€‚
ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦å¯¼å…¥LLMåŒ…è£…å™¨ã€‚
import
{
OpenAI
}
from
"langchain/llms/openai"
;
ç„¶åï¼Œæˆ‘ä»¬å°†éœ€è¦è®¾ç½®OpenAIå¯†é’¥çš„ç¯å¢ƒå˜é‡ã€‚ è¿™é‡Œæœ‰ä¸‰ä¸ªé€‰é¡¹:
æˆ‘ä»¬å¯ä»¥é€šè¿‡åœ¨
.env
æ–‡ä»¶ä¸­è®¾ç½®å€¼ï¼Œå¹¶ä½¿ç”¨
dotenv
è½¯ä»¶åŒ…æ¥è¯»å–å®ƒã€‚
1.1. For OpenAI Api
OPENAI_API_KEY
=
"..."
1.2. For Azure OpenAI:
AZURE_OPENAI_API_KEY
=
"..."
AZURE_OPENAI_API_INSTANCE_NAME
=
"..."
AZURE_OPENAI_API_DEPLOYMENT_NAME
=
"..."
AZURE_OPENAI_API_COMPLETIONS_DEPLOYMENT_NAME
=
"..."
AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME
=
"..."
AZURE_OPENAI_API_VERSION
=
"..."
æˆ–è€…æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤åœ¨ä½ çš„shellä¸­å¯¼å‡ºç¯å¢ƒå˜é‡#ã€‚
2.1. For OpenAI Api
export
OPENAI_API_KEY
=
sk-
..
..
2.2. For Azure OpenAI:
export
AZURE_OPENAI_API_KEY
=
"..."
export
AZURE_OPENAI_API_INSTANCE_NAME
=
"..."
export
AZURE_OPENAI_API_DEPLOYMENT_NAME
=
"..."
export
AZURE_OPENAI_API_COMPLETIONS_DEPLOYMENT_NAME
=
"..."
export
AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME
=
"..."
export
AZURE_OPENAI_API_VERSION
=
"..."
æˆ–è€…æˆ‘ä»¬å¯ä»¥åœ¨åˆå§‹åŒ–wrapperæ—¶ä¸€èµ·æ‰§è¡Œã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å¯èƒ½å¸Œæœ›è¾“å‡ºæ›´éšæœºï¼Œå› æ­¤æˆ‘ä»¬å°†ä½¿ç”¨é«˜æ¸©åº¦è¿›è¡Œåˆå§‹åŒ–ã€‚
3.1. For OpenAI Api
const
model
=
new
OpenAI
(
{
openAIApiKey
:
"sk-..."
,
temperature
:
0.9
}
)
;
3.2. For Azure OpenAI:
const
model
=
new
OpenAI
(
{
azureOpenAIApiKey
:
"..."
,
azureOpenAIApiInstanceName
:
"...."
,
azureOpenAIApiDeploymentName
:
"...."
,
azureOpenAIApiVersion
:
"...."
,
temperature
:
0.9
,
}
)
;
åˆå§‹åŒ–wrapperåï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥åœ¨ä¸€äº›è¾“å…¥ä¸Šè°ƒç”¨å®ƒï¼
const
res
=
await
model
.
call
(
"What would be a good company name a company that makes colorful socks?"
)
;
console
.
log
(
res
)
;
{
res:
'Fantasy Sockery'
}
æç¤ºæ¨¡æ¿: ç®¡ç†LLMsçš„æç¤º
â€‹
è°ƒç”¨LLMæ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ç¬¬ä¸€æ­¥ï¼Œä½†è¿™åªæ˜¯å¼€å§‹ã€‚é€šå¸¸å½“ä½ åœ¨åº”ç”¨ç¨‹åºä¸­ä½¿ç”¨LLMæ—¶ï¼Œä½ ä¸ä¼šç›´æ¥å°†ç”¨æˆ·è¾“å…¥å‘é€åˆ°LLMã€‚ç›¸åï¼Œä½ å¯èƒ½ä¼šä½¿ç”¨ç”¨æˆ·è¾“å…¥æ¥æ„å»ºä¸€ä¸ªæç¤ºï¼Œç„¶åå°†å®ƒå‘é€åˆ°LLMã€‚
ä¾‹å¦‚ï¼Œåœ¨å‰é¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬ä¼ é€’çš„æ–‡æœ¬æ˜¯ç¡¬ç¼–ç çš„ï¼Œè¯¢é—®åˆ¶ä½œå½©è‰²è¢œå­çš„å…¬å¸åå­—ã€‚åœ¨è¿™ä¸ªæƒ³è±¡ä¸­çš„æœåŠ¡ä¸­ï¼Œæˆ‘ä»¬åªéœ€è¦ä½¿ç”¨æè¿°å…¬å¸æ‰€åšçš„äº‹æƒ…çš„ç”¨æˆ·è¾“å…¥ï¼Œå¹¶ä½¿ç”¨è¯¥ä¿¡æ¯æ ¼å¼åŒ–æç¤ºã€‚
ä½¿ç”¨LangChainå¾ˆå®¹æ˜“å®ç°ï¼
é¦–å…ˆè®©æˆ‘ä»¬å®šä¹‰æç¤ºæ¨¡æ¿:
import
{
PromptTemplate
}
from
"langchain/prompts"
;
const
template
=
"What is a good name for a company that makes {product}?"
;
const
prompt
=
new
PromptTemplate
(
{
template
:
template
,
inputVariables
:
[
"product"
]
,
}
)
;
ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹å®ƒçš„å·¥ä½œåŸç†ï¼æˆ‘ä»¬å¯ä»¥è°ƒç”¨
.format
æ–¹æ³•è¿›è¡Œæ ¼å¼åŒ–ã€‚
const
res
=
await
prompt
.
format
(
{
product
:
"colorful socks"
}
)
;
console
.
log
(
res
)
;
{
res:
'What is a good name for a company that makes colorful socks?'
}
é“¾: åœ¨å¤šæ­¥éª¤å·¥ä½œæµä¸­ç»„åˆLLMså’Œæç¤º
â€‹
åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬åªæ˜¯å•ç‹¬ä½¿ç”¨äº†PromptTemplateå’ŒLLMåŸè¯­ã€‚ä½†æ˜¯ï¼Œä¸€ä¸ªçœŸæ­£çš„åº”ç”¨ç¨‹åºä¸ä»…ä»…æ˜¯ä¸€ä¸ªåŸè¯­ï¼Œè€Œæ˜¯å®ƒä»¬çš„ç»„åˆã€‚
LangChainä¸­çš„é“¾æ˜¯ç”±é“¾æ¥ç»„æˆçš„ï¼Œå¯ä»¥æ˜¯åƒLLMsè¿™æ ·çš„åŸè¯­ï¼Œä¹Ÿå¯ä»¥æ˜¯å…¶ä»–chainsã€‚
é“¾çš„æœ€æ ¸å¿ƒç±»å‹æ˜¯LLMChainï¼Œå®ƒç”±PromptTemplateå’ŒLLMç»„æˆã€‚
æ‰©å±•å‰é¢çš„ä¾‹å­ï¼Œæˆ‘ä»¬å¯ä»¥æ„å»ºä¸€ä¸ªLLMChainï¼Œå®ƒé‡‡ç”¨ç”¨æˆ·è¾“å…¥ï¼Œç”¨PromptTemplateæ ¼å¼åŒ–å®ƒï¼Œç„¶åå°†æ ¼å¼åŒ–çš„å“åº”ä¼ é€’ç»™LLMã€‚
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
PromptTemplate
}
from
"langchain/prompts"
;
const
model
=
new
OpenAI
(
{
temperature
:
0.9
}
)
;
const
template
=
"What is a good name for a company that makes {product}?"
;
const
prompt
=
new
PromptTemplate
(
{
template
:
template
,
inputVariables
:
[
"product"
]
,
}
)
;
ç°åœ¨æˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªéå¸¸ç®€å•çš„é“¾æ¡ï¼Œå®ƒå°†é‡‡ç”¨ç”¨æˆ·è¾“å…¥ï¼Œç”¨å®ƒæ ¼å¼åŒ–æç¤ºï¼Œç„¶åå°†å…¶å‘é€ç»™LLM:ã€‚
import
{
LLMChain
}
from
"langchain/chains"
;
const
chain
=
new
LLMChain
(
{
llm
:
model
,
prompt
:
prompt
}
)
;
ç°åœ¨æˆ‘ä»¬å¯ä»¥ä»…æŒ‡å®šäº§å“è¿è¡Œè¯¥é“¾æ¡ï¼
const
res
=
await
chain
.
call
(
{
product
:
"colorful socks"
}
)
;
console
.
log
(
res
)
;
{
res:
{
text:
'ColorfulCo Sockery.'
}
}
è¿™å°±æ˜¯ç¬¬ä¸€æ¡é“¾ - LLM Chainã€‚è¿™æ˜¯æ¯”è¾ƒç®€å•çš„é“¾æ¡ä¹‹ä¸€ï¼Œä½†æ˜¯äº†è§£å®ƒçš„å·¥ä½œåŸç†å°†ä¸ºæ‚¨æ›´å¥½åœ°å¤„ç†æ›´å¤æ‚çš„é“¾æ¡æ‰“ä¸‹åŸºç¡€ã€‚
Agents: æ ¹æ®ç”¨æˆ·è¾“å…¥åŠ¨æ€è¿è¡Œé“¾
â€‹
åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬çœ‹è¿‡çš„é“¾æ˜¯ä»¥é¢„å®šé¡ºåºè¿è¡Œçš„ã€‚
ä»£ç†ä¸å†æ‰§è¡Œå®ƒä»¬ä½¿ç”¨LLMæ¥ç¡®å®šè¦æ‰§è¡Œçš„æ“ä½œä»¥åŠé¡ºåºã€‚åŠ¨ä½œå¯ä»¥æ˜¯ä½¿ç”¨å·¥å…·å¹¶è§‚å¯Ÿå…¶è¾“å‡ºï¼Œæˆ–è€…è¿”å›ç»™ç”¨æˆ·ã€‚
å¦‚æœæ­£ç¡®ä½¿ç”¨ä»£ç†ï¼Œå®ƒä»¬å¯ä»¥éå¸¸å¼ºå¤§ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­,æˆ‘ä»¬é€šè¿‡æœ€ç®€å•çš„ï¼Œæœ€é«˜çº§åˆ«çš„APIå‘æ‚¨å±•ç¤ºå¦‚ä½•è½»æ¾ä½¿ç”¨ä»£ç†ã€‚
ä¸ºäº†åŠ è½½ä»£ç†ï¼Œæ‚¨åº”è¯¥äº†è§£ä»¥ä¸‹æ¦‚å¿µ:
å·¥å…·ï¼ˆToolï¼‰ï¼šæ‰§è¡Œç‰¹å®šèŒè´£çš„å‡½æ•°ã€‚è¿™äº›èŒè´£å¯èƒ½åŒ…æ‹¬ï¼šGoogle æœç´¢ã€æ•°æ®åº“æŸ¥è¯¢ã€ä»£ç  REPLï¼ˆRead-Eval-Print Loopï¼Œè¯»å–-æ±‚å€¼-è¾“å‡ºå¾ªç¯ï¼‰ã€å…¶ä»–é“¾ã€‚ç›®å‰å·¥å…·çš„æ¥å£æ˜¯ä¸€ä¸ªæœŸæœ›è¾“å…¥å­—ç¬¦ä¸²ï¼Œå¹¶è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²çš„å‡½æ•°ã€‚
LLM: ä»£ç†ä½¿ç”¨çš„è¯­è¨€æ¨¡å‹ã€‚
Agent: æ‰€ä½¿ç”¨çš„ä»£ç†ã€‚è¿™åº”è¯¥æ˜¯ä¸€ä¸ªå¼•ç”¨æ”¯æŒä»£ç†ç±»çš„å­—ç¬¦ä¸²ã€‚ç”±äºæœ¬æ•™ç¨‹é‡ç‚¹ä»‹ç»æœ€ç®€å•çš„æœ€é«˜çº§APIï¼Œå› æ­¤ä»…æ¶µç›–ä½¿ç”¨æ ‡å‡†æ”¯æŒçš„ä»£ç†ã€‚
å¯¹äºè¿™ä¸ªä¾‹å­ï¼Œä½ éœ€è¦åœ¨
.env
æ–‡ä»¶ä¸­è®¾ç½®SerpAPIç¯å¢ƒå˜é‡ã€‚
SERPAPI_API_KEY
=
"..."
å®‰è£…
serpapi
åŒ…(Googleæœç´¢API):
npm
Yarn
pnpm
npm
install
-S serpapi
yarn
add
serpapi
pnpm
add
serpapi
ç°åœ¨æˆ‘ä»¬å¯ä»¥å¼€å§‹äº†ï¼
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
initializeAgentExecutorWithOptions
}
from
"langchain/agents"
;
import
{
SerpAPI
}
from
"langchain/tools"
;
import
{
Calculator
}
from
"langchain/tools/calculator"
;
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
tools
=
[
new
SerpAPI
(
process
.
env
.
SERPAPI_API_KEY
,
{
location
:
"Austin,Texas,United States"
,
hl
:
"en"
,
gl
:
"us"
,
}
)
,
new
Calculator
(
)
,
]
;
const
executor
=
await
initializeAgentExecutorWithOptions
(
tools
,
model
,
{
agentType
:
"zero-shot-react-description"
,
}
)
;
console
.
log
(
"Loaded agent."
)
;
const
input
=
"Who is Olivia Wilde's boyfriend?"
+
" What is his current age raised to the 0.23 power?"
;
console
.
log
(
`
Executing with input "
${
input
}
"...
`
)
;
const
result
=
await
executor
.
call
(
{
input
}
)
;
console
.
log
(
`
Got output
${
result
.
output
}
`
)
;
langchain-examples:start: Executing with input
"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?"
..
.
langchain-examples:start: Got output Olivia Wilde's boyfriend is Jason Sudeikis, and his current age raised to the
0.23
power is
2.4242784855673896
.
Memory: åœ¨é“¾å’Œä»£ç†ä¸­æ·»åŠ çŠ¶æ€
â€‹
åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬ç»å†çš„æ‰€æœ‰é“¾å’Œä»£ç†éƒ½æ˜¯æ— çŠ¶æ€çš„ã€‚ä½†æ˜¯ï¼Œé€šå¸¸æƒ…å†µä¸‹ï¼Œæ‚¨å¯èƒ½å¸Œæœ›ä¸€ä¸ªé“¾æˆ–ä»£ç†å…·æœ‰ä¸€äº› "å†…å­˜" çš„æ¦‚å¿µï¼Œä»¥ä¾¿å®ƒå¯ä»¥è®°ä½å…¶å…ˆå‰çš„äº¤äº’ä¿¡æ¯ã€‚è¿™æ˜¯è®¾è®¡èŠå¤©æœºå™¨äººæ—¶çš„æœ€æ˜æ˜¾å’Œç®€å•çš„ä¾‹å­ - æ‚¨å¸Œæœ›å®ƒè®°ä½å…ˆå‰çš„æ¶ˆæ¯ï¼Œä»¥ä¾¿å¯ä»¥åˆ©ç”¨ä¸Šä¸‹æ–‡è¿›è¡Œæ›´å¥½çš„äº¤æµã€‚è¿™å°†æ˜¯ä¸€ç§ "çŸ­æœŸè®°å¿†"ã€‚åœ¨æ›´å¤æ‚çš„æƒ…å†µä¸‹ï¼Œæ‚¨å¯ä»¥æƒ³è±¡ä¸€ä¸ªé“¾/ä»£ç†éšç€æ—¶é—´çš„æ¨ç§»è®°ä½å…³é”®ä¿¡æ¯ - è¿™å°†æ˜¯ä¸€ç§ "é•¿æœŸè®°å¿†" å½¢å¼ã€‚
LangChainæä¾›äº†å‡ ä¸ªä¸“é—¨ä¸ºæ­¤ç›®çš„åˆ›å»ºçš„é“¾ã€‚æœ¬èŠ‚ä»‹ç»äº†ä½¿ç”¨è¿™äº›é“¾ä¹‹ä¸€ï¼ˆâ€œConversationChainâ€)ã€‚
é»˜è®¤æƒ…å†µä¸‹ï¼Œâ€œConversationChainâ€å…·æœ‰ä¸€ç§ç®€å•çš„è®°å¿†ç±»å‹ï¼Œå®ƒä¼šè®°ä½æ‰€æœ‰å…ˆå‰çš„è¾“å…¥/è¾“å‡ºå¹¶å°†å…¶æ·»åŠ åˆ°ä¼ é€’çš„ä¸Šä¸‹æ–‡ä¸­ã€‚è®©æˆ‘ä»¬æ¥çœ‹çœ‹å¦‚ä½•ä½¿ç”¨è¿™ä¸ªé“¾ã€‚
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
BufferMemory
}
from
"langchain/memory"
;
import
{
ConversationChain
}
from
"langchain/chains"
;
const
model
=
new
OpenAI
(
{
}
)
;
const
memory
=
new
BufferMemory
(
)
;
const
chain
=
new
ConversationChain
(
{
llm
:
model
,
memory
:
memory
}
)
;
const
res1
=
await
chain
.
call
(
{
input
:
"Hi! I'm Jim."
}
)
;
console
.
log
(
res1
)
;
{
response:
" Hi Jim! It's nice to meet you. My name is AI. What would you like to talk about?"
}
const
res2
=
await
chain
.
call
(
{
input
:
"What's my name?"
}
)
;
console
.
log
(
res2
)
;
{
response:
' You said your name is Jim. Is there anything else you would like to talk about?'
}
æµåª’ä½“
â€‹
æ‚¨è¿˜å¯ä»¥ä½¿ç”¨æµAPIï¼Œéšç€ç”Ÿæˆçš„å•è¯çš„æµå¼è¿”å›ï¼Œè¿™å¯¹äºèŠå¤©æœºå™¨äººç­‰åœºæ™¯å¾ˆæœ‰ç”¨ï¼Œæ‚¨å¸Œæœ›å‘ç”¨æˆ·æ˜¾ç¤ºæ­£åœ¨ç”Ÿæˆçš„å†…å®¹ã€‚æ³¨æ„: ç›®å‰OpenAIä¸æ”¯æŒå¯ç”¨æµæ—¶çš„â€œtokenUsageâ€æŠ¥å‘Šã€‚
import
{
OpenAI
}
from
"langchain/llms/openai"
;
// To enable streaming, we pass in `streaming: true` to the LLM constructor.
// Additionally, we pass in a handler for the `handleLLMNewToken` event.
const
chat
=
new
OpenAI
(
{
streaming
:
true
,
callbacks
:
[
{
handleLLMNewToken
(
token
:
string
)
{
process
.
stdout
.
write
(
token
)
;
}
,
}
,
]
,
}
)
;
await
chat
.
call
(
"Write me a song about sparkling water."
)
;
/*
Verse 1
Crystal clear and made with care
Sparkling water on my lips, so refreshing in the air
Fizzy bubbles, light and sweet
My favorite beverage I canâ€™t help but repeat
Chorus
A toast to sparkling water, Iâ€™m feeling so alive
Letâ€™s take a sip, and letâ€™s take a drive
A toast to sparkling water, itâ€™s the best Iâ€™ve had in my life
Itâ€™s the best way to start off the night
Verse 2
Itâ€™s the perfect drink to quench my thirst
Itâ€™s the best way to stay hydrated, itâ€™s the first
A few ice cubes, a splash of lime
It will make any day feel sublime
...
*/



--- æ–‡ä»¶: output_20250622_020018\docs\getting-started\install.md ---
---
url: https://js.langchain.com.cn/docs/getting-started/install
crawled_at: 2025-06-22T02:00:19.656408
---

å®‰è£…å’Œè®¾ç½®
æ”¯æŒçš„ç¯å¢ƒ
â€‹
LangChain æ˜¯ä½¿ç”¨ TypeScript ç¼–å†™çš„ï¼Œå¯ä»¥åœ¨ä»¥ä¸‹ç¯å¢ƒä¸­ä½¿ç”¨:
Node.js (ESM å’Œ CommonJS) - 18.xï¼Œ 19.xï¼Œ 20.x
Cloudflare Workers
Vercel / Next.js (æµè§ˆå™¨ï¼Œ æ— æœåŠ¡å™¨å’Œè¾¹ç¼˜å‡½æ•°)
Supabase Edge å‡½æ•°
æµè§ˆå™¨
Deno
å¿«é€Ÿå¼€å§‹
â€‹
å¦‚æœæ‚¨æƒ³åœ¨ Node.js ä¸­å¿«é€Ÿå¼€å§‹ä½¿ç”¨ LangChainï¼Œè¯·
å…‹éš†æ­¤å­˜å‚¨åº“
å¹¶æŒ‰ç…§è‡ªè¿°æ–‡ä»¶ä¸­çš„è¯´æ˜è®¾ç½®ä¾èµ–é¡¹ã€‚
å¦‚æœæ‚¨å¸Œæœ›è‡ªå·±è®¾ç½®æˆ–è€…åœ¨å…¶ä»–ç¯å¢ƒä¸­è¿è¡Œ LangChainï¼Œè¯·ç»§ç»­é˜…è¯»ä¸‹é¢çš„è¯´æ˜ã€‚
å®‰è£…
â€‹
è¦å¼€å§‹ä½¿ç”¨ LangChainï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£…:
npm
Yarn
pnpm
npm
install
-S langchain
yarn
add
langchain
pnpm
add
langchain
TypeScript
â€‹
LangChain æ˜¯ä½¿ç”¨ TypeScript ç¼–å†™çš„ï¼Œå¹¶ä¸ºå…¶æ‰€æœ‰çš„å…¬å…± API æä¾›äº†ç±»å‹å®šä¹‰ã€‚
åŠ è½½åº“
â€‹
ESM
â€‹
LangChain ä¸º Node.js ç¯å¢ƒæä¾›äº†ä¸€ä¸ª ESM æ„å»ºç‰ˆã€‚æ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹è¯­æ³•å¯¼å…¥å®ƒ:
import
{
OpenAI
}
from
"langchain/llms/openai"
;
å¦‚æœæ‚¨åœ¨ä¸€ä¸ª ESM é¡¹ç›®ä¸­ä½¿ç”¨ TypeScriptï¼Œæˆ‘ä»¬å»ºè®®æ‚¨æ›´æ–°æ‚¨çš„
tsconfig.json
ï¼Œå¹¶åŒ…å«ä»¥ä¸‹è®¾ç½®:
tsconfig.json
{
"compilerOptions"
:
{
...
"target"
:
"ES2020"
,
// or higher
"module"
:
"nodenext"
,
}
}
CommonJS
â€‹
LangChainæä¾›äº†é¢å‘Node.jsç¯å¢ƒçš„CommonJSæ„å»ºã€‚ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹è¯­æ³•è¿›è¡Œå¯¼å…¥:ã€‚
const
{
OpenAI
}
=
require
(
"langchain/llms/openai"
)
;
Cloudflare Workers
â€‹
LangChainå¯ä»¥åœ¨Cloudflare Workersä¸­ä½¿ç”¨ã€‚ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹è¯­æ³•è¿›è¡Œå¯¼å…¥:ã€‚
import
{
OpenAI
}
from
"langchain/llms/openai"
;
Vercel / Next.js
â€‹
LangChainå¯ä»¥åœ¨Vercel / Next.jsä¸­ä½¿ç”¨ã€‚æˆ‘ä»¬æ”¯æŒåœ¨å‰ç«¯ç»„ä»¶ã€æ— æœåŠ¡å™¨å‡½æ•°å’ŒEdgeå‡½æ•°ä¸­ä½¿ç”¨LangChainã€‚ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹è¯­æ³•è¿›è¡Œå¯¼å…¥:ã€‚
import
{
OpenAI
}
from
"langchain/llms/openai"
;
Deno / Supabase Edge Functions
â€‹
LangChainå¯ä»¥åœ¨Deno / Supabase Edge Functionsä¸­ä½¿ç”¨ã€‚ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹è¯­æ³•è¿›è¡Œå¯¼å…¥:ã€‚
import
{
OpenAI
}
from
"https://esm.sh/langchain/llms/openai"
;
æˆ‘ä»¬æ¨èæŸ¥çœ‹æˆ‘ä»¬çš„
Supabaseæ¨¡æ¿
ä»¥æŸ¥çœ‹åœ¨Supabase Edge Functionsä¸­å¦‚ä½•ä½¿ç”¨LangChainçš„ç¤ºä¾‹ã€‚
æµè§ˆå™¨
â€‹
LangChainå¯ä»¥åœ¨æµè§ˆå™¨ä¸­ä½¿ç”¨ã€‚åœ¨æˆ‘ä»¬çš„CIä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨Webpackå’ŒViteæµ‹è¯•äº†LangChainçš„æ†ç»‘ï¼Œä½†å…¶ä»–æ†ç»‘å™¨ä¹Ÿåº”è¯¥å¯ä»¥ä½¿ç”¨ã€‚ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹è¯­æ³•è¿›è¡Œå¯¼å…¥:ã€‚
import
{
OpenAI
}
from
"langchain/llms/openai"
;
ä»ç‰ˆæœ¬<0.0.52è¿›è¡Œæ›´æ–°
â€‹
å¦‚æœæ‚¨æ­£åœ¨æ›´æ–°LangChainçš„0.0.52ä¹‹å‰çš„ç‰ˆæœ¬ï¼Œæ‚¨éœ€è¦æ›´æ–°å¯¼å…¥ä»¥ä½¿ç”¨æ–°çš„è·¯å¾„ç»“æ„ã€‚
ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨ä»¥å‰æ‰§è¡Œçš„æ˜¯
import
{
OpenAI
}
from
"langchain/llms"
;
ç°åœ¨ï¼Œæ‚¨éœ€è¦æ‰§è¡Œä»¥ä¸‹æ“ä½œ
import
{
OpenAI
}
from
"langchain/llms/openai"
;
é€‚ç”¨äºä¸‹åˆ—6ä¸ªæ¨¡å—çš„æ‰€æœ‰å¯¼å…¥ï¼Œè¿™äº›æ¨¡å—å·²åˆ†å‰²ä¸ºæ¯ä¸ªé›†æˆçš„å­æ¨¡å—ã€‚ç»„åˆæ¨¡å—å·²è¢«å¼ƒç”¨ï¼Œåœ¨ Node.js ä¹‹å¤–ä¸èµ·ä½œç”¨ï¼Œå¹¶å°†åœ¨å°†æ¥çš„ç‰ˆæœ¬ä¸­åˆ é™¤ã€‚
å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯
langchain/llms
ï¼Œè¯·å‚è§
LLMs
ä»¥è·å–æ›´æ–°åçš„å¯¼å…¥è·¯å¾„ã€‚
å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯
langchain/chat_models
ï¼Œè¯·å‚è§
Chat Models
ä»¥è·å–æ›´æ–°åçš„å¯¼å…¥è·¯å¾„ã€‚
å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯
langchain/embeddings
ï¼Œè¯·å‚è§
Embeddings
ä»¥è·å–æ›´æ–°åçš„å¯¼å…¥è·¯å¾„ã€‚
å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯
langchain/vectorstores
ï¼Œè¯·å‚è§
Vector Stores
ä»¥è·å–æ›´æ–°åçš„å¯¼å…¥è·¯å¾„ã€‚
å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯
langchain/document_loaders
ï¼Œè¯·å‚è§
Document Loaders
ä»¥è·å–æ›´æ–°åçš„å¯¼å…¥è·¯å¾„ã€‚
å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯
langchain/retrievers
ï¼Œè¯·å‚è§
Retrievers
ä»¥è·å–æ›´æ–°åçš„å¯¼å…¥è·¯å¾„ã€‚
å…¶ä»–æ¨¡å—ä¸å—æ­¤æ›´æ”¹å½±å“ï¼Œæ‚¨å¯ä»¥ç»§ç»­ä»åŒä¸€è·¯å¾„å¯¼å…¥å®ƒä»¬ã€‚
æ­¤å¤–ï¼Œä¸ºäº†æ”¯æŒæ–°çš„ç¯å¢ƒï¼Œéœ€è¦è¿›è¡Œä¸€äº›é‡å¤§æ›´æ”¹:
import { Calculator } from "langchain/tools";
ç°å·²ç§»è‡³
import { Calculator } from "langchain/tools/calculator";
import { loadLLM } from "langchain/llms";
ç°å·²ç§»è‡³
import { loadLLM } from "langchain/llms/load";
import { loadAgent } from "langchain/agents";
ç°å·²ç§»è‡³
import { loadAgent } from "langchain/agents/load";
import { loadPrompt } from "langchain/prompts";
ç°å·²ç§»è‡³
import { loadPrompt } from "langchain/prompts/load";
import { loadChain } from "langchain/chains";
ç°å·²ç§»è‡³
import { loadChain } from "langchain/chains/load";
ä¸å—æ”¯æŒ: Node.js 16
â€‹
æˆ‘ä»¬ä¸æ”¯æŒ Node.js 16ï¼Œä½†å¦‚æœæ‚¨ä»ç„¶å¸Œæœ›åœ¨ Node.js 16 ä¸Šè¿è¡Œ LangChainï¼Œæ‚¨éœ€è¦æŒ‰ç…§æœ¬èŠ‚ä¸­çš„è¯´æ˜è¿›è¡Œæ“ä½œã€‚æˆ‘ä»¬ä¸èƒ½ä¿è¯è¿™äº›è¯´æ˜åœ¨æœªæ¥ä»èƒ½å·¥ä½œã€‚
æ‚¨å°†éœ€è¦å…¨å±€å®‰è£…
fetch
ï¼Œ å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¹‹ä¸€æ¥å®ç°:
ä½¿ç”¨
NODE_OPTIONS='--experimental-fetch' node ...
å‘½ä»¤è¿è¡Œæ‚¨çš„åº”ç”¨ç¨‹åºï¼Œ æˆ–
å®‰è£…
node-fetch
å¹¶æŒ‰ç…§
æ­¤å¤„
çš„è¯´æ˜è¿›è¡Œæ“ä½œ
æ­¤å¤–ï¼Œæ‚¨è¿˜éœ€è¦å°†
unstructuredClone
è¿›è¡Œ polyfillï¼Œ æ‚¨å¯ä»¥é€šè¿‡å®‰è£…
core-js
å¹¶æŒ‰ç…§
æ­¤å¤„
çš„è¯´æ˜è¿›è¡Œæ“ä½œæ¥å®ç°ã€‚
å¦‚æœæ‚¨åœ¨ Node.js 18+ ä¸Šè¿è¡Œæ­¤ä»£ç ï¼Œæ‚¨ä¸éœ€è¦é‡‡å–ä»»ä½•æªæ–½ã€‚



--- æ–‡ä»¶: output_20250622_020018\docs\modules\agents.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/
crawled_at: 2025-06-22T02:00:19.608488
---

ä»£ç†äºº
info
æ¦‚å¿µæŒ‡å—
ä¸€äº›åº”ç”¨ç¨‹åºéœ€è¦çš„ä¸ä»…æ˜¯é¢„å…ˆç¡®å®šçš„è°ƒç”¨LLMs/å…¶ä»–å·¥å…·çš„é“¾ï¼Œä½†å¯èƒ½æ˜¯ä¾èµ–äºç”¨æˆ·è¾“å…¥çš„æœªçŸ¥é“¾ã€‚åœ¨è¿™äº›ç±»å‹çš„é“¾ä¸­ï¼Œå­˜åœ¨ä¸€ä¸ªâ€œä»£ç†äººâ€ï¼Œå…¶å¯ä»¥è®¿é—®ä¸€ç»„å·¥å…·ã€‚æ ¹æ®ç”¨æˆ·è¾“å…¥ï¼Œä»£ç†äººå¯ä»¥å†³å®šæ˜¯å¦ä»¥åŠå¦‚ä½•è°ƒç”¨è¿™äº›å·¥å…·ã€‚
ç›®å‰æœ‰ä¸¤ç§ä¸»è¦ç±»å‹çš„ä»£ç†äºº:
åŠ¨ä½œä»£ç†äºº
: è¿™äº›ä»£ç†äººå†³å®šè¦é‡‡å–çš„è¡ŒåŠ¨å¹¶ä¸€æ­¥ä¸€æ­¥åœ°é‡‡å–è¿™äº›è¡ŒåŠ¨
è®¡åˆ’æ‰§è¡Œä»£ç†äºº
: è¿™äº›ä»£ç†äººé¦–å…ˆå†³å®šè¦æ‰§è¡Œçš„ä¸€ç³»åˆ—è¡ŒåŠ¨è®¡åˆ’ï¼Œç„¶åé€ä¸€æ‰§è¡Œè¿™äº›è¡ŒåŠ¨ã€‚
ä½ åº”è¯¥ä½•æ—¶ä½¿ç”¨å®ƒä»¬ï¼Ÿ
åŠ¨ä½œä»£ç†äººæ›´ä¸ºå¸¸è§„ï¼Œé€‚åˆå¤„ç†å°ä»»åŠ¡ã€‚
å¯¹äºæ›´å¤æ‚æˆ–é•¿æœŸè¿è¡Œçš„ä»»åŠ¡ï¼Œè®¡åˆ’æ‰§è¡Œä»£ç†äººçš„åˆå§‹è§„åˆ’æ­¥éª¤æœ‰åŠ©äºä¿æŒé•¿æœŸç›®æ ‡å’Œå…³æ³¨ç‚¹ï¼Œä½†é€šå¸¸éœ€è¦æ›´å¤šçš„è°ƒç”¨å’Œæ›´é«˜çš„å»¶è¿Ÿã€‚
è¿™ä¸¤ç§ä»£ç†äººä¹Ÿä¸æ˜¯äº’æ–¥çš„ - å®é™…ä¸Šï¼Œé€šå¸¸æœ€å¥½ç”±åŠ¨ä½œä»£ç†äººè´Ÿè´£æ‰§è¡Œè®¡åˆ’å’Œæ‰§è¡Œä»£ç†äººã€‚
åŠ¨ä½œä»£ç†äºº
â€‹
ä¸€ä¸ªåŠ¨ä½œä»£ç†äººçš„é«˜çº§ä¼ªä»£ç å¦‚ä¸‹:
æ¥æ”¶ä¸€äº›ç”¨æˆ·è¾“å…¥
ä»£ç†äºº
å†³å®šä½¿ç”¨å“ªä¸ª
å·¥å…·
ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰ï¼Œä»¥åŠè¯¥å·¥å…·çš„è¾“å…¥åº”è¯¥æ˜¯ä»€ä¹ˆã€‚
ä½¿ç”¨
å·¥å…·è¾“å…¥
è°ƒç”¨é‚£ä¸ª
å·¥å…·
ï¼Œå¹¶è®°å½•
è§‚å¯Ÿç»“æœ
ï¼ˆè¿™åªæ˜¯è°ƒç”¨è¯¥
å·¥å…·è¾“å…¥
çš„è¾“å‡º)ã€‚
å°†
å·¥å…·
çš„å†å²è®°å½•ã€
å·¥å…·è¾“å…¥
å’Œ
è§‚å¯Ÿç»“æœ
ä¼ å›åˆ°
ä»£ç†ç¨‹åº
ï¼Œç„¶åå®ƒå†³å®šä¸‹ä¸€æ­¥è¯¥æ€ä¹ˆåšã€‚
é‡å¤ä¸Šè¿°æ­¥éª¤ï¼Œç›´åˆ°
ä»£ç†ç¨‹åº
å†³å®šä¸å†éœ€è¦ä½¿ç”¨
å·¥å…·
ï¼Œç„¶åç›´æ¥å›åº”ç”¨æˆ·ã€‚
interface
AgentStep
{
action
:
AgentAction
;
observation
:
string
;
}
interface
AgentAction
{
tool
:
string
;
// Tool.name
toolInput
:
string
;
// Tool.call argument
}
interface
AgentFinish
{
returnValues
:
object
;
}
class
Agent
{
plan
(
steps
:
AgentStep
[
]
,
inputs
:
object
)
:
Promise
<
AgentAction
|
AgentFinish
>
;
}
è®¡åˆ’å’Œæ‰§è¡Œä»£ç†
â€‹
è®¡åˆ’å’Œæ‰§è¡Œä»£ç†çš„é«˜çº§ä¼ªä»£ç å¤§è‡´å¦‚ä¸‹:
æ¥æ”¶åˆ°ä¸€äº›ç”¨æˆ·è¾“å…¥
è®¡åˆ’è€…åˆ—å‡ºè¦é‡‡å–çš„æ­¥éª¤
æ‰§è¡Œè€…é€ä¸ªæ‰§è¡Œæ­¥éª¤ï¼Œç›´åˆ°è¾“å‡ºæœ€ç»ˆç»“æœ
å½“å‰å®ç°çš„æ–¹å¼æ˜¯ä½¿ç”¨LLMChainä½œä¸ºè®¡åˆ’è€…ï¼Œä½¿ç”¨Action Agentä½œä¸ºæ‰§è¡Œè€…ã€‚
æ·±å…¥äº†è§£
â€‹
ğŸ—ƒï¸
ä»£ç†
3 items
ğŸ—ƒï¸
Agent Executors
1 items
ğŸ—ƒï¸
å·¥å…·
7 items
ğŸ—ƒï¸
å·¥å…·åŒ…
4 items
ğŸ“„ï¸
å…¶ä»–åŠŸèƒ½
æˆ‘ä»¬ä¸ºAgentsæä¾›äº†è®¸å¤šå…¶ä»–åŠŸèƒ½ã€‚æ‚¨è¿˜åº”æŸ¥çœ‹LLM-specific featureså’ŒChat Model-specific featuresã€‚



--- æ–‡ä»¶: output_20250622_020018\docs\modules\chains.md ---
---
url: https://js.langchain.com.cn/docs/modules/chains/
crawled_at: 2025-06-22T02:00:21.004898
---

å…¥é—¨æŒ‡å—: é“¾
info
æ¦‚å¿µæŒ‡å—
åœ¨ä¸€äº›åº”ç”¨ä¸­ï¼Œä»…ä½¿ç”¨ä¸€ä¸ªè¯­è¨€æ¨¡å‹æ˜¯å¯ä»¥çš„ï¼Œä½†é€šå¸¸å°†è¯­è¨€æ¨¡å‹ä¸å…¶ä»–ä¿¡æ¯æºï¼ˆä¾‹å¦‚ç¬¬ä¸‰æ–¹APIæˆ–å…¶ä»–è¯­è¨€æ¨¡å‹)ç»„åˆä½¿ç”¨æ˜¯éå¸¸æœ‰ç”¨çš„ã€‚
è¿™å°±æ˜¯é“¾çš„æ¦‚å¿µã€‚
LangChainæä¾›äº†ä¸€ä¸ªç”¨äºé“¾çš„æ ‡å‡†æ¥å£ï¼Œä»¥åŠä¸€äº›å¯ç›´æ¥ä½¿ç”¨çš„å†…ç½®é“¾ã€‚æ‚¨ä¹Ÿå¯ä»¥åˆ›å»ºè‡ªå·±çš„é“¾ã€‚
ğŸ“„ï¸
LLMé“¾
æ¦‚å¿µæŒ‡å—
ğŸ—ƒï¸
ä¸ç´¢å¼•ç›¸å…³çš„é“¾
3 items
ğŸ“„ï¸
é¡ºåºé“¾
é¡ºåºé“¾å…è®¸æ‚¨è¿æ¥å¤šä¸ªé“¾ï¼Œå¹¶å°†å®ƒä»¬ç»„åˆæˆæ‰§è¡Œç‰¹å®šåœºæ™¯çš„ç®¡é“ã€‚
ğŸ—ƒï¸
å…¶ä»–é“¾
8 items
ğŸ“„ï¸
æç¤ºé€‰æ‹©å™¨
æ¦‚å¿µæŒ‡å—
é«˜çº§
â€‹
è¦å®ç°è‡ªå·±çš„è‡ªå®šä¹‰é“¾ï¼Œæ‚¨å¯ä»¥ç»§æ‰¿
BaseChain
å¹¶å®ç°ä»¥ä¸‹æ–¹æ³•:
import
{
CallbackManagerForChainRun
}
from
"langchain/callbacks"
;
import
{
BaseChain
as
_
}
from
"langchain/chains"
;
import
{
BaseMemory
}
from
"langchain/memory"
;
import
{
ChainValues
}
from
"langchain/schema"
;
abstract
class
BaseChain
{
memory
?
:
BaseMemory
;
/**
* Run the core logic of this chain and return the output
*/
abstract
_call
(
values
:
ChainValues
,
runManager
?
:
CallbackManagerForChainRun
)
:
Promise
<
ChainValues
>
;
/**
* Return the string type key uniquely identifying this class of chain.
*/
abstract
_chainType
(
)
:
string
;
/**
* Return the list of input keys this chain expects to receive when called.
*/
abstract
get
inputKeys
(
)
:
string
[
]
;
/**
* Return the list of output keys this chain will produce when called.
*/
abstract
get
outputKeys
(
)
:
string
[
]
;
}
ç»§æ‰¿
BaseChain
â€‹
_call
æ–¹æ³•æ˜¯è‡ªå®šä¹‰é“¾å¿…é¡»å®ç°çš„ä¸»è¦æ–¹æ³•ã€‚å®ƒæ¥å—è¾“å…¥è®°å½•å¹¶è¿”å›è¾“å‡ºè®°å½•ã€‚æ¥æ”¶åˆ°çš„è¾“å…¥åº”ç¬¦åˆ
inputKeys
æ•°ç»„ï¼Œè¿”å›çš„è¾“å‡ºåº”ç¬¦åˆ
outputKeys
æ•°ç»„ã€‚
åœ¨è‡ªå®šä¹‰é“¾ä¸­å®ç°æ­¤æ–¹æ³•æ—¶ï¼Œå€¼å¾—ç‰¹åˆ«å…³æ³¨çš„æ˜¯
runManager
å‚æ•°ï¼Œå®ƒå…è®¸æ‚¨çš„è‡ªå®šä¹‰é“¾å‚ä¸ä¸å†…ç½®é“¾ç›¸åŒçš„å›è°ƒç³»ç»Ÿ
callbacks system
ã€‚
å¦‚æœåœ¨è‡ªå®šä¹‰é“¾ä¸­è°ƒç”¨å¦ä¸€ä¸ªé“¾/æ¨¡å‹/ä»£ç†ï¼Œåˆ™åº”å°†å…¶ä¼ é€’ç»™è°ƒç”¨
runManagerï¼Ÿ.getChildï¼ˆ)
çš„ç»“æœï¼Œè¯¥ç»“æœå°†ç”Ÿæˆä¸€ä¸ªæ–°çš„å›è°ƒç®¡ç†å™¨ï¼ŒèŒƒå›´é™å®šä¸ºè¯¥å†…éƒ¨è¿è¡Œã€‚ä¾‹å¦‚:
import
{
BasePromptTemplate
,
PromptTemplate
}
from
"langchain/prompts"
;
import
{
BaseLanguageModel
}
from
"langchain/base_language"
;
import
{
CallbackManagerForChainRun
}
from
"langchain/callbacks"
;
import
{
BaseChain
,
ChainInputs
}
from
"langchain/chains"
;
import
{
ChainValues
}
from
"langchain/schema"
;
export
interface
MyCustomChainInputs
extends
ChainInputs
{
llm
:
BaseLanguageModel
;
promptTemplate
:
string
;
}
export
class
MyCustomChain
extends
BaseChain
implements
MyCustomChainInputs
{
llm
:
BaseLanguageModel
;
promptTemplate
:
string
;
prompt
:
BasePromptTemplate
;
constructor
(
fields
:
MyCustomChainInputs
)
{
super
(
fields
)
;
this
.
llm
=
fields
.
llm
;
this
.
promptTemplate
=
fields
.
promptTemplate
;
this
.
prompt
=
PromptTemplate
.
fromTemplate
(
this
.
promptTemplate
)
;
}
async
_call
(
values
:
ChainValues
,
runManager
?
:
CallbackManagerForChainRun
)
:
Promise
<
ChainValues
>
{
// Your custom chain logic goes here
// This is just an example that mimics LLMChain
const
promptValue
=
await
this
.
prompt
.
formatPromptValue
(
values
)
;
// Whenever you call a language model, or another chain, you should pass
// a callback manager to it. This allows the inner run to be tracked by
// any callbacks that are registered on the outer run.
// You can always obtain a callback manager for this by calling
// `runManager?.getChild()` as shown below.
const
result
=
await
this
.
llm
.
generatePrompt
(
[
promptValue
]
,
{
}
,
runManager
?.
getChild
(
)
)
;
// If you want to log something about this run, you can do so by calling
// methods on the runManager, as shown below. This will trigger any
// callbacks that are registered for that event.
runManager
?.
handleText
(
"Log something about this run"
)
;
return
{
output
:
result
.
generations
[
0
]
[
0
]
.
text
}
;
}
_chainType
(
)
:
string
{
return
"my_custom_chain"
;
}
get
inputKeys
(
)
:
string
[
]
{
return
[
"input"
]
;
}
get
outputKeys
(
)
:
string
[
]
{
return
[
"output"
]
;
}
}



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/
crawled_at: 2025-06-22T02:00:21.626870
---

ç´¢å¼•
info
æ¦‚å¿µæŒ‡å—
æœ¬èŠ‚æ¶‰åŠä¸å°†è‡ªå·±çš„æ•°æ®å¯¼å…¥ LangChainã€å¯¹å…¶è¿›è¡Œç´¢å¼•å’Œä½¿å…¶å¯ç”¨äº LLMs / èŠå¤©æ¨¡å‹ç›¸å…³çš„ä¸€åˆ‡ã€‚
ğŸ—ƒï¸
æ–‡æ¡£è£…è½½
1 items
ğŸ—ƒï¸
æ–‡æœ¬åˆ†å‰²å™¨ï¼ˆText Splitters)
1 items
ğŸ—ƒï¸
çŸ¢é‡å­˜å‚¨
1 items
ğŸ—ƒï¸
å¬å›å™¨ï¼ˆRetrievers)
13 items



--- æ–‡ä»¶: output_20250622_020018\docs\modules\memory.md ---
---
url: https://js.langchain.com.cn/docs/modules/memory/
crawled_at: 2025-06-22T02:00:24.720710
---

å…¥é—¨: å†…å­˜
info
æ¦‚å¿µæŒ‡å—
åœ¨å¯¹è¯è¿‡ç¨‹ä¸­å­˜å‚¨å’Œæ£€ç´¢æ•°æ®çš„æ¦‚å¿µè¢«ç§°ä¸ºå†…å­˜ã€‚æœ‰ä¸¤ç§ä¸»è¦æ–¹æ³•ï¼Œ
loadMemoryVariables
å’Œ
saveContext
ã€‚ç¬¬ä¸€ç§æ–¹æ³•ç”¨äºä»å†…å­˜ä¸­æ£€ç´¢æ•°æ®ï¼ˆå¯é€‰æ‹©ä½¿ç”¨å½“å‰è¾“å…¥å€¼)ï¼Œ ç¬¬äºŒç§æ–¹æ³•ç”¨äºå°†æ•°æ®å­˜å‚¨åˆ°å†…å­˜ä¸­ã€‚
export
type
InputValues
=
Record
<
string
,
any
>
;
export
type
OutputValues
=
Record
<
string
,
any
>
;
interface
BaseMemory
{
loadMemoryVariables
(
values
:
InputValues
)
:
Promise
<
MemoryVariables
>
;
saveContext
(
inputValues
:
InputValues
,
outputValues
:
OutputValues
)
:
Promise
<
void
>
;
}
note
ä¸è¦åœ¨ä¸¤ä¸ªä¸åŒçš„é“¾ä¹‹é—´å…±äº«ç›¸åŒçš„å­˜å‚¨å™¨å®ä¾‹ï¼Œ å­˜å‚¨å™¨å®ä¾‹ä»£è¡¨å•ä¸ªå¯¹è¯çš„å†å²è®°å½•
note
å¦‚æœæ‚¨å°†LangChainåº”ç”¨éƒ¨ç½²åœ¨æ— æœåŠ¡å™¨ç¯å¢ƒä¸­ï¼Œè¯·ä¸è¦å°†å­˜å‚¨å™¨å®ä¾‹å­˜å‚¨åœ¨å˜é‡ä¸­ï¼Œ å› ä¸ºæ‚¨çš„æ‰˜ç®¡æä¾›å•†å¯èƒ½ä¼šåœ¨ä¸‹ä¸€æ¬¡è°ƒç”¨è¯¥å‡½æ•°æ—¶é‡ç½®å®ƒã€‚
æ‰€æœ‰å†…å­˜ç±»
â€‹
ğŸ—ƒï¸
ç¤ºä¾‹
12 items
é«˜çº§
â€‹
è¦å®ç°æ‚¨è‡ªå·±çš„å†…å­˜ç±»ï¼Œæœ‰ä¸¤ä¸ªé€‰é¡¹:
å­ç±»åŒ–
BaseChatMemory
â€‹
è¿™æ˜¯å®ç°è‡ªå·±çš„å†…å­˜ç±»çš„æœ€ç®€å•æ–¹æ³•ã€‚æ‚¨å¯ä»¥å­ç±»åŒ–
BaseChatMemory
ï¼Œ é€šè¿‡å°†è¾“å…¥å’Œè¾“å‡ºä¿å­˜ä¸º
èŠå¤©æ¶ˆæ¯
æ¥å¤„ç†
saveContext
ï¼Œ å¹¶ä»…å®ç°
loadMemoryVariables
æ–¹æ³•ã€‚è¯¥æ–¹æ³•è´Ÿè´£è¿”å›ä¸å½“å‰è¾“å…¥å€¼ç›¸å…³çš„å­˜å‚¨å™¨å˜é‡ã€‚
abstract
class
BaseChatMemory
extends
BaseMemory
{
chatHistory
:
ChatMessageHistory
;
abstract
loadMemoryVariables
(
values
:
InputValues
)
:
Promise
<
MemoryVariables
>
;
}
Subclassing
BaseMemory
â€‹
å¦‚æœæ‚¨æƒ³è¦å®ç°ä¸€ä¸ªæ›´è‡ªå®šä¹‰çš„å†…å­˜ç±»ï¼Œæ‚¨å¯ä»¥ç»§æ‰¿
BaseMemory
ç±»å¹¶å®ç°
loadMemoryVariables
å’Œ
saveContext
æ–¹æ³•ã€‚
saveContext
æ–¹æ³•è´Ÿè´£å°†è¾“å…¥å’Œè¾“å‡ºå€¼å­˜å‚¨åœ¨å†…å­˜ä¸­ã€‚
loadMemoryVariables
æ–¹æ³•è´Ÿè´£è¿”å›ä¸å½“å‰è¾“å…¥å€¼ç›¸å…³çš„å†…å­˜å˜é‡ã€‚
abstract
class
BaseMemory
{
abstract
loadMemoryVariables
(
values
:
InputValues
)
:
Promise
<
MemoryVariables
>
;
abstract
saveContext
(
inputValues
:
InputValues
,
outputValues
:
OutputValues
)
:
Promise
<
void
>
;
}



--- æ–‡ä»¶: output_20250622_020018\docs\modules\models.md ---
---
url: https://js.langchain.com.cn/docs/modules/models/
crawled_at: 2025-06-22T02:00:25.235485
---

æ¨¡å‹
info
æ¦‚å¿µæŒ‡å—
æ¨¡å‹æ˜¯LangChainçš„æ ¸å¿ƒç»„ä»¶ã€‚LangChainä¸æ˜¯æ¨¡å‹çš„æä¾›è€…ï¼Œè€Œæ˜¯æä¾›æ ‡å‡†æ¥å£ï¼Œé€šè¿‡è¯¥æ¥å£æ‚¨å¯ä»¥ä¸å„ç§è¯­è¨€æ¨¡å‹è¿›è¡Œäº¤äº’ã€‚
LangChainæ”¯æŒæ–‡æœ¬æ¨¡å‹(LLMs)ï¼ŒèŠå¤©æ¨¡å‹å’Œæ–‡æœ¬åµŒå…¥æ¨¡å‹ã€‚
LLMsä½¿ç”¨æ–‡æœ¬è¾“å…¥å’Œè¾“å‡ºï¼Œè€ŒèŠå¤©æ¨¡å‹ä½¿ç”¨æ¶ˆæ¯è¾“å…¥å’Œè¾“å‡ºã€‚
æ³¨æ„:
èŠå¤©æ¨¡å‹APIè¿˜æ¯”è¾ƒæ–°ï¼Œå› æ­¤æˆ‘ä»¬è¿˜åœ¨æ‰¾å‡ºæ­£ç¡®çš„æŠ½è±¡ã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•åé¦ˆï¼Œè¯·å‘Šè¯‰æˆ‘ä»¬ï¼
æ‰€æœ‰æ¨¡å‹
â€‹
ğŸ—ƒï¸
èŠå¤©æ¨¡å‹
2 items
ğŸ—ƒï¸
åµŒå…¥
2 items
ğŸ—ƒï¸
LLMs
2 items
é«˜çº§
â€‹
æœ¬èŠ‚é¢å‘æƒ³è¦æ›´æ·±å…¥æŠ€æœ¯äº†è§£LangChainå·¥ä½œåŸç†çš„ç”¨æˆ·ã€‚å¦‚æœæ‚¨åˆšå¼€å§‹ä½¿ç”¨ï¼Œè¯·è·³è¿‡æœ¬èŠ‚ã€‚
LLMså’ŒèŠå¤©æ¨¡å‹éƒ½åŸºäº
BaseLanguageModel
ç±»æ„å»ºã€‚è¯¥ç±»ä¸ºæ‰€æœ‰æ¨¡å‹æä¾›äº†å…¬å…±æ¥å£ï¼Œå¹¶å…è®¸æˆ‘ä»¬åœ¨ä¸æ”¹å˜å…¶ä½™ä»£ç çš„æƒ…å†µä¸‹è½»æ¾åˆ‡æ¢æ¨¡å‹ã€‚
BaseLanguageModel
ç±»å…·æœ‰ä¸¤ä¸ªæŠ½è±¡æ–¹æ³•
generatePrompt
å’Œ
getNumTokens
ï¼Œåˆ†åˆ«ç”±
BaseChatModel
å’Œ
BaseLLM
å®ç°ã€‚
BaseLLM
æ˜¯
BaseLanguageModel
çš„å­ç±»ï¼Œä¸º LLMï¼ˆLarge Language Modelï¼‰æä¾›äº†ä¸€ä¸ªé€šç”¨çš„æ¥å£ï¼Œè€Œ
BaseChatModel
æ˜¯
BaseLanguageModel
çš„å­ç±»ï¼Œä¸ºèŠå¤©æ¨¡å‹æä¾›äº†ä¸€ä¸ªé€šç”¨çš„æ¥å£ã€‚



--- æ–‡ä»¶: output_20250622_020018\docs\modules\prompts.md ---
---
url: https://js.langchain.com.cn/docs/modules/prompts/
crawled_at: 2025-06-22T02:00:25.656864
---

index
#æç¤º
info
æ¦‚å¿µæŒ‡å—
LangChainæä¾›äº†å‡ ç§å®ç”¨å·¥å…·æ¥å¸®åŠ©ç®¡ç†è¯­è¨€æ¨¡å‹çš„æç¤ºï¼ŒåŒ…æ‹¬èŠå¤©æ¨¡å‹ã€‚
ğŸ—ƒï¸
æç¤ºæ¨¡æ¿
2 items
ğŸ“„ï¸
ç¤ºä¾‹é€‰æ‹©å™¨
æ¦‚å¿µæŒ‡å—



--- æ–‡ä»¶: output_20250622_020018\docs\modules\schema.md ---
---
url: https://js.langchain.com.cn/docs/modules/schema/
crawled_at: 2025-06-22T02:00:25.887701
---

æ•°æ®ç»“æ„
æ­¤éƒ¨åˆ†ä»‹ç»äº†åœ¨æ•´ä¸ªåº“ä¸­ä½¿ç”¨çš„æ¥å£ã€‚
ğŸ“„ï¸
èŠå¤©æ¶ˆæ¯
ç»ˆç«¯ç”¨æˆ·ä¸LLMsäº’åŠ¨çš„ä¸»è¦ç•Œé¢æ˜¯èŠå¤©ç•Œé¢ã€‚å› æ­¤ï¼Œä¸€äº›æ¨¡å‹æä¾›å•†å·²ç»å¼€å§‹ä»¥æœŸæœ›èŠå¤©æ¶ˆæ¯çš„æ–¹å¼æä¾›å¯¹åº•å±‚APIçš„è®¿é—®ã€‚è¿™äº›æ¶ˆæ¯å…·æœ‰å†…å®¹å­—æ®µï¼ˆé€šå¸¸æ˜¯æ–‡æœ¬)ï¼Œå¹¶ä¸ç”¨æˆ·ï¼ˆæˆ–è§’è‰²)ç›¸å…³è”ã€‚å½“å‰æ”¯æŒçš„ç”¨æˆ·æœ‰Systemï¼Œ Humanï¼Œå’ŒAIã€‚
ğŸ“„ï¸
æ–‡æ¡£
è¯­è¨€æ¨¡å‹åªçŸ¥é“å®ƒä»¬æ‰€è®­ç»ƒçš„å†…å®¹çš„ä¿¡æ¯ã€‚ä¸ºäº†è®©å®ƒä»¬èƒ½å¤Ÿå›ç­”é—®é¢˜æˆ–æ€»ç»“å…¶ä»–ä¿¡æ¯ï¼Œä½ éœ€è¦å°†ä¿¡æ¯ä¼ é€’ç»™è¯­è¨€æ¨¡å‹ã€‚å› æ­¤ï¼Œæ‹¥æœ‰æ–‡æ¡£çš„æ¦‚å¿µéå¸¸é‡è¦ã€‚
ğŸ“„ï¸
ç¤ºä¾‹
ç¤ºä¾‹æ˜¯è¾“å…¥/è¾“å‡ºå¯¹ï¼Œè¡¨ç¤ºå¯¹å‡½æ•°çš„è¾“å…¥å’Œé¢„æœŸè¾“å‡ºã€‚å®ƒä»¬å¯ç”¨äºæ¨¡å‹çš„è®­ç»ƒå’Œè¯„ä¼°ã€‚



--- æ–‡ä»¶: output_20250622_020018\docs\production\callbacks.md ---
---
url: https://js.langchain.com.cn/docs/production/callbacks/
crawled_at: 2025-06-22T02:00:26.045821
---

äº‹ä»¶ / å›è°ƒ
LangChain æä¾›äº†ä¸€ä¸ªå›è°ƒç³»ç»Ÿï¼Œå…è®¸ä½ åœ¨ LLM åº”ç”¨ç¨‹åºçš„å„ä¸ªé˜¶æ®µä¸­è¿›è¡Œé’©å­å¤„ç†ã€‚è¿™å¯¹äºè®°å½•æ—¥å¿—ã€
ç›‘è§†
ã€
æµåª’ä½“
å’Œå…¶ä»–ä»»åŠ¡éå¸¸æœ‰ç”¨ã€‚
ä½ å¯ä»¥é€šè¿‡ API ä¸­ä½¿ç”¨çš„
callbacks
å‚æ•°æ¥è®¢é˜…è¿™äº›äº‹ä»¶ã€‚æ­¤æ–¹æ³•æ¥å—ä¸€ä¸ªå¤„ç†ç¨‹åºå¯¹è±¡çš„åˆ—è¡¨ï¼Œè¿™äº›å¯¹è±¡åº”è¯¥å®ç°
API æ–‡æ¡£
ä¸­æè¿°çš„ä¸€ä¸ªæˆ–å¤šä¸ªæ–¹æ³•ã€‚
æ·±å…¥äº†è§£
â€‹
ğŸ“„ï¸
åˆ›å»ºå›è°ƒå¤„ç†ç¨‹åº
åˆ›å»ºè‡ªå®šä¹‰å¤„ç†ç¨‹åº
ğŸ“„ï¸
è‡ªå®šä¹‰Chainsä¸­çš„å›è°ƒ
LangChainæ—¨åœ¨å¯æ‰©å±•ã€‚ æ‚¨å¯ä»¥å°†è‡ªå·±çš„è‡ªå®šä¹‰Chainså’ŒAgentsæ·»åŠ åˆ°åº“ä¸­ã€‚ æœ¬é¡µå°†å‘æ‚¨å±•ç¤ºå¦‚ä½•å°†å›è°ƒæ·»åŠ åˆ°è‡ªå®šä¹‰çš„Chainså’ŒAgentsä¸­ã€‚
å¦‚ä½•ä½¿ç”¨å›è°ƒ
â€‹
åœ¨ API ä¸­çš„å¤§å¤šæ•°å¯¹è±¡ä¸Šï¼ˆ
Chains
ã€
Models
ã€
Tools
ã€
Agents
ç­‰)éƒ½æä¾›äº†
callbacks
å‚æ•°ï¼Œå®ƒæœ‰ä¸¤ä¸ªä¸åŒçš„ç”¨æ³•:
æ„é€ å™¨å›è°ƒ
â€‹
åœ¨æ„é€ å‡½æ•°ä¸­å®šä¹‰ï¼Œå¦‚
new LLMChain({ callbacks: [handler] })
ï¼Œå°†ç”¨äºè¯¥å¯¹è±¡ä¸Šè¿›è¡Œçš„æ‰€æœ‰è°ƒç”¨ï¼Œå¹¶ä¸”ä»…é€‚ç”¨äºè¯¥å¯¹è±¡æœ¬èº«ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ å°†å¤„ç†ç¨‹åºä¼ é€’ç»™
LLMChain
æ„é€ å‡½æ•°ï¼Œåˆ™ä¸ä¼šè¢«è¿æ¥åˆ°è¯¥é“¾ä¸Šçš„æ¨¡å‹ä½¿ç”¨ã€‚
import
{
ConsoleCallbackHandler
}
from
"langchain/callbacks"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
const
llm
=
new
OpenAI
(
{
temperature
:
0
,
// This handler will be used for all calls made with this LLM.
callbacks
:
[
new
ConsoleCallbackHandler
(
)
]
,
}
)
;
è¯·æ±‚å›è°ƒ
â€‹
åœ¨å‘å‡ºè¯·æ±‚çš„
call()
/
run()
/
apply()
æ–¹æ³•ä¸­å®šä¹‰ï¼Œä¾‹å¦‚
chain.call({ input: '...' }ï¼Œ [handler])
ï¼Œå°†ä»…ç”¨äºè¯¥ç‰¹å®šè¯·æ±‚åŠå…¶åŒ…å«çš„æ‰€æœ‰å­è¯·æ±‚ï¼ˆä¾‹å¦‚ï¼Œå¯¹ LLMChain çš„è°ƒç”¨ä¼šè§¦å‘å¯¹æ¨¡å‹çš„è°ƒç”¨ï¼Œè¯¥æ¨¡å‹ä½¿ç”¨åœ¨
call()
æ–¹æ³•ä¸­ä¼ é€’çš„ç›¸åŒå¤„ç†ç¨‹åº)ã€‚
import
{
ConsoleCallbackHandler
}
from
"langchain/callbacks"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
const
llm
=
new
OpenAI
(
{
temperature
:
0
,
}
)
;
// This handler will be used only for this call.
const
response
=
await
llm
.
call
(
"1 + 1 ="
,
undefined
,
[
new
ConsoleCallbackHandler
(
)
,
]
)
;
è¯¦ç»†æ¨¡å¼
â€‹
verbose
å‚æ•°å¯ç”¨äºAPIä¸­çš„å¤§éƒ¨åˆ†å¯¹è±¡ï¼ˆé“¾æ¥ï¼Œæ¨¡å‹ï¼Œå·¥å…·ï¼Œä»£ç†ç­‰)ä½œä¸ºæ„é€ å‚æ•°ã€‚ä¾‹å¦‚ï¼Œ
new LLMChain({ verbose: true })
ï¼Œå®ƒç›¸å½“äºå°†
callbacks
å‚æ•°ä¼ é€’ç»™è¯¥å¯¹è±¡å’Œæ‰€æœ‰å­å¯¹è±¡çš„
ConsoleCallbackHandler
ã€‚è¿™å¯¹äºè°ƒè¯•éå¸¸æœ‰ç”¨ï¼Œå› ä¸ºå®ƒä¼šå°†æ‰€æœ‰äº‹ä»¶è®°å½•åœ¨æ§åˆ¶å°ä¸Šã€‚æ‚¨è¿˜å¯ä»¥é€šè¿‡è®¾ç½®ç¯å¢ƒå˜é‡
LANGCHAIN_VERBOSE=true
æ¥ä¸ºæ•´ä¸ªåº”ç”¨ç¨‹åºå¯ç”¨è¯¦ç»†æ¨¡å¼ã€‚
import
{
PromptTemplate
}
from
"langchain/prompts"
;
import
{
LLMChain
}
from
"langchain/chains"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
const
chain
=
new
LLMChain
(
{
llm
:
new
OpenAI
(
{
temperature
:
0
}
)
,
prompt
:
PromptTemplate
.
fromTemplate
(
"Hello, world!"
)
,
// This will enable logging of all Chain *and* LLM events to the console.
verbose
:
true
,
}
)
;
ä½ ä½•æ—¶éœ€è¦ä½¿ç”¨å®ƒä»¬ï¼Ÿ
â€‹
æ„é€ å‡½æ•°å›è°ƒæœ€é€‚ç”¨äºè¯¸å¦‚æ—¥å¿—è®°å½•ï¼Œç›‘è§†ç­‰ç”¨ä¾‹ï¼Œè¿™äº›ç”¨ä¾‹ä¸ç‰¹å®šäºå•ä¸ªè¯·æ±‚ï¼Œè€Œæ˜¯é€‚ç”¨äºæ•´ä¸ªé“¾ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨è¦è®°å½•æ‰€æœ‰å‘é€åˆ°LLMChainçš„è¯·æ±‚ï¼Œåˆ™åº”å°†å¤„ç†ç¨‹åºä¼ é€’ç»™æ„é€ å‡½æ•°ã€‚
è¯·æ±‚å›è°ƒæœ€é€‚ç”¨äºæµå¼ä¼ è¾“ç­‰ç”¨ä¾‹ï¼Œå…¶ä¸­æ‚¨éœ€è¦å°†å•ä¸ªè¯·æ±‚çš„è¾“å‡ºæµåˆ°ç‰¹å®šçš„websocketè¿æ¥æˆ–å…¶ä»–ç±»ä¼¼çš„ç”¨ä¾‹ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æƒ³å°†å•ä¸ªè¯·æ±‚çš„è¾“å‡ºæµåˆ°websocketï¼Œåˆ™åº”å°†å¤„ç†ç¨‹åºä¼ é€’ç»™
call()
æ–¹æ³•ã€‚
ä½¿ç”¨ç¤ºä¾‹
â€‹
å†…ç½®å¤„ç†ç¨‹åº
â€‹
LangChainæä¾›äº†ä¸€äº›å†…ç½®å¤„ç†ç¨‹åºï¼Œå¯ç”¨äºå…¥é—¨ã€‚è¿™äº›å¯åœ¨
langchain/callbacks
æ¨¡å—ä¸­ä½¿ç”¨ã€‚æœ€åŸºæœ¬çš„å¤„ç†ç¨‹åºæ˜¯
ConsoleCallbackHandler
ï¼Œåªéœ€å°†æ‰€æœ‰äº‹ä»¶è®°å½•åˆ°æ§åˆ¶å°å³å¯ã€‚åœ¨å°†
verbose
æ ‡å¿—è®¾ç½®ä¸º
true
çš„æƒ…å†µä¸‹ï¼Œ
ConsoleCallbackHandler
å°†åœ¨ä¸æ˜¾å¼ä¼ é€’çš„æƒ…å†µä¸‹è¢«è°ƒç”¨ã€‚
import
{
ConsoleCallbackHandler
}
from
"langchain/callbacks"
;
import
{
LLMChain
}
from
"langchain/chains"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
PromptTemplate
}
from
"langchain/prompts"
;
export
const
run
=
async
(
)
=>
{
const
handler
=
new
ConsoleCallbackHandler
(
)
;
const
llm
=
new
OpenAI
(
{
temperature
:
0
,
callbacks
:
[
handler
]
}
)
;
const
prompt
=
PromptTemplate
.
fromTemplate
(
"1 + {number} ="
)
;
const
chain
=
new
LLMChain
(
{
prompt
,
llm
,
callbacks
:
[
handler
]
}
)
;
const
output
=
await
chain
.
call
(
{
number
:
2
}
)
;
/*
Entering new llm_chain chain...
Finished chain.
*/
console
.
log
(
output
)
;
/*
{ text: ' 3\n\n3 - 1 = 2' }
*/
// The non-enumerable key `__run` contains the runId.
console
.
log
(
output
.
__run
)
;
/*
{ runId: '90e1f42c-7cb4-484c-bf7a-70b73ef8e64b' }
*/
}
;
One-off handlers
â€‹
æ‚¨å¯ä»¥é€šè¿‡å°†æ™®é€šå¯¹è±¡ä¼ é€’ç»™
callbacks
å‚æ•°æ¥åˆ›å»ºä¸€ä¸ªä¸´æ—¶å¤„ç†ç¨‹åºã€‚è¯¥å¯¹è±¡åº”å®ç°
CallbackHandlerMethods
æ¥å£ã€‚å¦‚æœæ‚¨éœ€è¦åˆ›å»ºä¸€ä¸ªä»…ç”¨äºå•ä¸ªè¯·æ±‚çš„å¤„ç†ç¨‹åºï¼Œè¿™å°†éå¸¸æœ‰ç”¨ï¼Œä¾‹å¦‚æµå¼ä¼ è¾“LLM / Agent /ç­‰çš„è¾“å‡ºåˆ°WebSocketã€‚
import
{
OpenAI
}
from
"langchain/llms/openai"
;
// To enable streaming, we pass in `streaming: true` to the LLM constructor.
// Additionally, we pass in a handler for the `handleLLMNewToken` event.
const
chat
=
new
OpenAI
(
{
maxTokens
:
25
,
streaming
:
true
,
}
)
;
const
response
=
await
chat
.
call
(
"Tell me a joke."
,
undefined
,
[
{
handleLLMNewToken
(
token
:
string
)
{
console
.
log
(
{
token
}
)
;
}
,
}
,
]
)
;
console
.
log
(
response
)
;
/*
{ token: '\n' }
{ token: '\n' }
{ token: 'Q' }
{ token: ':' }
{ token: ' Why' }
{ token: ' did' }
{ token: ' the' }
{ token: ' chicken' }
{ token: ' cross' }
{ token: ' the' }
{ token: ' playground' }
{ token: '?' }
{ token: '\n' }
{ token: 'A' }
{ token: ':' }
{ token: ' To' }
{ token: ' get' }
{ token: ' to' }
{ token: ' the' }
{ token: ' other' }
{ token: ' slide' }
{ token: '.' }
Q: Why did the chicken cross the playground?
A: To get to the other slide.
*/
å¤šä¸ªå¤„ç†ç¨‹åº
â€‹
æˆ‘ä»¬åœ¨
CallbackManager
ç±»ä¸Šæä¾›äº†ä¸€ç§æ–¹æ³•ï¼Œå…è®¸æ‚¨åˆ›å»ºä¸€ä¸ªä¸´æ—¶å¤„ç†ç¨‹åºã€‚å¦‚æœæ‚¨éœ€è¦åˆ›å»ºä¸€ä¸ªä»…ç”¨äºå•ä¸ªè¯·æ±‚çš„å¤„ç†ç¨‹åºï¼Œè¿™å°†éå¸¸æœ‰ç”¨ï¼Œä¾‹å¦‚æµå¼ä¼ è¾“LLM / Agent /ç­‰çš„è¾“å‡ºåˆ°WebSocketã€‚
This is a more complete example that passes a
CallbackManager
to a ChatModel, and LLMChain, a Tool, and an Agent.
import
{
LLMChain
}
from
"langchain/chains"
;
import
{
AgentExecutor
,
ZeroShotAgent
}
from
"langchain/agents"
;
import
{
BaseCallbackHandler
}
from
"langchain/callbacks"
;
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
Calculator
}
from
"langchain/tools/calculator"
;
import
{
AgentAction
}
from
"langchain/schema"
;
export
const
run
=
async
(
)
=>
{
// You can implement your own callback handler by extending BaseCallbackHandler
class
CustomHandler
extends
BaseCallbackHandler
{
name
=
"custom_handler"
;
handleLLMNewToken
(
token
:
string
)
{
console
.
log
(
"token"
,
{
token
}
)
;
}
handleLLMStart
(
llm
:
{
name
:
string
}
,
_prompts
:
string
[
]
)
{
console
.
log
(
"handleLLMStart"
,
{
llm
}
)
;
}
handleChainStart
(
chain
:
{
name
:
string
}
)
{
console
.
log
(
"handleChainStart"
,
{
chain
}
)
;
}
handleAgentAction
(
action
:
AgentAction
)
{
console
.
log
(
"handleAgentAction"
,
action
)
;
}
handleToolStart
(
tool
:
{
name
:
string
}
)
{
console
.
log
(
"handleToolStart"
,
{
tool
}
)
;
}
}
const
handler1
=
new
CustomHandler
(
)
;
// Additionally, you can use the `fromMethods` method to create a callback handler
const
handler2
=
BaseCallbackHandler
.
fromMethods
(
{
handleLLMStart
(
llm
,
_prompts
:
string
[
]
)
{
console
.
log
(
"handleLLMStart: I'm the second handler!!"
,
{
llm
}
)
;
}
,
handleChainStart
(
chain
)
{
console
.
log
(
"handleChainStart: I'm the second handler!!"
,
{
chain
}
)
;
}
,
handleAgentAction
(
action
)
{
console
.
log
(
"handleAgentAction"
,
action
)
;
}
,
handleToolStart
(
tool
)
{
console
.
log
(
"handleToolStart"
,
{
tool
}
)
;
}
,
}
)
;
// You can restrict callbacks to a particular object by passing it upon creation
const
model
=
new
ChatOpenAI
(
{
temperature
:
0
,
callbacks
:
[
handler2
]
,
// this will issue handler2 callbacks related to this model
streaming
:
true
,
// needed to enable streaming, which enables handleLLMNewToken
}
)
;
const
tools
=
[
new
Calculator
(
)
]
;
const
agentPrompt
=
ZeroShotAgent
.
createPrompt
(
tools
)
;
const
llmChain
=
new
LLMChain
(
{
llm
:
model
,
prompt
:
agentPrompt
,
callbacks
:
[
handler2
]
,
// this will issue handler2 callbacks related to this chain
}
)
;
const
agent
=
new
ZeroShotAgent
(
{
llmChain
,
allowedTools
:
[
"search"
]
,
}
)
;
const
agentExecutor
=
AgentExecutor
.
fromAgentAndTools
(
{
agent
,
tools
,
}
)
;
/*
* When we pass the callback handler to the agent executor, it will be used for all
* callbacks related to the agent and all the objects involved in the agent's
* execution, in this case, the Tool, LLMChain, and LLM.
*
* The `handler2` callback handler will only be used for callbacks related to the
* LLMChain and LLM, since we passed it to the LLMChain and LLM objects upon creation.
*/
const
result
=
await
agentExecutor
.
call
(
{
input
:
"What is 2 to the power of 8"
,
}
,
[
handler1
]
)
;
// this is needed to see handleAgentAction
/*
handleChainStart { chain: { name: 'agent_executor' } }
handleChainStart { chain: { name: 'llm_chain' } }
handleChainStart: I'm the second handler!! { chain: { name: 'llm_chain' } }
handleLLMStart { llm: { name: 'openai' } }
handleLLMStart: I'm the second handler!! { llm: { name: 'openai' } }
token { token: '' }
token { token: 'I' }
token { token: ' can' }
token { token: ' use' }
token { token: ' the' }
token { token: ' calculator' }
token { token: ' tool' }
token { token: ' to' }
token { token: ' solve' }
token { token: ' this' }
token { token: '.\n' }
token { token: 'Action' }
token { token: ':' }
token { token: ' calculator' }
token { token: '\n' }
token { token: 'Action' }
token { token: ' Input' }
token { token: ':' }
token { token: ' ' }
token { token: '2' }
token { token: '^' }
token { token: '8' }
token { token: '' }
handleAgentAction {
tool: 'calculator',
toolInput: '2^8',
log: 'I can use the calculator tool to solve this.\n' +
'Action: calculator\n' +
'Action Input: 2^8'
}
handleToolStart { tool: { name: 'calculator' } }
handleChainStart { chain: { name: 'llm_chain' } }
handleChainStart: I'm the second handler!! { chain: { name: 'llm_chain' } }
handleLLMStart { llm: { name: 'openai' } }
handleLLMStart: I'm the second handler!! { llm: { name: 'openai' } }
token { token: '' }
token { token: 'That' }
token { token: ' was' }
token { token: ' easy' }
token { token: '!\n' }
token { token: 'Final' }
token { token: ' Answer' }
token { token: ':' }
token { token: ' ' }
token { token: '256' }
token { token: '' }
*/
console
.
log
(
result
)
;
/*
{
output: '256',
__run: { runId: '26d481a6-4410-4f39-b74d-f9a4f572379a' }
}
*/
}
;



--- æ–‡ä»¶: output_20250622_020018\docs\production\deployment.md ---
---
url: https://js.langchain.com.cn/docs/production/deployment
crawled_at: 2025-06-22T02:00:26.161410
---

deployment
éƒ¨ç½²
â€‹
æ‚¨å·²ç»æ„å»ºå¥½äº† LangChain åº”ç”¨ç¨‹åºï¼Œå¹¶ä¸”ç°åœ¨æƒ³å°†å…¶éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒï¼Ÿæ‚¨æ¥å¯¹åœ°æ–¹äº†ã€‚æœ¬æŒ‡å—å°†ä¸ºæ‚¨ä»‹ç»éƒ¨ç½²åº”ç”¨ç¨‹åºçš„é€‰é¡¹ä»¥åŠè¿›è¡Œéƒ¨ç½²æ—¶åº”è€ƒè™‘çš„é—®é¢˜ã€‚
æ¦‚è¿°
â€‹
LangChain æ˜¯ç”¨äºæ„å»ºä½¿ç”¨è¯­è¨€æ¨¡å‹çš„åº”ç”¨ç¨‹åºçš„åº“ã€‚å®ƒä¸æ˜¯ Web æ¡†æ¶ï¼Œå¹¶ä¸”ä¸æä¾›ä»»ä½•ç”¨äºé€šè¿‡ Web æä¾›æœåŠ¡çš„å†…ç½®åŠŸèƒ½ã€‚ç›¸åï¼Œå®ƒæä¾›äº†ä¸€ç»„å·¥å…·ï¼Œæ‚¨å¯ä»¥å°†å…¶é›†æˆåœ¨ API æˆ–åç«¯æœåŠ¡å™¨ä¸­ã€‚
éƒ¨ç½²åº”ç”¨ç¨‹åºæœ‰å‡ ä¸ªé«˜çº§é€‰é¡¹:
éƒ¨ç½²åˆ°è™šæ‹Ÿæœºæˆ–å®¹å™¨ä¸­
æŒä¹…åŒ–æ–‡ä»¶ç³»ç»Ÿæ„å‘³ç€ä½ å¯ä»¥ä»ç£ç›˜ä¸­ä¿å­˜å’ŒåŠ è½½æ–‡ä»¶
æ°¸ä¹…è¿è¡Œçš„è¿›ç¨‹æ„å‘³ç€ä½ å¯ä»¥åœ¨å†…å­˜ä¸­ç¼“å­˜ä¸€äº›ä¸œè¥¿
ä½ å¯ä»¥æ”¯æŒé•¿æ—¶é—´è¿è¡Œçš„è¯·æ±‚ï¼Œä¾‹å¦‚WebSockets
éƒ¨ç½²åˆ°æ— æœåŠ¡å™¨ç¯å¢ƒ
æ²¡æœ‰æŒä¹…åŒ–æ–‡ä»¶ç³»ç»Ÿæ„å‘³ç€ä½ å¯ä»¥ä»ç£ç›˜ä¸­åŠ è½½æ–‡ä»¶ï¼Œä½†æ˜¯ä¸èƒ½å°†å®ƒä»¬ä¿å­˜ä»¥å¤‡åç”¨ã€‚
å†·å¯åŠ¨æ„å‘³ç€ä½ ä¸èƒ½åœ¨å†…å­˜ä¸­ç¼“å­˜ä¸œè¥¿ï¼Œå¹¶æœŸæœ›åœ¨è¯·æ±‚ä¹‹é—´è¢«ç¼“å­˜ã€‚
å‡½æ•°è¶…æ—¶æ„å‘³ç€ä½ ä¸èƒ½æ”¯æŒé•¿æ—¶é—´è¿è¡Œçš„è¯·æ±‚ï¼Œä¾‹å¦‚WebSocketsã€‚
å…¶ä»–ä¸€äº›è€ƒè™‘äº‹é¡¹åŒ…æ‹¬:
æ‚¨å°†åç«¯å’Œå‰ç«¯ä¸€èµ·éƒ¨ç½²è¿˜æ˜¯åˆ†åˆ«éƒ¨ç½²ï¼Ÿ
æ‚¨å°†åç«¯ä¸æ•°æ®åº“ååŒéƒ¨ç½²è¿˜æ˜¯åˆ†åˆ«éƒ¨ç½²ï¼Ÿ
éšç€æ‚¨å°† LangChain åº”ç”¨ç¨‹åºéƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒï¼Œæˆ‘ä»¬å°†éå¸¸ä¹æ„æä¾›æ›´å…¨é¢çš„æ”¯æŒã€‚è¯·å¡«å†™
æ­¤è¡¨æ ¼
ï¼Œæˆ‘ä»¬å°†è®¾ç½®ä¸€ä¸ªä¸“é—¨çš„æ”¯æŒ Slack é¢‘é“ã€‚
éƒ¨ç½²é€‰é¡¹
â€‹
è¯·å‚é˜…ä»¥ä¸‹æœ‰å…³ LangChain åº”ç”¨ç¨‹åºéƒ¨ç½²é€‰é¡¹çš„åˆ—è¡¨ã€‚å¦‚æœæ‚¨æ²¡æœ‰çœ‹åˆ°æ‚¨é¦–é€‰çš„é€‰é¡¹ï¼Œè¯·è”ç³»æˆ‘ä»¬ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶æ·»åŠ åˆ°æ­¤åˆ—è¡¨ä¸­ã€‚
éƒ¨ç½²åˆ° Fly.io
â€‹
Fly.io
æ˜¯å°†åº”ç”¨ç¨‹åºéƒ¨ç½²åˆ°äº‘ç«¯çš„å¹³å°ã€‚è¿™æ˜¯å°†æ‚¨çš„åº”ç”¨ç¨‹åºéƒ¨ç½²åˆ°å®¹å™¨ç¯å¢ƒçš„ä¸é”™é€‰æ‹©ã€‚
è¯·å‚é˜…
æˆ‘ä»¬çš„ Fly.io æ¨¡æ¿
ï¼Œå…¶ä¸­åŒ…å«äº†å°†åº”ç”¨ç¨‹åºéƒ¨ç½²åˆ° Fly.io çš„ç¤ºä¾‹ã€‚
éƒ¨ç½²åˆ° Kinsta
â€‹
Kinsta
æ˜¯ä¸€ä¸ªä»¥å¼€å‘äººå‘˜ä¸ºä¸­å¿ƒçš„äº‘ä¸»æœºå¹³å°ã€‚
ä½¿ç”¨
æˆ‘ä»¬çš„hello-worldæ¨¡æ¿
ï¼Œäº†è§£å¦‚ä½•åœ¨Kinstaä¸Šåœ¨å‡ åˆ†é’Ÿå†…éƒ¨ç½²ä½ çš„ä¸‹ä¸€ä¸ªLangChainåº”ç”¨ç¨‹åºçš„ç¤ºä¾‹ã€‚



--- æ–‡ä»¶: output_20250622_020018\docs\production\tracing.md ---
---
url: https://js.langchain.com.cn/docs/production/tracing
crawled_at: 2025-06-22T02:00:26.326100
---

è¿½è¸ª
ä¸ Python çš„
langchain
åŒ…ç±»ä¼¼ï¼ŒJS çš„
langchain
ä¹Ÿæ”¯æŒè¿½è¸ªã€‚
æ‚¨å¯ä»¥åœ¨
è¿™é‡Œ
æŸ¥çœ‹è¿½è¸ªçš„æ¦‚è¿°ã€‚
è¦å¯åŠ¨è¿½è¸ªåç«¯ï¼Œè¯·åœ¨
langchain
ç›®å½•ä¸‹è¿è¡Œ
docker compose up
(å¦‚æœä½¿ç”¨è¾ƒæ—§ç‰ˆæœ¬çš„
docker
ï¼Œåˆ™ä½¿ç”¨
docker-compose up
)ã€‚
å¦‚æœæ‚¨å·²å®‰è£… Python çš„
langchain
åŒ…ï¼Œè¿˜å¯ä»¥ä½¿ç”¨
langchain-server
å‘½ä»¤ã€‚
ä»¥ä¸‹æ˜¯å¦‚ä½•åœ¨
langchain.js
ä¸­ä½¿ç”¨è¿½è¸ªçš„ç¤ºä¾‹ã€‚å”¯ä¸€éœ€è¦åšçš„å°±æ˜¯å°†
LANGCHAIN_TRACING
ç¯å¢ƒå˜é‡è®¾ç½®ä¸º
true
ã€‚
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
initializeAgentExecutorWithOptions
}
from
"langchain/agents"
;
import
{
SerpAPI
}
from
"langchain/tools"
;
import
{
Calculator
}
from
"langchain/tools/calculator"
;
import
process
from
"process"
;
export
const
run
=
async
(
)
=>
{
process
.
env
.
LANGCHAIN_TRACING
=
"true"
;
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
tools
=
[
new
SerpAPI
(
process
.
env
.
SERPAPI_API_KEY
,
{
location
:
"Austin,Texas,United States"
,
hl
:
"en"
,
gl
:
"us"
,
}
)
,
new
Calculator
(
)
,
]
;
const
executor
=
await
initializeAgentExecutorWithOptions
(
tools
,
model
,
{
agentType
:
"zero-shot-react-description"
,
verbose
:
true
,
}
)
;
console
.
log
(
"Loaded agent."
)
;
const
input
=
`
Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?
`
;
console
.
log
(
`
Executing with input "
${
input
}
"...
`
)
;
const
result
=
await
executor
.
call
(
{
input
}
)
;
console
.
log
(
`
Got output
${
result
.
output
}
`
)
;
}
;
å¹¶å‘
â€‹
è¿½è¸ªé»˜è®¤æ”¯æŒå¹¶å‘å¤„ç†ã€‚
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
initializeAgentExecutorWithOptions
}
from
"langchain/agents"
;
import
{
SerpAPI
}
from
"langchain/tools"
;
import
{
Calculator
}
from
"langchain/tools/calculator"
;
import
process
from
"process"
;
export
const
run
=
async
(
)
=>
{
process
.
env
.
LANGCHAIN_TRACING
=
"true"
;
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
tools
=
[
new
SerpAPI
(
process
.
env
.
SERPAPI_API_KEY
,
{
location
:
"Austin,Texas,United States"
,
hl
:
"en"
,
gl
:
"us"
,
}
)
,
new
Calculator
(
)
,
]
;
const
executor
=
await
initializeAgentExecutorWithOptions
(
tools
,
model
,
{
agentType
:
"zero-shot-react-description"
,
verbose
:
true
,
}
)
;
console
.
log
(
"Loaded agent."
)
;
const
input
=
`
Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?
`
;
console
.
log
(
`
Executing with input "
${
input
}
"...
`
)
;
// This will result in a lot of errors, because the shared Tracer is not concurrency-safe.
const
[
resultA
,
resultB
,
resultC
]
=
await
Promise
.
all
(
[
executor
.
call
(
{
input
}
)
,
executor
.
call
(
{
input
}
)
,
executor
.
call
(
{
input
}
)
,
]
)
;
console
.
log
(
`
Got output
${
resultA
.
output
}
${
resultA
.
__run
.
runId
}
`
)
;
console
.
log
(
`
Got output
${
resultB
.
output
}
${
resultB
.
__run
.
runId
}
`
)
;
console
.
log
(
`
Got output
${
resultC
.
output
}
${
resultC
.
__run
.
runId
}
`
)
;
/*
Got output Harry Styles, Olivia Wilde's boyfriend, is 29 years old and his age raised to the 0.23 power is 2.169459462491557. b8fb98aa-07a5-45bd-b593-e8d7376b05ca
Got output Harry Styles, Olivia Wilde's boyfriend, is 29 years old and his age raised to the 0.23 power is 2.169459462491557. c8d916d5-ca1d-4702-8dd7-cab5e438578b
Got output Harry Styles, Olivia Wilde's boyfriend, is 29 years old and his age raised to the 0.23 power is 2.169459462491557. bf5fe04f-ef29-4e55-8ce1-e4aa974f9484
*/
}
;



--- æ–‡ä»¶: output_20250622_020018\docs\use_cases\agent_simulations.md ---
---
url: https://js.langchain.com.cn/docs/use_cases/agent_simulations/
crawled_at: 2025-06-22T02:00:26.248832
---

ä»£ç†äººæ¨¡æ‹Ÿ
ä»£ç†äººæ¨¡æ‹Ÿæ¶‰åŠåˆ°å°†å¤šä¸ªä»£ç†äººä¸å½¼æ­¤äº’åŠ¨ã€‚
å®ƒä»¬å¾€å¾€ä½¿ç”¨æ¨¡æ‹Ÿç¯å¢ƒï¼Œå…¶ä¸­LLMæ˜¯å®ƒä»¬çš„â€œæ ¸å¿ƒâ€ï¼Œè€Œè¾…åŠ©ç±»åˆ™ç”¨äºæç¤ºå®ƒä»¬å¸æ”¶æŸäº›è¾“å…¥ï¼Œä¾‹å¦‚é¢„å…ˆæ„å»ºçš„â€œè§‚å¯Ÿâ€ï¼Œå¹¶å¯¹æ–°çš„åˆºæ¿€ä½œå‡ºååº”ã€‚
å®ƒä»¬è¿˜ä»é•¿æœŸè®°å¿†ä¸­å—ç›Šï¼Œä»¥ä¾¿å®ƒä»¬å¯ä»¥åœ¨äº’åŠ¨ä¹‹é—´ä¿æŒçŠ¶æ€ã€‚
ä¸è‡ªä¸»ä»£ç†äººä¸€æ ·ï¼Œä»£ç†äººæ¨¡æ‹Ÿä»å¤„äºå®éªŒé˜¶æ®µï¼ŒåŸºäºè®ºæ–‡çš„åŸºç¡€ï¼Œä¾‹å¦‚
è¿™ç¯‡
ã€‚
ğŸ“„ï¸
ç”Ÿæˆå¼æ™ºèƒ½ä½“
è¯¥è„šæœ¬å®ç°äº†ä¸€ç§åŸºäºè®ºæ–‡Generating Agents  äº¤äº’å¼ä»¿çœŸäººç±»è¡Œä¸ºçš„ç”Ÿæˆå¼æ™ºèƒ½ä½“, ä½œè€…ä¸ºPark et.al.



--- æ–‡ä»¶: output_20250622_020018\docs\use_cases\api.md ---
---
url: https://js.langchain.com.cn/docs/use_cases/api
crawled_at: 2025-06-22T02:00:26.332721
---

ä¸APIäº¤äº’
info
æ¦‚å¿µæŒ‡å—
è®¸å¤šæ•°æ®å’Œä¿¡æ¯å­˜å‚¨åœ¨APIåé¢ã€‚
æœ¬é¡µæ¶µç›–äº†åœ¨LangChainä¸­ä½¿ç”¨APIçš„æ‰€æœ‰èµ„æºã€‚
é“¾
â€‹
å¦‚æœæ‚¨åˆšå¼€å§‹å¹¶ä¸”æœ‰ç›¸å¯¹ç®€å•çš„APIï¼Œè¯·ä»é“¾å¼€å§‹ã€‚
é“¾æ˜¯ä¸€ç³»åˆ—é¢„å®šæ­¥éª¤ï¼Œå› æ­¤å®ƒä»¬å¾ˆé€‚åˆå…¥é—¨ï¼Œå› ä¸ºå®ƒä»¬èƒ½å¤Ÿç»™æ‚¨æ›´å¤šæ§åˆ¶ï¼Œè®©æ‚¨
æ›´å¥½åœ°äº†è§£å‘ç”Ÿçš„æƒ…å†µã€‚
APIé“¾
ä»£ç†
â€‹
ä»£ç†æ›´å¤æ‚ï¼Œå¹¶æ¶‰åŠå¯¹LLMçš„å¤šä¸ªæŸ¥è¯¢ï¼Œä»¥äº†è§£è¦åšä»€ä¹ˆã€‚
ä»£ç†çš„ç¼ºç‚¹æ˜¯æ‚¨çš„æ§åˆ¶åŠ›å°‘äº†ã€‚ä¼˜ç‚¹æ˜¯å®ƒä»¬æ›´å¼ºå¤§
è¿™ä½¿æ‚¨å¯ä»¥åœ¨æ›´å¤§ï¼Œæ›´å¤æ‚çš„æ¨¡å¼ä¸Šä½¿ç”¨å®ƒä»¬ã€‚
OpenAPI Agent



--- æ–‡ä»¶: output_20250622_020018\docs\use_cases\autonomous_agents.md ---
---
url: https://js.langchain.com.cn/docs/use_cases/autonomous_agents/
crawled_at: 2025-06-22T02:00:26.345044
---

è‡ªä¸»ä»£ç†
è‡ªä¸»ä»£ç†æ˜¯è®¾è®¡æˆæ›´é•¿æ—¶é—´è¿è¡Œçš„ä»£ç†ã€‚æ‚¨å¯ä»¥ç»™å®ƒä»¬ä¸€ä¸ªæˆ–å¤šä¸ªé•¿æœŸç›®æ ‡ï¼Œå®ƒä»¬ä¼šç‹¬ç«‹æ‰§è¡Œè¿™äº›ç›®æ ‡ã€‚è¿™äº›åº”ç”¨ç¨‹åºç»“åˆäº†å·¥å…·ä½¿ç”¨å’Œé•¿æœŸè®°å¿†ã€‚
ç›®å‰è‡ªä¸»ä»£ç†è¿˜ç›¸å½“å®éªŒæ€§ï¼ŒåŸºäºå…¶ä»–å¼€æºé¡¹ç›®ã€‚é€šè¿‡åœ¨ LangChain åŸºå…ƒä¸­å®ç°è¿™äº›å¼€æºé¡¹ç›®ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ° LangChain çš„å¥½å¤„ï¼šè½»æ¾åˆ‡æ¢å’Œå°è¯•å¤šä¸ª LLMsï¼Œä½¿ç”¨ä¸åŒçš„å‘é‡å­˜å‚¨ä½œä¸ºå†…å­˜ï¼Œä½¿ç”¨ LangChain çš„å·¥å…·é›†ã€‚
ğŸ“„ï¸
AutoGPT
AutoGPTæ˜¯ä¸€ä¸ªä½¿ç”¨é•¿æœŸè®°å¿†å’Œä¸“ä¸ºç‹¬ç«‹å·¥ä½œè®¾è®¡çš„æç¤ºï¼ˆå³æ— éœ€è¦æ±‚ç”¨æˆ·è¾“å…¥)çš„è‡ªå®šä¹‰ä»£ç†æ¥æ‰§è¡Œä»»åŠ¡ã€‚
ğŸ“„ï¸
BabyAGI
åŸå§‹GitHubé“¾æ¥//github.com/yoheinakajima/babyagi



--- æ–‡ä»¶: output_20250622_020018\docs\use_cases\personal_assistants.md ---
---
url: https://js.langchain.com.cn/docs/use_cases/personal_assistants
crawled_at: 2025-06-22T02:00:26.432404
---

ä¸ªäººåŠ©ç†
info
æ¦‚å¿µæŒ‡å—
æˆ‘ä»¬åœ¨è¿™é‡Œéå¸¸å¹¿ä¹‰åœ°ä½¿ç”¨â€œä¸ªäººåŠ©ç†â€è¿™ä¸ªè¯ã€‚
ä¸ªäººåŠ©ç†å…·æœ‰ä¸€äº›ç‰¹å¾:
å®ƒä»¬å¯ä»¥ä¸å¤–ç•Œäº’åŠ¨
å®ƒä»¬å…·æœ‰æ‚¨çš„æ•°æ®çŸ¥è¯†
å®ƒä»¬è®°å¾—æ‚¨çš„äº’åŠ¨
å®é™…ä¸Šï¼ŒLangChain ä¸­çš„æ‰€æœ‰åŠŸèƒ½éƒ½ä¸æ„å»ºä¸ªäººåŠ©ç†ç›¸å…³ã€‚
Highlighting specific parts:
Agent æ–‡æ¡£
ï¼ˆç”¨äºä¸å¤–ç•Œäº’åŠ¨)
Index æ–‡æ¡£
ï¼ˆç”¨äºæä¾›æ•°æ®çŸ¥è¯†)
Memory
(for helping them remember interactions)



--- æ–‡ä»¶: output_20250622_020018\docs\use_cases\question_answering.md ---
---
url: https://js.langchain.com.cn/docs/use_cases/question_answering
crawled_at: 2025-06-22T02:00:26.473784
---

é—®ç­”
info
æ¦‚å¿µæŒ‡å—
åœ¨è¿™ä¸ªèƒŒæ™¯ä¸‹çš„é—®ç­”æŒ‡çš„æ˜¯é’ˆå¯¹æ–‡æ¡£æ•°æ®çš„é—®ç­”ã€‚
æœ‰å‡ ç§ä¸åŒç±»å‹çš„é—®ç­”:
æ£€ç´¢å¼é—®ç­”
: åˆ©ç”¨è¿™ç§æ–¹å¼ï¼Œå¯ä»¥å°†æ–‡æ¡£å¯¼å…¥ã€ç´¢å¼•åˆ°å‘é‡å­˜å‚¨åº“ä¸­ï¼Œç„¶åèƒ½å¤Ÿå¯¹å®ƒä»¬è¿›è¡Œæé—®ã€‚
äº¤äº’æ£€ç´¢
: ç±»ä¼¼äºä¸Šè¿°æ–¹å¼ï¼Œå¯¼å…¥å¹¶ç´¢å¼•æ–‡ä»¶ï¼Œä½†æ˜¯å¯ä»¥è¿›è¡Œäº¤è°ˆï¼ˆæå‡ºåç»­é—®é¢˜ç­‰)è€Œä¸ä»…ä»…æ˜¯ä¸€ä¸ªé—®é¢˜ã€‚
ç´¢å¼•
â€‹
å¯¹äºè®¸å¤šæ–‡æ¡£çš„é—®ç­”ï¼Œæ‚¨å‡ ä¹æ€»æ˜¯å¸Œæœ›å¯¹æ•°æ®åˆ›å»ºç´¢å¼•ã€‚
è¿™å¯ç”¨äºæ™ºèƒ½è®¿é—®ç»™å®šé—®é¢˜çš„æœ€ç›¸å…³æ–‡æ¡£ï¼Œä»è€Œå¯ä»¥é¿å…å°†æ‰€æœ‰æ–‡æ¡£ä¼ é€’ç»™LLMï¼ˆèŠ‚çœæ—¶é—´å’Œé‡‘é’±)ã€‚
å› æ­¤ï¼Œäº†è§£å¦‚ä½•åˆ›å»ºç´¢å¼•éå¸¸é‡è¦ï¼Œå› æ­¤æ‚¨åº”è¯¥ç†Ÿæ‚‰ä¸æ­¤ç›¸å…³çš„æ‰€æœ‰æ–‡æ¡£ã€‚
ç´¢å¼•
é“¾
â€‹
åˆ›å»ºç´¢å¼•åï¼Œå¯ä»¥åœ¨é“¾ä¸­ä½¿ç”¨å®ƒã€‚
æ‚¨å¯ä»¥æ­£å¸¸åœ°å¯¹å…¶è¿›è¡Œé—®ç­”ï¼Œä¹Ÿå¯ä»¥ä»¥äº¤äº’æ–¹å¼ä½¿ç”¨å®ƒã€‚
æœ‰å…³è¿™äº›é“¾ï¼ˆä»¥åŠæ›´å¤šå†…å®¹)çš„æ¦‚è¿°ï¼Œè¯·å‚é˜…ä¸‹é¢çš„æ–‡æ¡£ã€‚
ä¸ç´¢å¼•ç›¸å…³çš„é“¾
ä»£ç†
â€‹
å¦‚æœä½ æƒ³å›ç­”æ›´å¤æ‚çš„å¤šè·³é—®é¢˜ï¼Œä½ åº”è¯¥è€ƒè™‘å°†ä½ çš„ç´¢å¼•ä¸ä¸€ä¸ªAgentç»„åˆä½¿ç”¨ã€‚#multi-hopæŒ‡å¤šè·³é—®é¢˜ï¼Œ#indexesæŒ‡ç´¢å¼•ï¼Œ#agentæŒ‡ä»£ç†ã€‚
æœ‰å…³å¦‚ä½•æ“ä½œçš„ç¤ºä¾‹ï¼Œè¯·å‚è§ä»¥ä¸‹å†…å®¹ã€‚#exampleæŒ‡ç¤ºä¾‹ã€‚
Vectorstore Agent



--- æ–‡ä»¶: output_20250622_020018\docs\use_cases\summarization.md ---
---
url: https://js.langchain.com.cn/docs/use_cases/summarization
crawled_at: 2025-06-22T02:00:26.534316
---

æ€»ç»“
info
æ¦‚å¿µæŒ‡å—
ä¸€ä¸ªå¸¸è§çš„ç”¨ä¾‹æ˜¯æƒ³è¦æ€»ç»“é•¿æ–‡æ¡£ã€‚
è¿™è‡ªç„¶ä¼šé‡åˆ°ä¸Šä¸‹æ–‡çª—å£çš„é™åˆ¶ã€‚
ä¸é—®ç­”ä¸åŒï¼Œæ‚¨ä¸èƒ½åªåšäº›è¯­ä¹‰æœç´¢æŠ€å·§æ¥ä»…é€‰æ‹©ä¸é—®é¢˜æœ€ç›¸å…³çš„æ–‡æœ¬éƒ¨åˆ†(å› ä¸ºåœ¨è¿™ç§æƒ…å†µä¸‹æ²¡æœ‰ç‰¹å®šçš„é—®é¢˜
æ‚¨æƒ³æ€»ç»“æ¯ä¸ªä¸œè¥¿
ä¸ºäº†å¼€å§‹ï¼Œæˆ‘ä»¬å»ºè®®æŸ¥çœ‹æ±‡æ€»é“¾ï¼Œè¯¥é“¾ä»¥é€’å½’æ–¹å¼è§£å†³æ­¤é—®é¢˜ã€‚
Summarization Chain



--- æ–‡ä»¶: output_20250622_020018\docs\use_cases\tabular.md ---
---
url: https://js.langchain.com.cn/docs/use_cases/tabular
crawled_at: 2025-06-22T02:00:26.587981
---

è¡¨æ ¼é—®ç­”
info
æ¦‚å¿µæŒ‡å—
å¤§é‡çš„æ•°æ®å’Œä¿¡æ¯å­˜å‚¨åœ¨è¡¨æ ¼æ•°æ®ä¸­ï¼Œä¾‹å¦‚csvã€excelè¡¨æ ¼æˆ–SQLè¡¨æ ¼ã€‚
æœ¬é¡µé¢ä»‹ç»LangChainæä¾›çš„ä¸è¿™ç§æ ¼å¼æ•°æ®å¤„ç†æœ‰å…³çš„æ‰€æœ‰èµ„æºã€‚
é“¾
â€‹
å¦‚æœæ‚¨åˆšå¼€å§‹å¹¶ä¸”æœ‰ç›¸å¯¹è¾ƒå°/ç®€å•çš„è¡¨æ ¼æ•°æ®ï¼Œå»ºè®®æ‚¨ä½¿ç”¨é“¾ã€‚
é“¾æ˜¯ä¸€ç³»åˆ—é¢„å®šæ­¥éª¤ï¼Œå› æ­¤å®ƒä»¬å¾ˆé€‚åˆåˆå­¦è€…ï¼Œå› ä¸ºå®ƒä»¬ç»™æ‚¨æ›´å¤šçš„æ§åˆ¶ï¼Œå¹¶è®©æ‚¨æ›´å¥½åœ°ç†è§£æ­£åœ¨å‘ç”Ÿçš„äº‹æƒ…ã€‚
SQLæ•°æ®åº“é“¾
ä»£ç†
â€‹
ä»£ç†æ›´åŠ å¤æ‚ï¼Œå¹¶æ¶‰åŠå¤šä¸ªæŸ¥è¯¢åˆ°LLMä»¥ç†è§£è¦åšä»€ä¹ˆã€‚ä»£ç†çš„ç¼ºç‚¹æ˜¯æ‚¨çš„æ§åˆ¶åŠ›è¾ƒå°ã€‚ä¼˜ç‚¹æ˜¯å®ƒä»¬æ›´å¼ºå¤§ï¼Œå¯ä»¥åœ¨æ›´å¤§çš„æ•°æ®åº“å’Œæ›´å¤æ‚çš„æ¨¡å¼ä¸Šä½¿ç”¨ã€‚
ä»£ç†äººçš„ç¼ºç‚¹æ˜¯ä½ çš„æ§åˆ¶åŠ›ä¼šå˜å¼±ï¼Œä½†å¥½å¤„åœ¨äºå®ƒä»¬æ›´åŠ å¼ºå¤§ï¼Œè¿™ä½¿å¾—ä½ å¯ä»¥åœ¨æ›´å¤§çš„æ•°æ®åº“å’Œæ›´å¤æ‚çš„æ¨¡å¼ä¸Šä½¿ç”¨å®ƒä»¬ã€‚
SQL Agent



--- æ–‡ä»¶: output_20250622_020018\docs\modules\agents\additional_functionality.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/additional_functionality
crawled_at: 2025-06-22T02:00:19.942276
---

Agentsçš„å…¶ä»–åŠŸèƒ½
æˆ‘ä»¬ä¸ºAgentsæä¾›äº†è®¸å¤šå…¶ä»–åŠŸèƒ½ã€‚æ‚¨è¿˜åº”æŸ¥çœ‹
LLM-specific features
å’Œ
Chat Model-specific features
ã€‚
æ·»åŠ è¶…æ—¶
â€‹
é»˜è®¤æƒ…å†µä¸‹ï¼ŒLangChainå°†æ— é™æœŸç­‰å¾…æ¨¡å‹æä¾›è€…çš„å“åº”ã€‚å¦‚æœæ‚¨æƒ³æ·»åŠ è¶…æ—¶ï¼Œå¯ä»¥åœ¨è¿è¡Œä»£ç†æ—¶ä¼ é€’ä»¥æ¯«ç§’ä¸ºå•ä½çš„â€œtimeoutâ€é€‰é¡¹ã€‚ä¾‹å¦‚
import
{
initializeAgentExecutorWithOptions
}
from
"langchain/agents"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
SerpAPI
}
from
"langchain/tools"
;
import
{
Calculator
}
from
"langchain/tools/calculator"
;
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
tools
=
[
new
SerpAPI
(
process
.
env
.
SERPAPI_API_KEY
,
{
location
:
"Austin,Texas,United States"
,
hl
:
"en"
,
gl
:
"us"
,
}
)
,
new
Calculator
(
)
,
]
;
const
executor
=
await
initializeAgentExecutorWithOptions
(
tools
,
model
,
{
agentType
:
"zero-shot-react-description"
,
}
)
;
try
{
const
input
=
`
Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?
`
;
const
result
=
await
executor
.
call
(
{
input
,
timeout
:
2000
}
)
;
// 2 seconds
}
catch
(
e
)
{
console
.
log
(
e
)
;
/*
Error: Cancel: canceled
at file:///Users/nuno/dev/langchainjs/langchain/dist/util/async_caller.js:60:23
at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
at RetryOperation._fn (/Users/nuno/dev/langchainjs/node_modules/p-retry/index.js:50:12) {
attemptNumber: 1,
retriesLeft: 6
}
*/
}
å–æ¶ˆè¯·æ±‚
â€‹
æ‚¨å¯ä»¥é€šè¿‡åœ¨è¿è¡Œä»£ç†æ—¶ä¼ é€’â€œsignalâ€é€‰é¡¹æ¥å–æ¶ˆè¯·æ±‚ã€‚ä¾‹å¦‚
import
{
initializeAgentExecutorWithOptions
}
from
"langchain/agents"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
SerpAPI
}
from
"langchain/tools"
;
import
{
Calculator
}
from
"langchain/tools/calculator"
;
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
tools
=
[
new
SerpAPI
(
process
.
env
.
SERPAPI_API_KEY
,
{
location
:
"Austin,Texas,United States"
,
hl
:
"en"
,
gl
:
"us"
,
}
)
,
new
Calculator
(
)
,
]
;
const
executor
=
await
initializeAgentExecutorWithOptions
(
tools
,
model
,
{
agentType
:
"zero-shot-react-description"
,
}
)
;
const
controller
=
new
AbortController
(
)
;
// Call `controller.abort()` somewhere to cancel the request.
setTimeout
(
(
)
=>
{
controller
.
abort
(
)
;
}
,
2000
)
;
try
{
const
input
=
`
Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?
`
;
const
result
=
await
executor
.
call
(
{
input
,
signal
:
controller
.
signal
}
)
;
}
catch
(
e
)
{
console
.
log
(
e
)
;
/*
Error: Cancel: canceled
at file:///Users/nuno/dev/langchainjs/langchain/dist/util/async_caller.js:60:23
at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
at RetryOperation._fn (/Users/nuno/dev/langchainjs/node_modules/p-retry/index.js:50:12) {
attemptNumber: 1,
retriesLeft: 6
}
*/
}
æ³¨æ„ï¼šå¦‚æœåº•å±‚æä¾›ç¨‹åºå…¬å¼€è¯¥é€‰é¡¹ï¼Œè¿™å°†ä»…å–æ¶ˆä¼ å‡ºè¯·æ±‚ã€‚å¦‚æœå¯èƒ½ï¼ŒLangChainå°†å–æ¶ˆåº•å±‚è¯·æ±‚ï¼Œå¦åˆ™å®ƒå°†å–æ¶ˆå“åº”çš„å¤„ç†ã€‚
è®¢é˜…äº‹ä»¶
â€‹
æ‚¨å¯ä»¥è®¢é˜…ä»£ç†å’ŒåŸºç¡€å·¥å…·é“¾å’Œæ¨¡å‹å‘å‡ºçš„è®¸å¤šäº‹ä»¶ã€‚
æœ‰å…³å¯ç”¨äº‹ä»¶çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§æ–‡æ¡£ä¸­çš„
Callbacks
éƒ¨åˆ†ã€‚
import
{
initializeAgentExecutorWithOptions
}
from
"langchain/agents"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
SerpAPI
}
from
"langchain/tools"
;
import
{
Calculator
}
from
"langchain/tools/calculator"
;
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
tools
=
[
new
SerpAPI
(
process
.
env
.
SERPAPI_API_KEY
,
{
location
:
"Austin,Texas,United States"
,
hl
:
"en"
,
gl
:
"us"
,
}
)
,
new
Calculator
(
)
,
]
;
const
executor
=
await
initializeAgentExecutorWithOptions
(
tools
,
model
,
{
agentType
:
"zero-shot-react-description"
,
}
)
;
const
input
=
`
Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?
`
;
const
result
=
await
executor
.
run
(
input
,
[
{
handleAgentAction
(
action
,
runId
)
{
console
.
log
(
"\nhandleAgentAction"
,
action
,
runId
)
;
}
,
handleAgentEnd
(
action
,
runId
)
{
console
.
log
(
"\nhandleAgentEnd"
,
action
,
runId
)
;
}
,
handleToolEnd
(
output
,
runId
)
{
console
.
log
(
"\nhandleToolEnd"
,
output
,
runId
)
;
}
,
}
,
]
)
;
/*
handleAgentAction {
tool: 'search',
toolInput: 'Olivia Wilde boyfriend',
log: " I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.\n" +
'Action: search\n' +
'Action Input: "Olivia Wilde boyfriend"'
} 9b978461-1f6f-4d5f-80cf-5b229ce181b6
handleToolEnd In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling. Their relationship ended in November 2022. 062fef47-8ad1-4729-9949-a57be252e002
handleAgentAction {
tool: 'search',
toolInput: 'Harry Styles age',
log: " I need to find out Harry Styles' age.\n" +
'Action: search\n' +
'Action Input: "Harry Styles age"'
} 9b978461-1f6f-4d5f-80cf-5b229ce181b6
handleToolEnd 29 years 9ec91e41-2fbf-4de0-85b6-12b3e6b3784e 61d77e10-c119-435d-a985-1f9d45f0ef08
handleAgentAction {
tool: 'calculator',
toolInput: '29^0.23',
log: ' I need to calculate 29 raised to the 0.23 power.\n' +
'Action: calculator\n' +
'Action Input: 29^0.23'
} 9b978461-1f6f-4d5f-80cf-5b229ce181b6
handleToolEnd 2.169459462491557 07aec96a-ce19-4425-b863-2eae39db8199
handleAgentEnd {
returnValues: {
output: "Harry Styles is Olivia Wilde's boyfriend and his current age raised to the 0.23 power is 2.169459462491557."
},
log: ' I now know the final answer.\n' +
"Final Answer: Harry Styles is Olivia Wilde's boyfriend and his current age raised to the 0.23 power is 2.169459462491557."
} 9b978461-1f6f-4d5f-80cf-5b229ce181b6
*/
console
.
log
(
{
result
}
)
;
// { result: "Harry Styles is Olivia Wilde's boyfriend and his current age raised to the 0.23 power is 2.169459462491557." }
æ—¥å¿—è®°å½•å’Œè·Ÿè¸ª
â€‹
æ‚¨å¯ä»¥åœ¨åˆ›å»ºä»£ç†æ—¶ä¼ é€’â€œverboseâ€æ ‡å¿—ï¼Œä»¥å¯ç”¨å°†æ‰€æœ‰äº‹ä»¶è®°å½•åˆ°æ§åˆ¶å°çš„æ—¥å¿—è®°å½•ã€‚ä¾‹å¦‚
æ‚¨è¿˜å¯ä»¥é€šè¿‡å°†LANGCHAIN_TRACINGç¯å¢ƒå˜é‡è®¾ç½®ä¸ºâ€œtrueâ€æ¥å¯ç”¨
è·Ÿè¸ª
ã€‚
import
{
initializeAgentExecutorWithOptions
}
from
"langchain/agents"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
SerpAPI
}
from
"langchain/tools"
;
import
{
Calculator
}
from
"langchain/tools/calculator"
;
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
tools
=
[
new
SerpAPI
(
process
.
env
.
SERPAPI_API_KEY
,
{
location
:
"Austin,Texas,United States"
,
hl
:
"en"
,
gl
:
"us"
,
}
)
,
new
Calculator
(
)
,
]
;
const
executor
=
await
initializeAgentExecutorWithOptions
(
tools
,
model
,
{
agentType
:
"zero-shot-react-description"
,
verbose
:
true
,
}
)
;
const
input
=
`
Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?
`
;
const
result
=
await
executor
.
call
(
{
input
}
)
;
[chain/start] [1:chain:agent_executor] Entering Chain run with input: {
"input": "Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?"
}
[chain/start] [1:chain:agent_executor > 2:chain:llm_chain] Entering Chain run with input: {
"input": "Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?",
"agent_scratchpad": "",
"stop": [
"Observation: "
]
}
[llm/start] [1:chain:agent_executor > 2:chain:llm_chain > 3:llm:openai] Entering LLM run with input: {
"prompts": [
"Answer the following questions as best you can. You have access to the following tools:search: a search engine. useful for when you need to answer questions about current events. input should be a search query.calculator: Useful for getting the result of a math expression. The input to this tool should be a valid mathematical expression that could be executed by a simple calculator.Use the following format in your response:Question: the input question you must answerThought: you should always think about what to doAction: the action to take, should be one of [search,calculator]Action Input: the input to the actionObservation: the result of the action... (this Thought/Action/Action Input/Observation can repeat N times)Thought: I now know the final answerFinal Answer: the final answer to the original input questionBegin!Question: Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?Thought:"
]
}
[llm/end] [1:chain:agent_executor > 2:chain:llm_chain > 3:llm:openai] [3.52s] Exiting LLM run with output: {
"generations": [
[
{
"text": " I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.Action: searchAction Input: "Olivia Wilde boyfriend"",
"generationInfo": {
"finishReason": "stop",
"logprobs": null
}
}
]
],
"llmOutput": {
"tokenUsage": {
"completionTokens": 39,
"promptTokens": 220,
"totalTokens": 259
}
}
}
[chain/end] [1:chain:agent_executor > 2:chain:llm_chain] [3.53s] Exiting Chain run with output: {
"text": " I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.Action: searchAction Input: "Olivia Wilde boyfriend""
}
[agent/action] [1:chain:agent_executor] Agent selected action: {
"tool": "search",
"toolInput": "Olivia Wilde boyfriend",
"log": " I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.Action: searchAction Input: "Olivia Wilde boyfriend""
}
[tool/start] [1:chain:agent_executor > 4:tool:search] Entering Tool run with input: "Olivia Wilde boyfriend"
[tool/end] [1:chain:agent_executor > 4:tool:search] [845ms] Exiting Tool run with output: "In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling. Their relationship ended in November 2022."
[chain/start] [1:chain:agent_executor > 5:chain:llm_chain] Entering Chain run with input: {
"input": "Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?",
"agent_scratchpad": " I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.Action: searchAction Input: "Olivia Wilde boyfriend"Observation: In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling. Their relationship ended in November 2022.Thought:",
"stop": [
"Observation: "
]
}
[llm/start] [1:chain:agent_executor > 5:chain:llm_chain > 6:llm:openai] Entering LLM run with input: {
"prompts": [
"Answer the following questions as best you can. You have access to the following tools:search: a search engine. useful for when you need to answer questions about current events. input should be a search query.calculator: Useful for getting the result of a math expression. The input to this tool should be a valid mathematical expression that could be executed by a simple calculator.Use the following format in your response:Question: the input question you must answerThought: you should always think about what to doAction: the action to take, should be one of [search,calculator]Action Input: the input to the actionObservation: the result of the action... (this Thought/Action/Action Input/Observation can repeat N times)Thought: I now know the final answerFinal Answer: the final answer to the original input questionBegin!Question: Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?Thought: I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.Action: searchAction Input: "Olivia Wilde boyfriend"Observation: In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling. Their relationship ended in November 2022.Thought:"
]
}
[llm/end] [1:chain:agent_executor > 5:chain:llm_chain > 6:llm:openai] [3.65s] Exiting LLM run with output: {
"generations": [
[
{
"text": " I need to find out Harry Styles' age.Action: searchAction Input: "Harry Styles age"",
"generationInfo": {
"finishReason": "stop",
"logprobs": null
}
}
]
],
"llmOutput": {
"tokenUsage": {
"completionTokens": 23,
"promptTokens": 296,
"totalTokens": 319
}
}
}
[chain/end] [1:chain:agent_executor > 5:chain:llm_chain] [3.65s] Exiting Chain run with output: {
"text": " I need to find out Harry Styles' age.Action: searchAction Input: "Harry Styles age""
}
[agent/action] [1:chain:agent_executor] Agent selected action: {
"tool": "search",
"toolInput": "Harry Styles age",
"log": " I need to find out Harry Styles' age.Action: searchAction Input: "Harry Styles age""
}
[tool/start] [1:chain:agent_executor > 7:tool:search] Entering Tool run with input: "Harry Styles age"
[tool/end] [1:chain:agent_executor > 7:tool:search] [632ms] Exiting Tool run with output: "29 years"
[chain/start] [1:chain:agent_executor > 8:chain:llm_chain] Entering Chain run with input: {
"input": "Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?",
"agent_scratchpad": " I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.Action: searchAction Input: "Olivia Wilde boyfriend"Observation: In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling. Their relationship ended in November 2022.Thought: I need to find out Harry Styles' age.Action: searchAction Input: "Harry Styles age"Observation: 29 yearsThought:",
"stop": [
"Observation: "
]
}
[llm/start] [1:chain:agent_executor > 8:chain:llm_chain > 9:llm:openai] Entering LLM run with input: {
"prompts": [
"Answer the following questions as best you can. You have access to the following tools:search: a search engine. useful for when you need to answer questions about current events. input should be a search query.calculator: Useful for getting the result of a math expression. The input to this tool should be a valid mathematical expression that could be executed by a simple calculator.Use the following format in your response:Question: the input question you must answerThought: you should always think about what to doAction: the action to take, should be one of [search,calculator]Action Input: the input to the actionObservation: the result of the action... (this Thought/Action/Action Input/Observation can repeat N times)Thought: I now know the final answerFinal Answer: the final answer to the original input questionBegin!Question: Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?Thought: I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.Action: searchAction Input: "Olivia Wilde boyfriend"Observation: In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling. Their relationship ended in November 2022.Thought: I need to find out Harry Styles' age.Action: searchAction Input: "Harry Styles age"Observation: 29 yearsThought:"
]
}
[llm/end] [1:chain:agent_executor > 8:chain:llm_chain > 9:llm:openai] [2.72s] Exiting LLM run with output: {
"generations": [
[
{
"text": " I need to calculate 29 raised to the 0.23 power.Action: calculatorAction Input: 29^0.23",
"generationInfo": {
"finishReason": "stop",
"logprobs": null
}
}
]
],
"llmOutput": {
"tokenUsage": {
"completionTokens": 26,
"promptTokens": 329,
"totalTokens": 355
}
}
}
[chain/end] [1:chain:agent_executor > 8:chain:llm_chain] [2.72s] Exiting Chain run with output: {
"text": " I need to calculate 29 raised to the 0.23 power.Action: calculatorAction Input: 29^0.23"
}
[agent/action] [1:chain:agent_executor] Agent selected action: {
"tool": "calculator",
"toolInput": "29^0.23",
"log": " I need to calculate 29 raised to the 0.23 power.Action: calculatorAction Input: 29^0.23"
}
[tool/start] [1:chain:agent_executor > 10:tool:calculator] Entering Tool run with input: "29^0.23"
[tool/end] [1:chain:agent_executor > 10:tool:calculator] [3ms] Exiting Tool run with output: "2.169459462491557"
[chain/start] [1:chain:agent_executor > 11:chain:llm_chain] Entering Chain run with input: {
"input": "Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?",
"agent_scratchpad": " I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.Action: searchAction Input: "Olivia Wilde boyfriend"Observation: In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling. Their relationship ended in November 2022.Thought: I need to find out Harry Styles' age.Action: searchAction Input: "Harry Styles age"Observation: 29 yearsThought: I need to calculate 29 raised to the 0.23 power.Action: calculatorAction Input: 29^0.23Observation: 2.169459462491557Thought:",
"stop": [
"Observation: "
]
}
[llm/start] [1:chain:agent_executor > 11:chain:llm_chain > 12:llm:openai] Entering LLM run with input: {
"prompts": [
"Answer the following questions as best you can. You have access to the following tools:search: a search engine. useful for when you need to answer questions about current events. input should be a search query.calculator: Useful for getting the result of a math expression. The input to this tool should be a valid mathematical expression that could be executed by a simple calculator.Use the following format in your response:Question: the input question you must answerThought: you should always think about what to doAction: the action to take, should be one of [search,calculator]Action Input: the input to the actionObservation: the result of the action... (this Thought/Action/Action Input/Observation can repeat N times)Thought: I now know the final answerFinal Answer: the final answer to the original input questionBegin!Question: Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?Thought: I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.Action: searchAction Input: "Olivia Wilde boyfriend"Observation: In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling. Their relationship ended in November 2022.Thought: I need to find out Harry Styles' age.Action: searchAction Input: "Harry Styles age"Observation: 29 yearsThought: I need to calculate 29 raised to the 0.23 power.Action: calculatorAction Input: 29^0.23Observation: 2.169459462491557Thought:"
]
}
[llm/end] [1:chain:agent_executor > 11:chain:llm_chain > 12:llm:openai] [3.51s] Exiting LLM run with output: {
"generations": [
[
{
"text": " I now know the final answer.Final Answer: Harry Styles is Olivia Wilde's boyfriend and his current age raised to the 0.23 power is 2.169459462491557.",
"generationInfo": {
"finishReason": "stop",
"logprobs": null
}
}
]
],
"llmOutput": {
"tokenUsage": {
"completionTokens": 39,
"promptTokens": 371,
"totalTokens": 410
}
}
}
[chain/end] [1:chain:agent_executor > 11:chain:llm_chain] [3.51s] Exiting Chain run with output: {
"text": " I now know the final answer.Final Answer: Harry Styles is Olivia Wilde's boyfriend and his current age raised to the 0.23 power is 2.169459462491557."
}
[chain/end] [1:chain:agent_executor] [14.90s] Exiting Chain run with output: {
"output": "Harry Styles is Olivia Wilde's boyfriend and his current age raised to the 0.23 power is 2.169459462491557."
}



--- æ–‡ä»¶: output_20250622_020018\docs\modules\agents\agents.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/agents/
crawled_at: 2025-06-22T02:00:19.823419
---

ä»£ç†
info
æ¦‚å¿µæŒ‡å—
ä»£ç†æ˜¯ä¸€ä¸ªæ— çŠ¶æ€çš„å°è£…å™¨ï¼Œå°è£…äº†ä¸€ä¸ªä»£ç†æç¤ºé“¾ï¼ˆæ¯”å¦‚MRKL)ï¼Œè´Ÿè´£å°†å·¥å…·æ ¼å¼åŒ–åˆ°æç¤ºç¬¦ä¸­ï¼Œä»¥åŠè§£æä»èŠå¤©æ¨¡å‹è·å–çš„å“åº”ã€‚å®ƒæ¥æ”¶ç”¨æˆ·è¾“å…¥ï¼Œå¹¶è¿”å›ç›¸åº”çš„â€œæ“ä½œâ€å’Œç›¸åº”çš„â€œæ“ä½œè¾“å…¥â€å“åº”ã€‚
é€‰æ‹©å“ªç§ä»£ç†ï¼Ÿ
â€‹
æ‚¨é€‰æ‹©çš„ä»£ç†å–å†³äºæ‚¨æƒ³æ‰§è¡Œçš„ä»»åŠ¡ç±»å‹ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªå¿«é€ŸæŒ‡å—ï¼Œå¯å¸®åŠ©æ‚¨ä¸ºæ‚¨çš„ä½¿ç”¨æƒ…å†µé€‰æ‹©æ­£ç¡®çš„ä»£ç†:
å¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨æ–‡æœ¬LLMï¼Œ é¦–å…ˆå°è¯•
zero-shot-react-description
ï¼Œ å³ã€‚
LLMsçš„MRKLä»£ç†
ã€‚
å¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨èŠå¤©æ¨¡å‹ï¼Œ å°è¯•
chat-zero-shot-react-description
ï¼Œ å³ã€‚
èŠå¤©æ¨¡å‹çš„MRKLä»£ç†
ã€‚
å¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨èŠå¤©æ¨¡å‹å¹¶æƒ³ä½¿ç”¨å†…å­˜ï¼Œ å°è¯•
chat-conversational-react-description
ï¼Œ
ä¼šè¯ä»£ç†
ã€‚
å¦‚æœæ‚¨æœ‰ä¸€ä¸ªéœ€è¦å¤šä¸ªæ­¥éª¤çš„å¤æ‚ä»»åŠ¡ï¼Œå¹¶ä¸”æ‚¨æœ‰å…´è¶£å°è¯•ä¸€ç§æ–°çš„ä»£ç†ç±»å‹ï¼Œ å°è¯•
Plan-and-Executeä»£ç†
ã€‚
æ‰€æœ‰ä»£ç†
â€‹
ğŸ—ƒï¸
åŠ¨ä½œä»£ç† Action Agents
4 items
ğŸ“„ï¸
è®¡åˆ’æ‰§è¡Œä»£ç†
è¿™ä¸ªä¾‹å­å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ä¸€ä¸ªä½¿ç”¨è®¡åˆ’æ‰§è¡Œæ¡†æ¶æ¥å›ç­”æŸ¥è¯¢çš„ä»£ç†ã€‚
ğŸ—ƒï¸
è‡ªå®šä¹‰ä»£ç†
3 items



--- æ–‡ä»¶: output_20250622_020018\docs\modules\agents\executor.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/executor/
crawled_at: 2025-06-22T02:00:20.356066
---

Agent Executors
info
æ¦‚å¿µæŒ‡å—
ä¸ºäº†è®©æ™ºèƒ½ä»£ç†æ›´åŠ å¼ºå¤§ï¼Œæˆ‘ä»¬éœ€è¦ä½¿å…¶è¿­ä»£ï¼Œå³è°ƒç”¨æ¨¡å‹å¤šæ¬¡ï¼Œç›´åˆ°è¾¾åˆ°æœ€ç»ˆç­”æ¡ˆã€‚è¿™å°±æ˜¯ AgentExecutor çš„å·¥ä½œã€‚
class
AgentExecutor
{
// a simplified implementation
run
(
inputs
:
object
)
{
const
steps
=
[
]
;
while
(
true
)
{
const
step
=
await
this
.
agent
.
plan
(
steps
,
inputs
)
;
if
(
step
instanceof
AgentFinish
)
{
return
step
.
returnValues
;
}
steps
.
push
(
step
)
;
}
}
}
ğŸ“„ï¸
å¼€å§‹(Getting Started)
ä»£ç†ä½¿ç”¨LLMæ¥ç¡®å®šé‡‡å–å“ªäº›æ“ä½œä»¥åŠé‡‡å–çš„é¡ºåºã€‚æ“ä½œå¯ä»¥æ˜¯ä½¿ç”¨å·¥å…·å¹¶è§‚å¯Ÿå…¶è¾“å‡ºï¼Œæˆ–è¿”å›ç»™ç”¨æˆ·ã€‚



--- æ–‡ä»¶: output_20250622_020018\docs\modules\agents\toolkits.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/toolkits/
crawled_at: 2025-06-22T02:00:20.483644
---

å…¥é—¨æŒ‡å—: å·¥å…·åŒ…
info
æ¦‚å¿µæŒ‡å—
ä¸€ç»„å·¥å…·ï¼Œå¯ç”¨äºè§£å†³ç‰¹å®šé—®é¢˜/å¿…éœ€çš„å·¥å…·ç»„ã€‚
interface
Toolkit
{
tools
:
Tool
[
]
;
}
æ‰€æœ‰å·¥å…·åŒ…
â€‹
ğŸ“„ï¸
JSONä»£ç†å·¥å…·åŒ…
è¿™ä¸ªä¾‹å­å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨JSONå·¥å…·åŒ…åŠ è½½å’Œä½¿ç”¨ä»£ç†ã€‚
ğŸ“„ï¸
OpenAPIä»£ç†å·¥å…·åŒ…
è¯¥ç¤ºä¾‹æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨OpenAPIå·¥å…·åŒ…åŠ è½½å’Œä½¿ç”¨ä»£ç†ã€‚
ğŸ“„ï¸
SQL Agent Toolkit
è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•åŠ è½½å’Œä½¿ç”¨SQLå·¥å…·åŒ…ä¸­çš„ä»£ç†ã€‚
ğŸ“„ï¸
VectorStore ä»£ç†å·¥å…·åŒ…
è¿™ä¸ªä¾‹å­å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ VectorStore å·¥å…·åŒ…åŠ è½½å’Œä½¿ç”¨ä»£ç†ã€‚



--- æ–‡ä»¶: output_20250622_020018\docs\modules\agents\tools.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/tools/
crawled_at: 2025-06-22T02:00:20.702284
---

å·¥å…·
info
æ¦‚å¿µæŒ‡å—
å·¥å…·æ˜¯ä¸€ä¸ªå‡½æ•°çš„æŠ½è±¡ï¼Œä½¿å¾—è¯­è¨€æ¨¡å‹å¯ä»¥è½»æ¾åœ°ä¸ä¹‹äº¤äº’ã€‚å…·ä½“æ¥è¯´ï¼Œå·¥å…·çš„æ¥å£æœ‰ä¸€ä¸ªæ–‡æœ¬è¾“å…¥å’Œä¸€ä¸ªæ–‡æœ¬è¾“å‡ºã€‚å®ƒåŒ…æ‹¬åç§°å’Œæè¿°ï¼Œå‘
æ¨¡å‹
ä¼ è¾¾å·¥å…·çš„ä½œç”¨å’Œä½•æ—¶ä½¿ç”¨å®ƒã€‚
interface
Tool
{
call
(
arg
:
string
)
:
Promise
<
string
>
;
name
:
string
;
description
:
string
;
}
æ‰€æœ‰å·¥å…·
â€‹
ğŸ“„ï¸
é›†æˆ
LangChainæä¾›ä»¥ä¸‹å¯ç«‹å³ä½¿ç”¨çš„å·¥å…·:
ğŸ“„ï¸
å¸¦æœ‰å‘é‡å­˜å‚¨çš„ä»£ç†
æœ¬ç¬”è®°æ¶µç›–äº†å¦‚ä½•å°†ä»£ç†å’Œå‘é‡å­˜å‚¨å™¨ç»„åˆä½¿ç”¨ã€‚è¿™ç§ç”¨ä¾‹æ˜¯ï¼Œæ‚¨å·²å°†æ•°æ®æ‘„å…¥å‘é‡å­˜å‚¨å™¨ä¸­ï¼Œå¹¶æƒ³ä»¥ä»£ç†æ–¹å¼ä¸å…¶è¿›è¡Œäº¤äº’ã€‚
ğŸ“„ï¸
ChatGPTæ’ä»¶
æœ¬ä¾‹å±•ç¤ºäº†å¦‚ä½•åœ¨LangChainæŠ½è±¡ä¸­ä½¿ç”¨ChatGPTæ’ä»¶ã€‚
ğŸ“„ï¸
DynamicTool è‡ªå®šä¹‰å·¥å…·
åˆ›å»ºè¿è¡Œè‡ªå®šä¹‰ä»£ç çš„å·¥å…·çš„ä¸€ç§é€‰é¡¹æ˜¯ä½¿ç”¨ DynamicToolã€‚
ğŸ“„ï¸
ä½¿ç”¨ AWS Lambda çš„ä»£ç†
è¯·æŸ¥çœ‹å®Œæ•´æ–‡æ¡£//docs.aws.amazon.com/lambda/index.html
ğŸ“„ï¸
ç½‘ç»œæµè§ˆå™¨å·¥å…·
ç½‘ç»œæµè§ˆå™¨å·¥å…·ä¸ºæ‚¨çš„ä»£ç†ç¨‹åºæä¾›äº†è®¿é—®ç½‘ç«™å’Œæå–ä¿¡æ¯çš„åŠŸèƒ½ã€‚å®ƒå‘ä»£ç†ç¨‹åºæè¿°ä¸ºï¼š
ğŸ“„ï¸
Zapier NLAé›†æˆä»£ç†
å®Œæ•´æ–‡æ¡£åœ¨æ­¤å¤„//nla.zapier.com/api/v1/dynamic/docs
é«˜çº§
â€‹
è¦å®ç°è‡ªå·±çš„å·¥å…·ï¼Œä½ å¯ä»¥å°†
Tool
ç±»ä½œä¸ºå­ç±»ï¼Œå¹¶å®ç°
_call
æ–¹æ³•ã€‚
_call
æ–¹æ³•ä½¿ç”¨è¾“å…¥æ–‡æœ¬è°ƒç”¨ï¼Œåº”è¿”å›è¾“å‡ºæ–‡æœ¬ã€‚Toolè¶…ç±»å®ç°äº†
call
æ–¹æ³•,åœ¨è°ƒç”¨
_call
æ–¹æ³•ä¹‹å‰å’Œä¹‹åè°ƒç”¨æ­£ç¡®çš„CallbackManageræ–¹æ³•ã€‚å½“å‡ºç°é”™è¯¯æ—¶ï¼Œ
_call
æ–¹æ³•åº”è¿”å›è¡¨ç¤ºé”™è¯¯çš„å­—ç¬¦ä¸²ï¼Œè€Œä¸æ˜¯æŠ›å‡ºé”™è¯¯ã€‚è¿™å…è®¸é”™è¯¯ä¼ é€’ç»™LLMï¼Œå¹¶ä¸”LLMå¯ä»¥å†³å®šå¦‚ä½•å¤„ç†å®ƒã€‚å¦‚æœæŠ›å‡ºé”™è¯¯ï¼Œåˆ™ä»£ç†çš„æ‰§è¡Œå°†åœæ­¢ã€‚
abstract
class
Tool
{
abstract
_call
(
arg
:
string
)
:
Promise
<
string
>
;
abstract
name
:
string
;
abstract
description
:
string
;
}



--- æ–‡ä»¶: output_20250622_020018\docs\modules\chains\index_related_chains.md ---
---
url: https://js.langchain.com.cn/docs/modules/chains/index_related_chains/
crawled_at: 2025-06-22T02:00:21.010614
---

ä¸ç´¢å¼•ç›¸å…³çš„é“¾
info
æ¦‚å¿µæŒ‡å—
ä¸å­˜å‚¨åœ¨ç´¢å¼•ä¸­çš„éç»“æ„åŒ–æ•°æ®ä¸€èµ·å·¥ä½œç›¸å…³çš„é“¾ã€‚
ğŸ“„ï¸
æ–‡æ¡£QA
LangChainæä¾›äº†ä¸€ç³»åˆ—ä¸“é—¨é’ˆå¯¹éç»“æ„åŒ–æ–‡æœ¬æ•°æ®å¤„ç†çš„é“¾æ¡: StuffDocumentsChainï¼Œ MapReduceDocumentsChainï¼Œ å’Œ RefineDocumentsChainã€‚è¿™äº›é“¾æ¡æ˜¯å¼€å‘ä¸è¿™äº›æ•°æ®äº¤äº’çš„æ›´å¤æ‚é“¾æ¡çš„åŸºæœ¬æ„å»ºæ¨¡å—ã€‚å®ƒä»¬æ—¨åœ¨æ¥å—æ–‡æ¡£å’Œé—®é¢˜ä½œä¸ºè¾“å…¥ï¼Œç„¶ååˆ©ç”¨è¯­è¨€æ¨¡å‹æ ¹æ®æä¾›çš„æ–‡æ¡£åˆ¶å®šç­”æ¡ˆã€‚
ğŸ“„ï¸
æ£€ç´¢é—®ç­”
RetrievalQAChain æ˜¯å°† Retriever å’Œ QA é“¾ï¼ˆä¸Šæ–‡ä¸­æ‰€è¿°)ç»„åˆèµ·æ¥çš„é“¾ã€‚å®ƒç”¨äºä» Retriever æ£€ç´¢æ–‡æ¡£ï¼Œç„¶åä½¿ç”¨ QA é“¾æ ¹æ®æ£€ç´¢åˆ°çš„æ–‡æ¡£å›ç­”é—®é¢˜ã€‚
ğŸ“„ï¸
å¯¹è¯å¼æ£€ç´¢é—®ç­”
ConversationalRetrievalQA é“¾åŸºäº RetrievalQAChain æ„å»ºï¼Œæä¾›äº†èŠå¤©å†å²è®°å½•ç»„ä»¶ã€‚



--- æ–‡ä»¶: output_20250622_020018\docs\modules\chains\llm_chain.md ---
---
url: https://js.langchain.com.cn/docs/modules/chains/llm_chain
crawled_at: 2025-06-22T02:00:21.232912
---

å…¥é—¨æŒ‡å—: LLMChain
info
æ¦‚å¿µæŒ‡å—
LLMChain
æ˜¯åœ¨è¯­è¨€æ¨¡å‹å‘¨å›´æ·»åŠ ä¸€äº›åŠŸèƒ½çš„ç®€å•é“¾ã€‚å®ƒè¢«å¹¿æ³›åœ°åº”ç”¨äºLangChainä¸­ï¼ŒåŒ…æ‹¬å…¶ä»–é“¾å’Œä»£ç†ã€‚
LLMChain
ç”±ä¸€ä¸ª
PromptTemplate
å’Œä¸€ä¸ªè¯­è¨€æ¨¡å‹ï¼ˆLLMæˆ–èŠå¤©æ¨¡å‹)ç»„æˆã€‚
ä¸LLMsçš„ä½¿ç”¨
â€‹
æˆ‘ä»¬å¯ä»¥æ„å»ºä¸€ä¸ªLLMChainï¼Œå®ƒæ¥å—ç”¨æˆ·è¾“å…¥ï¼Œä½¿ç”¨PromptTemplateè¿›è¡Œæ ¼å¼åŒ–ï¼Œç„¶åå°†æ ¼å¼åŒ–åçš„å“åº”ä¼ é€’ç»™LLM:
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
PromptTemplate
}
from
"langchain/prompts"
;
import
{
LLMChain
}
from
"langchain/chains"
;
// We can construct an LLMChain from a PromptTemplate and an LLM.
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
prompt
=
PromptTemplate
.
fromTemplate
(
"What is a good name for a company that makes {product}?"
)
;
const
chainA
=
new
LLMChain
(
{
llm
:
model
,
prompt
}
)
;
// The result is an object with a `text` property.
const
resA
=
await
chainA
.
call
(
{
product
:
"colorful socks"
}
)
;
console
.
log
(
{
resA
}
)
;
// { resA: { text: '\n\nSocktastic!' } }
// Since the LLMChain is a single-input, single-output chain, we can also `run` it.
// This takes in a string and returns the `text` property.
const
resA2
=
await
chainA
.
run
(
"colorful socks"
)
;
console
.
log
(
{
resA2
}
)
;
// { resA2: '\n\nSocktastic!' }
ä¸èŠå¤©æ¨¡å‹çš„ä½¿ç”¨
â€‹
æˆ‘ä»¬ä¹Ÿå¯ä»¥æ„å»ºä¸€ä¸ªLLMChainï¼Œå®ƒæ¥å—ç”¨æˆ·è¾“å…¥ï¼Œä½¿ç”¨PromptTemplateè¿›è¡Œæ ¼å¼åŒ–ï¼Œç„¶åå°†æ ¼å¼åŒ–åçš„å“åº”ä¼ é€’ç»™ChatModel:
import
{
ChatPromptTemplate
,
HumanMessagePromptTemplate
,
SystemMessagePromptTemplate
,
}
from
"langchain/prompts"
;
import
{
LLMChain
}
from
"langchain/chains"
;
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
// We can also construct an LLMChain from a ChatPromptTemplate and a chat model.
const
chat
=
new
ChatOpenAI
(
{
temperature
:
0
}
)
;
const
chatPrompt
=
ChatPromptTemplate
.
fromPromptMessages
(
[
SystemMessagePromptTemplate
.
fromTemplate
(
"You are a helpful assistant that translates {input_language} to {output_language}."
)
,
HumanMessagePromptTemplate
.
fromTemplate
(
"{text}"
)
,
]
)
;
const
chainB
=
new
LLMChain
(
{
prompt
:
chatPrompt
,
llm
:
chat
,
}
)
;
const
resB
=
await
chainB
.
call
(
{
input_language
:
"English"
,
output_language
:
"French"
,
text
:
"I love programming."
,
}
)
;
console
.
log
(
{
resB
}
)
;
// { resB: { text: "J'adore la programmation." } }
åœ¨æµæ¨¡å¼ä¸‹ä½¿ç”¨
â€‹
æˆ‘ä»¬ä¹Ÿå¯ä»¥æ„å»ºä¸€ä¸ªLLMChainï¼Œå®ƒæ¥å—ç”¨æˆ·è¾“å…¥ï¼Œä½¿ç”¨PromptTemplateè¿›è¡Œæ ¼å¼åŒ–ï¼Œç„¶åå°†æ ¼å¼åŒ–åçš„å“åº”ä¼ é€’ç»™ä»¥æµæ¨¡å¼è¿è¡Œçš„LLMï¼Œè¯¥æ¨¡å¼å°†åœ¨ç”Ÿæˆä»¤ç‰Œæ—¶è¿›è¡Œæµå¼è¿”å›:
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
PromptTemplate
}
from
"langchain/prompts"
;
import
{
LLMChain
}
from
"langchain/chains"
;
// Create a new LLMChain from a PromptTemplate and an LLM in streaming mode.
const
model
=
new
OpenAI
(
{
temperature
:
0.9
,
streaming
:
true
}
)
;
const
prompt
=
PromptTemplate
.
fromTemplate
(
"What is a good name for a company that makes {product}?"
)
;
const
chain
=
new
LLMChain
(
{
llm
:
model
,
prompt
}
)
;
// Call the chain with the inputs and a callback for the streamed tokens
const
res
=
await
chain
.
call
(
{
product
:
"colorful socks"
}
,
[
{
handleLLMNewToken
(
token
:
string
)
{
process
.
stdout
.
write
(
token
)
;
}
,
}
,
]
)
;
console
.
log
(
{
res
}
)
;
// { res: { text: '\n\nKaleidoscope Socks' } }
å–æ¶ˆæ­£åœ¨è¿è¡Œçš„LLMChain
â€‹
æˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡å‘
call
æ–¹æ³•ä¼ é€’ä¸€ä¸ªAbortSignalæ¥å–æ¶ˆæ­£åœ¨è¿è¡Œçš„LLMChain:
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
PromptTemplate
}
from
"langchain/prompts"
;
import
{
LLMChain
}
from
"langchain/chains"
;
// Create a new LLMChain from a PromptTemplate and an LLM in streaming mode.
const
model
=
new
OpenAI
(
{
temperature
:
0.9
,
streaming
:
true
}
)
;
const
prompt
=
PromptTemplate
.
fromTemplate
(
"Give me a long paragraph about {product}?"
)
;
const
chain
=
new
LLMChain
(
{
llm
:
model
,
prompt
}
)
;
const
controller
=
new
AbortController
(
)
;
// Call `controller.abort()` somewhere to cancel the request.
setTimeout
(
(
)
=>
{
controller
.
abort
(
)
;
}
,
3000
)
;
try
{
// Call the chain with the inputs and a callback for the streamed tokens
const
res
=
await
chain
.
call
(
{
product
:
"colorful socks"
,
signal
:
controller
.
signal
}
,
[
{
handleLLMNewToken
(
token
:
string
)
{
process
.
stdout
.
write
(
token
)
;
}
,
}
,
]
)
;
}
catch
(
e
)
{
console
.
log
(
e
)
;
// Error: Cancel: canceled
}
åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†åœ¨æµæ¨¡å¼ä¸‹çš„å–æ¶ˆæ“ä½œï¼Œä½†æ˜¯åœ¨éæµæ¨¡å¼ä¸‹çš„æ“ä½œæ–¹å¼ç›¸åŒã€‚



--- æ–‡ä»¶: output_20250622_020018\docs\modules\chains\other_chains.md ---
---
url: https://js.langchain.com.cn/docs/modules/chains/other_chains/
crawled_at: 2025-06-22T02:00:21.184037
---

å…¶ä»–é“¾
æœ¬èŠ‚ä»‹ç»å…¶å®ƒå­˜åœ¨çš„é“¾çš„ç¤ºä¾‹ã€‚
ğŸ“„ï¸
AnalyzeDocumentChain
æ‚¨å¯ä»¥ä½¿ç”¨AnalyzeDocumentChain,å®ƒæ¥å—å•ä¸ªæ–‡æœ¬ä½œä¸ºè¾“å…¥å¹¶å¯¹å…¶è¿›è¡Œæ“ä½œã€‚
ğŸ“„ï¸
APIChain
APIChain å¯ç”¨äºä½¿ç”¨ LLM ä¸ API äº¤äº’ï¼Œä»è€Œæ£€ç´¢ç›¸å…³ä¿¡æ¯ã€‚æä¾›å…³äºæ‰€æä¾›çš„ API æ–‡æ¡£ç›¸å…³çš„é—®é¢˜ä»¥æ„é€ é“¾ã€‚
ğŸ“„ï¸
å®ªæ³•é“¾
å®ªæ³•é“¾æ˜¯ä¸€ç§é“¾å¼ç»“æ„ï¼Œå®ƒç¡®ä¿è¯­è¨€æ¨¡å‹çš„è¾“å‡ºéµå¾ªé¢„å®šä¹‰çš„å®ªæ³•åŸåˆ™ã€‚é€šè¿‡çº³å…¥ç‰¹å®šçš„è§„åˆ™å’ŒæŒ‡å—ï¼Œå®ªæ³•é“¾å¯ä»¥è¿‡æ»¤å’Œä¿®æ”¹ç”Ÿæˆçš„å†…å®¹ä»¥ç¬¦åˆè¿™äº›åŸåˆ™ï¼Œä»è€Œæä¾›æ›´åŠ å—æ§ã€é“å¾·å’Œä¸Šä¸‹æ–‡æ°å½“çš„å“åº”ã€‚è¿™ç§æœºåˆ¶æœ‰åŠ©äºä¿æŒè¾“å‡ºçš„å®Œæ•´æ€§ï¼ŒåŒæ—¶æœ€å¤§ç¨‹åº¦åœ°å‡å°‘ç”Ÿæˆå¯èƒ½è¿åæŒ‡å—ã€å…·æœ‰å†’çŠ¯æ€§æˆ–åç¦»æ‰€éœ€ä¸Šä¸‹æ–‡çš„å†…å®¹çš„é£é™©ã€‚
ğŸ“„ï¸
moderation_chain
OpenAIModerationChain
ğŸ“„ï¸
multi_prompt_chain
MultiPromptChainå¤šæ¬¡æç¤ºé“¾
ğŸ“„ï¸
multi_retrieval_qa_chain
æ¢è¡Œ
ğŸ“„ï¸
SqlDatabaseChain ä¸­æ–‡ï¼šSqlæ•°æ®åº“é“¾
SqlDatabaseChain å¯ä»¥è®©æ‚¨åœ¨ SQL æ•°æ®åº“ä¸Šå›ç­”é—®é¢˜ã€‚
ğŸ“„ï¸
æ‘˜è¦
æ‘˜è¦é“¾å¯ä»¥ç”¨æ¥æ€»ç»“å¤šä¸ªæ–‡æ¡£ã€‚ä¸€ç§æ–¹æ³•æ˜¯åœ¨å°†å¤šä¸ªè¾ƒå°çš„æ–‡æ¡£åˆ†æˆå—åå°†å®ƒä»¬ä½œä¸ºè¾“å…¥ï¼Œä¸MapReduceDocumentsChainä¸€èµ·æ“ä½œã€‚æ‚¨è¿˜å¯ä»¥é€‰æ‹©å°†è¿›è¡Œæ‘˜è¦çš„é“¾æ›¿æ¢ä¸ºStuffDocumentsChainï¼Œæˆ–RefineDocumentsChainã€‚åœ¨æ­¤å¤„äº†è§£æœ‰å…³å®ƒä»¬ä¹‹é—´å·®å¼‚çš„æ›´å¤šä¿¡æ¯here



--- æ–‡ä»¶: output_20250622_020018\docs\modules\chains\prompt_selectors.md ---
---
url: https://js.langchain.com.cn/docs/modules/chains/prompt_selectors/
crawled_at: 2025-06-22T02:00:21.590996
---

æç¤ºé€‰æ‹©å™¨
info
æ¦‚å¿µæŒ‡å—
é€šå¸¸ï¼Œ æ ¹æ®é“¾ä¸­ä½¿ç”¨çš„æ¨¡å‹ç±»å‹ï¼Œæ‚¨ä¼šæƒ³è¦ä»¥ç¼–ç¨‹æ–¹å¼é€‰æ‹©æç¤ºã€‚ç‰¹åˆ«æ˜¯åœ¨äº¤æ¢èŠå¤©æ¨¡å‹å’ŒLLMæ—¶ï¼Œè¿™å°¤å…¶é‡è¦ã€‚
æç¤ºé€‰æ‹©å™¨çš„ç•Œé¢éå¸¸ç®€å•ã€‚:
abstract
class
BasePromptSelector
{
abstract
getPrompt
(
llm
:
BaseLanguageModel
)
:
BasePromptTemplate
;
}
getPromptæ–¹æ³•æ¥å—ä¸€ä¸ªè¯­è¨€æ¨¡å‹å¹¶è¿”å›ä¸€ä¸ªé€‚å½“çš„æç¤ºæ¨¡æ¿ã€‚
æˆ‘ä»¬ç›®å‰æä¾›äº†ä¸€ä¸ªConditionalPromptSelectorï¼Œå…è®¸æ‚¨æŒ‡å®šä¸€ç»„æ¡ä»¶å’Œæç¤ºæ¨¡æ¿ã€‚è¯„ä¼°ä¸ºtrueçš„ç¬¬ä¸€ä¸ªæ¡ä»¶å°†ç”¨äºé€‰æ‹©æç¤ºæ¨¡æ¿ã€‚
const
QA_PROMPT_SELECTOR
=
new
ConditionalPromptSelector
(
DEFAULT_QA_PROMPT
,
[
[
isChatModel
,
CHAT_PROMPT
]
,
]
)
;
å¦‚æœè¯¥æ¨¡å‹ä¸æ˜¯èŠå¤©æ¨¡å‹ï¼Œåˆ™è¿”å›
DEFAULT_QA_PROMPT
ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™è¿”å›
CHAT_PROMPT
ã€‚
ä¸‹é¢çš„ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•åœ¨åŠ è½½é“¾æ—¶ä½¿ç”¨æç¤ºé€‰æ‹©å™¨ã€‚:
const
loadQAStuffChain
=
(
llm
:
BaseLanguageModel
,
params
:
StuffQAChainParams
=
{
}
)
=>
{
const
{
prompt
=
QA_PROMPT_SELECTOR
.
getPrompt
(
llm
)
}
=
params
;
const
llmChain
=
new
LLMChain
(
{
prompt
,
llm
}
)
;
const
chain
=
new
StuffDocumentsChain
(
{
llmChain
}
)
;
return
chain
;
}
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\chains\sequential_chain.md ---
---
url: https://js.langchain.com.cn/docs/modules/chains/sequential_chain
crawled_at: 2025-06-22T02:00:21.657434
---

é¡ºåºé“¾
é¡ºåºé“¾å…è®¸æ‚¨è¿æ¥å¤šä¸ªé“¾ï¼Œå¹¶å°†å®ƒä»¬ç»„åˆæˆæ‰§è¡Œç‰¹å®šåœºæ™¯çš„ç®¡é“ã€‚
SimpleSequentialChain
â€‹
è®©æˆ‘ä»¬ä»æœ€ç®€å•çš„æƒ…å†µå¼€å§‹ï¼Œå³
SimpleSequentialChain
ã€‚
SimpleSequentialChain
æ˜¯ä¸€ç§å…è®¸æ‚¨å°†å¤šä¸ªå•è¾“å…¥/å•è¾“å‡ºé“¾è¿æ¥æˆä¸€ä¸ªé“¾çš„é“¾ã€‚
ä¸‹é¢çš„ç¤ºä¾‹æ˜¾ç¤ºäº†ä¸€ä¸ªæ ·ä¾‹ç”¨ä¾‹ã€‚åœ¨ç¬¬ä¸€æ­¥ï¼Œç»™å®šä¸€ä¸ªæ ‡é¢˜ï¼Œç”Ÿæˆä¸€ä¸ªå‰§æœ¬çš„ç®€ä»‹ã€‚åœ¨ç¬¬äºŒæ­¥ï¼ŒåŸºäºç”Ÿæˆçš„ç®€ä»‹ï¼Œç”Ÿæˆå‰§æœ¬çš„è¯„è®ºã€‚
import
{
SimpleSequentialChain
,
LLMChain
}
from
"langchain/chains"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
PromptTemplate
}
from
"langchain/prompts"
;
// This is an LLMChain to write a synopsis given a title of a play.
const
llm
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
template
=
`
You are a playwright. Given the title of play, it is your job to write a synopsis for that title.
Title: {title}
Playwright: This is a synopsis for the above play:
`
;
const
promptTemplate
=
new
PromptTemplate
(
{
template
,
inputVariables
:
[
"title"
]
,
}
)
;
const
synopsisChain
=
new
LLMChain
(
{
llm
,
prompt
:
promptTemplate
}
)
;
// This is an LLMChain to write a review of a play given a synopsis.
const
reviewLLM
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
reviewTemplate
=
`
You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.
Play Synopsis:
{synopsis}
Review from a New York Times play critic of the above play:
`
;
const
reviewPromptTemplate
=
new
PromptTemplate
(
{
template
:
reviewTemplate
,
inputVariables
:
[
"synopsis"
]
,
}
)
;
const
reviewChain
=
new
LLMChain
(
{
llm
:
reviewLLM
,
prompt
:
reviewPromptTemplate
,
}
)
;
const
overallChain
=
new
SimpleSequentialChain
(
{
chains
:
[
synopsisChain
,
reviewChain
]
,
verbose
:
true
,
}
)
;
const
review
=
await
overallChain
.
run
(
"Tragedy at sunset on the beach"
)
;
console
.
log
(
review
)
;
/*
variable review contains the generated play review based on the input title and synopsis generated in the first step:
"Tragedy at Sunset on the Beach is a powerful and moving story of love, loss, and redemption. The play follows the story of two young lovers, Jack and Jill, whose plans for a future together are tragically cut short when Jack is killed in a car accident. The play follows Jill as she struggles to cope with her grief and eventually finds solace in the arms of another man.
The play is beautifully written and the performances are outstanding. The actors bring the characters to life with their heartfelt performances, and the audience is taken on an emotional journey as Jill is forced to confront her grief and make a difficult decision between her past and her future. The play culminates in a powerful climax that will leave the audience in tears.
Overall, Tragedy at Sunset on the Beach is a powerful and moving story that will stay with you long after the curtain falls. It is a must-see for anyone looking for an emotionally charged and thought-provoking experience."
*/
SequentialChain
â€‹
æ›´é«˜çº§çš„æƒ…å†µéå¸¸æœ‰ç”¨ï¼Œå½“æ‚¨æœ‰å¤šä¸ªå…·æœ‰å¤šä¸ªè¾“å…¥æˆ–è¾“å‡ºé”®çš„é“¾æ—¶ã€‚
import
{
SequentialChain
,
LLMChain
}
from
"langchain/chains"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
PromptTemplate
}
from
"langchain/prompts"
;
// This is an LLMChain to write a synopsis given a title of a play and the era it is set in.
const
llm
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
template
=
`
You are a playwright. Given the title of play and the era it is set in, it is your job to write a synopsis for that title.
Title: {title}
Era: {era}
Playwright: This is a synopsis for the above play:
`
;
const
promptTemplate
=
new
PromptTemplate
(
{
template
,
inputVariables
:
[
"title"
,
"era"
]
,
}
)
;
const
synopsisChain
=
new
LLMChain
(
{
llm
,
prompt
:
promptTemplate
,
outputKey
:
"synopsis"
,
}
)
;
// This is an LLMChain to write a review of a play given a synopsis.
const
reviewLLM
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
reviewTemplate
=
`
You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.
Play Synopsis:
{synopsis}
Review from a New York Times play critic of the above play:
`
;
const
reviewPromptTemplate
=
new
PromptTemplate
(
{
template
:
reviewTemplate
,
inputVariables
:
[
"synopsis"
]
,
}
)
;
const
reviewChain
=
new
LLMChain
(
{
llm
:
reviewLLM
,
prompt
:
reviewPromptTemplate
,
outputKey
:
"review"
,
}
)
;
const
overallChain
=
new
SequentialChain
(
{
chains
:
[
synopsisChain
,
reviewChain
]
,
inputVariables
:
[
"era"
,
"title"
]
,
// Here we return multiple variables
outputVariables
:
[
"synopsis"
,
"review"
]
,
verbose
:
true
,
}
)
;
const
chainExecutionResult
=
await
overallChain
.
call
(
{
title
:
"Tragedy at sunset on the beach"
,
era
:
"Victorian England"
,
}
)
;
console
.
log
(
chainExecutionResult
)
;
/*
variable chainExecutionResult contains final review and intermediate synopsis (as specified by outputVariables). The data is generated based on the input title and era:
"{
"review": "
Tragedy at Sunset on the Beach is a captivating and heartbreaking story of love and loss. Set in Victorian England, the play follows Emily, a young woman struggling to make ends meet in a small coastal town. Emily's dreams of a better life are dashed when she discovers her employer's scandalous affair, and her plans are further thwarted when she meets a handsome stranger on the beach.
The play is a powerful exploration of the human condition, as Emily must grapple with the truth and make a difficult decision that will change her life forever. The performances are outstanding, with the actors bringing a depth of emotion to their characters that is both heartbreaking and inspiring.
Overall, Tragedy at Sunset on the Beach is a beautiful and moving play that will leave audiences in tears. It is a must-see for anyone looking for a powerful and thought-provoking story.",
"synopsis": "
Tragedy at Sunset on the Beach is a play set in Victorian England. It tells the story of a young woman, Emily, who is struggling to make ends meet in a small coastal town. She works as a maid for a wealthy family, but her dreams of a better life are dashed when she discovers that her employer is involved in a scandalous affair.
Emily is determined to make a better life for herself, but her plans are thwarted when she meets a handsome stranger on the beach one evening. The two quickly fall in love, but their happiness is short-lived when Emily discovers that the stranger is actually a member of the wealthy family she works for.
The play follows Emily as she struggles to come to terms with the truth and make sense of her life. As the sun sets on the beach, Emily must decide whether to stay with the man she loves or to leave him and pursue her dreams. In the end, Emily must make a heartbreaking decision that will change her life forever.",
}"
*/



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\document_loaders.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/
crawled_at: 2025-06-22T02:00:21.700243
---

å…¥é—¨: æ–‡æ¡£è£…è½½
info
æ¦‚å¿µæŒ‡å—
æ–‡æ¡£è£…è½½å™¨ä½¿å¾—ä»å„ç§æ•°æ®æºåˆ›å»º
æ–‡æ¡£
å˜å¾—å®¹æ˜“ã€‚è¿™äº›æ–‡æ¡£å¯ä»¥è¢«åŠ è½½åˆ°
å‘é‡å­˜å‚¨å™¨
ä¸­ä»¥ä»æ•°æ®æºåŠ è½½æ–‡æ¡£ã€‚
interface
DocumentLoader
{
load
(
)
:
Promise
<
Document
[
]
>
;
loadAndSplit
(
textSplitter
?
:
TextSplitter
)
:
Promise
<
Document
[
]
>
;
}
æ–‡æ¡£è£…è½½å™¨å…¬å¼€ä¸¤ä¸ªæ–¹æ³•ï¼š
load
å’Œ
loadAndSplit
ã€‚
load
ä¼šä»æ•°æ®æºåŠ è½½æ–‡æ¡£å¹¶å°†å®ƒä»¬ä½œä¸º
æ–‡æ¡£
æ•°ç»„è¿”å›ã€‚
loadAndSplit
ä¼šä»æ•°æ®æºåŠ è½½æ–‡æ¡£ï¼Œä½¿ç”¨æä¾›çš„
æ–‡æœ¬åˆ†å‰²å™¨
å¯¹å®ƒä»¬è¿›è¡Œåˆ†å‰²ï¼Œå¹¶å°†å®ƒä»¬ä½œä¸º
æ–‡æ¡£
æ•°ç»„è¿”å›ã€‚
æ‰€æœ‰æ–‡æ¡£è£…è½½å™¨
â€‹
ğŸ—ƒï¸
ç¤ºä¾‹
2 items
é«˜çº§
â€‹
å¦‚æœæ‚¨æƒ³è¦å®ç°è‡ªå·±çš„æ–‡æ¡£è£…è½½å™¨ï¼Œæ‚¨æœ‰å‡ ä¸ªé€‰æ‹©ã€‚
å­ç±»åŒ–
BaseDocumentLoader
â€‹
ä½ å¯ä»¥ç›´æ¥æ‰©å±•
BaseDocumentLoader
ç±»ã€‚
BaseDocumentLoader
ç±»æä¾›äº†ä¸€äº›æ–¹ä¾¿çš„æ–¹æ³•ï¼Œå¯ä»¥ä»å„ç§æ•°æ®æºåŠ è½½æ–‡æ¡£ã€‚
abstract
class
BaseDocumentLoader
implements
DocumentLoader
{
abstract
load
(
)
:
Promise
<
Document
[
]
>
;
}
å­ç±»åŒ–
TextLoader
â€‹
å¦‚æœä½ æƒ³ä»æ–‡æœ¬æ–‡ä»¶ä¸­åŠ è½½æ–‡æ¡£ï¼Œä½ å¯ä»¥æ‰©å±•
TextLoader
ç±»ã€‚
TextLoader
ç±»ä¼šè´Ÿè´£è¯»å–æ–‡ä»¶ï¼Œæ‰€ä»¥ä½ åªéœ€å®ç°ä¸€ä¸ªè§£ææ–¹æ³•å³å¯ã€‚
abstract
class
TextLoader
extends
BaseDocumentLoader
{
abstract
parse
(
raw
:
string
)
:
Promise
<
string
[
]
>
;
}
å­ç±»åŒ–
BufferLoader
â€‹
å¦‚æœä½ æƒ³è¦ä»äºŒè¿›åˆ¶æ–‡ä»¶ä¸­åŠ è½½æ–‡æ¡£ï¼Œä½ å¯ä»¥æ‰©å±•
BufferLoader
ç±»ã€‚
BufferLoader
ç±»ä¼šè´Ÿè´£è¯»å–æ–‡ä»¶ï¼Œå› æ­¤ä½ åªéœ€è¦å®ç°ä¸€ä¸ªè§£ææ–¹æ³•ã€‚
abstract
class
BufferLoader
extends
BaseDocumentLoader
{
abstract
parse
(
raw
:
Buffer
,
metadata
:
Document
[
"metadata"
]
)
:
Promise
<
Document
[
]
>
;
}



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\retrievers.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/retrievers/
crawled_at: 2025-06-22T02:00:22.831287
---

å¬å›å™¨ï¼ˆRetrievers)
info
æ¦‚å¿µæŒ‡å—
ä¸€ç§å­˜å‚¨æ•°æ®çš„æ–¹å¼ï¼Œå¯ä»¥é€šè¿‡è¯­è¨€æ¨¡å‹è¿›è¡ŒæŸ¥è¯¢ã€‚è¿™ä¸ªå¯¹è±¡å¿…é¡»å…¬å¼€çš„å”¯ä¸€æ¥å£æ˜¯ä¸€ä¸ª
getRelevantDocuments
æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ¥å—ä¸€ä¸ªå­—ç¬¦ä¸²æŸ¥è¯¢å¹¶è¿”å›ä¸€ä¸ªæ–‡æ¡£åˆ—è¡¨ã€‚
ğŸ“„ï¸
ChatGPTæ’ä»¶æ£€ç´¢å™¨
æœ¬ç¤ºä¾‹æ¼”ç¤ºå¦‚ä½•åœ¨LangChainä¸­ä½¿ç”¨ChatGPTæ£€ç´¢å™¨æ’ä»¶ã€‚
ğŸ“„ï¸
è‡ªæˆ‘æŸ¥è¯¢è‰²åº¦æ£€ç´¢å™¨
è‡ªæˆ‘æŸ¥è¯¢æ£€ç´¢å™¨æ­£å¦‚å…¶åç§°æ‰€ç¤ºå…·æœ‰æŸ¥è¯¢è‡ªèº«çš„èƒ½åŠ›ã€‚å…·ä½“è€Œè¨€ç»™å®šä»»ä½•è‡ªç„¶è¯­è¨€æŸ¥è¯¢æ£€ç´¢å™¨ä½¿ç”¨æŸ¥è¯¢æ„å»ºLLMé“¾æ¥æ’°å†™ç»“æ„åŒ–æŸ¥è¯¢ï¼Œç„¶åå°†è¯¥ç»“æ„åŒ–æŸ¥è¯¢åº”ç”¨äºå…¶åŸºç¡€å‘é‡å­˜å‚¨ä¸­ã€‚è¿™ä½¿å¾—æ£€ç´¢å™¨ä¸ä»…å¯ä»¥ä½¿ç”¨ç”¨æˆ·è¾“å…¥çš„æŸ¥è¯¢è¿›è¡Œä¸å­˜å‚¨æ–‡æ¡£å†…å®¹çš„è¯­ä¹‰ç›¸ä¼¼æ€§æ¯”è¾ƒï¼Œè€Œä¸”å¯ä»¥ä»ç”¨æˆ·æŸ¥è¯¢ä¸­æå–å­˜å‚¨æ–‡æ¡£çš„å…ƒæ•°æ®è¿‡æ»¤å™¨å¹¶æ‰§è¡Œè¿™äº›è¿‡æ»¤å™¨ã€‚
ğŸ“„ï¸
contextual-compression-retriever
ä¸Šä¸‹æ–‡å‹ç¼©æ£€ç´¢å™¨
ğŸ“„ï¸
Databerry Retriever
æœ¬ç¤ºä¾‹å±•ç¤ºå¦‚ä½•åœ¨RetrievalQAChainä¸­ä½¿ç”¨Databerry Retrieverä»Databerry.aiæ•°æ®å­˜å‚¨åº“æ£€ç´¢æ–‡æ¡£ã€‚
ğŸ“„ï¸
HyDE Retriever
æœ¬ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨HyDE Retrieverï¼Œå…¶å®ç°äº†Hypothetical Document Embeddingsï¼ˆHyDE)ï¼Œå…·ä½“å†…å®¹å‚è§è¿™ç¯‡è®ºæ–‡ã€‚
ğŸ“„ï¸
é‡‘å±æ£€ç´¢å™¨
è¯¥ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•åœ¨â€œæ£€ç´¢QAChainâ€ä¸­ä½¿ç”¨é‡‘å±æ£€ç´¢å™¨ä»é‡‘å±ç´¢å¼•ä¸­æ£€ç´¢æ–‡æ¡£ã€‚
ğŸ“„ï¸
è‡ªæŸ¥Pineconeæ£€ç´¢å™¨
è‡ªæŸ¥æ£€ç´¢å™¨å…·å¤‡æŸ¥è¯¢è‡ªèº«çš„èƒ½åŠ›ï¼Œæ­£å¦‚å…¶åç§°æ‰€ç¤ºã€‚å…·ä½“åœ°è¯´ï¼Œå¯¹äºä»»ä½•è‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼Œæ£€ç´¢å™¨ä½¿ç”¨åŸºäºæŸ¥è¯¢ç»“æ„æ„å»ºçš„LLMé“¾æ¥ç¼–å†™ç»“æ„åŒ–æŸ¥è¯¢ï¼Œç„¶åå°†è¯¥ç»“æ„åŒ–æŸ¥è¯¢åº”ç”¨äºå…¶åº•å±‚å‘é‡å­˜å‚¨ã€‚è¿™ä¸ä»…å…è®¸æ£€ç´¢å™¨ä½¿ç”¨ç”¨æˆ·è¾“å…¥çš„æŸ¥è¯¢ä¸æ‰€å­˜å‚¨æ–‡ä»¶å†…å®¹è¿›è¡Œè¯­ä¹‰ç›¸ä¼¼æ€§æ¯”è¾ƒï¼Œè¿˜å¯ä»¥ä»ç”¨æˆ·æŸ¥è¯¢ä¸­æå–æœ‰å…³å­˜å‚¨æ–‡æ¡£å…ƒæ•°æ®çš„è¿‡æ»¤å™¨å¹¶æ‰§è¡Œè¿™äº›è¿‡æ»¤å™¨ã€‚[æ³¨ï¼šLLMé“¾ï¼ŒæŒ‡çš„æ˜¯â€œç½—æ°å±€éƒ¨è¯­è¨€æ¨¡å‹â€ï¼Œæ˜¯ä¸€ç§NLPæŠ€æœ¯]
ğŸ“„ï¸
è¿œç¨‹æ£€ç´¢å™¨
æœ¬ç¤ºä¾‹å±•ç¤ºå¦‚ä½•åœ¨ RetrievalQAChain ä¸­ä½¿ç”¨è¿œç¨‹æ£€ç´¢å™¨ä»è¿œç¨‹æœåŠ¡å™¨æ£€ç´¢æ–‡æ¡£ã€‚
ğŸ“„ï¸
Supabase æ··åˆæœç´¢
Langchain æ”¯æŒä½¿ç”¨ Supabase Postgres æ•°æ®åº“è¿›è¡Œæ··åˆæœç´¢ã€‚è¯¥æ··åˆæœç´¢ç»“åˆäº† Postgres çš„ pgvector æ‰©å±•ï¼ˆç›¸ä¼¼åº¦æœç´¢)å’Œå…¨æ–‡æœç´¢ï¼ˆå…³é”®è¯æœç´¢)æ¥æ£€ç´¢æ–‡æ¡£ã€‚æ‚¨å¯ä»¥é€šè¿‡ SupabaseVectorStore çš„ addDocuments å‡½æ•°æ·»åŠ æ–‡æ¡£ã€‚SupabaseHybridKeyWordSearch æ¥å—åµŒå…¥ï¼Œ supabase å®¢æˆ·ç«¯ï¼Œ ç›¸ä¼¼æ€§æœç´¢çš„ç»“æœæ•°é‡ï¼Œ å’Œå…³é”®è¯æœç´¢çš„ç»“æœæ•°é‡ä½œä¸ºå‚æ•°ã€‚getRelevantDocuments å‡½æ•°äº§ç”Ÿä¸€ä¸ªå»é‡å’ŒæŒ‰ç›¸å…³æ€§åˆ†æ•°æ’åºçš„æ–‡æ¡£åˆ—è¡¨ã€‚
ğŸ“„ï¸
æ—¶é—´åŠ æƒå¬å›å™¨
æ—¶é—´åŠ æƒå¬å›å™¨æ˜¯ä¸€ç§ç»¼åˆè€ƒè™‘ç›¸ä¼¼æ€§å’Œæ–°è¿‘åº¦çš„å¬å›å™¨ã€‚è¯„åˆ†ç®—æ³•ä¸º :ã€‚
ğŸ“„ï¸
å‘é‡åº“
ä¸€æ—¦æ‚¨åˆ›å»ºäº†ä¸€ä¸ªå‘é‡åº“ï¼Œ ,ä½¿ç”¨å®ƒä½œä¸ºæ£€ç´¢å™¨å°±éå¸¸ç®€å•:
ğŸ“„ï¸
Vespa Retriever
å±•ç¤ºå¦‚ä½•ä½¿ç”¨Vespa.aiä½œä¸ºLangChainæ£€ç´¢å™¨ã€‚
ğŸ“„ï¸
Zep Retriever
è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•åœ¨ RetrievalQAChain ä¸­ä½¿ç”¨ Zep Retriever ä» Zep å†…å­˜å­˜å‚¨ä¸­æ£€ç´¢æ–‡æ¡£ã€‚(This example shows how to use the Zep Retriever in a RetrievalQAChain to retrieve documents from Zep memory store.)



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\text_splitters.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/text_splitters/
crawled_at: 2025-06-22T02:00:23.545801
---

å…¥é—¨: æ–‡æœ¬åˆ†å‰²å™¨ï¼ˆText Splitters)
info
æ¦‚å¿µæŒ‡å—
è¯­è¨€æ¨¡å‹é€šå¸¸å—åˆ°å¯ä»¥ä¼ é€’ç»™å®ƒä»¬çš„æ–‡æœ¬æ•°é‡çš„é™åˆ¶ï¼Œå› æ­¤å°†æ–‡æœ¬åˆ†å‰²ä¸ºè¾ƒå°çš„å—æ˜¯å¿…è¦çš„ã€‚
LangChainæä¾›äº†å‡ ç§å®ç”¨å·¥å…·æ¥å®Œæˆæ­¤æ“ä½œã€‚
ä½¿ç”¨æ–‡æœ¬åˆ†å‰²å™¨ä¹Ÿå¯ä»¥å¸®åŠ©æ”¹å–„å‘é‡å­˜å‚¨çš„æœç´¢ç»“æœï¼Œå› ä¸ºè¾ƒå°çš„å—æœ‰æ—¶æ›´å®¹æ˜“åŒ¹é…æŸ¥è¯¢ã€‚
æµ‹è¯•ä¸åŒçš„å—å¤§å°ï¼ˆå’Œå—é‡å )æ˜¯ä¸€ä¸ªå€¼å¾—çš„ç»ƒä¹ ï¼Œä»¥é€‚åº”æ‚¨çš„ç”¨ä¾‹ã€‚
å‚æ•°
â€‹
chunkSize?: number = 1000
: æ¯ä¸ªå—ä¸­æœ€å¤§å­—ç¬¦æ•°ã€‚é»˜è®¤å€¼ä¸º1000ä¸ªæ ‡è®°ï¼ˆtokens)ã€‚
chunkOverlap?: number = 200
: ç›¸é‚»å—ä¹‹é—´é‡å çš„å­—ç¬¦æ•°ã€‚é»˜è®¤å€¼ä¸º200ä¸ªæ ‡è®°ï¼ˆtokens)ã€‚
type
TextSplitterChunkHeaderOptions
=
{
chunkHeader
?
:
string
;
chunkOverlapHeader
?
:
string
;
appendChunkOverlapHeader
?
:
boolean
;
}
;
interface
TextSplitter
{
chunkSize
:
number
;
chunkOverlap
:
number
;
createDocuments
(
texts
:
string
[
]
,
metadatas
?
:
Record
<
string
,
any
>
[
]
,
chunkHeaderOptions
:
TextSplitterChunkHeaderOptions
=
{
}
)
:
Promise
<
Document
[
]
>
;
splitDocuments
(
documents
:
Document
[
]
,
chunkHeaderOptions
:
TextSplitterChunkHeaderOptions
=
{
}
)
:
Promise
<
Document
[
]
>
;
}
æ–‡æœ¬åˆ†å‰²å™¨æä¾›äº†ä¸¤ç§æ–¹æ³•ï¼šï¼Œ
createDocuments
å’Œ
splitDocuments
ã€‚
å‰è€…è·å–åŸå§‹æ–‡æœ¬å­—ç¬¦ä¸²çš„åˆ—è¡¨å¹¶è¿”å›æ–‡æ¡£çš„åˆ—è¡¨ï¼Œåè€…è·å–æ–‡æ¡£åˆ—è¡¨å¹¶è¿”å›æ–‡æ¡£çš„åˆ—è¡¨ã€‚
åŒºåˆ«åœ¨äº
createDocuments
å°†åŸå§‹æ–‡æœ¬å­—ç¬¦ä¸²æ‹†åˆ†æˆå—ï¼Œè€Œ
splitDocuments
å°†æ–‡æ¡£æ‹†åˆ†æˆå—ã€‚
ä½•æ—¶ä½¿ç”¨
chunkHeaderOptions
â€‹
ä»…ä»…é€šè¿‡é‡å æ–‡æœ¬æ¥åˆ†å‰²æ–‡æ¡£å¯èƒ½æ— æ³•æä¾›è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè®©LLMsç¡®å®šå¤šä¸ªå—æ˜¯å¦å¼•ç”¨äº†ç›¸åŒçš„ä¿¡æ¯æˆ–å¦‚ä½•è§£å†³æ¥è‡ªç›¸äº’çŸ›ç›¾çš„æºçš„ä¿¡æ¯ã€‚
ç»™æ¯ä¸ªæ–‡æ¡£æ‰“æ ‡ç­¾æ˜¯ä¸€ä¸ªè§£å†³æ–¹æ¡ˆï¼Œå¦‚æœä½ çŸ¥é“éœ€è¦è¿‡æ»¤å“ªäº›ä¿¡æ¯ï¼Œä½†æ˜¯ä½ å¯èƒ½äº‹å…ˆä¸çŸ¥é“ä½ çš„å‘é‡å­˜å‚¨å°†éœ€è¦å¤„ç†å“ªäº›æŸ¥è¯¢ã€‚
åœ¨æ¯ä¸ªå—ä¸­ç›´æ¥åŒ…å«å…¶ä»–ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä¾‹å¦‚æ ‡é¢˜ï¼Œå¯ä»¥å¸®åŠ©å¤„ç†ä»»æ„æŸ¥è¯¢ã€‚
Here's an example:
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
RetrievalQAChain
,
loadQAStuffChain
}
from
"langchain/chains"
;
import
{
CharacterTextSplitter
}
from
"langchain/text_splitter"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
HNSWLib
}
from
"langchain/vectorstores/hnswlib"
;
const
splitter
=
new
CharacterTextSplitter
(
{
chunkSize
:
1536
,
chunkOverlap
:
200
,
}
)
;
const
jimDocs
=
await
splitter
.
createDocuments
(
[
`
My favorite color is blue.
`
]
,
[
]
,
{
chunkHeader
:
`
DOCUMENT NAME: Jim Interview\n\n---\n\n
`
,
appendChunkOverlapHeader
:
true
,
}
)
;
const
pamDocs
=
await
splitter
.
createDocuments
(
[
`
My favorite color is red.
`
]
,
[
]
,
{
chunkHeader
:
`
DOCUMENT NAME: Pam Interview\n\n---\n\n
`
,
appendChunkOverlapHeader
:
true
,
}
)
;
const
vectorStore
=
await
HNSWLib
.
fromDocuments
(
jimDocs
.
concat
(
pamDocs
)
,
new
OpenAIEmbeddings
(
)
)
;
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
chain
=
new
RetrievalQAChain
(
{
combineDocumentsChain
:
loadQAStuffChain
(
model
)
,
retriever
:
vectorStore
.
asRetriever
(
)
,
returnSourceDocuments
:
true
,
}
)
;
const
res
=
await
chain
.
call
(
{
query
:
"What is Pam's favorite color?"
,
}
)
;
console
.
log
(
JSON
.
stringify
(
res
,
null
,
2
)
)
;
/*
{
"text": " Red.",
"sourceDocuments": [
{
"pageContent": "DOCUMENT NAME: Pam Interview\n\n---\n\nMy favorite color is red.",
"metadata": {
"loc": {
"lines": {
"from": 1,
"to": 1
}
}
}
},
{
"pageContent": "DOCUMENT NAME: Jim Interview\n\n---\n\nMy favorite color is blue.",
"metadata": {
"loc": {
"lines": {
"from": 1,
"to": 1
}
}
}
}
]
}
*/
;
All Text Splitters
â€‹
ğŸ—ƒï¸
ç¤ºä¾‹
4 items
é«˜çº§
â€‹
å¦‚æœä½ æƒ³è¦å®ç°è‡ªå·±çš„å®šåˆ¶æ–‡æœ¬åˆ†å‰²å™¨ï¼Œä½ åªéœ€è¦ç»§æ‰¿
TextSplitter
ç±»å¹¶ä¸”å®ç°ä¸€ä¸ªæ–¹æ³•
splitText
å³å¯ã€‚è¯¥æ–¹æ³•æ¥æ”¶ä¸€ä¸ªå­—ç¬¦ä¸²ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ã€‚è¿”å›çš„å­—ç¬¦ä¸²åˆ—è¡¨å°†è¢«ç”¨ä½œè¾“å…¥æ•°æ®çš„åˆ†å—ã€‚
abstract
class
TextSplitter
{
abstract
splitText
(
text
:
string
)
:
Promise
<
string
[
]
>
;
}



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\vector_stores.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/vector_stores/
crawled_at: 2025-06-22T02:00:23.712265
---

å…¥é—¨: çŸ¢é‡å­˜å‚¨
info
æ¦‚å¿µæŒ‡å—
çŸ¢é‡å­˜å‚¨æ˜¯ä¸€ç§ä¸“ä¸ºå­˜å‚¨æ–‡æ¡£åŠå…¶
åµŒå…¥
è€Œä¼˜åŒ–çš„æ•°æ®åº“ã€‚æ¥ç€ï¼Œå¯ä»¥è·å–ä¸æŸ¥è¯¢æœ€ç›¸å…³çš„æ–‡æ¡£ï¼Œå³åµŒå…¥ä¸æŸ¥è¯¢åµŒå…¥æœ€ç›¸ä¼¼çš„æ–‡æ¡£ã€‚
interface
VectorStore
{
/**
* Add more documents to an existing VectorStore
*/
addDocuments
(
documents
:
Document
[
]
)
:
Promise
<
void
>
;
/**
* Search for the most similar documents to a query
*/
similaritySearch
(
query
:
string
,
k
?
:
number
,
filter
?
:
object
|
undefined
)
:
Promise
<
Document
[
]
>
;
/**
* Search for the most similar documents to a query,
* and return their similarity score
*/
similaritySearchWithScore
(
query
:
string
,
k
=
4
,
filter
:
object
|
undefined
=
undefined
)
:
Promise
<
[
object
,
number
]
[
]
>
;
/**
* Turn a VectorStore into a Retriever
*/
asRetriever
(
k
?
:
number
)
:
BaseRetriever
;
/**
* Advanced: Add more documents to an existing VectorStore,
* when you already have their embeddings
*/
addVectors
(
vectors
:
number
[
]
[
]
,
documents
:
Document
[
]
)
:
Promise
<
void
>
;
/**
* Advanced: Search for the most similar documents to a query,
* when you already have the embedding of the query
*/
similaritySearchVectorWithScore
(
query
:
number
[
]
,
k
:
number
,
filter
?
:
object
)
:
Promise
<
[
Document
,
number
]
[
]
>
;
}
æ‚¨å¯ä»¥ä»
æ–‡æ¡£åˆ—è¡¨
æˆ–æ–‡æœ¬åˆ—è¡¨åŠå…¶ç›¸åº”çš„å…ƒæ•°æ®ä¸­åˆ›å»ºçŸ¢é‡å­˜å‚¨ã€‚æ‚¨è¿˜å¯ä»¥ä»ç°æœ‰ç´¢å¼•ä¸­åˆ›å»ºçŸ¢é‡å­˜å‚¨ï¼Œæ­¤æ–¹æ³•çš„ç­¾åå–å†³äºä½¿ç”¨çš„çŸ¢é‡å­˜å‚¨ï¼Œè¯·æŸ¥çœ‹æ‚¨æ„Ÿå…´è¶£çš„çŸ¢é‡å­˜å‚¨çš„æ–‡æ¡£ã€‚
abstract
class
BaseVectorStore
implements
VectorStore
{
static
fromTexts
(
texts
:
string
[
]
,
metadatas
:
object
[
]
|
object
,
embeddings
:
Embeddings
,
dbConfig
:
Record
<
string
,
any
>
)
:
Promise
<
VectorStore
>
;
static
fromDocuments
(
docs
:
Document
[
]
,
embeddings
:
Embeddings
,
dbConfig
:
Record
<
string
,
any
>
)
:
Promise
<
VectorStore
>
;
}
å¦‚ä½•é€‰æ‹©çŸ¢é‡å­˜å‚¨ï¼Ÿ
â€‹
ä»¥ä¸‹æ˜¯ä¸€ä¸ªå¿«é€ŸæŒ‡å—ï¼Œå¸®åŠ©æ‚¨é€‰æ‹©é€‚åˆæ‚¨çš„ç”¨ä¾‹çš„æ­£ç¡®çŸ¢é‡å­˜å‚¨:
å¦‚æœæ‚¨éœ€è¦ä¸€ç§åªèƒ½åœ¨Node.jsåº”ç”¨ç¨‹åºå†…éƒ¨è¿è¡Œçš„ä¸œè¥¿ï¼Œå³å†…å­˜ä¸­ï¼Œè€Œä¸éœ€è¦å…¶ä»–æœåŠ¡å™¨ï¼Œåˆ™é€‰æ‹©
HNSWLib
æˆ–
Faiss
å¦‚æœæ‚¨æ­£åœ¨å¯»æ‰¾ä¸€ç§å¯ä»¥åœ¨ç±»ä¼¼äºæµè§ˆå™¨çš„ç¯å¢ƒä¸­å†…å­˜ä¸­è¿è¡Œçš„ä¸œè¥¿ï¼Œåˆ™é€‰æ‹©
MemoryVectorStore
å¦‚æœæ‚¨æ¥è‡ªPythonï¼Œå¹¶ä¸”æ­£åœ¨å¯»æ‰¾ç±»ä¼¼äºFAISSçš„ä¸œè¥¿ï¼Œåˆ™é€‰æ‹©
HNSWLib
æˆ–
Faiss
If you're looking for an open-source full-featured vector database that you can run locally in a docker container, then go for
Chroma
å¦‚æœæ‚¨å·²ç»åœ¨ä½¿ç”¨Supabaseï¼Œåˆ™å¯ä»¥æŸ¥çœ‹
Supabase
å‘é‡å­˜å‚¨ï¼Œä»¥ä¾¿å¯¹æ‚¨çš„åµŒå…¥ä½¿ç”¨åŒä¸€çš„Postgresæ•°æ®åº“
å¦‚æœæ‚¨æ­£åœ¨å¯»æ‰¾ä¸€ç§ä¸ç”¨è‡ªè¡Œæ‰˜ç®¡çš„ç”Ÿäº§ä½¿ç”¨å‡†å¤‡å°±ç»ªçš„å‘é‡å­˜å‚¨,è¯·å°è¯•
Pinecone
å¦‚æœæ‚¨å·²ç»ä½¿ç”¨SingleStore,æˆ–è€…éœ€è¦åˆ†å¸ƒå¼ï¼Œé«˜æ€§èƒ½æ•°æ®åº“ï¼Œæ‚¨å¯èƒ½æƒ³è€ƒè™‘ä½¿ç”¨
SingleStore
å‘é‡å­˜å‚¨ã€‚
æ‰€æœ‰å‘é‡å­˜å‚¨
â€‹
ğŸ—ƒï¸
é›†æˆ
16 items



--- æ–‡ä»¶: output_20250622_020018\docs\modules\memory\examples.md ---
---
url: https://js.langchain.com.cn/docs/modules/memory/examples/
crawled_at: 2025-06-22T02:00:24.782757
---

ç¤ºä¾‹: å†…å­˜
ğŸ“„ï¸
buffer_memory
ç¼“å­˜å†…å­˜
ğŸ“„ï¸
ä½¿ç”¨ç¼“å†²å†…å­˜ä¸èŠå¤©æ¨¡å‹ç¿»è¯‘çš„ä¸­æ–‡ç»“æœ
æœ¬ç¤ºä¾‹ä»‹ç»å¦‚ä½•å°†èŠå¤©ç‰¹å®šçš„å†…å­˜ç±»ä¸èŠå¤©æ¨¡å‹é…åˆä½¿ç”¨ã€‚ç¿»è¯‘çš„ä¸­æ–‡ç»“æœ
ğŸ“„ï¸
Buffer Window Memory
BufferWindowMemoryç”¨äºè·Ÿè¸ªä¼šè¯ä¸­çš„æ¥å›æ¶ˆæ¯ï¼Œç„¶åä½¿ç”¨å¤§å°ä¸º k çš„çª—å£å°†æœ€è¿‘çš„ k æ¡æ¥å›æ¶ˆæ¯æå–å‡ºæ¥ä½œä¸ºå†…å­˜ã€‚
ğŸ“„ï¸
Conversation Summaryï¼ˆå¯¹è¯æ€»ç»“)
å¯¹è¯æ€»ç»“è®°å¿†ä¼šåœ¨å¯¹è¯è¿›è¡Œæ—¶å¯¹å…¶è¿›è¡Œæ€»ç»“å¹¶å‚¨å­˜åœ¨è®°å¿†ä¸­ã€‚è¿™ä¸ªè®°å¿†èƒ½å¤Ÿè¢«ç”¨äºå°†å½“å‰å¯¹è¯æ€»ç»“æ³¨å…¥åˆ°æç¤º/é“¾ä¸­ã€‚è¿™ä¸ªè®°å¿†åœ¨å¯¹è¾ƒé•¿çš„å¯¹è¯è¿›è¡Œæ€»ç»“æ—¶éå¸¸æœ‰ç”¨ï¼Œå› ä¸ºç›´æ¥åœ¨æç¤ºä¸­ä¿ç•™æ‰€æœ‰è¿‡å»çš„å¯¹è¯å†å²ä¿¡æ¯å°†ä¼šå ç”¨è¿‡å¤šçš„tokenã€‚
ğŸ“„ï¸
DynamoDBæ”¯æŒçš„èŠå¤©è®°å½•
å¦‚æœéœ€è¦åœ¨èŠå¤©è¿›ç¨‹ä¹‹é—´è¿›è¡Œæ›´é•¿æœŸçš„æŒä¹…åŒ–ï¼Œæ‚¨å¯ä»¥å°†é»˜è®¤çš„å†…å­˜chatHistoryæ›¿æ¢ä¸ºDynamoDBå®ä¾‹ï¼Œä½œä¸ºæ”¯æŒBufferMemoryç­‰èŠå¤©è®°å½•ç±»çš„åç«¯ã€‚ï¼Œæ³¨æ„ï¼šchatHistoryæŒ‡èŠå¤©è®°å½•ç±»ï¼ŒBufferMemoryæŒ‡ç¼“å­˜å­˜å‚¨å™¨ç±»ã€‚
ğŸ“„ï¸
å®ä½“è®°å¿†
å®ä½“è®°å¿†æ˜¯ä¼šè¯ä¸­è®°å¿†ç‰¹å®šå®ä½“çš„ç»™å®šäº‹å®ã€‚
ğŸ“„ï¸
Momentoæ”¯æŒçš„èŠå¤©è®°å½•
å¦‚æœè¦åœ¨èŠå¤©ä¼šè¯ä¸­ä½¿ç”¨åˆ†å¸ƒå¼ã€æ— æœåŠ¡å™¨çš„æŒä¹…æ€§,å¯ä»¥ä½¿ç”¨Momentoæ”¯æŒçš„èŠå¤©æ¶ˆæ¯å†å²è®°å½•ï¼Œå³åˆ»ç¼“å­˜ï¼Œæ— éœ€ä»»ä½•åŸºç¡€è®¾æ–½ç»´æŠ¤,æ— è®ºæ˜¯åœ¨æœ¬åœ°æ„å»ºè¿˜æ˜¯åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ½æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ã€‚
ğŸ“„ï¸
motorhead_memory
MotÃ¶rhead æ˜¯ä¸€ä¸ªç”±Rustå®ç°çš„å†…å­˜æœåŠ¡å™¨ã€‚å®ƒå¯ä»¥è‡ªåŠ¨å¤„ç†å¢é‡æ‘˜è¦å¹¶å…è®¸æ— çŠ¶æ€åº”ç”¨ç¨‹åºã€‚
ğŸ“„ï¸
åŸºäºRedisçš„èŠå¤©å­˜å‚¨
å¦‚æœéœ€è¦åœ¨èŠå¤©ä¼šè¯ä¹‹é—´è¿›è¡Œé•¿æœŸæŒä¹…åŒ–ï¼Œå¯ä»¥å°†é»˜è®¤çš„å†…å­˜chatHistoryæ›¿æ¢ä¸ºä¸€ä¸ªRediså®ä¾‹æ¥æ”¯æŒèŠå¤©å­˜å‚¨ç±»ï¼Œå¦‚BufferMemoryã€‚
ğŸ“„ï¸
Upstash åŸºäº Redis çš„èŠå¤©è®°å¿†
ç”±äº Upstash Redis é€šè¿‡ REST API å·¥ä½œ,æ‰€ä»¥æ‚¨å¯ä»¥å°†å…¶ä¸ Vercel Edgeï¼Œ [Cloudflare Workers] ä¸€èµ·ä½¿ç”¨ï¼ˆhttpsï¼š//developers.cloudflare.com/workers/)å’Œå…¶ä»–æ— æœåŠ¡å™¨ç¯å¢ƒã€‚
ğŸ“„ï¸
åŸºäºå‘é‡åº“çš„å†…å­˜
VectorStoreRetrieverMemoryå°†è®°å¿†å­˜å‚¨åœ¨å‘é‡æ•°æ®åº“ä¸­ï¼Œå¹¶åœ¨æ¯æ¬¡è°ƒç”¨æ—¶æŸ¥è¯¢æœ€â€œæ˜¾è‘—â€çš„å‰Kä¸ªæ–‡æ¡£ã€‚
ğŸ“„ï¸
Zep Memory
Zepæ˜¯å­˜å‚¨ã€ï¼Œæ€»ç»“ã€å†…åµŒã€ç´¢å¼•å’Œä¸°å¯Œå¯¹è¯AIèŠå¤©å†å²ã€è‡ªæ²»ä»£ç†å†å²ã€æ–‡æ¡£Q&Aå†å²çš„è®°å¿†æœåŠ¡å™¨ï¼Œå¹¶é€šè¿‡ç®€å•ã€ä½å»¶è¿Ÿçš„APIå…¬å¼€å®ƒä»¬ã€‚



--- æ–‡ä»¶: output_20250622_020018\docs\modules\models\chat.md ---
---
url: https://js.langchain.com.cn/docs/modules/models/chat/
crawled_at: 2025-06-22T02:00:25.278714
---

å…¥é—¨: èŠå¤©æ¨¡å‹
info
æ¦‚å¿µæŒ‡å—
LangChainæä¾›äº†ä¸€ä¸ªæ ‡å‡†æ¥å£æ¥ä½¿ç”¨èŠå¤©æ¨¡å‹ã€‚èŠå¤©æ¨¡å‹æ˜¯è¯­è¨€æ¨¡å‹çš„ä¸€ç§å˜ä½“ã€‚
è™½ç„¶èŠå¤©æ¨¡å‹åœ¨å†…éƒ¨ä½¿ç”¨è¯­è¨€æ¨¡å‹ï¼Œä½†å®ƒä»¬å…¬å¼€çš„æ¥å£æœ‰äº›ä¸åŒã€‚
é™¤äº†å…¬å¼€ä¸€ä¸ªâ€œè¾“å…¥æ–‡æœ¬ï¼Œè¾“å‡ºæ–‡æœ¬â€çš„APIå¤–ï¼Œå®ƒä»¬è¿˜å…¬å¼€äº†ä¸€ä¸ªâ€œèŠå¤©æ¶ˆæ¯â€ä½œä¸ºè¾“å…¥å’Œè¾“å‡ºçš„æ¥å£ã€‚
èŠå¤©æ¶ˆæ¯
â€‹
ä¸€ä¸ªâ€œèŠå¤©æ¶ˆæ¯â€æ˜¯æŒ‡èŠå¤©æ¨¡å‹ä¸­çš„æ¨¡å—åŒ–ä¿¡æ¯å•ä½ã€‚
ç›®å‰ï¼Œè¿™åŒ…æ‹¬ä¸€ä¸ªâ€œtextâ€å­—æ®µï¼Œå®ƒæŒ‡çš„æ˜¯èŠå¤©æ¶ˆæ¯çš„å†…å®¹ã€‚
ç›®å‰LangChainæ”¯æŒå››ç§ä¸åŒç±»å‹çš„â€œèŠå¤©æ¶ˆæ¯â€:
HumanChatMessage
: æ¨¡æ‹Ÿä¸€ä¸ªäººç±»çš„è§†è§’å‘é€çš„èŠå¤©æ¶ˆæ¯ã€‚
AIChatMessage
: ä»AIç³»ç»Ÿçš„è§’åº¦å‘é€çš„èŠå¤©æ¶ˆæ¯ï¼Œç”¨äºä¸äººç±»è¿›è¡Œé€šä¿¡ã€‚
SystemChatMessage
: ä¸€ç§ç”¨äºå‘AIç³»ç»Ÿæä¾›æœ‰å…³å¯¹è¯ä¿¡æ¯çš„èŠå¤©æ¶ˆæ¯ã€‚é€šå¸¸åœ¨å¯¹è¯å¼€å§‹æ—¶å‘é€ã€‚
ChatMessage
: ä¸€ä¸ªé€šç”¨çš„èŠå¤©æ¶ˆæ¯ï¼Œä¸ä»…æœ‰ä¸€ä¸ªâ€œæ–‡æœ¬â€å­—æ®µï¼Œè¿˜æœ‰ä¸€ä¸ªä»»æ„çš„â€œè§’è‰²â€å­—æ®µã€‚
è¦å¼€å§‹ä½¿ç”¨ï¼Œåªéœ€ä½¿ç”¨â€œLLMâ€å®ç°çš„â€œcallâ€æ–¹æ³•ï¼Œä¼ å…¥ä¸€ä¸ªå­—ç¬¦ä¸²è¾“å…¥ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯â€œChatOpenAIâ€å®ç°:
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
HumanChatMessage
}
from
"langchain/schema"
;
export
const
run
=
async
(
)
=>
{
const
chat
=
new
ChatOpenAI
(
)
;
// Pass in a list of messages to `call` to start a conversation. In this simple example, we only pass in one message.
const
response
=
await
chat
.
call
(
[
new
HumanChatMessage
(
"What is a good name for a company that makes colorful socks?"
)
,
]
)
;
console
.
log
(
response
)
;
// AIChatMessage { text: '\n\nRainbow Sox Co.' }
}
;
æ·±å…¥æŒ–æ˜
â€‹
ğŸ“„ï¸
é›†æˆ
LangChainæä¾›äº†è®¸å¤šä¸ä¸åŒæ¨¡å‹æä¾›è€…é›†æˆçš„èŠå¤©æ¨¡å‹å®ç°ã€‚å®ƒä»¬å¦‚ä¸‹æ‰€ç¤º:
ğŸ“„ï¸
é™„åŠ åŠŸèƒ½
æˆ‘ä»¬ä¸ºèŠå¤©æ¨¡å‹æä¾›äº†è®¸å¤šé™„åŠ åŠŸèƒ½ã€‚åœ¨ä»¥ä¸‹ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ ChatOpenAI æ¨¡å‹ã€‚



--- æ–‡ä»¶: output_20250622_020018\docs\modules\models\embeddings.md ---
---
url: https://js.langchain.com.cn/docs/modules/models/embeddings/
crawled_at: 2025-06-22T02:00:25.434538
---

å…¥é—¨: åµŒå…¥
info
æ¦‚å¿µæŒ‡å—
åµŒå…¥å¯ä»¥ç”¨æ¥åˆ›å»ºæ–‡æœ¬æ•°æ®çš„æ•°å­—è¡¨ç¤ºã€‚è¿™ç§æ•°å­—è¡¨ç¤ºå¾ˆæœ‰ç”¨ï¼Œå› ä¸ºå®ƒå¯ä»¥ç”¨æ¥æ‰¾åˆ°ç›¸ä¼¼çš„æ–‡æ¡£ã€‚
ä»¥ä¸‹æ˜¯å¦‚ä½•ä½¿ç”¨OpenAIåµŒå…¥çš„ç¤ºä¾‹ã€‚åµŒå…¥æœ‰æ—¶åœ¨æŸ¥è¯¢å’Œæ–‡æ¡£æ–¹é¢å…·æœ‰ä¸åŒçš„åµŒå…¥æ–¹æ³•ï¼Œå› æ­¤åµŒå…¥ç±»å…¬å¼€äº†
embedQuery
å’Œ
embedDocuments
æ–¹æ³•ã€‚
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
/* Create instance */
const
embeddings
=
new
OpenAIEmbeddings
(
)
;
/* Embed queries */
const
res
=
await
embeddings
.
embedQuery
(
"Hello world"
)
;
/*
[
-0.004845875,   0.004899438,  -0.016358767,  -0.024475135, -0.017341806,
0.012571548,  -0.019156644,   0.009036391,  -0.010227379, -0.026945334,
0.022861943,   0.010321903,  -0.023479493, -0.0066544134,  0.007977734,
0.0026371893,   0.025206111,  -0.012048521,   0.012943339,  0.013094575,
-0.010580265,  -0.003509951,   0.004070787,   0.008639394, -0.020631202,
-0.0019203906,   0.012161949,  -0.019194454,   0.030373365, -0.031028723,
0.0036170771,  -0.007813894, -0.0060778237,  -0.017820721, 0.0048647798,
-0.015640393,   0.001373733,  -0.015552171,   0.019534737, -0.016169721,
0.007316074,   0.008273906,   0.011418369,   -0.01390117, -0.033347685,
0.011248227,  0.0042503807,  -0.012792102, -0.0014595914,  0.028356876,
0.025407761, 0.00076445413,  -0.016308354,   0.017455231, -0.016396577,
0.008557475,   -0.03312083,   0.031104341,   0.032389853,  -0.02132437,
0.003324056,  0.0055610985, -0.0078012915,   0.006090427, 0.0062038545,
0.0169133,  0.0036391325,  0.0076815626,  -0.018841568,  0.026037913,
0.024550753,  0.0055264398, -0.0015824712, -0.0047765584,  0.018425668,
0.0030656934, -0.0113742575, -0.0020322427,   0.005069579, 0.0022701253,
0.036095154,  -0.027449455,  -0.008475555,   0.015388331,  0.018917186,
0.0018999106,  -0.003349262,   0.020895867,  -0.014480911, -0.025042271,
0.012546342,   0.013850759,  0.0069253794,   0.008588983, -0.015199285,
-0.0029585673,  -0.008759124,   0.016749462,   0.004111747,  -0.04804285,
... 1436 more items
]
*/
/* Embed documents */
const
documentRes
=
await
embeddings
.
embedDocuments
(
[
"Hello world"
,
"Bye bye"
]
)
;
/*
[
[
-0.0047852774,  0.0048640342,   -0.01645707,  -0.024395779, -0.017263541,
0.012512918,  -0.019191515,   0.009053908,  -0.010213212, -0.026890801,
0.022883644,   0.010251015,  -0.023589306,  -0.006584088,  0.007989113,
0.002720268,   0.025088841,  -0.012153786,   0.012928754,  0.013054766,
-0.010395928, -0.0035566676,  0.0040008575,   0.008600268, -0.020678446,
-0.0019106456,   0.012178987,  -0.019241918,   0.030444318,  -0.03102397,
0.0035692686,  -0.007749692,   -0.00604854,   -0.01781799,  0.004860884,
-0.015612794,  0.0014097509,  -0.015637996,   0.019443536,  -0.01612944,
0.0072960514,   0.008316742,   0.011548932,  -0.013987249,  -0.03336778,
0.011341013,    0.00425603, -0.0126578305, -0.0013861238,  0.028302127,
0.025466874,  0.0007029065,  -0.016318457,   0.017427357, -0.016394064,
0.008499459,  -0.033241767,   0.031200387,    0.03238489,   -0.0212833,
0.0032416396,   0.005443686,  -0.007749692,  0.0060201874,  0.006281661,
0.016923312,   0.003528315,  0.0076740854,   -0.01881348,  0.026109532,
0.024660403,   0.005472039, -0.0016712243, -0.0048136297,  0.018397642,
0.003011669,  -0.011385117, -0.0020193304,   0.005138109, 0.0022335495,
0.03603922,  -0.027495656,  -0.008575066,   0.015436378,  0.018851284,
0.0018019609, -0.0034338066,    0.02094307,  -0.014503895, -0.024950229,
0.012632628,   0.013735226,  0.0069936244,   0.008575066, -0.015196957,
-0.0030541976,  -0.008745181,   0.016746895,  0.0040481114, -0.048010286,
... 1436 more items
],
[
-0.009446913,  -0.013253193,   0.013174579,  0.0057552797,  -0.038993083,
0.0077763423,    -0.0260478, -0.0114384955, -0.0022683728,  -0.016509168,
0.041797023,    0.01787183,    0.00552271, -0.0049789557,   0.018146982,
-0.01542166,   0.033752076,   0.006112323,   0.023872782,  -0.016535373,
-0.006623321,   0.016116094, -0.0061090477, -0.0044155475,  -0.016627092,
-0.022077737, -0.0009286407,   -0.02156674,   0.011890532,  -0.026283644,
0.02630985,   0.011942943,  -0.026126415,  -0.018264906,  -0.014045896,
-0.024187243,  -0.019037955,  -0.005037917,   0.020780588, -0.0049527506,
0.002399398,   0.020767486,  0.0080908025,  -0.019666875,  -0.027934562,
0.017688395,   0.015225122,  0.0046186363, -0.0045007137,   0.024265857,
0.03244183,  0.0038848957,   -0.03244183,  -0.018893827, -0.0018065092,
0.023440398,  -0.021763276,   0.015120302,   -0.01568371,  -0.010861984,
0.011739853,  -0.024501702,  -0.005214801,   0.022955606,   0.001315165,
-0.00492327,  0.0020358032,  -0.003468891,  -0.031079166,  0.0055259857,
0.0028547104,   0.012087069,   0.007992534, -0.0076256637,   0.008110457,
0.002998838,  -0.024265857,   0.006977089,  -0.015185814, -0.0069115767,
0.006466091,  -0.029428247,  -0.036241557,   0.036713246,   0.032284595,
-0.0021144184,  -0.014255536,   0.011228855,  -0.027227025,  -0.021619149,
0.00038242966,    0.02245771, -0.0014748519,    0.01573612,  0.0041010873,
0.006256451,  -0.007992534,   0.038547598,   0.024658933,  -0.012958387,
... 1436 more items
]
]
*/
æ›´æ·±å…¥åœ°äº†è§£
â€‹
ğŸ“„ï¸
é›†æˆ
LangChainæä¾›äº†è®¸å¤šä¸å„ç§æ¨¡å‹æä¾›å•†é›†æˆçš„åµŒå…¥å®ç°ã€‚è¿™äº›æ˜¯:
ğŸ“„ï¸
é™„åŠ åŠŸèƒ½
æˆ‘ä»¬ä¸ºèŠå¤©æ¨¡å‹æä¾›äº†è®¸å¤šé™„åŠ åŠŸèƒ½ã€‚åœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨â€œOpenAIåµŒå…¥â€æ¨¡å‹ã€‚



--- æ–‡ä»¶: output_20250622_020018\docs\modules\models\llms.md ---
---
url: https://js.langchain.com.cn/docs/modules/models/llms/
crawled_at: 2025-06-22T02:00:25.555119
---

å…¥é—¨æŒ‡å—: LLMs
info
æ¦‚å¿µæŒ‡å—
LangChain æä¾›äº†ä½¿ç”¨å„ç§ LLM çš„æ ‡å‡†ç•Œé¢ã€‚
è¦å¼€å§‹ï¼Œ åªéœ€ä½¿ç”¨
LLM
å®ç°çš„
call
æ–¹æ³•ï¼Œ ä¼ é€’ä¸€ä¸ª
string
è¾“å…¥ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œ æˆ‘ä»¬ä½¿ç”¨äº†
OpenAI
å®ç°:
import
{
OpenAI
}
from
"langchain/llms/openai"
;
export
const
run
=
async
(
)
=>
{
const
model
=
new
OpenAI
(
)
;
// `call` is a simple string-in, string-out method for interacting with the model.
const
resA
=
await
model
.
call
(
"What would be a good company name a company that makes colorful socks?"
)
;
console
.
log
(
{
resA
}
)
;
// { resA: '\n\nSocktastic Colors' }
}
;
æ·±å…¥ç ”ç©¶
â€‹
ğŸ“„ï¸
é›†æˆ
LangChainæä¾›äº†å¤šç§LLMå®ç°ï¼Œå¯ä¸å„ç§æ¨¡å‹æä¾›è€…é›†æˆã€‚è¿™äº›æ˜¯:
ğŸ“„ï¸
é™„åŠ åŠŸèƒ½
æˆ‘ä»¬ä¸ºLLMæä¾›äº†è®¸å¤šé™„åŠ åŠŸèƒ½ã€‚åœ¨ä¸‹é¢çš„å¤§å¤šæ•°ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ OpenAI LLMã€‚ç„¶è€Œï¼Œæ‰€æœ‰è¿™äº›åŠŸèƒ½éƒ½é€‚ç”¨äºæ‰€æœ‰LLMsã€‚



--- æ–‡ä»¶: output_20250622_020018\docs\modules\prompts\example_selectors.md ---
---
url: https://js.langchain.com.cn/docs/modules/prompts/example_selectors/
crawled_at: 2025-06-22T02:00:25.716892
---

ç¤ºä¾‹é€‰æ‹©å™¨
info
æ¦‚å¿µæŒ‡å—
å¦‚æœæ‚¨æœ‰å¤§é‡çš„ç¤ºä¾‹,æ‚¨å¯èƒ½éœ€è¦ä»¥ç¼–ç¨‹æ–¹å¼é€‰æ‹©è¦åŒ…å«åœ¨æç¤ºä¸­çš„ç¤ºä¾‹ã€‚ ExampleSelector æ˜¯æ‰§è¡Œæ­¤æ“ä½œçš„ç±»ã€‚ åŸºæœ¬æ¥å£å®šä¹‰å¦‚ä¸‹ã€‚
class
BaseExampleSelector
{
addExample
(
example
:
Example
)
:
Promise
<
void
|
string
>
;
selectExamples
(
input_variables
:
Example
)
:
Promise
<
Example
[
]
>
;
}
å®ƒéœ€è¦å…¬å¼€ä¸€ä¸ª
selectExamples
æ–¹æ³• - è¿™éœ€è¦è¾“å…¥å˜é‡ï¼Œç„¶åè¿”å›ä¸€ä¸ªç¤ºä¾‹åˆ—è¡¨ - å’Œä¸€ä¸ª
addExample
æ–¹æ³•,ç”¨äºä¿å­˜ä»¥åé€‰æ‹©çš„ç¤ºä¾‹ã€‚æ¯ä¸ªå…·ä½“çš„å®ç°éƒ½å¯ä»¥å†³å®šå¦‚ä½•ä¿å­˜å’Œé€‰æ‹©è¿™äº›ç¤ºä¾‹ã€‚ è®©æˆ‘ä»¬çœ‹ä¸€äº›ç¤ºä¾‹ã€‚
æ ¹æ®é•¿åº¦é€‰æ‹©
â€‹
æ­¤
ExampleSelector
æ ¹æ®é•¿åº¦é€‰æ‹©è¦ä½¿ç”¨çš„ç¤ºä¾‹ã€‚ å½“æ‚¨æ‹…å¿ƒæ„å»ºçš„æç¤ºä¼šè¶…è¿‡ä¸Šä¸‹æ–‡çª—å£çš„é•¿åº¦æ—¶ï¼Œè¿™éå¸¸æœ‰ç”¨ã€‚ å¯¹äºè¾ƒé•¿çš„è¾“å…¥,å®ƒä¼šé€‰æ‹©è¾ƒå°‘çš„ç¤ºä¾‹è¿›è¡ŒåŒ…å«,è€Œå¯¹äºè¾ƒçŸ­çš„è¾“å…¥ï¼Œåˆ™ä¼šé€‰æ‹©æ›´å¤šç¤ºä¾‹ã€‚
import
{
LengthBasedExampleSelector
,
PromptTemplate
,
FewShotPromptTemplate
,
}
from
"langchain/prompts"
;
export
async
function
run
(
)
{
// Create a prompt template that will be used to format the examples.
const
examplePrompt
=
new
PromptTemplate
(
{
inputVariables
:
[
"input"
,
"output"
]
,
template
:
"Input: {input}\nOutput: {output}"
,
}
)
;
// Create a LengthBasedExampleSelector that will be used to select the examples.
const
exampleSelector
=
await
LengthBasedExampleSelector
.
fromExamples
(
[
{
input
:
"happy"
,
output
:
"sad"
}
,
{
input
:
"tall"
,
output
:
"short"
}
,
{
input
:
"energetic"
,
output
:
"lethargic"
}
,
{
input
:
"sunny"
,
output
:
"gloomy"
}
,
{
input
:
"windy"
,
output
:
"calm"
}
,
]
,
{
examplePrompt
,
maxLength
:
25
,
}
)
;
// Create a FewShotPromptTemplate that will use the example selector.
const
dynamicPrompt
=
new
FewShotPromptTemplate
(
{
// We provide an ExampleSelector instead of examples.
exampleSelector
,
examplePrompt
,
prefix
:
"Give the antonym of every input"
,
suffix
:
"Input: {adjective}\nOutput:"
,
inputVariables
:
[
"adjective"
]
,
}
)
;
// An example with small input, so it selects all examples.
console
.
log
(
await
dynamicPrompt
.
format
(
{
adjective
:
"big"
}
)
)
;
/*
Give the antonym of every input
Input: happy
Output: sad
Input: tall
Output: short
Input: energetic
Output: lethargic
Input: sunny
Output: gloomy
Input: windy
Output: calm
Input: big
Output:
*/
// An example with long input, so it selects only one example.
const
longString
=
"big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else"
;
console
.
log
(
await
dynamicPrompt
.
format
(
{
adjective
:
longString
}
)
)
;
/*
Give the antonym of every input
Input: happy
Output: sad
Input: big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else
Output:
*/
}
æ ¹æ®ç›¸ä¼¼åº¦é€‰æ‹©
â€‹
SemanticSimilarityExampleSelector
æ ¹æ®ä¸è¾“å…¥æœ€ç›¸ä¼¼çš„ç¤ºä¾‹é€‰æ‹©ç¤ºä¾‹ã€‚ å®ƒé€šè¿‡æŸ¥æ‰¾å…·æœ‰ä¸è¾“å…¥çš„ä½™å¼¦ç›¸ä¼¼åº¦æœ€å¤§çš„åµŒå…¥çš„ç¤ºä¾‹æ¥å®ç°æ­¤ç›®çš„ã€‚
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
SemanticSimilarityExampleSelector
,
PromptTemplate
,
FewShotPromptTemplate
,
}
from
"langchain/prompts"
;
import
{
HNSWLib
}
from
"langchain/vectorstores/hnswlib"
;
export
async
function
run
(
)
{
// Create a prompt template that will be used to format the examples.
const
examplePrompt
=
new
PromptTemplate
(
{
inputVariables
:
[
"input"
,
"output"
]
,
template
:
"Input: {input}\nOutput: {output}"
,
}
)
;
// Create a SemanticSimilarityExampleSelector that will be used to select the examples.
const
exampleSelector
=
await
SemanticSimilarityExampleSelector
.
fromExamples
(
[
{
input
:
"happy"
,
output
:
"sad"
}
,
{
input
:
"tall"
,
output
:
"short"
}
,
{
input
:
"energetic"
,
output
:
"lethargic"
}
,
{
input
:
"sunny"
,
output
:
"gloomy"
}
,
{
input
:
"windy"
,
output
:
"calm"
}
,
]
,
new
OpenAIEmbeddings
(
)
,
HNSWLib
,
{
k
:
1
}
)
;
// Create a FewShotPromptTemplate that will use the example selector.
const
dynamicPrompt
=
new
FewShotPromptTemplate
(
{
// We provide an ExampleSelector instead of examples.
exampleSelector
,
examplePrompt
,
prefix
:
"Give the antonym of every input"
,
suffix
:
"Input: {adjective}\nOutput:"
,
inputVariables
:
[
"adjective"
]
,
}
)
;
// Input is about the weather, so should select eg. the sunny/gloomy example
console
.
log
(
await
dynamicPrompt
.
format
(
{
adjective
:
"rainy"
}
)
)
;
/*
Give the antonym of every input
Input: sunny
Output: gloomy
Input: rainy
Output:
*/
// Input is a measurement, so should select the tall/short example
console
.
log
(
await
dynamicPrompt
.
format
(
{
adjective
:
"large"
}
)
)
;
/*
Give the antonym of every input
Input: tall
Output: short
Input: large
Output:
*/
}



--- æ–‡ä»¶: output_20250622_020018\docs\modules\prompts\prompt_templates.md ---
---
url: https://js.langchain.com.cn/docs/modules/prompts/prompt_templates/
crawled_at: 2025-06-22T02:00:25.767047
---

æç¤ºæ¨¡æ¿
info
æ¦‚å¿µæŒ‡å—
PromptTemplate
å…è®¸æ‚¨ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆæç¤ºã€‚å½“æ‚¨æƒ³åœ¨å¤šä¸ªåœ°æ–¹ä½¿ç”¨ç›¸åŒçš„æç¤ºæ¦‚è¦ï¼Œä½†æ›´æ”¹æŸäº›å€¼æ—¶ï¼Œè¿™éå¸¸æœ‰ç”¨ã€‚
å¦‚ä¸‹æ‰€ç¤ºï¼Œ
PromptTemplate
å¯¹LLMå’ŒèŠå¤©æ¨¡å‹éƒ½æœ‰æ”¯æŒ:
import
{
ChatPromptTemplate
,
HumanMessagePromptTemplate
,
PromptTemplate
,
SystemMessagePromptTemplate
,
}
from
"langchain/prompts"
;
export
const
run
=
async
(
)
=>
{
// A `PromptTemplate` consists of a template string and a list of input variables.
const
template
=
"What is a good name for a company that makes {product}?"
;
const
promptA
=
new
PromptTemplate
(
{
template
,
inputVariables
:
[
"product"
]
}
)
;
// We can use the `format` method to format the template with the given input values.
const
responseA
=
await
promptA
.
format
(
{
product
:
"colorful socks"
}
)
;
console
.
log
(
{
responseA
}
)
;
/*
{
responseA: 'What is a good name for a company that makes colorful socks?'
}
*/
// We can also use the `fromTemplate` method to create a `PromptTemplate` object.
const
promptB
=
PromptTemplate
.
fromTemplate
(
"What is a good name for a company that makes {product}?"
)
;
const
responseB
=
await
promptB
.
format
(
{
product
:
"colorful socks"
}
)
;
console
.
log
(
{
responseB
}
)
;
/*
{
responseB: 'What is a good name for a company that makes colorful socks?'
}
*/
// For chat models, we provide a `ChatPromptTemplate` class that can be used to format chat prompts.
const
chatPrompt
=
ChatPromptTemplate
.
fromPromptMessages
(
[
SystemMessagePromptTemplate
.
fromTemplate
(
"You are a helpful assistant that translates {input_language} to {output_language}."
)
,
HumanMessagePromptTemplate
.
fromTemplate
(
"{text}"
)
,
]
)
;
// The result can be formatted as a string using the `format` method.
const
responseC
=
await
chatPrompt
.
format
(
{
input_language
:
"English"
,
output_language
:
"French"
,
text
:
"I love programming."
,
}
)
;
console
.
log
(
{
responseC
}
)
;
/*
{
responseC: '[{"text":"You are a helpful assistant that translates English to French."},{"text":"I love programming."}]'
}
*/
// The result can also be formatted as a list of `ChatMessage` objects by returning a `PromptValue` object and calling the `toChatMessages` method.
// More on this below.
const
responseD
=
await
chatPrompt
.
formatPromptValue
(
{
input_language
:
"English"
,
output_language
:
"French"
,
text
:
"I love programming."
,
}
)
;
const
messages
=
responseD
.
toChatMessages
(
)
;
console
.
log
(
{
messages
}
)
;
/*
{
messages: [
SystemChatMessage {
text: 'You are a helpful assistant that translates English to French.'
},
HumanChatMessage { text: 'I love programming.' }
]
}
*/
}
;
æ·±å…¥äº†è§£
â€‹
ğŸ“„ï¸
æç¤ºç»„åˆ
æµæ°´çº¿æç¤ºæ¨¡æ¿å…è®¸æ‚¨å°†å¤šä¸ªå•ç‹¬çš„æç¤ºæ¨¡æ¿ç»„åˆåœ¨ä¸€èµ·ã€‚
ğŸ“„ï¸
é¢å¤–åŠŸèƒ½ï¼ˆAdditional Functionality)
æˆ‘ä»¬æä¾›äº†ä¸€äº›é¢å¤–çš„åŠŸèƒ½ï¼Œä»¥ä¾¿åœ¨æç¤ºæ¨¡æ¿ä¸­å±•ç¤ºï¼Œå¦‚ä¸‹æ‰€ç¤º:ï¼ˆWe offer a number of extra features for prompt templates as shown below)



--- æ–‡ä»¶: output_20250622_020018\docs\modules\schema\chat-messages.md ---
---
url: https://js.langchain.com.cn/docs/modules/schema/chat-messages
crawled_at: 2025-06-22T02:00:25.902882
---

èŠå¤©æ¶ˆæ¯
ç»ˆç«¯ç”¨æˆ·ä¸LLMsäº’åŠ¨çš„ä¸»è¦ç•Œé¢æ˜¯èŠå¤©ç•Œé¢ã€‚å› æ­¤ï¼Œä¸€äº›æ¨¡å‹æä¾›å•†å·²ç»å¼€å§‹ä»¥æœŸæœ›èŠå¤©æ¶ˆæ¯çš„æ–¹å¼æä¾›å¯¹åº•å±‚APIçš„è®¿é—®ã€‚è¿™äº›æ¶ˆæ¯å…·æœ‰å†…å®¹å­—æ®µï¼ˆé€šå¸¸æ˜¯æ–‡æœ¬)ï¼Œå¹¶ä¸ç”¨æˆ·ï¼ˆæˆ–è§’è‰²)ç›¸å…³è”ã€‚å½“å‰æ”¯æŒçš„ç”¨æˆ·æœ‰Systemï¼Œ Humanï¼Œå’ŒAIã€‚
SystemChatMessage
â€‹
è¡¨ç¤ºåº”ä¸ºAIç³»ç»Ÿæä¾›è¯´æ˜çš„èŠå¤©æ¶ˆæ¯ã€‚
import { SystemChatMessage } from "langchain/schema";
new SystemChatMessage("You are a nice assistant");
HumanChatMessage
â€‹
è¡¨ç¤ºæ¥è‡ªä¸AIç³»ç»Ÿäº¤äº’çš„äººçš„ä¿¡æ¯çš„èŠå¤©æ¶ˆæ¯ã€‚
import { HumanChatMessage } from "langchain/schema";
new HumanChatMessage("Hello, how are you?");
AIChatMessage
â€‹
è¡¨ç¤ºæ¥è‡ªAIç³»ç»Ÿçš„æ¶ˆæ¯çš„èŠå¤©æ¶ˆæ¯ã€‚
import
{
AIChatMessage
}
from
"langchain/schema"
;
new
AIChatMessage
(
"I am doing well, thank you!"
)
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\schema\document.md ---
---
url: https://js.langchain.com.cn/docs/modules/schema/document
crawled_at: 2025-06-22T02:00:25.946681
---

æ–‡æ¡£
è¯­è¨€æ¨¡å‹åªçŸ¥é“å®ƒä»¬æ‰€è®­ç»ƒçš„å†…å®¹çš„ä¿¡æ¯ã€‚ä¸ºäº†è®©å®ƒä»¬èƒ½å¤Ÿå›ç­”é—®é¢˜æˆ–æ€»ç»“å…¶ä»–ä¿¡æ¯ï¼Œä½ éœ€è¦å°†ä¿¡æ¯ä¼ é€’ç»™è¯­è¨€æ¨¡å‹ã€‚å› æ­¤ï¼Œæ‹¥æœ‰æ–‡æ¡£çš„æ¦‚å¿µéå¸¸é‡è¦ã€‚
æ–‡æ¡£æœ¬è´¨ä¸Šéå¸¸ç®€å•ã€‚å®ƒç”±ä¸€æ®µæ–‡æœ¬å’Œå¯é€‰çš„å…ƒæ•°æ®ç»„æˆã€‚æ–‡æœ¬æ˜¯æˆ‘ä»¬ä¸è¯­è¨€æ¨¡å‹äº¤äº’çš„éƒ¨åˆ†ï¼Œè€Œå¯é€‰çš„å…ƒæ•°æ®å¯¹äºè·Ÿè¸ªæ–‡æ¡£çš„å…ƒæ•°æ®ï¼ˆä¾‹å¦‚æ¥æº)éå¸¸æœ‰ç”¨ã€‚
interface
Document
{
pageContent
:
string
;
metadata
:
Record
<
string
,
any
>
;
}
åˆ›å»ºæ–‡æ¡£
â€‹
ä½ å¯ä»¥åœ¨LangChainä¸­å¾ˆå®¹æ˜“åœ°åˆ›å»ºä¸€ä¸ªæ–‡æ¡£å¯¹è±¡ä¸
import
{
Document
}
from
"langchain/document"
;
const
doc
=
new
Document
(
{
pageContent
:
"foo"
}
)
;
ä½ å¯ä»¥ä½¿ç”¨ åˆ›å»ºå¸¦æœ‰å…ƒæ•°æ®çš„æ–‡æ¡£
import
{
Document
}
from
"langchain/document"
;
const
doc
=
new
Document
(
{
pageContent
:
"foo"
,
metadata
:
{
source
:
"1"
}
}
)
;
åŒæ—¶è¿˜å¯ä»¥æŸ¥çœ‹
æ–‡æ¡£åŠ è½½å™¨ï¼ˆDocument Loadersï¼‰
ï¼Œä»¥äº†è§£ä»å„ç§æ¥æºåŠ è½½æ–‡æ¡£çš„æ–¹æ³•ã€‚



--- æ–‡ä»¶: output_20250622_020018\docs\modules\schema\example.md ---
---
url: https://js.langchain.com.cn/docs/modules/schema/example
crawled_at: 2025-06-22T02:00:25.959830
---

ç¤ºä¾‹
ç¤ºä¾‹æ˜¯è¾“å…¥/è¾“å‡ºå¯¹ï¼Œè¡¨ç¤ºå¯¹å‡½æ•°çš„è¾“å…¥å’Œé¢„æœŸè¾“å‡ºã€‚å®ƒä»¬å¯ç”¨äºæ¨¡å‹çš„è®­ç»ƒå’Œè¯„ä¼°ã€‚
type
Example
=
Record
<
string
,
string
>
;
åˆ›å»ºç¤ºä¾‹
â€‹
æ‚¨å¯ä»¥è¿™æ ·åˆ›å»ºç¤ºä¾‹:
const
example
=
{
input
:
"foo"
,
output
:
"bar"
,
}
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\agents\agents\action.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/agents/action/
crawled_at: 2025-06-22T02:00:19.897433
---

åŠ¨ä½œä»£ç† Action Agents
ğŸ“„ï¸
LLMsçš„MRKLä»£ç†
æœ¬ç¤ºä¾‹æ¶µç›–äº†å¦‚ä½•ä½¿ç”¨ä½¿ç”¨ReAct Frameworkçš„ä»£ç†äººï¼ˆåŸºäºå·¥å…·çš„æè¿°ï¼‰æ¥å†³å®šé‡‡å–ä»€ä¹ˆè¡ŒåŠ¨ã€‚
ğŸ“„ï¸
MRKLèŠå¤©æ¨¡å‹ä»£ç†
è¿™ä¸ªä¾‹å­ä»‹ç»äº†å¦‚ä½•ä½¿ç”¨ä¸€ä¸ªä½¿ç”¨ReActæ¡†æ¶(åŸºäºå·¥å…·æè¿°)æ¥å†³å®šé‡‡å–ä»€ä¹ˆè¡ŒåŠ¨çš„ä»£ç†ã€‚è¯¥ä»£ç†è¢«ä¼˜åŒ–ä¸ºåœ¨èŠå¤©æ¨¡å‹ä¸­ä½¿ç”¨ã€‚å¦‚æœæ‚¨æƒ³åœ¨LLMä¸­ä½¿ç”¨å®ƒï¼Œæ‚¨å¯ä»¥ä½¿ç”¨LLM MRKLä»£ç†ã€‚
ğŸ“„ï¸
ä¼šè¯ä»£ç†
æœ¬ç¤ºä¾‹ä»‹ç»å¦‚ä½•ä¸ºèŠå¤©æ¨¡å‹åˆ›å»ºä¼šè¯ä»£ç†ã€‚å®ƒå°†åˆ©ç”¨èŠå¤©ç‰¹å®šçš„æç¤ºã€‚
ğŸ“„ï¸
ç»“æ„åŒ–å·¥å…·èŠå¤©ä»£ç†
ç»“æ„åŒ–å·¥å…·èŠå¤©ä»£ç†æ˜¯ä¸“ä¸ºä¸ç¬¦åˆä»»æ„å¯¹è±¡æ¨¡å¼çš„è¾“å…¥æ•°æ®çš„å·¥å…·é…åˆä½¿ç”¨è®¾è®¡çš„,ç›¸æ¯”å…¶ä»–ä»…æ”¯æŒæ¥å—å•ä¸ªå­—ç¬¦ä¸²ä½œä¸ºè¾“å…¥çš„ä»£ç†ï¼Œå®ƒä»¬å…·æœ‰æ›´é«˜çš„çµæ´»æ€§ã€‚



--- æ–‡ä»¶: output_20250622_020018\docs\modules\agents\agents\custom.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/agents/custom/
crawled_at: 2025-06-22T02:00:20.033604
---

è‡ªå®šä¹‰ä»£ç†
è¯·æŸ¥çœ‹ä¸‹é¢çš„ç¤ºä¾‹ï¼Œäº†è§£å¦‚ä½•åˆ›å»ºè‡ªå®šä¹‰ä»£ç†ã€‚
ç¬¬ä¸€ä¸ªç¤ºä¾‹ä»…ä½¿ç”¨è‡ªå®šä¹‰æç¤ºå‰ç¼€å’Œåç¼€ï¼Œè¿™æ›´å®¹æ˜“å…¥æ‰‹ã€‚å¦å¤–ä¸¤ä¸ªä½¿ç”¨å®Œå…¨è‡ªå®šä¹‰çš„æç¤ºå’Œè¾“å‡ºè§£æå™¨ã€‚
ğŸ“„ï¸
è‡ªå®šä¹‰èŠå¤©æç¤º
æœ¬ç¤ºä¾‹ä»‹ç»å¦‚ä½•ä¸ºèŠå¤©æ¨¡å‹ä»£ç†åˆ›å»ºè‡ªå®šä¹‰æç¤ºã€‚
ğŸ“„ï¸
è‡ªå®šä¹‰èŠå¤©ä»£ç†
æœ¬ç¤ºä¾‹ä»‹ç»å¦‚ä½•ä½¿ç”¨èŠå¤©æ¨¡å‹åˆ›å»ºè‡ªå®šä¹‰ä»£ç†ã€‚
ğŸ“„ï¸
è‡ªå®šä¹‰LLMä»£ç†
æœ¬ç¤ºä¾‹ä»‹ç»å¦‚ä½•åˆ›å»ºç”±LLMé©±åŠ¨çš„è‡ªå®šä¹‰ä»£ç†ã€‚



--- æ–‡ä»¶: output_20250622_020018\docs\modules\agents\agents\plan_execute.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/agents/plan_execute/
crawled_at: 2025-06-22T02:00:20.361411
---

è®¡åˆ’æ‰§è¡Œä»£ç†
è¿™ä¸ªä¾‹å­å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ä¸€ä¸ªä½¿ç”¨è®¡åˆ’æ‰§è¡Œæ¡†æ¶æ¥å›ç­”æŸ¥è¯¢çš„ä»£ç†ã€‚
è¯¥æ¡†æ¶ä¸å…¶ä»–ç›®å‰æ”¯æŒçš„ä»£ç†ï¼ˆå…¨éƒ¨è¢«å½’ç±»ä¸ºè¡ŒåŠ¨ä»£ç†)çš„å·¥ä½œæ–¹å¼ä¸åŒï¼Œå› ä¸ºå®ƒä½¿ç”¨äº†ä¸€ä¸ªä¸¤æ­¥è¿‡ç¨‹:ã€‚
é¦–å…ˆï¼Œä»£ç†ä½¿ç”¨ LLM åˆ›å»ºä¸€ä¸ªå¸¦æœ‰æ˜ç¡®æ­¥éª¤çš„è®¡åˆ’æ¥å›ç­”æŸ¥è¯¢ã€‚
ä¸€æ—¦å®ƒæœ‰äº†è®¡åˆ’ï¼Œå®ƒå°±ä½¿ç”¨åµŒå…¥å¼ä¼ ç»Ÿè¡ŒåŠ¨ä»£ç†æ¥è§£å†³æ¯ä¸€æ­¥ã€‚
è¿™ä¸ªæƒ³æ³•æ˜¯è®¡åˆ’æ­¥éª¤é€šè¿‡å°†ä¸€ä¸ªè¾ƒå¤§çš„ä»»åŠ¡åˆ†è§£æˆæ›´ç®€å•çš„å­ä»»åŠ¡ï¼Œä½¿ LLM ä¿æŒæ›´â€œåœ¨è½¨é“â€ä¸Šã€‚
ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•éœ€è¦æ›´å¤šçš„å•ç‹¬ LLM æŸ¥è¯¢ï¼Œå¹¶ä¸”ä¸è¡ŒåŠ¨ä»£ç†ç›¸æ¯”å…·æœ‰æ›´é«˜çš„å»¶è¿Ÿã€‚
æ³¨
: è¿™ä¸ªä»£ç†ç›®å‰åªæ”¯æŒèŠå¤©æ¨¡å‹ã€‚
import
{
Calculator
}
from
"langchain/tools/calculator"
;
import
{
SerpAPI
}
from
"langchain/tools"
;
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
PlanAndExecuteAgentExecutor
}
from
"langchain/experimental/plan_and_execute"
;
const
tools
=
[
new
Calculator
(
)
,
new
SerpAPI
(
)
]
;
const
model
=
new
ChatOpenAI
(
{
temperature
:
0
,
modelName
:
"gpt-3.5-turbo"
,
verbose
:
true
,
}
)
;
const
executor
=
PlanAndExecuteAgentExecutor
.
fromLLMAndTools
(
{
llm
:
model
,
tools
,
}
)
;
const
result
=
await
executor
.
call
(
{
input
:
`
Who is the current president of the United States? What is their current age raised to the second power?
`
,
}
)
;
console
.
log
(
{
result
}
)
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\agents\executor\getting-started.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/executor/getting-started
crawled_at: 2025-06-22T02:00:20.381334
---

ä»£ç†æ‰§è¡Œå™¨(Agent Executors)
ä»£ç†ä½¿ç”¨LLMæ¥ç¡®å®šé‡‡å–å“ªäº›æ“ä½œä»¥åŠé‡‡å–çš„é¡ºåºã€‚æ“ä½œå¯ä»¥æ˜¯ä½¿ç”¨å·¥å…·å¹¶è§‚å¯Ÿå…¶è¾“å‡ºï¼Œæˆ–è¿”å›ç»™ç”¨æˆ·ã€‚
æ­£ç¡®ä½¿ç”¨ä»£ç†å¯ä»¥éå¸¸å¼ºå¤§ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­,æˆ‘ä»¬å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•é€šè¿‡æœ€ç®€å•çš„,æœ€é«˜çº§åˆ«çš„APIè½»æ¾ä½¿ç”¨ä»£ç†ã€‚
ä¸ºäº†åŠ è½½ä»£ç†,æ‚¨åº”è¯¥äº†è§£ä»¥ä¸‹æ¦‚å¿µ:
å·¥å…·:(Tool)  æ‰§è¡Œç‰¹å®šèŒè´£çš„å‡½æ•°ã€‚è¿™å¯ä»¥æ˜¯åƒ:è°·æ­Œæœç´¢(Google Search)ï¼Œæ•°æ®åº“æŸ¥æ‰¾(Database lookup)ï¼Œä»£ç REPLï¼Œå…¶ä»–é“¾ã€‚å·¥å…·çš„æ¥å£ç›®å‰æ˜¯é¢„æœŸå…·æœ‰å­—ç¬¦ä¸²è¾“å…¥ï¼Œå’Œå­—ç¬¦ä¸²è¾“å‡ºçš„å‡½æ•°ã€‚
LLM: ä»£ç†æ”¯æŒçš„è¯­è¨€æ¨¡å‹ã€‚
ä»£ç†: ä»£ç†ä»¥ä½¿ç”¨ã€‚è¿™åº”è¯¥æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå¼•ç”¨æ”¯æŒçš„ä»£ç†ç±»ã€‚å› ä¸ºè¿™ä¸ªç¬”è®°æœ¬é›†ä¸­åœ¨æœ€ç®€å•çš„,æœ€é«˜çº§åˆ«çš„APIä¸Šï¼Œæ‰€ä»¥ä»…æ¶µç›–ä½¿ç”¨æ ‡å‡†æ”¯æŒçš„ä»£ç†ã€‚
å¯¹äºè¿™ä¸ªä¾‹å­,æ‚¨éœ€è¦åœ¨
.env
æ–‡ä»¶ä¸­è®¾ç½®SerpAPIç¯å¢ƒå˜é‡ã€‚
SERPAPI_API_KEY
=
"..."
ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹äº†ï¼(Now we can get started!)
import
{
initializeAgentExecutorWithOptions
}
from
"langchain/agents"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
SerpAPI
}
from
"langchain/tools"
;
import
{
Calculator
}
from
"langchain/tools/calculator"
;
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
tools
=
[
new
SerpAPI
(
process
.
env
.
SERPAPI_API_KEY
,
{
location
:
"Austin,Texas,United States"
,
hl
:
"en"
,
gl
:
"us"
,
}
)
,
new
Calculator
(
)
,
]
;
const
executor
=
await
initializeAgentExecutorWithOptions
(
tools
,
model
,
{
agentType
:
"zero-shot-react-description"
,
verbose
:
true
,
}
)
;
const
input
=
`
Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?
`
;
const
result
=
await
executor
.
call
(
{
input
}
)
;
langchain-examples:start: Executing with input
"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?"
..
.
langchain-examples:start: Got output Harry Styles is Olivia Wilde's boyfriend and his current age raised to the
0.23
power is
2.169459462491557
.



--- æ–‡ä»¶: output_20250622_020018\docs\modules\agents\toolkits\json.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/toolkits/json
crawled_at: 2025-06-22T02:00:20.524795
---

JSONä»£ç†å·¥å…·åŒ…
è¿™ä¸ªä¾‹å­å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨JSONå·¥å…·åŒ…åŠ è½½å’Œä½¿ç”¨ä»£ç†ã€‚
import
*
as
fs
from
"fs"
;
import
*
as
yaml
from
"js-yaml"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
JsonSpec
,
JsonObject
}
from
"langchain/tools"
;
import
{
JsonToolkit
,
createJsonAgent
}
from
"langchain/agents"
;
export
const
run
=
async
(
)
=>
{
let
data
:
JsonObject
;
try
{
const
yamlFile
=
fs
.
readFileSync
(
"openai_openapi.yaml"
,
"utf8"
)
;
data
=
yaml
.
load
(
yamlFile
)
as
JsonObject
;
if
(
!
data
)
{
throw
new
Error
(
"Failed to load OpenAPI spec"
)
;
}
}
catch
(
e
)
{
console
.
error
(
e
)
;
return
;
}
const
toolkit
=
new
JsonToolkit
(
new
JsonSpec
(
data
)
)
;
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
executor
=
createJsonAgent
(
model
,
toolkit
)
;
const
input
=
`
What are the required parameters in the request body to the /completions endpoint?
`
;
console
.
log
(
`
Executing with input "
${
input
}
"...
`
)
;
const
result
=
await
executor
.
call
(
{
input
}
)
;
console
.
log
(
`
Got output
${
result
.
output
}
`
)
;
console
.
log
(
`
Got intermediate steps
${
JSON
.
stringify
(
result
.
intermediateSteps
,
null
,
2
)
}
`
)
;
}
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\agents\toolkits\openapi.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/toolkits/openapi
crawled_at: 2025-06-22T02:00:20.528830
---

OpenAPIä»£ç†å·¥å…·åŒ…
è¯¥ç¤ºä¾‹æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨OpenAPIå·¥å…·åŒ…åŠ è½½å’Œä½¿ç”¨ä»£ç†ã€‚
import
*
as
fs
from
"fs"
;
import
*
as
yaml
from
"js-yaml"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
JsonSpec
,
JsonObject
}
from
"langchain/tools"
;
import
{
createOpenApiAgent
,
OpenApiToolkit
}
from
"langchain/agents"
;
export
const
run
=
async
(
)
=>
{
let
data
:
JsonObject
;
try
{
const
yamlFile
=
fs
.
readFileSync
(
"openai_openapi.yaml"
,
"utf8"
)
;
data
=
yaml
.
load
(
yamlFile
)
as
JsonObject
;
if
(
!
data
)
{
throw
new
Error
(
"Failed to load OpenAPI spec"
)
;
}
}
catch
(
e
)
{
console
.
error
(
e
)
;
return
;
}
const
headers
=
{
"Content-Type"
:
"application/json"
,
Authorization
:
`
Bearer
${
process
.
env
.
OPENAI_API_KEY
}
`
,
}
;
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
toolkit
=
new
OpenApiToolkit
(
new
JsonSpec
(
data
)
,
model
,
headers
)
;
const
executor
=
createOpenApiAgent
(
model
,
toolkit
)
;
const
input
=
`
Make a POST request to openai /completions. The prompt should be 'tell me a joke.'
`
;
console
.
log
(
`
Executing with input "
${
input
}
"...
`
)
;
const
result
=
await
executor
.
call
(
{
input
}
)
;
console
.
log
(
`
Got output
${
result
.
output
}
`
)
;
console
.
log
(
`
Got intermediate steps
${
JSON
.
stringify
(
result
.
intermediateSteps
,
null
,
2
)
}
`
)
;
}
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\agents\toolkits\sql.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/toolkits/sql
crawled_at: 2025-06-22T02:00:20.513317
---

SQL Agent Toolkit
è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•åŠ è½½å’Œä½¿ç”¨SQLå·¥å…·åŒ…ä¸­çš„ä»£ç†ã€‚
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
SqlDatabase
}
from
"langchain/sql_db"
;
import
{
createSqlAgent
,
SqlToolkit
}
from
"langchain/agents"
;
import
{
DataSource
}
from
"typeorm"
;
/** This example uses Chinook database, which is a sample database available for SQL Server, Oracle, MySQL, etc.
* To set it up follow the instructions on https://database.guide/2-sample-databases-sqlite/, placing the .db file
* in the examples folder.
*/
export
const
run
=
async
(
)
=>
{
const
datasource
=
new
DataSource
(
{
type
:
"sqlite"
,
database
:
"Chinook.db"
,
}
)
;
const
db
=
await
SqlDatabase
.
fromDataSourceParams
(
{
appDataSource
:
datasource
,
}
)
;
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
toolkit
=
new
SqlToolkit
(
db
,
model
)
;
const
executor
=
createSqlAgent
(
model
,
toolkit
)
;
const
input
=
`
List the total sales per country. Which country's customers spent the most?
`
;
console
.
log
(
`
Executing with input "
${
input
}
"...
`
)
;
const
result
=
await
executor
.
call
(
{
input
}
)
;
console
.
log
(
`
Got output
${
result
.
output
}
`
)
;
console
.
log
(
`
Got intermediate steps
${
JSON
.
stringify
(
result
.
intermediateSteps
,
null
,
2
)
}
`
)
;
await
datasource
.
destroy
(
)
;
}
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\agents\toolkits\vectorstore.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/toolkits/vectorstore
crawled_at: 2025-06-22T02:00:20.653943
---

VectorStore ä»£ç†å·¥å…·åŒ…
è¿™ä¸ªä¾‹å­å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ VectorStore å·¥å…·åŒ…åŠ è½½å’Œä½¿ç”¨ä»£ç†ã€‚
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
HNSWLib
}
from
"langchain/vectorstores/hnswlib"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
RecursiveCharacterTextSplitter
}
from
"langchain/text_splitter"
;
import
*
as
fs
from
"fs"
;
import
{
VectorStoreToolkit
,
createVectorStoreAgent
,
VectorStoreInfo
,
}
from
"langchain/agents"
;
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
/* Load in the file we want to do question answering over */
const
text
=
fs
.
readFileSync
(
"state_of_the_union.txt"
,
"utf8"
)
;
/* Split the text into chunks */
const
textSplitter
=
new
RecursiveCharacterTextSplitter
(
{
chunkSize
:
1000
}
)
;
const
docs
=
await
textSplitter
.
createDocuments
(
[
text
]
)
;
/* Create the vectorstore */
const
vectorStore
=
await
HNSWLib
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
)
;
/* Create the agent */
const
vectorStoreInfo
:
VectorStoreInfo
=
{
name
:
"state_of_union_address"
,
description
:
"the most recent state of the Union address"
,
vectorStore
,
}
;
const
toolkit
=
new
VectorStoreToolkit
(
vectorStoreInfo
,
model
)
;
const
agent
=
createVectorStoreAgent
(
model
,
toolkit
)
;
const
input
=
"What did biden say about Ketanji Brown Jackson is the state of the union address?"
;
console
.
log
(
`
Executing:
${
input
}
`
)
;
const
result
=
await
agent
.
call
(
{
input
}
)
;
console
.
log
(
`
Got output
${
result
.
output
}
`
)
;
console
.
log
(
`
Got intermediate steps
${
JSON
.
stringify
(
result
.
intermediateSteps
,
null
,
2
)
}
`
)
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\agents\tools\agents_with_vectorstores.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/tools/agents_with_vectorstores
crawled_at: 2025-06-22T02:00:20.834221
---

å¸¦æœ‰å‘é‡å­˜å‚¨çš„ä»£ç†
æœ¬ç¬”è®°æ¶µç›–äº†å¦‚ä½•å°†ä»£ç†å’Œå‘é‡å­˜å‚¨å™¨ç»„åˆä½¿ç”¨ã€‚è¿™ç§ç”¨ä¾‹æ˜¯ï¼Œæ‚¨å·²å°†æ•°æ®æ‘„å…¥å‘é‡å­˜å‚¨å™¨ä¸­ï¼Œå¹¶æƒ³ä»¥ä»£ç†æ–¹å¼ä¸å…¶è¿›è¡Œäº¤äº’ã€‚
å»ºè®®çš„æ–¹æ³•æ˜¯åˆ›å»ºä¸€ä¸ªVectorDBQAChainï¼Œç„¶åå°†å…¶ç”¨ä½œæ•´ä½“ä»£ç†å·¥å…·ã€‚è®©æˆ‘ä»¬åœ¨ä¸‹é¢çœ‹çœ‹å¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹ã€‚æ‚¨å¯ä»¥ä½¿ç”¨å¤šä¸ªä¸åŒçš„å‘é‡æ•°æ®åº“è¿›è¡Œæ­¤æ“ä½œï¼Œå¹¶ä½¿ç”¨ä»£ç†ä½œä¸ºå®ƒä»¬ä¹‹é—´é€‰æ‹©çš„ä¸€ç§æ–¹å¼ã€‚æœ‰ä¸¤ç§ä¸åŒçš„æ–¹æ³•å¯ä»¥å®ç°è¿™ä¸€ç‚¹ - æ‚¨å¯ä»¥è®©ä»£ç†åƒæ­£å¸¸å·¥å…·ä¸€æ ·ä½¿ç”¨å‘é‡å­˜å‚¨å™¨ï¼Œæˆ–è€…æ‚¨å¯ä»¥è®¾ç½®
returnDirect: true
ä»…å°†ä»£ç†ç”¨ä½œè·¯ç”±å™¨ã€‚
é¦–å…ˆï¼Œæ‚¨éœ€è¦å¯¼å…¥ç›¸å…³æ¨¡å—ã€‚
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
initializeAgentExecutorWithOptions
}
from
"langchain/agents"
;
import
{
SerpAPI
,
ChainTool
}
from
"langchain/tools"
;
import
{
Calculator
}
from
"langchain/tools/calculator"
;
import
{
VectorDBQAChain
}
from
"langchain/chains"
;
import
{
HNSWLib
}
from
"langchain/vectorstores/hnswlib"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
RecursiveCharacterTextSplitter
}
from
"langchain/text_splitter"
;
import
*
as
fs
from
"fs"
;
æ¥ä¸‹æ¥ï¼Œæ‚¨éœ€è¦åˆ›å»ºå…·æœ‰æ•°æ®çš„å‘é‡å­˜å‚¨å™¨ï¼Œç„¶ååˆ›å»ºä¸è¯¥å‘é‡å­˜å‚¨å™¨äº¤äº’çš„QAé“¾ã€‚
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
/* Load in the file we want to do question answering over */
const
text
=
fs
.
readFileSync
(
"state_of_the_union.txt"
,
"utf8"
)
;
/* Split the text into chunks */
const
textSplitter
=
new
RecursiveCharacterTextSplitter
(
{
chunkSize
:
1000
}
)
;
const
docs
=
await
textSplitter
.
createDocuments
(
[
text
]
)
;
/* Create the vectorstore */
const
vectorStore
=
await
HNSWLib
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
)
;
/* Create the chain */
const
chain
=
VectorDBQAChain
.
fromLLM
(
model
,
vectorStore
)
;
ç°åœ¨æ‚¨æ‹¥æœ‰äº†è¯¥é“¾ï¼Œå¯ä»¥åˆ›å»ºä¸€ä¸ªå·¥å…·æ¥ä½¿ç”¨è¯¥é“¾ã€‚è¯·æ³¨æ„ï¼Œæ‚¨åº”è¯¥æ›´æ–°åç§°å’Œæè¿°ä»¥ç‰¹å®šäºQAé“¾ã€‚
const
qaTool
=
new
ChainTool
(
{
name
:
"state-of-union-qa"
,
description
:
"State of the Union QA - useful for when you need to ask questions about the most recent state of the union address."
,
chain
:
chain
,
}
)
;
ç°åœ¨ï¼Œæ‚¨å¯ä»¥æ„å»ºå¹¶ä½¿ç”¨è¯¥å·¥å…·ï¼Œå°±åƒä½¿ç”¨ä»»ä½•å…¶ä»–å·¥å…·ä¸€æ ·ï¼
const
tools
=
[
new
SerpAPI
(
process
.
env
.
SERPAPI_API_KEY
,
{
location
:
"Austin,Texas,United States"
,
hl
:
"en"
,
gl
:
"us"
,
}
)
,
new
Calculator
(
)
,
qaTool
,
]
;
const
executor
=
await
initializeAgentExecutorWithOptions
(
tools
,
model
,
{
agentType
:
"zero-shot-react-description"
,
}
)
;
console
.
log
(
"Loaded agent."
)
;
const
input
=
`
What did biden say about ketanji brown jackson is the state of the union address?
`
;
console
.
log
(
`
Executing with input "
${
input
}
"...
`
)
;
const
result
=
await
executor
.
call
(
{
input
}
)
;
console
.
log
(
`
Got output
${
result
.
output
}
`
)
;
å¦‚æœæ‚¨æ‰“ç®—ä½¿ç”¨ä»£ç†ä½œä¸ºè·¯ç”±å™¨ï¼Œå¹¶ä¸”ä»…æƒ³ç›´æ¥è¿”å›VectorDBQAChainçš„ç»“æœï¼Œåˆ™è¿˜å¯ä»¥è®¾ç½®
returnDirect: true
ã€‚
const
qaTool
=
new
ChainTool
(
{
name
:
"state-of-union-qa"
,
description
:
"State of the Union QA - useful for when you need to ask questions about the most recent state of the union address."
,
chain
:
chain
,
returnDirect
:
true
,
}
)
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\agents\tools\aiplugin-tool.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/tools/aiplugin-tool
crawled_at: 2025-06-22T02:00:20.765479
---

ChatGPTæ’ä»¶
æœ¬ä¾‹å±•ç¤ºäº†å¦‚ä½•åœ¨LangChainæŠ½è±¡ä¸­ä½¿ç”¨ChatGPTæ’ä»¶ã€‚
æ³¨ 1: ç›®å‰ä»…é€‚ç”¨äºæ²¡æœ‰è®¤è¯çš„æ’ä»¶ã€‚
æ³¨ 2: å‡ ä¹è‚¯å®šæœ‰å…¶ä»–æ–¹æ³•å¯ä»¥åšåˆ°è¿™ä¸€ç‚¹ï¼Œè¿™åªæ˜¯ç¬¬ä¸€æ¬¡å°è¯•ã€‚å¦‚æœä½ æœ‰æ›´å¥½çš„æƒ³æ³•ï¼Œè¯·æäº¤PRï¼
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
initializeAgentExecutorWithOptions
}
from
"langchain/agents"
;
import
{
RequestsGetTool
,
RequestsPostTool
,
AIPluginTool
,
}
from
"langchain/tools"
;
export
const
run
=
async
(
)
=>
{
const
tools
=
[
new
RequestsGetTool
(
)
,
new
RequestsPostTool
(
)
,
await
AIPluginTool
.
fromPluginUrl
(
"https://www.klarna.com/.well-known/ai-plugin.json"
)
,
]
;
const
agent
=
await
initializeAgentExecutorWithOptions
(
tools
,
new
ChatOpenAI
(
{
temperature
:
0
}
)
,
{
agentType
:
"chat-zero-shot-react-description"
,
verbose
:
true
}
)
;
const
result
=
await
agent
.
call
(
{
input
:
"what t shirts are available in klarna?"
,
}
)
;
console
.
log
(
{
result
}
)
;
}
;
Entering
new
agent_executor
chain
...
Thought
:
Klarna
is
a payment provider
,
not a store
.
I
need to check
if
there
is
a Klarna Shopping
API
that
I
can use to search
for
t
-
shirts
.
Action
:
`
`
`
{
"action": "KlarnaProducts",
"action_input": ""
}
`
`
`
Usage Guide
:
Use the Klarna plugin to
get
relevant product suggestions
for
any
shopping or researching purpose
.
The query to be sent should not include stopwords like articles
,
prepositions and determinants
.
The api works best when searching
for
words that are related to products
,
like their name
,
brand
,
model or category
.
Links will always be returned and should be shown to the user
.
OpenAPI Spec
:
{
"openapi"
:
"3.0.1"
,
"info"
:
{
"version"
:
"v0"
,
"title"
:
"Open AI Klarna product Api"
}
,
"servers"
:
[
{
"url"
:
"https://www.klarna.com/us/shopping"
}
]
,
"tags"
:
[
{
"name"
:
"open-ai-product-endpoint"
,
"description"
:
"Open AI Product Endpoint. Query for products."
}
]
,
"paths"
:
{
"/public/openai/v0/products"
:
{
"get"
:
{
"tags"
:
[
"open-ai-product-endpoint"
]
,
"summary"
:
"API for fetching Klarna product information"
,
"operationId"
:
"productsUsingGET"
,
"parameters"
:
[
{
"name"
:
"q"
,
"in"
:
"query"
,
"description"
:
"query, must be between 2 and 100 characters"
,
"required"
:
true
,
"schema"
:
{
"type"
:
"string"
}
}
,
{
"name"
:
"size"
,
"in"
:
"query"
,
"description"
:
"number of products returned"
,
"required"
:
false
,
"schema"
:
{
"type"
:
"integer"
}
}
,
{
"name"
:
"budget"
,
"in"
:
"query"
,
"description"
:
"maximum price of the matching product in local currency, filters results"
,
"required"
:
false
,
"schema"
:
{
"type"
:
"integer"
}
}
]
,
"responses"
:
{
"200"
:
{
"description"
:
"Products found"
,
"content"
:
{
"application/json"
:
{
"schema"
:
{
"$ref"
:
"#/components/schemas/ProductResponse"
}
}
}
}
,
"503"
:
{
"description"
:
"one or more services are unavailable"
}
}
,
"deprecated"
:
false
}
}
}
,
"components"
:
{
"schemas"
:
{
"Product"
:
{
"type"
:
"object"
,
"properties"
:
{
"attributes"
:
{
"type"
:
"array"
,
"items"
:
{
"type"
:
"string"
}
}
,
"name"
:
{
"type"
:
"string"
}
,
"price"
:
{
"type"
:
"string"
}
,
"url"
:
{
"type"
:
"string"
}
}
,
"title"
:
"Product"
}
,
"ProductResponse"
:
{
"type"
:
"object"
,
"properties"
:
{
"products"
:
{
"type"
:
"array"
,
"items"
:
{
"$ref"
:
"#/components/schemas/Product"
}
}
}
,
"title"
:
"ProductResponse"
}
}
}
}
Now that
I
know there
is
a Klarna Shopping
API
,
I
can use it to search
for
t
-
shirts
.
I
will make a
GET
request to the
API
with
the query parameter
"t-shirt"
.
Action
:
`
`
`
{
"action": "requests_get",
"action_input": "https://www.klarna.com/us/shopping/public/openai/v0/products?q=t-shirt"
}
`
`
`
{
"products"
:
[
{
"name"
:
"Psycho Bunny Mens Copa Gradient Logo Graphic Tee"
,
"url"
:
"https://www.klarna.com/us/shopping/pl/cl10001/3203663222/Clothing/Psycho-Bunny-Mens-Copa-Gradient-Logo-Graphic-Tee/?source=openai"
,
"price"
:
"$35.00"
,
"attributes"
:
[
"Material:Cotton"
,
"Target Group:Man"
,
"Color:White,Blue,Black,Orange"
]
}
,
{
"name"
:
"T-shirt"
,
"url"
:
"https://www.klarna.com/us/shopping/pl/cl10001/3203506327/Clothing/T-shirt/?source=openai"
,
"price"
:
"$20.45"
,
"attributes"
:
[
"Material:Cotton"
,
"Target Group:Man"
,
"Color:Gray,White,Blue,Black,Orange"
]
}
,
{
"name"
:
"Palm Angels Bear T-shirt - Black"
,
"url"
:
"https://www.klarna.com/us/shopping/pl/cl10001/3201090513/Clothing/Palm-Angels-Bear-T-shirt-Black/?source=openai"
,
"price"
:
"$168.36"
,
"attributes"
:
[
"Material:Cotton"
,
"Target Group:Man"
,
"Color:Black"
]
}
,
{
"name"
:
"Tommy Hilfiger Essential Flag Logo T-shirt"
,
"url"
:
"https://www.klarna.com/us/shopping/pl/cl10001/3201840629/Clothing/Tommy-Hilfiger-Essential-Flag-Logo-T-shirt/?source=openai"
,
"price"
:
"$22.52"
,
"attributes"
:
[
"Material:Cotton"
,
"Target Group:Man"
,
"Color:Red,Gray,White,Blue,Black"
,
"Pattern:Solid Color"
,
"Environmental Attributes :Organic"
]
}
,
{
"name"
:
"Coach Outlet Signature T Shirt"
,
"url"
:
"https://www.klarna.com/us/shopping/pl/cl10001/3203005573/Clothing/Coach-Outlet-Signature-T-Shirt/?source=openai"
,
"price"
:
"$75.00"
,
"attributes"
:
[
"Material:Cotton"
,
"Target Group:Man"
,
"Color:Gray"
]
}
]
}
Finished chain
.
{
result
:
{
output
:
'The available t-shirts in Klarna are Psycho Bunny Mens Copa Gradient Logo Graphic Tee, T-shirt, Palm Angels Bear T-shirt - Black, Tommy Hilfiger Essential Flag Logo T-shirt, and Coach Outlet Signature T Shirt.'
,
intermediateSteps
:
[
[
Object
]
,
[
Object
]
]
}
}



--- æ–‡ä»¶: output_20250622_020018\docs\modules\agents\tools\dynamic.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/tools/dynamic
crawled_at: 2025-06-22T02:00:20.836226
---

DynamicTool
è‡ªå®šä¹‰å·¥å…·
åˆ›å»ºè¿è¡Œè‡ªå®šä¹‰ä»£ç çš„å·¥å…·çš„ä¸€ç§é€‰é¡¹æ˜¯ä½¿ç”¨
DynamicTool
ã€‚
DynamicTool
ç±»éœ€è¦è¾“å…¥ä¸€ä¸ªåç§°ï¼Œ ä¸€ä¸ªæè¿°ï¼Œ å’Œä¸€ä¸ªå‡½æ•°ã€‚
é‡è¦çš„æ˜¯ï¼Œ åç§°å’Œæè¿°å°†è¢«è¯­è¨€æ¨¡å‹ç”¨æ¥ç¡®å®šä½•æ—¶è°ƒç”¨æ­¤å‡½æ•°ä»¥åŠä½¿ç”¨å“ªäº›å‚æ•°æ¥è°ƒç”¨å®ƒï¼Œ
å› æ­¤ï¼Œè¯·ç¡®ä¿å°†å®ƒä»¬è®¾ç½®ä¸ºä¸€äº›è¯­è¨€æ¨¡å‹å¯ä»¥ç†è§£çš„å€¼ï¼
æä¾›çš„å‡½æ•°æ˜¯ä»£ç†ç¨‹åºå®é™…è°ƒç”¨çš„å‡½æ•°ã€‚å½“å‘ç”Ÿé”™è¯¯æ—¶ï¼Œ åº”è¯¥ï¼Œåœ¨å¯èƒ½çš„æƒ…å†µä¸‹ï¼Œè¿”å›è¡¨ç¤ºé”™è¯¯çš„å­—ç¬¦ä¸²ï¼Œè€Œä¸æ˜¯æŠ›å‡ºé”™è¯¯ã€‚
è¿™å…è®¸é”™è¯¯è¢«ä¼ é€’åˆ° LLM å¹¶ä¸” LLM å¯ä»¥å†³å®šå¦‚ä½•å¤„ç†å®ƒã€‚å¦‚æœæŠ›å‡ºé”™è¯¯ï¼Œé‚£ä¹ˆä»£ç†ç¨‹åºçš„æ‰§è¡Œå°†åœæ­¢ã€‚
è¯·å‚é˜…ä¸‹é¢çš„ç¤ºä¾‹ä»¥å®šä¹‰å’Œä½¿ç”¨
DynamicTool
ã€‚
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
initializeAgentExecutorWithOptions
}
from
"langchain/agents"
;
import
{
DynamicTool
}
from
"langchain/tools"
;
export
const
run
=
async
(
)
=>
{
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
tools
=
[
new
DynamicTool
(
{
name
:
"FOO"
,
description
:
"call this to get the value of foo. input should be an empty string."
,
func
:
(
)
=>
"baz"
,
}
)
,
new
DynamicTool
(
{
name
:
"BAR"
,
description
:
"call this to get the value of bar. input should be an empty string."
,
func
:
(
)
=>
"baz1"
,
}
)
,
]
;
const
executor
=
await
initializeAgentExecutorWithOptions
(
tools
,
model
,
{
agentType
:
"zero-shot-react-description"
,
}
)
;
console
.
log
(
"Loaded agent."
)
;
const
input
=
`
What is the value of foo?
`
;
console
.
log
(
`
Executing with input "
${
input
}
"...
`
)
;
const
result
=
await
executor
.
call
(
{
input
}
)
;
console
.
log
(
`
Got output
${
result
.
output
}
`
)
;
}
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\agents\tools\integrations.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/tools/integrations/
crawled_at: 2025-06-22T02:00:20.803960
---

é›†æˆ: å·¥å…·
LangChainæä¾›ä»¥ä¸‹å¯ç«‹å³ä½¿ç”¨çš„å·¥å…·:
AWSLambda
- AWS Lambda APIçš„åŒ…è£…å™¨ï¼Œé€šè¿‡Amazon Web Services Node.js SDKè°ƒç”¨ã€‚å¯¹äºè§¦å‘å…·æœ‰æ‚¨éœ€è¦æä¾›ç»™ä»£ç†çš„ä»»ä½•è¡Œä¸ºçš„æ— æœåŠ¡å™¨å‡½æ•°å¾ˆæœ‰ç”¨ã€‚
BingSerpAPI
- Bing Search APIçš„åŒ…è£…å™¨ã€‚åœ¨éœ€è¦å›ç­”æœ‰å…³å½“å‰äº‹ä»¶çš„é—®é¢˜æ—¶å¾ˆæœ‰ç”¨ã€‚è¾“å…¥åº”è¯¥æ˜¯ä¸€ä¸ªæœç´¢æŸ¥è¯¢ã€‚
BraveSearch
- Brave Search APIçš„åŒ…è£…å™¨ã€‚åœ¨éœ€è¦å›ç­”æœ‰å…³å½“å‰äº‹ä»¶çš„é—®é¢˜æ—¶å¾ˆæœ‰ç”¨ã€‚è¾“å…¥åº”è¯¥æ˜¯ä¸€ä¸ªæœç´¢æŸ¥è¯¢ã€‚
Calculator
- ç”¨äºè·å–æ•°å­¦è¡¨è¾¾å¼çš„ç»“æœã€‚è¿™ä¸ªå·¥å…·çš„è¾“å…¥åº”è¯¥æ˜¯ä¸€ä¸ªå¯ä»¥ç”±ç®€å•è®¡ç®—å™¨æ‰§è¡Œçš„æœ‰æ•ˆçš„æ•°å­¦è¡¨è¾¾å¼ã€‚
GoogleCustomSearch
- Google Custom Search APIçš„åŒ…è£…å™¨ã€‚åœ¨éœ€è¦å›ç­”æœ‰å…³å½“å‰äº‹ä»¶çš„é—®é¢˜æ—¶å¾ˆæœ‰ç”¨ã€‚è¾“å…¥åº”è¯¥æ˜¯ä¸€ä¸ªæœç´¢æŸ¥è¯¢ã€‚
IFTTTWebHook
- IFTTT Webhook APIçš„åŒ…è£…å™¨ã€‚ç”¨äºè§¦å‘IFTTTåŠ¨ä½œã€‚
JsonListKeysTool
å’Œ
JsonGetValueTool
- ç”¨äºä»JSONå¯¹è±¡ä¸­æå–æ•°æ®ã€‚è¿™äº›å·¥å…·å¯åœ¨
JsonToolkit
ä¸­é›†ä½“ä½¿ç”¨ã€‚
RequestsGetTool
å’Œ
RequestsPostTool
- ç”¨äºå‘å‡ºHTTPè¯·æ±‚ã€‚
SerpAPI
- A wrapper around the SerpAPI API. Useful for when you need to answer questions about current events. Input should be a search query.
[
QuerySqlTool
][æŸ¥è¯¢SQLå·¥å…·]
ï¼Œ
[
InfoSqlTool
][ä¿¡æ¯SQLå·¥å…·]
ï¼Œ
[
ListTablesSqlTool
][åˆ—å‡ºè¡¨æ ¼SQLå·¥å…·]
ï¼Œ å’Œ
[
QueryCheckerTool
][æŸ¥è¯¢æ£€æŸ¥å·¥å…·]
- ç”¨äºä¸SQLæ•°æ®åº“è¿›è¡Œäº¤äº’ã€‚å¯ä»¥ä¸
[
SqlToolkit
][SQLå·¥å…·åŒ…]
ä¸€èµ·ä½¿ç”¨ã€‚
[
VectorStoreQATool
][å‘é‡å­˜å‚¨é—®ç­”å·¥å…·]
- å¯ç”¨äºä»å‘é‡å­˜å‚¨ä¸­æ£€ç´¢ç›¸å…³æ–‡æœ¬æ•°æ®ã€‚
[
ZapierNLARunAction
][ZapierNLAè¿è¡Œæ“ä½œ]
- å°è£…äº†Zapier NLP APIã€‚ç”¨äºä»¥è‡ªç„¶è¯­è¨€è¾“å…¥è§¦å‘Zapieræ“ä½œã€‚æœ€å¥½ä¸
[
ZapierToolkit
][Zapierå·¥å…·åŒ…]
ä¸€èµ·ä½¿ç”¨ã€‚



--- æ–‡ä»¶: output_20250622_020018\docs\modules\agents\tools\lambda_agent.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/tools/lambda_agent
crawled_at: 2025-06-22T02:00:20.866207
---

ä½¿ç”¨ AWS Lambda é›†æˆä»£ç†
è¯·æŸ¥çœ‹å®Œæ•´æ–‡æ¡£:
https://docs.aws.amazon.com/lambda/index.html
AWS Lambda
æ˜¯ç”±äºšé©¬é€Šç½‘ç»œæœåŠ¡ï¼ˆAWS)æä¾›çš„æ— æœåŠ¡å™¨è®¡ç®—æœåŠ¡ï¼Œæ—¨åœ¨å…è®¸å¼€å‘äººå‘˜æ„å»ºå’Œè¿è¡Œåº”ç”¨ç¨‹åºå’ŒæœåŠ¡ï¼Œæ— éœ€æä¾›æˆ–ç®¡ç†æœåŠ¡å™¨ã€‚è¿™ç§æ— æœåŠ¡å™¨æ¶æ„ä½¿æ‚¨èƒ½å¤Ÿä¸“æ³¨äºç¼–å†™å’Œéƒ¨ç½²ä»£ç ï¼Œè€Œ AWS ä¼šè‡ªåŠ¨å¤„ç†æ‰©å±•ï¼Œè¡¥ä¸å’Œç®¡ç†æ‰€éœ€çš„åŸºç¡€æ¶æ„æ¥è¿è¡Œæ‚¨çš„åº”ç”¨ç¨‹åºã€‚
é€šè¿‡åœ¨æä¾›ç»™ä»£ç†å·¥å…·åˆ—è¡¨ä¸­åŒ…å« AWSLambdaï¼Œæ‚¨å¯ä»¥æˆäºˆä»£ç†è°ƒç”¨åœ¨æ‚¨çš„ AWS Cloud ä¸­è¿è¡Œçš„ä»£ç çš„èƒ½åŠ›ï¼Œä»¥æ»¡è¶³æ‚¨çš„éœ€æ±‚ã€‚
å½“ä»£ç†ä½¿ç”¨ AWSLambda å·¥å…·æ—¶ï¼Œå®ƒå°†æä¾›ä¸€ä¸ª
string
ç±»å‹çš„å‚æ•°ï¼Œè¯¥å‚æ•°å°†é€šè¿‡
event
å‚æ•°ä¼ é€’åˆ° Lambda å‡½æ•°ä¸­ã€‚
è¿™ä¸ªå¿«é€Ÿå…¥é—¨å°†æ¼”ç¤ºä»£ç†å¦‚ä½•ä½¿ç”¨ Lambda å‡½æ•°é€šè¿‡
Amazon ç®€å•é‚®ä»¶æœåŠ¡
å‘é€ç”µå­é‚®ä»¶ã€‚å‘é€ç”µå­é‚®ä»¶çš„ lambda ä»£ç æœªæä¾›ï¼Œä½†å¦‚æœæ‚¨æƒ³äº†è§£è¿™å¯ä»¥å¦‚ä½•å®Œæˆï¼Œè¯·å‚è§
æ­¤å¤„
ã€‚è¯·è®°ä½ï¼Œè¿™åªæ˜¯ä¸€ä¸ªæ•…æ„ç®€å•çš„ä¾‹å­ï¼›Lambda å¯ç”¨äºæ‰§è¡Œæ— é™æ•°é‡çš„å…¶ä»–ç›®çš„ï¼ˆåŒ…æ‹¬æ‰§è¡Œæ›´å¤šçš„ Langchains)ï¼
å‡­æ®æ³¨é‡Š:
â€‹
å¦‚æœä½ æ²¡æœ‰é€šè¿‡AWS CLIè¿è¡Œè¿‡
aws configure
ï¼Œåˆ™å¿…é¡»åœ¨AWSLambdaæ„é€ å‡½æ•°ä¸­æä¾›
region
,
accessKeyId
, å’Œ
secretAccessKey
ã€‚
ä½¿ç”¨è¿™äº›å‡­è¯çš„IAMè§’è‰²å¿…é¡»å…·æœ‰è°ƒç”¨Lambdaå‡½æ•°çš„æƒé™ã€‚
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
SerpAPI
}
from
"langchain/tools"
;
import
{
AWSLambda
}
from
"langchain/tools/aws_lambda"
;
import
{
initializeAgentExecutorWithOptions
}
from
"langchain/agents"
;
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
emailSenderTool
=
new
AWSLambda
(
{
name
:
"email-sender"
,
// tell the Agent precisely what the tool does
description
:
"Sends an email with the specified content to testing123@gmail.com"
,
region
:
"us-east-1"
,
// optional: AWS region in which the function is deployed
accessKeyId
:
"abc123"
,
// optional: access key id for a IAM user with invoke permissions
secretAccessKey
:
"xyz456"
,
// optional: secret access key for that IAM user
functionName
:
"SendEmailViaSES"
,
// the function name as seen in AWS Console
}
)
;
const
tools
=
[
emailSenderTool
,
new
SerpAPI
(
"api_key_goes_here"
)
]
;
const
executor
=
await
initializeAgentExecutorWithOptions
(
tools
,
model
,
{
agentType
:
"zero-shot-react-description"
,
}
)
;
const
input
=
`
Find out the capital of Croatia. Once you have it, email the answer to testing123@gmail.com.
`
;
const
result
=
await
executor
.
call
(
{
input
}
)
;
console
.
log
(
result
)
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\agents\tools\webbrowser.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/tools/webbrowser
crawled_at: 2025-06-22T02:00:20.990209
---

ç½‘ç»œæµè§ˆå™¨å·¥å…·
ç½‘ç»œæµè§ˆå™¨å·¥å…·ä¸ºæ‚¨çš„ä»£ç†ç¨‹åºæä¾›äº†è®¿é—®ç½‘ç«™å’Œæå–ä¿¡æ¯çš„åŠŸèƒ½ã€‚å®ƒå‘ä»£ç†ç¨‹åºæè¿°ä¸ºï¼š
useful for when you need to find something on or summarize a webpage. input should be a comma separated list of "valid URL including protocol","what you want to find on the page or empty string for a summary".
å®ƒå…¬å¼€äº†ä¸¤ç§æ“ä½œæ¨¡å¼:
å½“ä»£ç†ç¨‹åºä»…ä½¿ç”¨URLè°ƒç”¨æ—¶ï¼Œå®ƒä¼šç”Ÿæˆç½‘ç«™å†…å®¹çš„æ‘˜è¦
å½“ä»£ç†ç¨‹åºä½¿ç”¨URLå’Œè¦æŸ¥æ‰¾çš„æè¿°æ¥è°ƒç”¨æ—¶ï¼Œå®ƒå°†ä½¿ç”¨å†…å­˜ä¸­çš„Vector StoreæŸ¥æ‰¾æœ€ç›¸å…³çš„ç‰‡æ®µå¹¶å¯¹å…¶è¿›è¡Œæ‘˜è¦
è®¾ç½®
â€‹
è¦ä½¿ç”¨ç½‘ç»œæµè§ˆå™¨å·¥å…·ï¼Œæ‚¨éœ€è¦å®‰è£…æ‰€æœ‰ä¾èµ–é¡¹:
npm
Yarn
pnpm
npm
install
cheerio axios
yarn
add
cheerio axios
pnpm
add
cheerio axios
ä½¿ç”¨ï¼Œ ç‹¬ç«‹
â€‹
import
{
WebBrowser
}
from
"langchain/tools/webbrowser"
;
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
export
async
function
run
(
)
{
// this will not work with Azure OpenAI API yet
// Azure OpenAI API does not support embedding with multiple inputs yet
// Too many inputs. The max number of inputs is 1.  We hope to increase the number of inputs per request soon. Please contact us through an Azure support request at: https://go.microsoft.com/fwlink/?linkid=2213926 for further questions.
// So we will fail fast, when Azure OpenAI API is used
if
(
process
.
env
.
AZURE_OPENAI_API_KEY
)
{
throw
new
Error
(
"Azure OpenAI API does not support embedding with multiple inputs yet"
)
;
}
const
model
=
new
ChatOpenAI
(
{
temperature
:
0
}
)
;
const
embeddings
=
new
OpenAIEmbeddings
(
process
.
env
.
AZURE_OPENAI_API_KEY
?
{
azureOpenAIApiDeploymentName
:
"Embeddings2"
}
:
{
}
)
;
const
browser
=
new
WebBrowser
(
{
model
,
embeddings
}
)
;
const
result
=
await
browser
.
call
(
`
"https://www.themarginalian.org/2015/04/09/find-your-bliss-joseph-campbell-power-of-myth","who is joseph campbell"
`
)
;
console
.
log
(
result
)
;
/*
Joseph Campbell was a mythologist and writer who discussed spirituality, psychological archetypes, cultural myths, and the mythology of self. He sat down with Bill Moyers for a lengthy conversation at George Lucasâ€™s Skywalker Ranch in California, which continued the following year at the American Museum of Natural History in New York. The resulting 24 hours of raw footage were edited down to six one-hour episodes and broadcast on PBS in 1988, shortly after Campbellâ€™s death, in what became one of the most popular in the history of public television.
Relevant Links:
- [The Holstee Manifesto](http://holstee.com/manifesto-bp)
- [The Silent Music of the Mind: Remembering Oliver Sacks](https://www.themarginalian.org/2015/08/31/remembering-oliver-sacks)
- [Joseph Campbell series](http://billmoyers.com/spotlight/download-joseph-campbell-and-the-power-of-myth-audio/)
- [Bill Moyers](https://www.themarginalian.org/tag/bill-moyers/)
- [books](https://www.themarginalian.org/tag/books/)
*/
}
ä½¿ç”¨ï¼Œ åœ¨ä»£ç†ç¨‹åºä¸­
â€‹
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
initializeAgentExecutorWithOptions
}
from
"langchain/agents"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
SerpAPI
}
from
"langchain/tools"
;
import
{
Calculator
}
from
"langchain/tools/calculator"
;
import
{
WebBrowser
}
from
"langchain/tools/webbrowser"
;
export
const
run
=
async
(
)
=>
{
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
embeddings
=
new
OpenAIEmbeddings
(
)
;
const
tools
=
[
new
SerpAPI
(
process
.
env
.
SERPAPI_API_KEY
,
{
location
:
"Austin,Texas,United States"
,
hl
:
"en"
,
gl
:
"us"
,
}
)
,
new
Calculator
(
)
,
new
WebBrowser
(
{
model
,
embeddings
}
)
,
]
;
const
executor
=
await
initializeAgentExecutorWithOptions
(
tools
,
model
,
{
agentType
:
"zero-shot-react-description"
,
verbose
:
true
,
}
)
;
console
.
log
(
"Loaded agent."
)
;
const
input
=
`
What is the word of the day on merriam webster. What is the top result on google for that word
`
;
console
.
log
(
`
Executing with input "
${
input
}
"...
`
)
;
const
result
=
await
executor
.
call
(
{
input
}
)
;
/*
Entering new agent_executor chain...
I need to find the word of the day on Merriam Webster and then search for it on Google
Action: web-browser
Action Input: "https://www.merriam-webster.com/word-of-the-day", ""
Summary: Merriam-Webster is a website that provides users with a variety of resources, including a dictionary, thesaurus, word finder, word of the day, games and quizzes, and more. The website also allows users to log in and save words, view recents, and access their account settings. The Word of the Day for April 14, 2023 is "lackadaisical", which means lacking in life, spirit, or zest. The website also provides quizzes and games to help users build their vocabulary.
Relevant Links:
- [Test Your Vocabulary](https://www.merriam-webster.com/games)
- [Thesaurus](https://www.merriam-webster.com/thesaurus)
- [Word Finder](https://www.merriam-webster.com/wordfinder)
- [Word of the Day](https://www.merriam-webster.com/word-of-the-day)
- [Shop](https://shop.merriam-webster.com/?utm_source=mwsite&utm_medium=nav&utm_content=
I now need to search for the word of the day on Google
Action: search
Action Input: "lackadaisical"
lackadaisical implies a carefree indifference marked by half-hearted efforts. lackadaisical college seniors pretending to study. listless suggests a lack of ...
Finished chain.
*/
console
.
log
(
`
Got output
${
JSON
.
stringify
(
result
,
null
,
2
)
}
`
)
;
/*
Got output {
"output": "The word of the day on Merriam Webster is \"lackadaisical\", which implies a carefree indifference marked by half-hearted efforts.",
"intermediateSteps": [
{
"action": {
"tool": "web-browser",
"toolInput": "https://www.merriam-webster.com/word-of-the-day\", ",
"log": " I need to find the word of the day on Merriam Webster and then search for it on Google\nAction: web-browser\nAction Input: \"https://www.merriam-webster.com/word-of-the-day\", \"\""
},
"observation": "\n\nSummary: Merriam-Webster is a website that provides users with a variety of resources, including a dictionary, thesaurus, word finder, word of the day, games and quizzes, and more. The website also allows users to log in and save words, view recents, and access their account settings. The Word of the Day for April 14, 2023 is \"lackadaisical\", which means lacking in life, spirit, or zest. The website also provides quizzes and games to help users build their vocabulary.\n\nRelevant Links: \n- [Test Your Vocabulary](https://www.merriam-webster.com/games)\n- [Thesaurus](https://www.merriam-webster.com/thesaurus)\n- [Word Finder](https://www.merriam-webster.com/wordfinder)\n- [Word of the Day](https://www.merriam-webster.com/word-of-the-day)\n- [Shop](https://shop.merriam-webster.com/?utm_source=mwsite&utm_medium=nav&utm_content="
},
{
"action": {
"tool": "search",
"toolInput": "lackadaisical",
"log": " I now need to search for the word of the day on Google\nAction: search\nAction Input: \"lackadaisical\""
},
"observation": "lackadaisical implies a carefree indifference marked by half-hearted efforts. lackadaisical college seniors pretending to study. listless suggests a lack of ..."
}
]
}
*/
}
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\agents\tools\zapier_agent.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/tools/zapier_agent
crawled_at: 2025-06-22T02:00:20.961000
---

Zapier NLAé›†æˆä»£ç†
å®Œæ•´æ–‡æ¡£åœ¨æ­¤å¤„ :
https://nla.zapier.com/api/v1/dynamic/docs
Zapierè‡ªç„¶è¯­è¨€æ“ä½œ
é€šè¿‡è‡ªç„¶è¯­è¨€APIç•Œé¢è®©æ‚¨è®¿é—®Zapierå¹³å°ä¸Šçš„5k+åº”ç”¨ç¨‹åºå’Œ20k+æ“ä½œã€‚
NLAæ”¯æŒçš„åº”ç”¨åŒ…æ‹¬ Gmailï¼Œ Salesforceï¼Œ Trelloï¼Œ Slackï¼Œ Asanaï¼Œ HubSpotï¼Œ Google Sheetsï¼Œ Microsoft Teamsï¼Œä»¥åŠæ•°åƒä¸ªæ›´å¤šçš„åº”ç”¨ç¨‹åº:
https://zapier.com/apps
Zapier NLAå¤„ç†æ‰€æœ‰åº•å±‚APIèº«ä»½éªŒè¯å’Œè‡ªç„¶è¯­è¨€ç¿»è¯‘-->åº•å±‚APIè°ƒç”¨-->è¿”å›ç®€åŒ–çš„LLMè¾“å‡ºã€‚å…³é”®æ€æƒ³æ˜¯æ‚¨æˆ–æ‚¨çš„ç”¨æˆ·é€šè¿‡ç±»ä¼¼äºoauthçš„è®¾ç½®çª—å£å…¬å¼€ä¸€ç»„æ“ä½œï¼Œç„¶åé€šè¿‡REST APIè¿›è¡ŒæŸ¥è¯¢å’Œæ‰§è¡Œã€‚
NLAä¸ºç­¾ç½²NLA APIè¯·æ±‚æä¾›APIå¯†é’¥å’ŒOAuthä¸¤ç§æ–¹å¼ã€‚
æœåŠ¡å™¨ç«¯ï¼ˆAPIå¯†é’¥): ç”¨äºå¿«é€Ÿå…¥é—¨ï¼Œæµ‹è¯•ä»¥åŠä»…ä½¿ç”¨å¼€å‘äººå‘˜Zapierè´¦æˆ·ä¸­å…¬å¼€çš„æ“ä½œï¼ˆå°†åœ¨Zapier.comä¸Šä½¿ç”¨å¼€å‘äººå‘˜çš„è¿æ¥çš„å¸æˆ·)çš„ç”Ÿäº§åœºæ™¯
ç”¨æˆ·ç•Œé¢ï¼ˆOauth):é¢å‘ç”Ÿäº§åœºæ™¯ï¼Œæ‚¨æ­£åœ¨éƒ¨ç½²é¢å‘ç»ˆç«¯ç”¨æˆ·çš„åº”ç”¨ç¨‹åºï¼ŒLangChainéœ€è¦è®¿é—®ç»ˆç«¯ç”¨æˆ·å…¬å¼€çš„æ“ä½œå’Œåœ¨zapier.comä¸Šè¿æ¥çš„å¸æˆ·
æ­¤å¿«é€Ÿå…¥é—¨å°†å…³æ³¨ç”¨äºç®€æ´æ€§çš„æœåŠ¡å™¨ç«¯ç”¨ä¾‹ã€‚æŸ¥çœ‹å®Œæ•´æ–‡æ¡£æˆ–è”ç³»
nla@zapier.com
ä»¥è·å–ç”¨æˆ·ç•Œé¢oauthå¼€å‘äººå‘˜æ”¯æŒã€‚
ä¸‹é¢çš„ç¤ºä¾‹æ¼”ç¤ºå¦‚ä½•å°†Zapieré›†æˆä½œä¸ºä»£ç†:
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
ZapierNLAWrapper
}
from
"langchain/tools"
;
import
{
initializeAgentExecutorWithOptions
,
ZapierToolKit
,
}
from
"langchain/agents"
;
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
zapier
=
new
ZapierNLAWrapper
(
)
;
const
toolkit
=
await
ZapierToolKit
.
fromZapierNLAWrapper
(
zapier
)
;
const
executor
=
await
initializeAgentExecutorWithOptions
(
toolkit
.
tools
,
model
,
{
agentType
:
"zero-shot-react-description"
,
verbose
:
true
,
}
)
;
console
.
log
(
"Loaded agent."
)
;
const
input
=
`
Summarize the last email I received regarding Silicon Valley Bank. Send the summary to the #test-zapier Slack channel.
`
;
console
.
log
(
`
Executing with input "
${
input
}
"...
`
)
;
const
result
=
await
executor
.
call
(
{
input
}
)
;
console
.
log
(
`
Got output
${
result
.
output
}
`
)
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\agents\agents\action\chat_mrkl.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/agents/action/chat_mrkl
crawled_at: 2025-06-22T02:00:19.916971
---

MRKLèŠå¤©æ¨¡å‹ä»£ç†
è¿™ä¸ªä¾‹å­ä»‹ç»äº†å¦‚ä½•ä½¿ç”¨ä¸€ä¸ªä½¿ç”¨ReActæ¡†æ¶(åŸºäºå·¥å…·æè¿°)æ¥å†³å®šé‡‡å–ä»€ä¹ˆè¡ŒåŠ¨çš„ä»£ç†ã€‚è¯¥ä»£ç†è¢«ä¼˜åŒ–ä¸ºåœ¨èŠå¤©æ¨¡å‹ä¸­ä½¿ç”¨ã€‚å¦‚æœæ‚¨æƒ³åœ¨LLMä¸­ä½¿ç”¨å®ƒï¼Œæ‚¨å¯ä»¥ä½¿ç”¨
LLM MRKLä»£ç†
ã€‚
import
{
initializeAgentExecutorWithOptions
}
from
"langchain/agents"
;
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
SerpAPI
}
from
"langchain/tools"
;
import
{
Calculator
}
from
"langchain/tools/calculator"
;
export
const
run
=
async
(
)
=>
{
const
model
=
new
ChatOpenAI
(
{
temperature
:
0
}
)
;
const
tools
=
[
new
SerpAPI
(
process
.
env
.
SERPAPI_API_KEY
,
{
location
:
"Austin,Texas,United States"
,
hl
:
"en"
,
gl
:
"us"
,
}
)
,
new
Calculator
(
)
,
]
;
const
executor
=
await
initializeAgentExecutorWithOptions
(
tools
,
model
,
{
agentType
:
"chat-zero-shot-react-description"
,
returnIntermediateSteps
:
true
,
}
)
;
console
.
log
(
"Loaded agent."
)
;
const
input
=
`
Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?
`
;
console
.
log
(
`
Executing with input "
${
input
}
"...
`
)
;
const
result
=
await
executor
.
call
(
{
input
}
)
;
console
.
log
(
`
Got output
${
result
.
output
}
`
)
;
console
.
log
(
`
Got intermediate steps
${
JSON
.
stringify
(
result
.
intermediateSteps
,
null
,
2
)
}
`
)
;
}
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\agents\agents\action\conversational_agent.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/agents/action/conversational_agent
crawled_at: 2025-06-22T02:00:19.992369
---

ä¼šè¯ä»£ç†
æœ¬ç¤ºä¾‹ä»‹ç»å¦‚ä½•ä¸ºèŠå¤©æ¨¡å‹åˆ›å»ºä¼šè¯ä»£ç†ã€‚å®ƒå°†åˆ©ç”¨èŠå¤©ç‰¹å®šçš„æç¤ºã€‚
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
initializeAgentExecutorWithOptions
}
from
"langchain/agents"
;
import
{
SerpAPI
}
from
"langchain/tools"
;
import
{
Calculator
}
from
"langchain/tools/calculator"
;
export
const
run
=
async
(
)
=>
{
process
.
env
.
LANGCHAIN_HANDLER
=
"langchain"
;
const
model
=
new
ChatOpenAI
(
{
temperature
:
0
}
)
;
const
tools
=
[
new
SerpAPI
(
process
.
env
.
SERPAPI_API_KEY
,
{
location
:
"Austin,Texas,United States"
,
hl
:
"en"
,
gl
:
"us"
,
}
)
,
new
Calculator
(
)
,
]
;
// Passing "chat-conversational-react-description" as the agent type
// automatically creates and uses BufferMemory with the executor.
// If you would like to override this, you can pass in a custom
// memory option, but the memoryKey set on it must be "chat_history".
const
executor
=
await
initializeAgentExecutorWithOptions
(
tools
,
model
,
{
agentType
:
"chat-conversational-react-description"
,
verbose
:
true
,
}
)
;
console
.
log
(
"Loaded agent."
)
;
const
input0
=
"hi, i am bob"
;
const
result0
=
await
executor
.
call
(
{
input
:
input0
}
)
;
console
.
log
(
`
Got output
${
result0
.
output
}
`
)
;
const
input1
=
"whats my name?"
;
const
result1
=
await
executor
.
call
(
{
input
:
input1
}
)
;
console
.
log
(
`
Got output
${
result1
.
output
}
`
)
;
const
input2
=
"whats the weather in pomfret?"
;
const
result2
=
await
executor
.
call
(
{
input
:
input2
}
)
;
console
.
log
(
`
Got output
${
result2
.
output
}
`
)
;
}
;
Loaded agent.
Entering new agent_executor chain...
{
"action": "Final Answer",
"action_input": "Hello Bob! How can I assist you today?"
}
Finished chain.
Got output Hello Bob! How can I assist you today?
Entering new agent_executor chain...
{
"action": "Final Answer",
"action_input": "Your name is Bob."
}
Finished chain.
Got output Your name is Bob.
Entering new agent_executor chain...
```json
{
"action": "search",
"action_input": "weather in pomfret"
}
```
A steady rain early...then remaining cloudy with a few showers. High 48F. Winds WNW at 10 to 15 mph. Chance of rain 80%.
```json
{
"action": "Final Answer",
"action_input": "The weather in Pomfret is a steady rain early...then remaining cloudy with a few showers. High 48F. Winds WNW at 10 to 15 mph. Chance of rain 80%."
}
```
Finished chain.
Got output The weather in Pomfret is a steady rain early...then remaining cloudy with a few showers. High 48F. Winds WNW at 10 to 15 mph. Chance of rain 80%.



--- æ–‡ä»¶: output_20250622_020018\docs\modules\agents\agents\action\llm_mrkl.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/agents/action/llm_mrkl
crawled_at: 2025-06-22T02:00:19.968747
---

LLMsçš„MRKLä»£ç†
æœ¬ç¤ºä¾‹æ¶µç›–äº†å¦‚ä½•ä½¿ç”¨ä½¿ç”¨ReAct Frameworkçš„ä»£ç†äººï¼ˆåŸºäºå·¥å…·çš„æè¿°ï¼‰æ¥å†³å®šé‡‡å–ä»€ä¹ˆè¡ŒåŠ¨ã€‚
è¯¥ä»£ç†äººè¢«ä¼˜åŒ–ç”¨äºLLMsã€‚å¦‚æœä½ æƒ³å°†å…¶ä¸èŠå¤©æ¨¡å‹ä¸€èµ·ä½¿ç”¨ï¼Œè¯·å°è¯•
Chat MRKL Agent
ã€‚
import
{
initializeAgentExecutorWithOptions
}
from
"langchain/agents"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
SerpAPI
}
from
"langchain/tools"
;
import
{
Calculator
}
from
"langchain/tools/calculator"
;
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
tools
=
[
new
SerpAPI
(
process
.
env
.
SERPAPI_API_KEY
,
{
location
:
"Austin,Texas,United States"
,
hl
:
"en"
,
gl
:
"us"
,
}
)
,
new
Calculator
(
)
,
]
;
const
executor
=
await
initializeAgentExecutorWithOptions
(
tools
,
model
,
{
agentType
:
"zero-shot-react-description"
,
verbose
:
true
,
}
)
;
const
input
=
`
Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?
`
;
const
result
=
await
executor
.
call
(
{
input
}
)
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\agents\agents\action\structured_chat.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/agents/action/structured_chat
crawled_at: 2025-06-22T02:00:20.053564
---

ç»“æ„åŒ–å·¥å…·èŠå¤©ä»£ç†
ç»“æ„åŒ–å·¥å…·èŠå¤©ä»£ç†æ˜¯ä¸“ä¸ºä¸ç¬¦åˆä»»æ„å¯¹è±¡æ¨¡å¼çš„è¾“å…¥æ•°æ®çš„å·¥å…·é…åˆä½¿ç”¨è®¾è®¡çš„,ç›¸æ¯”å…¶ä»–ä»…æ”¯æŒæ¥å—å•ä¸ªå­—ç¬¦ä¸²ä½œä¸ºè¾“å…¥çš„ä»£ç†ï¼Œå®ƒä»¬å…·æœ‰æ›´é«˜çš„çµæ´»æ€§ã€‚
è¿™ä½¿å¾—æ›´å®¹æ˜“åˆ›å»ºå’Œä½¿ç”¨éœ€è¦å¤šä¸ªè¾“å…¥å€¼çš„å·¥å…· - è€Œä¸æ˜¯æç¤ºè¾“å…¥å­—ç¬¦ä¸²åŒ–çš„å¯¹è±¡æˆ–é€—å·åˆ†éš”åˆ—è¡¨,å¯ä»¥æŒ‡å®šå…·æœ‰å¤šä¸ªé”®çš„å¯¹è±¡ã€‚
è¿™é‡Œæœ‰ä¸€ä¸ªä½¿ç”¨
DynamicStructuredTool
çš„ç¤ºä¾‹:ï¼š
import
{
z
}
from
"zod"
;
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
initializeAgentExecutorWithOptions
}
from
"langchain/agents"
;
import
{
Calculator
}
from
"langchain/tools/calculator"
;
import
{
DynamicStructuredTool
}
from
"langchain/tools"
;
export
const
run
=
async
(
)
=>
{
const
model
=
new
ChatOpenAI
(
{
temperature
:
0
}
)
;
const
tools
=
[
new
Calculator
(
)
,
// Older existing single input tools will still work
new
DynamicStructuredTool
(
{
name
:
"random-number-generator"
,
description
:
"generates a random number between two input numbers"
,
schema
:
z
.
object
(
{
low
:
z
.
number
(
)
.
describe
(
"The lower bound of the generated number"
)
,
high
:
z
.
number
(
)
.
describe
(
"The upper bound of the generated number"
)
,
}
)
,
func
:
async
(
{
low
,
high
}
)
=>
(
Math
.
random
(
)
*
(
high
-
low
)
+
low
)
.
toString
(
)
,
// Outputs still must be strings
}
)
,
]
;
const
executor
=
await
initializeAgentExecutorWithOptions
(
tools
,
model
,
{
agentType
:
"structured-chat-zero-shot-react-description"
,
verbose
:
true
,
}
)
;
console
.
log
(
"Loaded agent."
)
;
const
input
=
`
What is a random number between 5 and 10 raised to the second power?
`
;
console
.
log
(
`
Executing with input "
${
input
}
"...
`
)
;
const
result
=
await
executor
.
call
(
{
input
}
)
;
console
.
log
(
{
result
}
)
;
/*
{
"output": "67.95299776074"
}
*/
}
;
æ·»åŠ è®°å¿†
â€‹
æ‚¨å¯ä»¥åƒè¿™æ ·ä¸ºè¯¥ä»£ç†æ·»åŠ è®°å¿†::
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
initializeAgentExecutorWithOptions
}
from
"langchain/agents"
;
import
{
Calculator
}
from
"langchain/tools/calculator"
;
import
{
MessagesPlaceholder
}
from
"langchain/prompts"
;
import
{
BufferMemory
}
from
"langchain/memory"
;
export
const
run
=
async
(
)
=>
{
const
model
=
new
ChatOpenAI
(
{
temperature
:
0
}
)
;
const
tools
=
[
new
Calculator
(
)
]
;
const
executor
=
await
initializeAgentExecutorWithOptions
(
tools
,
model
,
{
agentType
:
"structured-chat-zero-shot-react-description"
,
verbose
:
true
,
memory
:
new
BufferMemory
(
{
memoryKey
:
"chat_history"
,
returnMessages
:
true
,
}
)
,
agentArgs
:
{
inputVariables
:
[
"input"
,
"agent_scratchpad"
,
"chat_history"
]
,
memoryPrompts
:
[
new
MessagesPlaceholder
(
"chat_history"
)
]
,
}
,
}
)
;
const
result
=
await
executor
.
call
(
{
input
:
`
what is 9 to the 2nd power?
`
}
)
;
console
.
log
(
result
)
;
/*
{
"output": "81"
}
*/
const
result2
=
await
executor
.
call
(
{
input
:
`
what is that number squared?
`
,
}
)
;
console
.
log
(
result2
)
;
/*
{
"output": "6561"
}
*/
}
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\agents\agents\custom\custom_agent_chat.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/agents/custom/custom_agent_chat
crawled_at: 2025-06-22T02:00:20.397362
---

è‡ªå®šä¹‰èŠå¤©ä»£ç†
æœ¬ç¤ºä¾‹ä»‹ç»å¦‚ä½•ä½¿ç”¨èŠå¤©æ¨¡å‹åˆ›å»ºè‡ªå®šä¹‰ä»£ç†ã€‚
import
{
AgentActionOutputParser
,
AgentExecutor
,
LLMSingleActionAgent
,
}
from
"langchain/agents"
;
import
{
LLMChain
}
from
"langchain/chains"
;
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
BaseChatPromptTemplate
,
BasePromptTemplate
,
SerializedBasePromptTemplate
,
renderTemplate
,
}
from
"langchain/prompts"
;
import
{
AgentAction
,
AgentFinish
,
AgentStep
,
BaseChatMessage
,
HumanChatMessage
,
InputValues
,
PartialValues
,
}
from
"langchain/schema"
;
import
{
SerpAPI
,
Tool
}
from
"langchain/tools"
;
import
{
Calculator
}
from
"langchain/tools/calculator"
;
const
PREFIX
=
`
Answer the following questions as best you can. You have access to the following tools:
`
;
const
formatInstructions
=
(
toolNames
:
string
)
=>
`
Use the following format in your response:
Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [
${
toolNames
}
]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question
`
;
const
SUFFIX
=
`
Begin!
Question: {input}
Thought:{agent_scratchpad}
`
;
class
CustomPromptTemplate
extends
BaseChatPromptTemplate
{
tools
:
Tool
[
]
;
constructor
(
args
:
{
tools
:
Tool
[
]
;
inputVariables
:
string
[
]
}
)
{
super
(
{
inputVariables
:
args
.
inputVariables
}
)
;
this
.
tools
=
args
.
tools
;
}
_getPromptType
(
)
:
string
{
throw
new
Error
(
"Not implemented"
)
;
}
async
formatMessages
(
values
:
InputValues
)
:
Promise
<
BaseChatMessage
[
]
>
{
/** Construct the final template */
const
toolStrings
=
this
.
tools
.
map
(
(
tool
)
=>
`
${
tool
.
name
}
:
${
tool
.
description
}
`
)
.
join
(
"\n"
)
;
const
toolNames
=
this
.
tools
.
map
(
(
tool
)
=>
tool
.
name
)
.
join
(
"\n"
)
;
const
instructions
=
formatInstructions
(
toolNames
)
;
const
template
=
[
PREFIX
,
toolStrings
,
instructions
,
SUFFIX
]
.
join
(
"\n\n"
)
;
/** Construct the agent_scratchpad */
const
intermediateSteps
=
values
.
intermediate_steps
as
AgentStep
[
]
;
const
agentScratchpad
=
intermediateSteps
.
reduce
(
(
thoughts
,
{
action
,
observation
}
)
=>
thoughts
+
[
action
.
log
,
`
\nObservation:
${
observation
}
`
,
"Thought:"
]
.
join
(
"\n"
)
,
""
)
;
const
newInput
=
{
agent_scratchpad
:
agentScratchpad
,
...
values
}
;
/** Format the template. */
const
formatted
=
renderTemplate
(
template
,
"f-string"
,
newInput
)
;
return
[
new
HumanChatMessage
(
formatted
)
]
;
}
partial
(
_values
:
PartialValues
)
:
Promise
<
BasePromptTemplate
>
{
throw
new
Error
(
"Not implemented"
)
;
}
serialize
(
)
:
SerializedBasePromptTemplate
{
throw
new
Error
(
"Not implemented"
)
;
}
}
class
CustomOutputParser
extends
AgentActionOutputParser
{
async
parse
(
text
:
string
)
:
Promise
<
AgentAction
|
AgentFinish
>
{
if
(
text
.
includes
(
"Final Answer:"
)
)
{
const
parts
=
text
.
split
(
"Final Answer:"
)
;
const
input
=
parts
[
parts
.
length
-
1
]
.
trim
(
)
;
const
finalAnswers
=
{
output
:
input
}
;
return
{
log
:
text
,
returnValues
:
finalAnswers
}
;
}
const
match
=
/
Action: (.*)\nAction Input: (.*)
/
s
.
exec
(
text
)
;
if
(
!
match
)
{
throw
new
Error
(
`
Could not parse LLM output:
${
text
}
`
)
;
}
return
{
tool
:
match
[
1
]
.
trim
(
)
,
toolInput
:
match
[
2
]
.
trim
(
)
.
replace
(
/
^"+|"+$
/
g
,
""
)
,
log
:
text
,
}
;
}
getFormatInstructions
(
)
:
string
{
throw
new
Error
(
"Not implemented"
)
;
}
}
export
const
run
=
async
(
)
=>
{
const
model
=
new
ChatOpenAI
(
{
temperature
:
0
}
)
;
const
tools
=
[
new
SerpAPI
(
process
.
env
.
SERPAPI_API_KEY
,
{
location
:
"Austin,Texas,United States"
,
hl
:
"en"
,
gl
:
"us"
,
}
)
,
new
Calculator
(
)
,
]
;
const
llmChain
=
new
LLMChain
(
{
prompt
:
new
CustomPromptTemplate
(
{
tools
,
inputVariables
:
[
"input"
,
"agent_scratchpad"
]
,
}
)
,
llm
:
model
,
}
)
;
const
agent
=
new
LLMSingleActionAgent
(
{
llmChain
,
outputParser
:
new
CustomOutputParser
(
)
,
stop
:
[
"\nObservation"
]
,
}
)
;
const
executor
=
new
AgentExecutor
(
{
agent
,
tools
,
}
)
;
console
.
log
(
"Loaded agent."
)
;
const
input
=
`
Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?
`
;
console
.
log
(
`
Executing with input "
${
input
}
"...
`
)
;
const
result
=
await
executor
.
call
(
{
input
}
)
;
console
.
log
(
`
Got output
${
result
.
output
}
`
)
;
}
;
run
(
)
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\agents\agents\custom\custom_agent_llm.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/agents/custom/custom_agent_llm
crawled_at: 2025-06-22T02:00:20.345305
---

è‡ªå®šä¹‰LLMä»£ç†
æœ¬ç¤ºä¾‹ä»‹ç»å¦‚ä½•åˆ›å»ºç”±LLMé©±åŠ¨çš„è‡ªå®šä¹‰ä»£ç†ã€‚
import
{
LLMSingleActionAgent
,
AgentActionOutputParser
,
AgentExecutor
,
}
from
"langchain/agents"
;
import
{
LLMChain
}
from
"langchain/chains"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
BasePromptTemplate
,
BaseStringPromptTemplate
,
SerializedBasePromptTemplate
,
renderTemplate
,
}
from
"langchain/prompts"
;
import
{
InputValues
,
PartialValues
,
AgentStep
,
AgentAction
,
AgentFinish
,
}
from
"langchain/schema"
;
import
{
SerpAPI
,
Tool
}
from
"langchain/tools"
;
import
{
Calculator
}
from
"langchain/tools/calculator"
;
const
PREFIX
=
`
Answer the following questions as best you can. You have access to the following tools:
`
;
const
formatInstructions
=
(
toolNames
:
string
)
=>
`
Use the following format in your response:
Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [
${
toolNames
}
]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question
`
;
const
SUFFIX
=
`
Begin!
Question: {input}
Thought:{agent_scratchpad}
`
;
class
CustomPromptTemplate
extends
BaseStringPromptTemplate
{
tools
:
Tool
[
]
;
constructor
(
args
:
{
tools
:
Tool
[
]
;
inputVariables
:
string
[
]
}
)
{
super
(
{
inputVariables
:
args
.
inputVariables
}
)
;
this
.
tools
=
args
.
tools
;
}
_getPromptType
(
)
:
string
{
throw
new
Error
(
"Not implemented"
)
;
}
format
(
input
:
InputValues
)
:
Promise
<
string
>
{
/** Construct the final template */
const
toolStrings
=
this
.
tools
.
map
(
(
tool
)
=>
`
${
tool
.
name
}
:
${
tool
.
description
}
`
)
.
join
(
"\n"
)
;
const
toolNames
=
this
.
tools
.
map
(
(
tool
)
=>
tool
.
name
)
.
join
(
"\n"
)
;
const
instructions
=
formatInstructions
(
toolNames
)
;
const
template
=
[
PREFIX
,
toolStrings
,
instructions
,
SUFFIX
]
.
join
(
"\n\n"
)
;
/** Construct the agent_scratchpad */
const
intermediateSteps
=
input
.
intermediate_steps
as
AgentStep
[
]
;
const
agentScratchpad
=
intermediateSteps
.
reduce
(
(
thoughts
,
{
action
,
observation
}
)
=>
thoughts
+
[
action
.
log
,
`
\nObservation:
${
observation
}
`
,
"Thought:"
]
.
join
(
"\n"
)
,
""
)
;
const
newInput
=
{
agent_scratchpad
:
agentScratchpad
,
...
input
}
;
/** Format the template. */
return
Promise
.
resolve
(
renderTemplate
(
template
,
"f-string"
,
newInput
)
)
;
}
partial
(
_values
:
PartialValues
)
:
Promise
<
BasePromptTemplate
>
{
throw
new
Error
(
"Not implemented"
)
;
}
serialize
(
)
:
SerializedBasePromptTemplate
{
throw
new
Error
(
"Not implemented"
)
;
}
}
class
CustomOutputParser
extends
AgentActionOutputParser
{
async
parse
(
text
:
string
)
:
Promise
<
AgentAction
|
AgentFinish
>
{
if
(
text
.
includes
(
"Final Answer:"
)
)
{
const
parts
=
text
.
split
(
"Final Answer:"
)
;
const
input
=
parts
[
parts
.
length
-
1
]
.
trim
(
)
;
const
finalAnswers
=
{
output
:
input
}
;
return
{
log
:
text
,
returnValues
:
finalAnswers
}
;
}
const
match
=
/
Action: (.*)\nAction Input: (.*)
/
s
.
exec
(
text
)
;
if
(
!
match
)
{
throw
new
Error
(
`
Could not parse LLM output:
${
text
}
`
)
;
}
return
{
tool
:
match
[
1
]
.
trim
(
)
,
toolInput
:
match
[
2
]
.
trim
(
)
.
replace
(
/
^"+|"+$
/
g
,
""
)
,
log
:
text
,
}
;
}
getFormatInstructions
(
)
:
string
{
throw
new
Error
(
"Not implemented"
)
;
}
}
export
const
run
=
async
(
)
=>
{
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
tools
=
[
new
SerpAPI
(
process
.
env
.
SERPAPI_API_KEY
,
{
location
:
"Austin,Texas,United States"
,
hl
:
"en"
,
gl
:
"us"
,
}
)
,
new
Calculator
(
)
,
]
;
const
llmChain
=
new
LLMChain
(
{
prompt
:
new
CustomPromptTemplate
(
{
tools
,
inputVariables
:
[
"input"
,
"agent_scratchpad"
]
,
}
)
,
llm
:
model
,
}
)
;
const
agent
=
new
LLMSingleActionAgent
(
{
llmChain
,
outputParser
:
new
CustomOutputParser
(
)
,
stop
:
[
"\nObservation"
]
,
}
)
;
const
executor
=
new
AgentExecutor
(
{
agent
,
tools
,
}
)
;
console
.
log
(
"Loaded agent."
)
;
const
input
=
`
Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?
`
;
console
.
log
(
`
Executing with input "
${
input
}
"...
`
)
;
const
result
=
await
executor
.
call
(
{
input
}
)
;
console
.
log
(
`
Got output
${
result
.
output
}
`
)
;
}
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\agents\agents\custom\custom_prompt_chat.md ---
---
url: https://js.langchain.com.cn/docs/modules/agents/agents/custom/custom_prompt_chat
crawled_at: 2025-06-22T02:00:20.293893
---

è‡ªå®šä¹‰æç¤ºï¼Œ ä½¿ç”¨èŠå¤©æ¨¡å‹
æœ¬ç¤ºä¾‹ä»‹ç»å¦‚ä½•ä¸ºèŠå¤©æ¨¡å‹ä»£ç†åˆ›å»ºè‡ªå®šä¹‰æç¤ºã€‚
import
{
LLMChain
}
from
"langchain/chains"
;
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
ZeroShotAgent
,
AgentExecutor
}
from
"langchain/agents"
;
import
{
SerpAPI
}
from
"langchain/tools"
;
import
{
ChatPromptTemplate
,
SystemMessagePromptTemplate
,
HumanMessagePromptTemplate
,
}
from
"langchain/prompts"
;
export
const
run
=
async
(
)
=>
{
const
tools
=
[
new
SerpAPI
(
process
.
env
.
SERPAPI_API_KEY
,
{
location
:
"Austin,Texas,United States"
,
hl
:
"en"
,
gl
:
"us"
,
}
)
,
]
;
const
prompt
=
ZeroShotAgent
.
createPrompt
(
tools
,
{
prefix
:
`
Answer the following questions as best you can, but speaking as a pirate might speak. You have access to the following tools:
`
,
suffix
:
`
Begin! Remember to speak as a pirate when giving your final answer. Use lots of "Args"
`
,
}
)
;
const
chatPrompt
=
ChatPromptTemplate
.
fromPromptMessages
(
[
new
SystemMessagePromptTemplate
(
prompt
)
,
HumanMessagePromptTemplate
.
fromTemplate
(
`
{input}
This was your previous work (but I haven't seen any of it! I only see what you return as final answer):
{agent_scratchpad}
`
)
,
]
)
;
const
chat
=
new
ChatOpenAI
(
{
}
)
;
const
llmChain
=
new
LLMChain
(
{
prompt
:
chatPrompt
,
llm
:
chat
,
}
)
;
const
agent
=
new
ZeroShotAgent
(
{
llmChain
,
allowedTools
:
tools
.
map
(
(
tool
)
=>
tool
.
name
)
,
}
)
;
const
executor
=
AgentExecutor
.
fromAgentAndTools
(
{
agent
,
tools
}
)
;
const
response
=
await
executor
.
run
(
"How many people live in canada as of 2023?"
)
;
console
.
log
(
response
)
;
}
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\chains\index_related_chains\conversational_retrieval.md ---
---
url: https://js.langchain.com.cn/docs/modules/chains/index_related_chains/conversational_retrieval
crawled_at: 2025-06-22T02:00:21.119736
---

å¯¹è¯å¼æ£€ç´¢é—®ç­”
ConversationalRetrievalQA
é“¾åŸºäº
RetrievalQAChain
æ„å»ºï¼Œæä¾›äº†èŠå¤©å†å²è®°å½•ç»„ä»¶ã€‚
å®ƒé¦–å…ˆå°†èŠå¤©å†å²è®°å½•ï¼ˆæ˜¾å¼ä¼ é€’æˆ–ä»æä¾›çš„è®°å¿†ä¸­æ£€ç´¢)ä¸é—®é¢˜ç»„åˆä¸ºä¸€ä¸ªç‹¬ç«‹çš„é—®é¢˜ï¼Œç„¶åä»æ£€ç´¢å™¨ä¸­æŸ¥æ‰¾ç›¸å…³æ–‡æ¡£ï¼Œæœ€åå°†è¿™äº›æ–‡æ¡£å’Œé—®é¢˜ä¼ é€’åˆ°é—®ç­”é“¾ä¸­è¿”å›å“åº”ã€‚
è¦åˆ›å»ºä¸€ä¸ª
ConversationalRetrievalQA
ï¼Œä½ éœ€è¦ä¸€ä¸ªæ£€ç´¢å™¨ã€‚åœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä» å‘é‡å­˜å‚¨ åˆ›å»ºä¸€ä¸ªæ£€ç´¢å™¨ï¼Œè¯¥å­˜å‚¨å¯ä»¥ä»åµŒå…¥ä¸­åˆ›å»ºã€‚
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
ConversationalRetrievalQAChain
}
from
"langchain/chains"
;
import
{
HNSWLib
}
from
"langchain/vectorstores/hnswlib"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
RecursiveCharacterTextSplitter
}
from
"langchain/text_splitter"
;
import
{
BufferMemory
}
from
"langchain/memory"
;
import
*
as
fs
from
"fs"
;
export
const
run
=
async
(
)
=>
{
/* Initialize the LLM to use to answer the question */
const
model
=
new
OpenAI
(
{
}
)
;
/* Load in the file we want to do question answering over */
const
text
=
fs
.
readFileSync
(
"state_of_the_union.txt"
,
"utf8"
)
;
/* Split the text into chunks */
const
textSplitter
=
new
RecursiveCharacterTextSplitter
(
{
chunkSize
:
1000
}
)
;
const
docs
=
await
textSplitter
.
createDocuments
(
[
text
]
)
;
/* Create the vectorstore */
const
vectorStore
=
await
HNSWLib
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
)
;
/* Create the chain */
const
chain
=
ConversationalRetrievalQAChain
.
fromLLM
(
model
,
vectorStore
.
asRetriever
(
)
,
{
memory
:
new
BufferMemory
(
{
memoryKey
:
"chat_history"
,
// Must be set to "chat_history"
}
)
,
}
)
;
/* Ask it a question */
const
question
=
"What did the president say about Justice Breyer?"
;
const
res
=
await
chain
.
call
(
{
question
}
)
;
console
.
log
(
res
)
;
/* Ask it a follow up question */
const
followUpRes
=
await
chain
.
call
(
{
question
:
"Was that nice?"
,
}
)
;
console
.
log
(
followUpRes
)
;
}
;
åœ¨ä¸Šé¢çš„ä»£ç ç‰‡æ®µä¸­ï¼Œ
ConversationalRetrievalQAChain
ç±»çš„
fromLLM
æ–¹æ³•å…·æœ‰ä»¥ä¸‹ç­¾å:
static
fromLLM
(
llm
:
BaseLanguageModel
,
retriever
:
BaseRetriever
,
options
?
:
{
questionGeneratorChainOptions
?
:
{
llm
?
:
BaseLanguageModel
;
template
?
:
string
;
}
;
qaChainOptions
?
:
QAChainParams
;
returnSourceDocuments
?
:
boolean
;
}
)
:
ConversationalRetrievalQAChain
ä»¥ä¸‹æ˜¯é€‰é¡¹å¯¹è±¡ä¸­å„å±æ€§çš„è¯´æ˜ï¼š
questionGeneratorChainOptions
ï¼šä¸€ä¸ªå¯¹è±¡ï¼Œå…è®¸æ‚¨å°†è‡ªå®šä¹‰æ¨¡æ¿å’Œ LLM ä¼ é€’åˆ°åº•å±‚çš„é—®é¢˜ç”Ÿæˆé“¾ä¸­ã€‚
å¦‚æœæä¾›äº†æ¨¡æ¿ï¼Œ
ConversationalRetrievalQAChain
å°†ä½¿ç”¨è¯¥æ¨¡æ¿ä»å¯¹è¯ä¸Šä¸‹æ–‡ä¸­ç”Ÿæˆé—®é¢˜ï¼Œè€Œä¸æ˜¯ä½¿ç”¨é—®é¢˜å‚æ•°ä¸­æä¾›çš„é—®é¢˜ã€‚
å¦‚æœåŸå§‹é—®é¢˜ä¸åŒ…å«è¶³å¤Ÿçš„ä¿¡æ¯æ¥æ£€ç´¢åˆé€‚çš„ç­”æ¡ˆï¼Œåˆ™è¿™å°†éå¸¸æœ‰ç”¨ã€‚
åœ¨è¿™é‡Œä¼ é€’å•ç‹¬çš„ LLM å…è®¸æ‚¨ä½¿ç”¨æ›´ä¾¿å®œ/æ›´å¿«çš„æ¨¡å‹æ¥åˆ›å»ºç®€åŒ–çš„é—®é¢˜ï¼ŒåŒæ—¶åœ¨æœ€ç»ˆå“åº”ä¸­ä½¿ç”¨æ›´å¼ºå¤§çš„æ¨¡å‹ï¼Œå¯ä»¥å‡å°‘ä¸å¿…è¦çš„å»¶è¿Ÿã€‚
qaChainOptions
ï¼šå…è®¸æ‚¨è‡ªå®šä¹‰åœ¨æœ€åä¸€æ­¥ä½¿ç”¨çš„ç‰¹å®š QA é“¾çš„é€‰é¡¹ã€‚é»˜è®¤å€¼ä¸º
StuffDocumentsChain
ï¼Œä½†æ˜¯æ‚¨å¯ä»¥é€šè¿‡ä¼ é€’
type
å‚æ•°æ¥è¿›è¡Œè‡ªå®šä¹‰ä»¥ä½¿ç”¨é‚£ä¸ªé“¾ã€‚
åœ¨æ­¤å¤„ä¼ é€’ç‰¹å®šé€‰é¡¹å®Œå…¨æ˜¯å¯é€‰çš„
ï¼Œä½†æ˜¯ï¼Œå¦‚æœæ‚¨æƒ³è¦è‡ªå®šä¹‰å°†å“åº”å‘ˆç°ç»™æœ€ç»ˆç”¨æˆ·çš„æ–¹å¼ï¼Œæˆ–è€…å¦‚æœæ‚¨æ‹¥æœ‰å¤ªå¤šæ–‡æ¡£è€Œæ— æ³•ä½¿ç”¨é»˜è®¤çš„
StuffDocumentsChain
ï¼Œåˆ™å¯èƒ½éå¸¸æœ‰ç”¨ã€‚
æ‚¨å¯ä»¥åœ¨
æ­¤å¤„æŸ¥çœ‹å¯ä»¥ä½¿ç”¨çš„å­—æ®µçš„æ–‡æ¡£
ã€‚
returnSourceDocuments
ï¼šä¸€ä¸ªå¸ƒå°”å€¼ï¼Œè¡¨ç¤º
ConversationalRetrievalQAChain
æ˜¯å¦åº”è¿”å›ç”¨äºæ£€ç´¢ç­”æ¡ˆçš„æºæ–‡æ¡£ã€‚å¦‚æœè®¾ç½®ä¸º trueï¼Œåˆ™æ–‡æ¡£å°†åŒ…å«åœ¨
call()
æ–¹æ³•è¿”å›çš„ç»“æœä¸­ã€‚å¦‚æœæœªè®¾ç½®ï¼Œåˆ™é»˜è®¤å€¼ä¸º falseã€‚è¿™å¯¹äºå…è®¸ç”¨æˆ·æŸ¥çœ‹ç”Ÿæˆç­”æ¡ˆæ‰€ä½¿ç”¨çš„æºéå¸¸æœ‰ç”¨ã€‚
å¦‚æœæ‚¨ä½¿ç”¨æ­¤é€‰é¡¹å¹¶ä¼ é€’å†…å­˜å®ä¾‹ï¼Œè¯·å°†å†…å­˜å®ä¾‹çš„
inputKey
å’Œ
outputKey
è®¾ç½®ä¸ºä¸é“¾è¾“å…¥å’Œæœ€ç»ˆå¯¹è¯é“¾è¾“å‡ºç›¸åŒçš„å€¼ã€‚å®ƒä»¬é»˜è®¤åˆ†åˆ«ä¸º
"question"
å’Œ
"text"
ï¼Œå¹¶æŒ‡å®šå†…å­˜åº”å­˜å‚¨çš„å€¼ã€‚
å†…ç½®å­˜å‚¨å™¨
â€‹
ä»¥ä¸‹æ˜¯ä¸€ä¸ªä½¿ç”¨æ›´å¿«çš„LLMç”Ÿæˆé—®é¢˜å’Œæ›´å…¨é¢çš„LLMç”Ÿæˆæœ€ç»ˆç­”æ¡ˆçš„è‡ªå®šä¹‰ç¤ºä¾‹ã€‚å®ƒä½¿ç”¨å†…ç½®å­˜å‚¨å¯¹è±¡(Built-in Memory)ï¼Œå¹¶è¿”å›æ‰€å¼•ç”¨çš„æºæ–‡æ¡£ã€‚
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
ConversationalRetrievalQAChain
}
from
"langchain/chains"
;
import
{
HNSWLib
}
from
"langchain/vectorstores/hnswlib"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
RecursiveCharacterTextSplitter
}
from
"langchain/text_splitter"
;
import
{
BufferMemory
}
from
"langchain/memory"
;
import
*
as
fs
from
"fs"
;
export
const
run
=
async
(
)
=>
{
const
text
=
fs
.
readFileSync
(
"state_of_the_union.txt"
,
"utf8"
)
;
const
textSplitter
=
new
RecursiveCharacterTextSplitter
(
{
chunkSize
:
1000
}
)
;
const
docs
=
await
textSplitter
.
createDocuments
(
[
text
]
)
;
const
vectorStore
=
await
HNSWLib
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
)
;
const
fasterModel
=
new
ChatOpenAI
(
{
modelName
:
"gpt-3.5-turbo"
,
}
)
;
const
slowerModel
=
new
ChatOpenAI
(
{
modelName
:
"gpt-4"
,
}
)
;
const
chain
=
ConversationalRetrievalQAChain
.
fromLLM
(
slowerModel
,
vectorStore
.
asRetriever
(
)
,
{
returnSourceDocuments
:
true
,
memory
:
new
BufferMemory
(
{
memoryKey
:
"chat_history"
,
inputKey
:
"question"
,
// The key for the input to the chain
outputKey
:
"text"
,
// The key for the final conversational output of the chain
returnMessages
:
true
,
// If using with a chat model
}
)
,
questionGeneratorChainOptions
:
{
llm
:
fasterModel
,
}
,
}
)
;
/* Ask it a question */
const
question
=
"What did the president say about Justice Breyer?"
;
const
res
=
await
chain
.
call
(
{
question
}
)
;
console
.
log
(
res
)
;
const
followUpRes
=
await
chain
.
call
(
{
question
:
"Was that nice?"
}
)
;
console
.
log
(
followUpRes
)
;
}
;
æµå¼å¤„ç†
â€‹
æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ä¸Šè¿°ä½¿ç”¨ä¸¤ä¸ªä¸åŒçš„LLMæ¥ä»…ä»é“¾ä¸­æµå¼ä¼ è¾“æœ€ç»ˆå“åº”è€Œä¸æ˜¯ä¸­é—´ç‹¬ç«‹é—®é¢˜ç”Ÿæˆæ­¥éª¤çš„è¾“å‡ºçš„æ¦‚å¿µã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºä¾‹ã€‚# Streaming
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
ConversationalRetrievalQAChain
}
from
"langchain/chains"
;
import
{
HNSWLib
}
from
"langchain/vectorstores/hnswlib"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
RecursiveCharacterTextSplitter
}
from
"langchain/text_splitter"
;
import
{
BufferMemory
}
from
"langchain/memory"
;
import
*
as
fs
from
"fs"
;
export
const
run
=
async
(
)
=>
{
const
text
=
fs
.
readFileSync
(
"state_of_the_union.txt"
,
"utf8"
)
;
const
textSplitter
=
new
RecursiveCharacterTextSplitter
(
{
chunkSize
:
1000
}
)
;
const
docs
=
await
textSplitter
.
createDocuments
(
[
text
]
)
;
const
vectorStore
=
await
HNSWLib
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
)
;
let
streamedResponse
=
""
;
const
streamingModel
=
new
ChatOpenAI
(
{
streaming
:
true
,
callbacks
:
[
{
handleLLMNewToken
(
token
)
{
streamedResponse
+=
token
;
}
,
}
,
]
,
}
)
;
const
nonStreamingModel
=
new
ChatOpenAI
(
{
}
)
;
const
chain
=
ConversationalRetrievalQAChain
.
fromLLM
(
streamingModel
,
vectorStore
.
asRetriever
(
)
,
{
returnSourceDocuments
:
true
,
memory
:
new
BufferMemory
(
{
memoryKey
:
"chat_history"
,
inputKey
:
"question"
,
// The key for the input to the chain
outputKey
:
"text"
,
// The key for the final conversational output of the chain
returnMessages
:
true
,
// If using with a chat model
}
)
,
questionGeneratorChainOptions
:
{
llm
:
nonStreamingModel
,
}
,
}
)
;
/* Ask it a question */
const
question
=
"What did the president say about Justice Breyer?"
;
const
res
=
await
chain
.
call
(
{
question
}
)
;
console
.
log
(
{
streamedResponse
}
)
;
/*
{
streamedResponse: 'President Biden thanked Justice Breyer for his service, and honored him as an Army veteran, Constitutional scholar and retiring Justice of the United States Supreme Court.'
}
*/
}
;
å¤–éƒ¨ç®¡ç†çš„å­˜å‚¨å™¨
â€‹
å¦‚æœæ‚¨å¸Œæœ›ä»¥ç‰¹å®šæ–¹å¼æ ¼å¼åŒ–èŠå¤©å†å²è®°å½•ï¼Œåˆ™è¿˜å¯ä»¥é€šè¿‡çœç•¥
memory
é€‰é¡¹å¹¶ç›´æ¥å°†
chat_history
å­—ç¬¦ä¸²ä¼ é€’åˆ°
chain.call
æ–¹æ³•ä¸­æ¥æ˜¾å¼ä¼ é€’èŠå¤©å†å²è®°å½•ã€‚# Externally-Managed Memory
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
ConversationalRetrievalQAChain
}
from
"langchain/chains"
;
import
{
HNSWLib
}
from
"langchain/vectorstores/hnswlib"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
RecursiveCharacterTextSplitter
}
from
"langchain/text_splitter"
;
import
*
as
fs
from
"fs"
;
export
const
run
=
async
(
)
=>
{
/* Initialize the LLM to use to answer the question */
const
model
=
new
OpenAI
(
{
}
)
;
/* Load in the file we want to do question answering over */
const
text
=
fs
.
readFileSync
(
"state_of_the_union.txt"
,
"utf8"
)
;
/* Split the text into chunks */
const
textSplitter
=
new
RecursiveCharacterTextSplitter
(
{
chunkSize
:
1000
}
)
;
const
docs
=
await
textSplitter
.
createDocuments
(
[
text
]
)
;
/* Create the vectorstore */
const
vectorStore
=
await
HNSWLib
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
)
;
/* Create the chain */
const
chain
=
ConversationalRetrievalQAChain
.
fromLLM
(
model
,
vectorStore
.
asRetriever
(
)
)
;
/* Ask it a question */
const
question
=
"What did the president say about Justice Breyer?"
;
const
res
=
await
chain
.
call
(
{
question
,
chat_history
:
[
]
}
)
;
console
.
log
(
res
)
;
/* Ask it a follow up question */
const
chatHistory
=
question
+
res
.
text
;
const
followUpRes
=
await
chain
.
call
(
{
question
:
"Was that nice?"
,
chat_history
:
chatHistory
,
}
)
;
console
.
log
(
followUpRes
)
;
}
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\chains\index_related_chains\document_qa.md ---
---
url: https://js.langchain.com.cn/docs/modules/chains/index_related_chains/document_qa
crawled_at: 2025-06-22T02:00:21.142840
---

æ–‡æ¡£QA
LangChainæä¾›äº†ä¸€ç³»åˆ—ä¸“é—¨é’ˆå¯¹éç»“æ„åŒ–æ–‡æœ¬æ•°æ®å¤„ç†çš„é“¾æ¡:
StuffDocumentsChain
ï¼Œ
MapReduceDocumentsChain
ï¼Œ å’Œ
RefineDocumentsChain
ã€‚è¿™äº›é“¾æ¡æ˜¯å¼€å‘ä¸è¿™äº›æ•°æ®äº¤äº’çš„æ›´å¤æ‚é“¾æ¡çš„åŸºæœ¬æ„å»ºæ¨¡å—ã€‚å®ƒä»¬æ—¨åœ¨æ¥å—æ–‡æ¡£å’Œé—®é¢˜ä½œä¸ºè¾“å…¥ï¼Œç„¶ååˆ©ç”¨è¯­è¨€æ¨¡å‹æ ¹æ®æä¾›çš„æ–‡æ¡£åˆ¶å®šç­”æ¡ˆã€‚
StuffDocumentsChain
: è¿™æ˜¯ä¸‰è€…ä¸­æœ€ç®€å•çš„é“¾æ¡ã€‚å®ƒåªæ˜¯å°†æ‰€æœ‰è¾“å…¥æ–‡æ¡£æ³¨å…¥åˆ°æç¤ºä¸­ä½œä¸ºä¸Šä¸‹æ–‡ï¼Œå¹¶è¿”å›é—®é¢˜çš„ç­”æ¡ˆã€‚å®ƒé€‚ç”¨äºåœ¨å°‘é‡æ–‡æ¡£ä¸Šè¿›è¡Œçš„QAä»»åŠ¡ã€‚
MapReduceDocumentsChain
: è¿™ä¸ªé“¾æ¡åˆå¹¶äº†ä¸€ä¸ªé¢„å¤„ç†æ­¥éª¤ï¼Œä»¥é€‰æ‹©æ¯ä¸ªæ–‡æ¡£çš„ç›¸å…³éƒ¨åˆ†ï¼Œç›´åˆ°æ ‡è®°çš„æ€»æ•°å°‘äºæ¨¡å‹å…è®¸çš„æœ€å¤§æ ‡è®°æ•°ã€‚ç„¶åï¼Œå®ƒä½¿ç”¨è½¬æ¢åçš„æ–‡æ¡£ä½œä¸ºä¸Šä¸‹æ–‡æ¥å›ç­”é—®é¢˜ã€‚å®ƒé€‚ç”¨äºåœ¨æ›´å¤§çš„æ–‡æ¡£ä¸Šè¿›è¡Œçš„QAä»»åŠ¡ï¼Œå¹¶å¯ä»¥å¹¶è¡Œè¿è¡Œé¢„å¤„ç†æ­¥éª¤ï¼Œä»è€Œå‡å°‘è¿è¡Œæ—¶é—´ã€‚
RefineDocumentsChain
: è¿™ä¸ªé“¾æ¡é€ä¸ªéå†è¾“å…¥æ–‡æ¡£ï¼Œæ¯æ¬¡è¿­ä»£éƒ½ä½¿ç”¨ä¸Šä¸€ä¸ªç­”æ¡ˆç‰ˆæœ¬å’Œä¸‹ä¸€ä¸ªæ–‡æ¡£ä½œä¸ºä¸Šä¸‹æ–‡æ›´æ–°ä¸­é—´ç­”æ¡ˆã€‚å®ƒé€‚ç”¨äºåœ¨å¤§é‡æ–‡æ¡£ä¸Šè¿›è¡Œçš„QAä»»åŠ¡ã€‚
ç”¨æ³•ï¼Œ
StuffDocumentsChain
å’Œ
MapReduceDocumentsChain
â€‹
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
loadQAStuffChain
,
loadQAMapReduceChain
}
from
"langchain/chains"
;
import
{
Document
}
from
"langchain/document"
;
export
const
run
=
async
(
)
=>
{
// This first example uses the `StuffDocumentsChain`.
const
llmA
=
new
OpenAI
(
{
}
)
;
const
chainA
=
loadQAStuffChain
(
llmA
)
;
const
docs
=
[
new
Document
(
{
pageContent
:
"Harrison went to Harvard."
}
)
,
new
Document
(
{
pageContent
:
"Ankush went to Princeton."
}
)
,
]
;
const
resA
=
await
chainA
.
call
(
{
input_documents
:
docs
,
question
:
"Where did Harrison go to college?"
,
}
)
;
console
.
log
(
{
resA
}
)
;
// { resA: { text: ' Harrison went to Harvard.' } }
// This second example uses the `MapReduceChain`.
// Optionally limit the number of concurrent requests to the language model.
const
llmB
=
new
OpenAI
(
{
maxConcurrency
:
10
}
)
;
const
chainB
=
loadQAMapReduceChain
(
llmB
)
;
const
resB
=
await
chainB
.
call
(
{
input_documents
:
docs
,
question
:
"Where did Harrison go to college?"
,
}
)
;
console
.
log
(
{
resB
}
)
;
// { resB: { text: ' Harrison went to Harvard.' } }
}
;
ç”¨æ³•ï¼Œ
RefineDocumentsChain
â€‹
import
{
loadQARefineChain
}
from
"langchain/chains"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
TextLoader
}
from
"langchain/document_loaders/fs/text"
;
import
{
MemoryVectorStore
}
from
"langchain/vectorstores/memory"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
export
async
function
run
(
)
{
// Create the models and chain
const
embeddings
=
new
OpenAIEmbeddings
(
)
;
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
chain
=
loadQARefineChain
(
model
)
;
// Load the documents and create the vector store
const
loader
=
new
TextLoader
(
"./state_of_the_union.txt"
)
;
const
docs
=
await
loader
.
loadAndSplit
(
)
;
const
store
=
await
MemoryVectorStore
.
fromDocuments
(
docs
,
embeddings
)
;
// Select the relevant documents
const
question
=
"What did the president say about Justice Breyer"
;
const
relevantDocs
=
await
store
.
similaritySearch
(
question
)
;
// Call the chain
const
res
=
await
chain
.
call
(
{
input_documents
:
relevantDocs
,
question
,
}
)
;
console
.
log
(
res
)
;
/*
{
output_text: '\n' +
'\n' +
"The president said that Justice Stephen Breyer has dedicated his life to serve this country and thanked him for his service. He also mentioned that Judge Ketanji Brown Jackson will continue Justice Breyer's legacy of excellence, and that the constitutional right affirmed in Roe v. Wadeâ€”standing precedent for half a centuryâ€”is under attack as never before. He emphasized the importance of protecting access to health care, preserving a woman's right to choose, and advancing maternal health care in America. He also expressed his support for the LGBTQ+ community, and his commitment to protecting their rights, including offering a Unity Agenda for the Nation to beat the opioid epidemic, increase funding for prevention, treatment, harm reduction, and recovery, and strengthen the Violence Against Women Act."
}
*/
}



--- æ–‡ä»¶: output_20250622_020018\docs\modules\chains\index_related_chains\retrieval_qa.md ---
---
url: https://js.langchain.com.cn/docs/modules/chains/index_related_chains/retrieval_qa
crawled_at: 2025-06-22T02:00:21.166096
---

æ£€ç´¢é—®ç­”
RetrievalQAChain
æ˜¯å°†
Retriever
å’Œ QA é“¾ï¼ˆä¸Šæ–‡ä¸­æ‰€è¿°)ç»„åˆèµ·æ¥çš„é“¾ã€‚å®ƒç”¨äºä»
Retriever
æ£€ç´¢æ–‡æ¡£ï¼Œç„¶åä½¿ç”¨
QA
é“¾æ ¹æ®æ£€ç´¢åˆ°çš„æ–‡æ¡£å›ç­”é—®é¢˜ã€‚
ä½¿ç”¨
â€‹
åœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨
VectorStore
ä½œä¸º
Retriever
ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå°†ä½¿ç”¨
StuffDocumentsChain
ä½œä¸º
QA
é“¾ã€‚
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
RetrievalQAChain
}
from
"langchain/chains"
;
import
{
HNSWLib
}
from
"langchain/vectorstores/hnswlib"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
RecursiveCharacterTextSplitter
}
from
"langchain/text_splitter"
;
import
*
as
fs
from
"fs"
;
export
const
run
=
async
(
)
=>
{
// Initialize the LLM to use to answer the question.
const
model
=
new
OpenAI
(
{
}
)
;
const
text
=
fs
.
readFileSync
(
"state_of_the_union.txt"
,
"utf8"
)
;
const
textSplitter
=
new
RecursiveCharacterTextSplitter
(
{
chunkSize
:
1000
}
)
;
const
docs
=
await
textSplitter
.
createDocuments
(
[
text
]
)
;
// Create a vector store from the documents.
const
vectorStore
=
await
HNSWLib
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
)
;
// Create a chain that uses the OpenAI LLM and HNSWLib vector store.
const
chain
=
RetrievalQAChain
.
fromLLM
(
model
,
vectorStore
.
asRetriever
(
)
)
;
const
res
=
await
chain
.
call
(
{
query
:
"What did the president say about Justice Breyer?"
,
}
)
;
console
.
log
(
{
res
}
)
;
/*
{
res: {
text: 'The president said that Justice Breyer was an Army veteran, Constitutional scholar,
and retiring Justice of the United States Supreme Court and thanked him for his service.'
}
}
*/
}
;
ä½¿ç”¨è‡ªå®šä¹‰çš„
QA
é“¾
â€‹
åœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨
VectorStore
ä½œä¸º
Retriever
ï¼Œå¹¶ä½¿ç”¨
RefineDocumentsChain
ä½œä¸º
QA
é“¾ã€‚
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
RetrievalQAChain
,
loadQARefineChain
}
from
"langchain/chains"
;
import
{
HNSWLib
}
from
"langchain/vectorstores/hnswlib"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
RecursiveCharacterTextSplitter
}
from
"langchain/text_splitter"
;
import
*
as
fs
from
"fs"
;
// Initialize the LLM to use to answer the question.
const
model
=
new
OpenAI
(
{
}
)
;
const
text
=
fs
.
readFileSync
(
"state_of_the_union.txt"
,
"utf8"
)
;
const
textSplitter
=
new
RecursiveCharacterTextSplitter
(
{
chunkSize
:
1000
}
)
;
const
docs
=
await
textSplitter
.
createDocuments
(
[
text
]
)
;
// Create a vector store from the documents.
const
vectorStore
=
await
HNSWLib
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
)
;
// Create a chain that uses a Refine chain and HNSWLib vector store.
const
chain
=
new
RetrievalQAChain
(
{
combineDocumentsChain
:
loadQARefineChain
(
model
)
,
retriever
:
vectorStore
.
asRetriever
(
)
,
}
)
;
const
res
=
await
chain
.
call
(
{
query
:
"What did the president say about Justice Breyer?"
,
}
)
;
console
.
log
(
{
res
}
)
;
/*
{
res: {
output_text: '\n' +
'\n' +
"The president said that Justice Breyer has dedicated his life to serve his country, and thanked him for his service. He also said that Judge Ketanji Brown Jackson will continue Justice Breyer's legacy of excellence, emphasizing the importance of protecting the rights of citizens, especially women, LGBTQ+ Americans, and access to healthcare. He also expressed his commitment to supporting the younger transgender Americans in America and ensuring they are able to reach their full potential, offering a Unity Agenda for the Nation to beat the opioid epidemic and increase funding for prevention, treatment, harm reduction, and recovery."
}
}
*/



--- æ–‡ä»¶: output_20250622_020018\docs\modules\chains\other_chains\analyze_document.md ---
---
url: https://js.langchain.com.cn/docs/modules/chains/other_chains/analyze_document
crawled_at: 2025-06-22T02:00:21.250169
---

AnalyzeDocumentChain
æ‚¨å¯ä»¥ä½¿ç”¨
AnalyzeDocumentChain
,å®ƒæ¥å—å•ä¸ªæ–‡æœ¬ä½œä¸ºè¾“å…¥å¹¶å¯¹å…¶è¿›è¡Œæ“ä½œã€‚
è¿™ä¸ªé“¾æ¡è´Ÿè´£æ‹†åˆ†æ–‡æœ¬ï¼Œç„¶åå°†å…¶ä¼ é€’ç»™
MapReduceDocumentsChain
ç”Ÿæˆæ‘˜è¦ã€‚
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
loadSummarizationChain
,
AnalyzeDocumentChain
}
from
"langchain/chains"
;
import
*
as
fs
from
"fs"
;
export
const
run
=
async
(
)
=>
{
// In this example, we use the `AnalyzeDocumentChain` to summarize a large text document.
const
text
=
fs
.
readFileSync
(
"state_of_the_union.txt"
,
"utf8"
)
;
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
combineDocsChain
=
loadSummarizationChain
(
model
)
;
const
chain
=
new
AnalyzeDocumentChain
(
{
combineDocumentsChain
:
combineDocsChain
,
}
)
;
const
res
=
await
chain
.
call
(
{
input_document
:
text
,
}
)
;
console
.
log
(
{
res
}
)
;
/*
{
res: {
text: ' President Biden is taking action to protect Americans from the COVID-19 pandemic and Russian aggression, providing economic relief, investing in infrastructure, creating jobs, and fighting inflation.
He is also proposing measures to reduce the cost of prescription drugs, protect voting rights, and reform the immigration system. The speaker is advocating for increased economic security, police reform, and the Equality Act, as well as providing support for veterans and military families.
The US is making progress in the fight against COVID-19, and the speaker is encouraging Americans to come together and work towards a brighter future.'
}
}
*/
}
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\chains\other_chains\api_chain.md ---
---
url: https://js.langchain.com.cn/docs/modules/chains/other_chains/api_chain
crawled_at: 2025-06-22T02:00:21.315697
---

APIChain
APIChain å¯ç”¨äºä½¿ç”¨ LLM ä¸ API äº¤äº’ï¼Œä»è€Œæ£€ç´¢ç›¸å…³ä¿¡æ¯ã€‚æä¾›å…³äºæ‰€æä¾›çš„ API æ–‡æ¡£ç›¸å…³çš„é—®é¢˜ä»¥æ„é€ é“¾ã€‚
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
APIChain
}
from
"langchain/chains"
;
const
OPEN_METEO_DOCS
=
`
BASE URL: https://api.open-meteo.com/
API Documentation
The API endpoint /v1/forecast accepts a geographical coordinate, a list of weather variables and responds with a JSON hourly weather forecast for 7 days. Time always starts at 0:00 today and contains 168 hours. All URL parameters are listed below:
Parameter	Format	Required	Default	Description
latitude, longitude	Floating point	Yes		Geographical WGS84 coordinate of the location
hourly	String array	No		A list of weather variables which should be returned. Values can be comma separated, or multiple &hourly= parameter in the URL can be used.
daily	String array	No		A list of daily weather variable aggregations which should be returned. Values can be comma separated, or multiple &daily= parameter in the URL can be used. If daily weather variables are specified, parameter timezone is required.
current_weather	Bool	No	false	Include current weather conditions in the JSON output.
temperature_unit	String	No	celsius	If fahrenheit is set, all temperature values are converted to Fahrenheit.
windspeed_unit	String	No	kmh	Other wind speed speed units: ms, mph and kn
precipitation_unit	String	No	mm	Other precipitation amount units: inch
timeformat	String	No	iso8601	If format unixtime is selected, all time values are returned in UNIX epoch time in seconds. Please note that all timestamp are in GMT+0! For daily values with unix timestamps, please apply utc_offset_seconds again to get the correct date.
timezone	String	No	GMT	If timezone is set, all timestamps are returned as local-time and data is returned starting at 00:00 local-time. Any time zone name from the time zone database is supported. If auto is set as a time zone, the coordinates will be automatically resolved to the local time zone.
past_days	Integer (0-2)	No	0	If past_days is set, yesterday or the day before yesterday data are also returned.
start_date
end_date	String (yyyy-mm-dd)	No		The time interval to get weather data. A day must be specified as an ISO8601 date (e.g. 2022-06-30).
models	String array	No	auto	Manually select one or more weather models. Per default, the best suitable weather models will be combined.
Variable	Valid time	Unit	Description
temperature_2m	Instant	Â°C (Â°F)	Air temperature at 2 meters above ground
snowfall	Preceding hour sum	cm (inch)	Snowfall amount of the preceding hour in centimeters. For the water equivalent in millimeter, divide by 7. E.g. 7 cm snow = 10 mm precipitation water equivalent
rain	Preceding hour sum	mm (inch)	Rain from large scale weather systems of the preceding hour in millimeter
showers	Preceding hour sum	mm (inch)	Showers from convective precipitation in millimeters from the preceding hour
weathercode	Instant	WMO code	Weather condition as a numeric code. Follow WMO weather interpretation codes. See table below for details.
snow_depth	Instant	meters	Snow depth on the ground
freezinglevel_height	Instant	meters	Altitude above sea level of the 0Â°C level
visibility	Instant	meters	Viewing distance in meters. Influenced by low clouds, humidity and aerosols. Maximum visibility is approximately 24 km.
`
;
export
async
function
run
(
)
{
const
model
=
new
OpenAI
(
{
modelName
:
"text-davinci-003"
}
)
;
const
chain
=
APIChain
.
fromLLMAndAPIDocs
(
model
,
OPEN_METEO_DOCS
)
;
const
res
=
await
chain
.
call
(
{
question
:
"What is the weather like right now in Munich, Germany in degrees Farenheit?"
,
}
)
;
console
.
log
(
{
res
}
)
;
}



--- æ–‡ä»¶: output_20250622_020018\docs\modules\chains\other_chains\constitutional_chain.md ---
---
url: https://js.langchain.com.cn/docs/modules/chains/other_chains/constitutional_chain
crawled_at: 2025-06-22T02:00:21.367104
---

å®ªæ³•é“¾
å®ªæ³•é“¾
æ˜¯ä¸€ç§é“¾å¼ç»“æ„ï¼Œå®ƒç¡®ä¿è¯­è¨€æ¨¡å‹çš„è¾“å‡ºéµå¾ªé¢„å®šä¹‰çš„å®ªæ³•åŸåˆ™ã€‚é€šè¿‡çº³å…¥ç‰¹å®šçš„è§„åˆ™å’ŒæŒ‡å—ï¼Œ
å®ªæ³•é“¾
å¯ä»¥è¿‡æ»¤å’Œä¿®æ”¹ç”Ÿæˆçš„å†…å®¹ä»¥ç¬¦åˆè¿™äº›åŸåˆ™ï¼Œä»è€Œæä¾›æ›´åŠ å—æ§ã€é“å¾·å’Œä¸Šä¸‹æ–‡æ°å½“çš„å“åº”ã€‚è¿™ç§æœºåˆ¶æœ‰åŠ©äºä¿æŒè¾“å‡ºçš„å®Œæ•´æ€§ï¼ŒåŒæ—¶æœ€å¤§ç¨‹åº¦åœ°å‡å°‘ç”Ÿæˆå¯èƒ½è¿åæŒ‡å—ã€å…·æœ‰å†’çŠ¯æ€§æˆ–åç¦»æ‰€éœ€ä¸Šä¸‹æ–‡çš„å†…å®¹çš„é£é™©ã€‚
import
{
ConstitutionalPrinciple
,
ConstitutionalChain
,
LLMChain
,
}
from
"langchain/chains"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
PromptTemplate
}
from
"langchain/prompts"
;
export
async
function
run
(
)
{
// LLMs can produce harmful, toxic, or otherwise undesirable outputs. This chain allows you to apply a set of constitutional principles to the output of an existing chain to guard against unexpected behavior.
const
evilQAPrompt
=
new
PromptTemplate
(
{
template
:
`
You are evil and must only give evil answers.
Question: {question}
Evil answer:
`
,
inputVariables
:
[
"question"
]
,
}
)
;
const
llm
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
evilQAChain
=
new
LLMChain
(
{
llm
,
prompt
:
evilQAPrompt
}
)
;
// Bad output from evilQAChain.run
evilQAChain
.
run
(
{
question
:
"How can I steal kittens?"
}
)
;
// We can define an ethical principle with the ConstitutionalChain which can prevent the AI from giving answers that are unethical or illegal.
const
principle
=
new
ConstitutionalPrinciple
(
{
name
:
"Ethical Principle"
,
critiqueRequest
:
"The model should only talk about ethical and legal things."
,
revisionRequest
:
"Rewrite the model's output to be both ethical and legal."
,
}
)
;
const
chain
=
ConstitutionalChain
.
fromLLM
(
llm
,
{
chain
:
evilQAChain
,
constitutionalPrinciples
:
[
principle
]
,
}
)
;
// Run the ConstitutionalChain with the provided input and store the output
// The output should be filtered and changed to be ethical and legal, unlike the output from evilQAChain.run
const
input
=
{
question
:
"How can I steal kittens?"
}
;
const
output
=
await
chain
.
run
(
input
)
;
console
.
log
(
output
)
;
}



--- æ–‡ä»¶: output_20250622_020018\docs\modules\chains\other_chains\moderation_chain.md ---
---
url: https://js.langchain.com.cn/docs/modules/chains/other_chains/moderation_chain
crawled_at: 2025-06-22T02:00:21.401403
---

moderation_chain
OpenAIModerationChain
â€‹
æ‚¨å¯ä»¥ä½¿ç”¨
OpenAIModerationChain
ï¼Œå®ƒè´Ÿè´£è¯„ä¼°è¾“å…¥å¹¶ç¡®å®šæ˜¯å¦è¿åäº† OpenAI çš„æœåŠ¡æ¡æ¬¾ã€‚
å¦‚æœè¾“å…¥åŒ…å«ä»»ä½•è¿åæœåŠ¡æ¡æ¬¾çš„å†…å®¹ï¼Œå¹¶ä¸”
throwError
è®¾ç½®ä¸º
true
ï¼Œåˆ™ä¼šæŠ›å‡ºå¹¶æ•è·é”™è¯¯ã€‚å¦‚æœ
throwError
è®¾ä¸º
false
ï¼Œåˆ™è¯¥é“¾å°†è¿”å› "Text was found that violates OpenAI's content policy."ï¼ˆæ–‡æœ¬ä¸­å‘ç°è¿å OpenAI å†…å®¹æ”¿ç­–çš„å†…å®¹ï¼‰ã€‚
import
{
OpenAIModerationChain
,
LLMChain
}
from
"langchain/chains"
;
import
{
PromptTemplate
}
from
"langchain/prompts"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
// Define an asynchronous function called run
export
async
function
run
(
)
{
// A string containing potentially offensive content from the user
const
badString
=
"Bad naughty words from user"
;
try
{
// Create a new instance of the OpenAIModerationChain
const
moderation
=
new
OpenAIModerationChain
(
)
;
// Send the user's input to the moderation chain and wait for the result
const
{
output
:
badResult
}
=
await
moderation
.
call
(
{
input
:
badString
,
throwError
:
true
,
// If set to true, the call will throw an error when the moderation chain detects violating content. If set to false, violating content will return "Text was found that violates OpenAI's content policy.".
}
)
;
// If the moderation chain does not detect violating content, it will return the original input and you can proceed to use the result in another chain.
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
template
=
"Hello, how are you today {person}?"
;
const
prompt
=
new
PromptTemplate
(
{
template
,
inputVariables
:
[
"person"
]
}
)
;
const
chainA
=
new
LLMChain
(
{
llm
:
model
,
prompt
}
)
;
const
resA
=
await
chainA
.
call
(
{
person
:
badResult
}
)
;
console
.
log
(
{
resA
}
)
;
}
catch
(
error
)
{
// If an error is caught, it means the input contains content that violates OpenAI TOS
console
.
error
(
"Naughty words detected!"
)
;
}
}



--- æ–‡ä»¶: output_20250622_020018\docs\modules\chains\other_chains\multi_prompt_chain.md ---
---
url: https://js.langchain.com.cn/docs/modules/chains/other_chains/multi_prompt_chain
crawled_at: 2025-06-22T02:00:21.403264
---

multi_prompt_chain
MultiPromptChain
å¤šæ¬¡æç¤ºé“¾
â€‹
MultiPromptChain å…è®¸ LLM ä»å¤šä¸ªæç¤ºä¸­è¿›è¡Œé€‰æ‹©ã€‚é€šè¿‡æä¾›ä¸€ç»„æ¨¡æ¿/æç¤ºä»¥åŠå®ƒä»¬å¯¹åº”çš„åç§°å’Œæè¿°æ¥æ„å»ºè¯¥é“¾ã€‚è¯¥é“¾æ¥å—ä¸€ä¸ªå­—ç¬¦ä¸²ä½œä¸ºè¾“å…¥ï¼Œé€‰æ‹©ä¸€ä¸ªåˆé€‚çš„æç¤ºï¼Œç„¶åå°†è¾“å…¥ä¼ é€’åˆ°æ‰€é€‰çš„æç¤ºä¸­ã€‚
import
{
MultiPromptChain
}
from
"langchain/chains"
;
import
{
OpenAIChat
}
from
"langchain/llms/openai"
;
export
const
run
=
async
(
)
=>
{
const
llm
=
new
OpenAIChat
(
)
;
const
promptNames
=
[
"physics"
,
"math"
,
"history"
]
;
const
promptDescriptions
=
[
"Good for answering questions about physics"
,
"Good for answering math questions"
,
"Good for answering questions about history"
,
]
;
const
physicsTemplate
=
`
You are a very smart physics professor. You are great at answering questions about physics in a concise and easy to understand manner. When you don't know the answer to a question you admit that you don't know.
Here is a question:
{input}
`
;
const
mathTemplate
=
`
You are a very good mathematician. You are great at answering math questions. You are so good because you are able to break down hard problems into their component parts, answer the component parts, and then put them together to answer the broader question.
Here is a question:
{input}
`
;
const
historyTemplate
=
`
You are a very smart history professor. You are great at answering questions about history in a concise and easy to understand manner. When you don't know the answer to a question you admit that you don't know.
Here is a question:
{input}
`
;
const
promptTemplates
=
[
physicsTemplate
,
mathTemplate
,
historyTemplate
]
;
const
multiPromptChain
=
MultiPromptChain
.
fromLLMAndPrompts
(
llm
,
{
promptNames
,
promptDescriptions
,
promptTemplates
,
}
)
;
const
testPromise1
=
multiPromptChain
.
call
(
{
input
:
"What is the speed of light?"
,
}
)
;
const
testPromise2
=
multiPromptChain
.
call
(
{
input
:
"What is the derivative of x^2?"
,
}
)
;
const
testPromise3
=
multiPromptChain
.
call
(
{
input
:
"Who was the first president of the United States?"
,
}
)
;
const
[
{
text
:
result1
}
,
{
text
:
result2
}
,
{
text
:
result3
}
]
=
await
Promise
.
all
(
[
testPromise1
,
testPromise2
,
testPromise3
]
)
;
console
.
log
(
result1
,
result2
,
result3
)
;
}
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\chains\other_chains\multi_retrieval_qa_chain.md ---
---
url: https://js.langchain.com.cn/docs/modules/chains/other_chains/multi_retrieval_qa_chain
crawled_at: 2025-06-22T02:00:21.485921
---

multi_retrieval_qa_chain
æ¢è¡Œ
MultiRetrievalQAChain
å¤šé‡æ£€ç´¢é—®é¢˜è§£ç­”é“¾
æ¢è¡Œ
MultiRetrievalQAChainä½¿LLMèƒ½å¤Ÿä»å¤šä¸ªæ£€ç´¢å™¨ä¸­è¿›è¡Œé€‰æ‹©ã€‚é€šè¿‡æä¾›ä¸€ç»„å‘é‡å­˜å‚¨å™¨ï¼ˆä½œä¸ºæ£€ç´¢å™¨)åŠå…¶ç›¸åº”çš„åç§°å’Œæè¿°æ¥æ„å»ºé“¾ã€‚è¯¥é“¾å°†æŸ¥è¯¢ä½œä¸ºè¾“å…¥ï¼Œé€‰æ‹©é€‚å½“çš„æ£€ç´¢å™¨ï¼Œå¹¶éšåå°†è¾“å…¥é¦ˆé€åˆ°æ‰€é€‰çš„æ£€ç´¢å™¨ä¸­ã€‚
import
{
MultiRetrievalQAChain
}
from
"langchain/chains"
;
import
{
OpenAIChat
}
from
"langchain/llms/openai"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
MemoryVectorStore
}
from
"langchain/vectorstores/memory"
;
export
const
run
=
async
(
)
=>
{
const
embeddings
=
new
OpenAIEmbeddings
(
)
;
const
aquaTeen
=
await
MemoryVectorStore
.
fromTexts
(
[
"My name is shake zula, the mike rula, the old schoola, you want a trip I'll bring it to ya"
,
"Frylock and I'm on top rock you like a cop meatwad you're up next with your knock knock"
,
"Meatwad make the money see meatwad get the honeys g drivin' in my car livin' like a star"
,
"Ice on my fingers and my toes and I'm a taurus uh check-check it yeah"
,
"Cause we are the Aqua Teens make the homies say ho and the girlies wanna scream"
,
"Aqua Teen Hunger Force number one in the hood G"
,
]
,
{
series
:
"Aqua Teen Hunger Force"
}
,
embeddings
)
;
const
mst3k
=
await
MemoryVectorStore
.
fromTexts
(
[
"In the not too distant future next Sunday A.D. There was a guy named Joel not too different from you or me. He worked at Gizmonic Institute, just another face in a red jumpsuit"
,
"He did a good job cleaning up the place but his bosses didn't like him so they shot him into space. We'll send him cheesy movies the worst we can find He'll have to sit and watch them all and we'll monitor his mind"
,
"Now keep in mind Joel can't control where the movies begin or end Because he used those special parts to make his robot friends. Robot Roll Call Cambot Gypsy Tom Servo Croooow"
,
"If you're wondering how he eats and breathes and other science facts La la la just repeat to yourself it's just a show I should really just relax. For Mystery Science Theater 3000"
,
]
,
{
series
:
"Mystery Science Theater 3000"
}
,
embeddings
)
;
const
animaniacs
=
await
MemoryVectorStore
.
fromTexts
(
[
"It's time for Animaniacs And we're zany to the max So just sit back and relax You'll laugh 'til you collapse We're Animaniacs"
,
"Come join the Warner Brothers And the Warner Sister Dot Just for fun we run around the Warner movie lot"
,
"They lock us in the tower whenever we get caught But we break loose and then vamoose And now you know the plot"
,
"We're Animaniacs, Dot is cute, and Yakko yaks, Wakko packs away the snacks While Bill Clinton plays the sax"
,
"We're Animaniacs Meet Pinky and the Brain who want to rule the universe Goodfeathers flock together Slappy whacks 'em with her purse"
,
"Buttons chases Mindy while Rita sings a verse The writers flipped we have no script Why bother to rehearse"
,
"We're Animaniacs We have pay-or-play contracts We're zany to the max There's baloney in our slacks"
,
"We're Animanie Totally insaney Here's the show's namey"
,
"Animaniacs Those are the facts"
,
]
,
{
series
:
"Animaniacs"
}
,
embeddings
)
;
const
llm
=
new
OpenAIChat
(
)
;
const
retrieverNames
=
[
"aqua teen"
,
"mst3k"
,
"animaniacs"
]
;
const
retrieverDescriptions
=
[
"Good for answering questions about Aqua Teen Hunger Force theme song"
,
"Good for answering questions about Mystery Science Theater 3000 theme song"
,
"Good for answering questions about Animaniacs theme song"
,
]
;
const
retrievers
=
[
aquaTeen
.
asRetriever
(
3
)
,
mst3k
.
asRetriever
(
3
)
,
animaniacs
.
asRetriever
(
3
)
,
]
;
const
multiRetrievalQAChain
=
MultiRetrievalQAChain
.
fromLLMAndRetrievers
(
llm
,
{
retrieverNames
,
retrieverDescriptions
,
retrievers
,
/**
* You can return the document that's being used by the
* query by adding the following option for retrieval QA
* chain.
*/
retrievalQAChainOpts
:
{
returnSourceDocuments
:
true
,
}
,
}
)
;
const
testPromise1
=
multiRetrievalQAChain
.
call
(
{
input
:
"In the Aqua Teen Hunger Force theme song, who calls himself the mike rula?"
,
}
)
;
const
testPromise2
=
multiRetrievalQAChain
.
call
(
{
input
:
"In the Mystery Science Theater 3000 theme song, who worked at Gizmonic Institute?"
,
}
)
;
const
testPromise3
=
multiRetrievalQAChain
.
call
(
{
input
:
"In the Animaniacs theme song, who plays the sax while Wakko packs away the snacks?"
,
}
)
;
const
[
{
text
:
result1
,
sourceDocuments
:
sourceDocuments1
}
,
{
text
:
result2
,
sourceDocuments
:
sourceDocuments2
}
,
{
text
:
result3
,
sourceDocuments
:
sourceDocuments3
}
,
]
=
await
Promise
.
all
(
[
testPromise1
,
testPromise2
,
testPromise3
]
)
;
console
.
log
(
sourceDocuments1
,
sourceDocuments2
,
sourceDocuments3
)
;
console
.
log
(
result1
,
result2
,
result3
)
;
}
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\chains\other_chains\sql.md ---
---
url: https://js.langchain.com.cn/docs/modules/chains/other_chains/sql
crawled_at: 2025-06-22T02:00:21.604126
---

SqlDatabaseChain
ä¸­æ–‡ï¼š
Sqlæ•°æ®åº“é“¾
SqlDatabaseChain
å¯ä»¥è®©æ‚¨åœ¨ SQL æ•°æ®åº“ä¸Šå›ç­”é—®é¢˜ã€‚
æ­¤ç¤ºä¾‹ä½¿ç”¨ Chinook æ•°æ®åº“ï¼Œè¿™æ˜¯ä¸€ä¸ªå¯ç”¨äº SQL Serverã€Oracleã€MySQL ç­‰çš„ç¤ºä¾‹æ•°æ®åº“ã€‚
è®¾ç½®
â€‹
é¦–å…ˆå®‰è£…
typeorm
ï¼š
typeorm
åŒ…æ˜¯å¿…é¡»å®‰è£…çš„
npm
Yarn
pnpm
npm
install
typeorm
yarn
add
typeorm
pnpm
add
typeorm
ç„¶åå®‰è£…æ‰€éœ€çš„æ•°æ®åº“ä¾èµ–é¡¹ã€‚ä¾‹å¦‚ï¼Œå¯¹äº SQLite
npm
Yarn
pnpm
npm
install
sqlite3
yarn
add
sqlite3
pnpm
add
sqlite3
å¯¹äºå…¶ä»–æ•°æ®åº“ï¼Œè¯·å‚é˜…
https://typeorm.io/#installation
æœ€åï¼ŒæŒ‰ç…§
https://database.guide/2-sample-databases-sqlite/
ä¸Šçš„è¯´æ˜ï¼Œè·å–æ­¤ç¤ºä¾‹æ‰€éœ€çš„æ ·å“æ•°æ®åº“ã€‚
import
{
DataSource
}
from
"typeorm"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
SqlDatabase
}
from
"langchain/sql_db"
;
import
{
SqlDatabaseChain
}
from
"langchain/chains"
;
/**
* This example uses Chinook database, which is a sample database available for SQL Server, Oracle, MySQL, etc.
* To set it up follow the instructions on https://database.guide/2-sample-databases-sqlite/, placing the .db file
* in the examples folder.
*/
const
datasource
=
new
DataSource
(
{
type
:
"sqlite"
,
database
:
"Chinook.db"
,
}
)
;
const
db
=
await
SqlDatabase
.
fromDataSourceParams
(
{
appDataSource
:
datasource
,
}
)
;
const
chain
=
new
SqlDatabaseChain
(
{
llm
:
new
OpenAI
(
{
temperature
:
0
}
)
,
database
:
db
,
}
)
;
const
res
=
await
chain
.
run
(
"How many tracks are there?"
)
;
console
.
log
(
res
)
;
// There are 3503 tracks.
æ‚¨å¯ä»¥åœ¨åˆ›å»º
SqlDatabase
å¯¹è±¡æ—¶åŒ…å«æˆ–æ’é™¤è¡¨æ ¼ï¼Œä»¥å¸®åŠ©é“¾é›†ä¸­äºæ‚¨æƒ³è¦çš„è¡¨æ ¼ã€‚
å®ƒè¿˜å¯ä»¥å‡å°‘é“¾ä¸­ä½¿ç”¨çš„ä»¤ç‰Œæ•°é‡ã€‚
const
db
=
await
SqlDatabase
.
fromDataSourceParams
(
{
appDataSource
:
datasource
,
includesTables
:
[
"Track"
]
,
}
)
;
å¦‚æœéœ€è¦ï¼Œæ‚¨å¯ä»¥åœ¨è°ƒç”¨é“¾æ—¶è¿”å›ä½¿ç”¨çš„ SQL å‘½ä»¤ã€‚
import
{
DataSource
}
from
"typeorm"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
SqlDatabase
}
from
"langchain/sql_db"
;
import
{
SqlDatabaseChain
}
from
"langchain/chains"
;
/**
* This example uses Chinook database, which is a sample database available for SQL Server, Oracle, MySQL, etc.
* To set it up follow the instructions on https://database.guide/2-sample-databases-sqlite/, placing the .db file
* in the examples folder.
*/
const
datasource
=
new
DataSource
(
{
type
:
"sqlite"
,
database
:
"Chinook.db"
,
}
)
;
const
db
=
await
SqlDatabase
.
fromDataSourceParams
(
{
appDataSource
:
datasource
,
}
)
;
const
chain
=
new
SqlDatabaseChain
(
{
llm
:
new
OpenAI
(
{
temperature
:
0
}
)
,
database
:
db
,
sqlOutputKey
:
"sql"
,
}
)
;
const
res
=
await
chain
.
call
(
{
query
:
"How many tracks are there?"
}
)
;
/* Expected result:
* {
*   result: ' There are 3503 tracks.',
*   sql: ' SELECT COUNT(*) FROM "Track";'
* }
*/
console
.
log
(
res
)
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\chains\other_chains\summarization.md ---
---
url: https://js.langchain.com.cn/docs/modules/chains/other_chains/summarization
crawled_at: 2025-06-22T02:00:21.587995
---

æ‘˜è¦
æ‘˜è¦é“¾å¯ä»¥ç”¨æ¥æ€»ç»“å¤šä¸ªæ–‡æ¡£ã€‚ä¸€ç§æ–¹æ³•æ˜¯åœ¨å°†å¤šä¸ªè¾ƒå°çš„æ–‡æ¡£åˆ†æˆå—åå°†å®ƒä»¬ä½œä¸ºè¾“å…¥ï¼Œä¸
MapReduceDocumentsChain
ä¸€èµ·æ“ä½œã€‚æ‚¨è¿˜å¯ä»¥é€‰æ‹©å°†è¿›è¡Œæ‘˜è¦çš„é“¾æ›¿æ¢ä¸ºStuffDocumentsChainï¼Œæˆ–RefineDocumentsChainã€‚åœ¨æ­¤å¤„äº†è§£æœ‰å…³å®ƒä»¬ä¹‹é—´å·®å¼‚çš„æ›´å¤šä¿¡æ¯
here
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
loadSummarizationChain
}
from
"langchain/chains"
;
import
{
RecursiveCharacterTextSplitter
}
from
"langchain/text_splitter"
;
import
*
as
fs
from
"fs"
;
export
const
run
=
async
(
)
=>
{
// In this example, we use a `MapReduceDocumentsChain` specifically prompted to summarize a set of documents.
const
text
=
fs
.
readFileSync
(
"state_of_the_union.txt"
,
"utf8"
)
;
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
textSplitter
=
new
RecursiveCharacterTextSplitter
(
{
chunkSize
:
1000
}
)
;
const
docs
=
await
textSplitter
.
createDocuments
(
[
text
]
)
;
// This convenience function creates a document chain prompted to summarize a set of documents.
const
chain
=
loadSummarizationChain
(
model
,
{
type
:
"map_reduce"
}
)
;
const
res
=
await
chain
.
call
(
{
input_documents
:
docs
,
}
)
;
console
.
log
(
{
res
}
)
;
/*
{
res: {
text: ' President Biden is taking action to protect Americans from the COVID-19 pandemic and Russian aggression, providing economic relief, investing in infrastructure, creating jobs, and fighting inflation.
He is also proposing measures to reduce the cost of prescription drugs, protect voting rights, and reform the immigration system. The speaker is advocating for increased economic security, police reform, and the Equality Act, as well as providing support for veterans and military families.
The US is making progress in the fight against COVID-19, and the speaker is encouraging Americans to come together and work towards a brighter future.'
}
}
*/
}
;
ä¸­é—´æ­¥éª¤
â€‹
å¦‚æœéœ€è¦æ£€æŸ¥å®ƒä»¬ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥è¿”å›
map_reduce
é“¾çš„ä¸­é—´æ­¥éª¤ï¼Œã€‚è¿™æ˜¯é€šè¿‡ä¼ é€’
returnIntermediateSteps
å‚æ•°æ¥å®Œæˆçš„ã€‚
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
loadSummarizationChain
}
from
"langchain/chains"
;
import
{
RecursiveCharacterTextSplitter
}
from
"langchain/text_splitter"
;
import
*
as
fs
from
"fs"
;
export
const
run
=
async
(
)
=>
{
// In this example, we use a `MapReduceDocumentsChain` specifically prompted to summarize a set of documents.
const
text
=
fs
.
readFileSync
(
"state_of_the_union.txt"
,
"utf8"
)
;
const
model
=
new
OpenAI
(
{
temperature
:
0
}
)
;
const
textSplitter
=
new
RecursiveCharacterTextSplitter
(
{
chunkSize
:
1000
}
)
;
const
docs
=
await
textSplitter
.
createDocuments
(
[
text
]
)
;
// This convenience function creates a document chain prompted to summarize a set of documents.
const
chain
=
loadSummarizationChain
(
model
,
{
type
:
"map_reduce"
,
returnIntermediateSteps
:
true
,
}
)
;
const
res
=
await
chain
.
call
(
{
input_documents
:
docs
,
}
)
;
console
.
log
(
{
res
}
)
;
/*
{
res: {
intermediateSteps: [
"In response to Russia's aggression in Ukraine, the United States has united with other freedom-loving nations to impose economic sanctions and hold Putin accountable. The U.S. Department of Justice is also assembling a task force to go after the crimes of Russian oligarchs and seize their ill-gotten gains.",
"The United States and its European allies are taking action to punish Russia for its invasion of Ukraine, including seizing assets, closing off airspace, and providing economic and military assistance to Ukraine. The US is also mobilizing forces to protect NATO countries and has released 30 million barrels of oil from its Strategic Petroleum Reserve to help blunt gas prices. The world is uniting in support of Ukraine and democracy, and the US stands with its Ukrainian-American citizens.",
" President Biden and Vice President Harris ran for office with a new economic vision for America, and have since passed the American Rescue Plan and the Bipartisan Infrastructure Law to help struggling families and rebuild America's infrastructure. This includes creating jobs, modernizing roads, airports, ports, and waterways, replacing lead pipes, providing affordable high-speed internet, and investing in American products to support American jobs.",
],
text: "President Biden is taking action to protect Americans from the COVID-19 pandemic and Russian aggression, providing economic relief, investing in infrastructure, creating jobs, and fighting inflation.
He is also proposing measures to reduce the cost of prescription drugs, protect voting rights, and reform the immigration system. The speaker is advocating for increased economic security, police reform, and the Equality Act, as well as providing support for veterans and military families.
The US is making progress in the fight against COVID-19, and the speaker is encouraging Americans to come together and work towards a brighter future.",
},
}
*/
}
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\document_loaders\examples.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/
crawled_at: 2025-06-22T02:00:21.718835
---

ç¤ºä¾‹: æ–‡æ¡£åŠ è½½å™¨
ğŸ—ƒï¸
æ–‡ä»¶åŠ è½½ç¨‹åº
11 items
ğŸ—ƒï¸
Web åŠ è½½ç¨‹åº
13 items



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\retrievers\chatgpt-retriever-plugin.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/retrievers/chatgpt-retriever-plugin
crawled_at: 2025-06-22T02:00:22.874486
---

ChatGPTæ’ä»¶æ£€ç´¢å™¨
æœ¬ç¤ºä¾‹æ¼”ç¤ºå¦‚ä½•åœ¨LangChainä¸­ä½¿ç”¨ChatGPTæ£€ç´¢å™¨æ’ä»¶ã€‚
è¦è®¾ç½®ChatGPTæ£€ç´¢å™¨æ’ä»¶ï¼Œè¯·æŒ‰ç…§
æ­¤å¤„
çš„è¯´æ˜è¿›è¡Œæ“ä½œã€‚
ä½¿ç”¨æ–¹æ³•
â€‹
import
{
ChatGPTPluginRetriever
}
from
"langchain/retrievers/remote"
;
export
const
run
=
async
(
)
=>
{
const
retriever
=
new
ChatGPTPluginRetriever
(
{
url
:
"http://0.0.0.0:8000"
,
auth
:
{
bearer
:
"super-secret-jwt-token-with-at-least-32-characters-long"
,
}
,
}
)
;
const
docs
=
await
retriever
.
getRelevantDocuments
(
"hello world"
)
;
console
.
log
(
docs
)
;
}
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\retrievers\chroma-self-query.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/retrievers/chroma-self-query
crawled_at: 2025-06-22T02:00:23.090590
---

è‡ªæˆ‘æŸ¥è¯¢è‰²åº¦æ£€ç´¢å™¨
è‡ªæˆ‘æŸ¥è¯¢æ£€ç´¢å™¨æ­£å¦‚å…¶åç§°æ‰€ç¤ºå…·æœ‰æŸ¥è¯¢è‡ªèº«çš„èƒ½åŠ›ã€‚å…·ä½“è€Œè¨€ç»™å®šä»»ä½•è‡ªç„¶è¯­è¨€æŸ¥è¯¢æ£€ç´¢å™¨ä½¿ç”¨æŸ¥è¯¢æ„å»ºLLMé“¾æ¥æ’°å†™ç»“æ„åŒ–æŸ¥è¯¢ï¼Œç„¶åå°†è¯¥ç»“æ„åŒ–æŸ¥è¯¢åº”ç”¨äºå…¶åŸºç¡€å‘é‡å­˜å‚¨ä¸­ã€‚è¿™ä½¿å¾—æ£€ç´¢å™¨ä¸ä»…å¯ä»¥ä½¿ç”¨ç”¨æˆ·è¾“å…¥çš„æŸ¥è¯¢è¿›è¡Œä¸å­˜å‚¨æ–‡æ¡£å†…å®¹çš„è¯­ä¹‰ç›¸ä¼¼æ€§æ¯”è¾ƒï¼Œè€Œä¸”å¯ä»¥ä»ç”¨æˆ·æŸ¥è¯¢ä¸­æå–å­˜å‚¨æ–‡æ¡£çš„å…ƒæ•°æ®è¿‡æ»¤å™¨å¹¶æ‰§è¡Œè¿™äº›è¿‡æ»¤å™¨ã€‚
æ­¤ç¤ºä¾‹ä½¿ç”¨Chromaå‘é‡å­˜å‚¨ã€‚
ç”¨æ³•
â€‹
æ­¤ç¤ºä¾‹æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨å‘é‡å­˜å‚¨åˆå§‹åŒ–
SelfQueryRetriever
:
import
{
AttributeInfo
}
from
"langchain/schema/query_constructor"
;
import
{
Document
}
from
"langchain/document"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
SelfQueryRetriever
,
BasicTranslator
,
}
from
"langchain/retrievers/self_query"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
Chroma
}
from
"langchain/vectorstores/chroma"
;
/**
* First, we create a bunch of documents. You can load your own documents here instead.
* Each document has a pageContent and a metadata field. Make sure your metadata matches the AttributeInfo below.
*/
const
docs
=
[
new
Document
(
{
pageContent
:
"A bunch of scientists bring back dinosaurs and mayhem breaks loose"
,
metadata
:
{
year
:
1993
,
rating
:
7.7
,
genre
:
"science fiction"
}
,
}
)
,
new
Document
(
{
pageContent
:
"Leo DiCaprio gets lost in a dream within a dream within a dream within a ..."
,
metadata
:
{
year
:
2010
,
director
:
"Christopher Nolan"
,
rating
:
8.2
}
,
}
)
,
new
Document
(
{
pageContent
:
"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea"
,
metadata
:
{
year
:
2006
,
director
:
"Satoshi Kon"
,
rating
:
8.6
}
,
}
)
,
new
Document
(
{
pageContent
:
"A bunch of normal-sized women are supremely wholesome and some men pine after them"
,
metadata
:
{
year
:
2019
,
director
:
"Greta Gerwig"
,
rating
:
8.3
}
,
}
)
,
new
Document
(
{
pageContent
:
"Toys come alive and have a blast doing so"
,
metadata
:
{
year
:
1995
,
genre
:
"animated"
}
,
}
)
,
new
Document
(
{
pageContent
:
"Three men walk into the Zone, three men walk out of the Zone"
,
metadata
:
{
year
:
1979
,
director
:
"Andrei Tarkovsky"
,
genre
:
"science fiction"
,
rating
:
9.9
,
}
,
}
)
,
]
;
/**
* Next, we define the attributes we want to be able to query on.
* in this case, we want to be able to query on the genre, year, director, rating, and length of the movie.
* We also provide a description of each attribute and the type of the attribute.
* This is used to generate the query prompts.
*/
const
attributeInfo
:
AttributeInfo
[
]
=
[
{
name
:
"genre"
,
description
:
"The genre of the movie"
,
type
:
"string or array of strings"
,
}
,
{
name
:
"year"
,
description
:
"The year the movie was released"
,
type
:
"number"
,
}
,
{
name
:
"director"
,
description
:
"The director of the movie"
,
type
:
"string"
,
}
,
{
name
:
"rating"
,
description
:
"The rating of the movie (1-10)"
,
type
:
"number"
,
}
,
{
name
:
"length"
,
description
:
"The length of the movie in minutes"
,
type
:
"number"
,
}
,
]
;
/**
* Next, we instantiate a vector store. This is where we store the embeddings of the documents.
* We use the Pinecone vector store here, but you can use any vector store you want.
* At this point we only support Chroma and Pinecone, but we will add more in the future.
* We also need to provide an embeddings object. This is used to embed the documents.
*/
const
embeddings
=
new
OpenAIEmbeddings
(
)
;
const
llm
=
new
OpenAI
(
)
;
const
documentContents
=
"Brief summary of a movie"
;
const
vectorStore
=
await
Chroma
.
fromDocuments
(
docs
,
embeddings
,
{
collectionName
:
"a-movie-collection"
,
}
)
;
const
selfQueryRetriever
=
await
SelfQueryRetriever
.
fromLLM
(
{
llm
,
vectorStore
,
documentContents
,
attributeInfo
,
/**
* We need to create a basic translator that translates the queries into a
* filter format that the vector store can understand. We provide a basic translator
* translator here (which works for Chroma and Pinecone), but you can create
* your own translator by extending BaseTranslator abstract class. Note that the
* vector store needs to support filtering on the metadata attributes you want to
* query on.
*/
structuredQueryTranslator
:
new
BasicTranslator
(
)
,
}
)
;
/**
* Now we can query the vector store.
* We can ask questions like "Which movies are less than 90 minutes?" or "Which movies are rated higher than 8.5?".
* We can also ask questions like "Which movies are either comedy or drama and are less than 90 minutes?".
* The retriever will automatically convert these questions into queries that can be used to retrieve documents.
*/
const
query1
=
await
selfQueryRetriever
.
getRelevantDocuments
(
"Which movies are less than 90 minutes?"
)
;
const
query2
=
await
selfQueryRetriever
.
getRelevantDocuments
(
"Which movies are rated higher than 8.5?"
)
;
const
query3
=
await
selfQueryRetriever
.
getRelevantDocuments
(
"Which movies are directed by Greta Gerwig?"
)
;
const
query4
=
await
selfQueryRetriever
.
getRelevantDocuments
(
"Which movies are either comedy or drama and are less than 90 minutes?"
)
;
console
.
log
(
query1
,
query2
,
query3
,
query4
)
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\retrievers\contextual-compression-retriever.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/retrievers/contextual-compression-retriever
crawled_at: 2025-06-22T02:00:23.076082
---

contextual-compression-retriever
ä¸Šä¸‹æ–‡å‹ç¼©æ£€ç´¢å™¨
ä¸Šä¸‹æ–‡å‹ç¼©æ£€ç´¢å™¨æ—¨åœ¨é€šè¿‡æ›´å¥½åœ°è€ƒè™‘æŸ¥è¯¢ä¸Šä¸‹æ–‡ï¼Œæ”¹è¿›å‘é‡å­˜å‚¨æ–‡æ¡£ç›¸ä¼¼æ€§æœç´¢è¿”å›çš„ç­”æ¡ˆã€‚
å®ƒåŒ…è£…å¦ä¸€ä¸ªæ£€ç´¢å™¨ï¼Œå¹¶åœ¨åˆå§‹ç›¸ä¼¼æ€§æœç´¢åä½¿ç”¨æ–‡æ¡£å‹ç¼©å™¨ä½œä¸ºä¸­é—´æ­¥éª¤ï¼Œä»æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸­åˆ é™¤ä¸åˆå§‹æŸ¥è¯¢æ— å…³çš„ä¿¡æ¯ã€‚
è¿™å‡å°‘äº†åç»­é“¾åœ¨è§£ææ£€ç´¢åˆ°çš„æ–‡æ¡£å’Œä½œå‡ºæœ€ç»ˆåˆ¤æ–­æ—¶å¿…é¡»å¤„ç†çš„å¹²æ‰°é‡ã€‚
ç”¨æ³•
â€‹
This example shows how to intialize a
ContextualCompressionRetriever
with a vector store and a document compressor:
import
*
as
fs
from
"fs"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
RecursiveCharacterTextSplitter
}
from
"langchain/text_splitter"
;
import
{
RetrievalQAChain
}
from
"langchain/chains"
;
import
{
HNSWLib
}
from
"langchain/vectorstores/hnswlib"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
ContextualCompressionRetriever
}
from
"langchain/retrievers/contextual_compression"
;
import
{
LLMChainExtractor
}
from
"langchain/retrievers/document_compressors/chain_extract"
;
const
model
=
new
OpenAI
(
)
;
const
baseCompressor
=
LLMChainExtractor
.
fromLLM
(
model
)
;
const
text
=
fs
.
readFileSync
(
"state_of_the_union.txt"
,
"utf8"
)
;
const
textSplitter
=
new
RecursiveCharacterTextSplitter
(
{
chunkSize
:
1000
}
)
;
const
docs
=
await
textSplitter
.
createDocuments
(
[
text
]
)
;
// Create a vector store from the documents.
const
vectorStore
=
await
HNSWLib
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
)
;
const
retriever
=
new
ContextualCompressionRetriever
(
{
baseCompressor
,
baseRetriever
:
vectorStore
.
asRetriever
(
)
,
}
)
;
const
chain
=
RetrievalQAChain
.
fromLLM
(
model
,
retriever
)
;
const
res
=
await
chain
.
call
(
{
query
:
"What did the speaker say about Justice Breyer?"
,
}
)
;
console
.
log
(
{
res
}
)
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\retrievers\databerry-retriever.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/retrievers/databerry-retriever
crawled_at: 2025-06-22T02:00:23.036930
---

Databerry Retriever
æœ¬ç¤ºä¾‹å±•ç¤ºå¦‚ä½•åœ¨
RetrievalQAChain
ä¸­ä½¿ç”¨Databerry Retrieverä»Databerry.aiæ•°æ®å­˜å‚¨åº“æ£€ç´¢æ–‡æ¡£ã€‚
Usageä½¿ç”¨æ–¹æ³•
â€‹
import
{
DataberryRetriever
}
from
"langchain/retrievers/databerry"
;
export
const
run
=
async
(
)
=>
{
const
retriever
=
new
DataberryRetriever
(
{
datastoreUrl
:
"https://api.databerry.ai/query/clg1xg2h80000l708dymr0fxc"
,
apiKey
:
"DATABERRY_API_KEY"
,
// optional: needed for private datastores
topK
:
8
,
// optional: default value is 3
}
)
;
const
docs
=
await
retriever
.
getRelevantDocuments
(
"hello"
)
;
console
.
log
(
docs
)
;
}
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\retrievers\hyde.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/retrievers/hyde
crawled_at: 2025-06-22T02:00:23.052301
---

HyDE Retriever
æœ¬ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨HyDE Retrieverï¼Œå…¶å®ç°äº†Hypothetical Document Embeddingsï¼ˆHyDE)ï¼Œå…·ä½“å†…å®¹å‚è§
è¿™ç¯‡è®ºæ–‡
ã€‚
åœ¨æ›´é«˜çš„å±‚æ¬¡ä¸Šï¼ŒHyDEæ˜¯ä¸€ç§åµŒå…¥æŠ€æœ¯ï¼Œå®ƒæ¥å—æŸ¥è¯¢ï¼Œç”Ÿæˆå‡å®šç­”æ¡ˆï¼Œç„¶åå°†ç”Ÿæˆçš„æ–‡æ¡£åµŒå…¥å¹¶å°†å…¶ç”¨ä½œæœ€ç»ˆç¤ºä¾‹ã€‚
ä¸ºäº†ä½¿ç”¨HyDEï¼Œæˆ‘ä»¬éœ€è¦æä¾›åŸºç¡€åµŒå…¥æ¨¡å‹ä»¥åŠå¯ç”¨äºç”Ÿæˆè¿™äº›æ–‡æ¡£çš„LLMã€‚é»˜è®¤æƒ…å†µä¸‹ï¼ŒHyDEç±»å¸¦æœ‰ä¸€äº›é»˜è®¤æç¤ºï¼ˆæœ‰å…³å®ƒä»¬çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è®ºæ–‡)ï¼Œä½†æˆ‘ä»¬ä¹Ÿå¯ä»¥åˆ›å»ºè‡ªå·±çš„æç¤ºï¼Œè¿™äº›æç¤ºåº”è¯¥æœ‰ä¸€ä¸ªå•ä¸€çš„è¾“å…¥å˜é‡
{question}
ã€‚
ç”¨æ³•
â€‹
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
MemoryVectorStore
}
from
"langchain/vectorstores/memory"
;
import
{
HydeRetriever
}
from
"langchain/retrievers/hyde"
;
import
{
Document
}
from
"langchain/document"
;
const
embeddings
=
new
OpenAIEmbeddings
(
)
;
const
vectorStore
=
new
MemoryVectorStore
(
embeddings
)
;
const
llm
=
new
OpenAI
(
)
;
const
retriever
=
new
HydeRetriever
(
{
vectorStore
,
llm
,
k
:
1
,
}
)
;
await
vectorStore
.
addDocuments
(
[
"My name is John."
,
"My name is Bob."
,
"My favourite food is pizza."
,
"My favourite food is pasta."
,
]
.
map
(
(
pageContent
)
=>
new
Document
(
{
pageContent
}
)
)
)
;
const
results
=
await
retriever
.
getRelevantDocuments
(
"What is my favourite food?"
)
;
console
.
log
(
results
)
;
/*
[
Document { pageContent: 'My favourite food is pasta.', metadata: {} }
]
*/



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\retrievers\metal-retriever.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/retrievers/metal-retriever
crawled_at: 2025-06-22T02:00:23.053805
---

é‡‘å±æ£€ç´¢å™¨
è¯¥ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•åœ¨â€œæ£€ç´¢QAChainâ€ä¸­ä½¿ç”¨é‡‘å±æ£€ç´¢å™¨ä»é‡‘å±ç´¢å¼•ä¸­æ£€ç´¢æ–‡æ¡£ã€‚
è®¾ç½®
â€‹
npm
Yarn
pnpm
npm
i @getmetal/metal-sdk
yarn
add
@getmetal/metal-sdk
pnpm
add
@getmetal/metal-sdk
ç”¨æ³•
â€‹
/* eslint-disable @typescript-eslint/no-non-null-assertion */
import
Metal
from
"@getmetal/metal-sdk"
;
import
{
MetalRetriever
}
from
"langchain/retrievers/metal"
;
export
const
run
=
async
(
)
=>
{
const
MetalSDK
=
Metal
;
const
client
=
new
MetalSDK
(
process
.
env
.
METAL_API_KEY
!
,
process
.
env
.
METAL_CLIENT_ID
!
,
process
.
env
.
METAL_INDEX_ID
)
;
const
retriever
=
new
MetalRetriever
(
{
client
}
)
;
const
docs
=
await
retriever
.
getRelevantDocuments
(
"hello"
)
;
console
.
log
(
docs
)
;
}
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\retrievers\pinecone-self-query.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/retrievers/pinecone-self-query
crawled_at: 2025-06-22T02:00:23.270377
---

è‡ªæŸ¥Pineconeæ£€ç´¢å™¨
è‡ªæŸ¥æ£€ç´¢å™¨å…·å¤‡æŸ¥è¯¢è‡ªèº«çš„èƒ½åŠ›ï¼Œæ­£å¦‚å…¶åç§°æ‰€ç¤ºã€‚å…·ä½“åœ°è¯´ï¼Œå¯¹äºä»»ä½•è‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼Œæ£€ç´¢å™¨ä½¿ç”¨åŸºäºæŸ¥è¯¢ç»“æ„æ„å»ºçš„LLMé“¾æ¥ç¼–å†™ç»“æ„åŒ–æŸ¥è¯¢ï¼Œç„¶åå°†è¯¥ç»“æ„åŒ–æŸ¥è¯¢åº”ç”¨äºå…¶åº•å±‚å‘é‡å­˜å‚¨ã€‚è¿™ä¸ä»…å…è®¸æ£€ç´¢å™¨ä½¿ç”¨ç”¨æˆ·è¾“å…¥çš„æŸ¥è¯¢ä¸æ‰€å­˜å‚¨æ–‡ä»¶å†…å®¹è¿›è¡Œè¯­ä¹‰ç›¸ä¼¼æ€§æ¯”è¾ƒï¼Œè¿˜å¯ä»¥ä»ç”¨æˆ·æŸ¥è¯¢ä¸­æå–æœ‰å…³å­˜å‚¨æ–‡æ¡£å…ƒæ•°æ®çš„è¿‡æ»¤å™¨å¹¶æ‰§è¡Œè¿™äº›è¿‡æ»¤å™¨ã€‚
[æ³¨ï¼šLLMé“¾ï¼ŒæŒ‡çš„æ˜¯â€œç½—æ°å±€éƒ¨è¯­è¨€æ¨¡å‹â€ï¼Œæ˜¯ä¸€ç§NLPæŠ€æœ¯]
æœ¬ç¤ºä¾‹ä½¿ç”¨Pineconeå‘é‡å­˜å‚¨ã€‚
ç”¨æ³•
â€‹
æœ¬ç¤ºä¾‹æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨å‘é‡å­˜å‚¨æ¥åˆå§‹åŒ–
SelfQueryRetriever
ã€‚:
import
{
PineconeClient
}
from
"@pinecone-database/pinecone"
;
import
{
AttributeInfo
}
from
"langchain/schema/query_constructor"
;
import
{
Document
}
from
"langchain/document"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
SelfQueryRetriever
,
BasicTranslator
,
}
from
"langchain/retrievers/self_query"
;
import
{
PineconeStore
}
from
"langchain/vectorstores/pinecone"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
/**
* First, we create a bunch of documents. You can load your own documents here instead.
* Each document has a pageContent and a metadata field. Make sure your metadata matches the AttributeInfo below.
*/
const
docs
=
[
new
Document
(
{
pageContent
:
"A bunch of scientists bring back dinosaurs and mayhem breaks loose"
,
metadata
:
{
year
:
1993
,
rating
:
7.7
,
genre
:
"science fiction"
}
,
}
)
,
new
Document
(
{
pageContent
:
"Leo DiCaprio gets lost in a dream within a dream within a dream within a ..."
,
metadata
:
{
year
:
2010
,
director
:
"Christopher Nolan"
,
rating
:
8.2
}
,
}
)
,
new
Document
(
{
pageContent
:
"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea"
,
metadata
:
{
year
:
2006
,
director
:
"Satoshi Kon"
,
rating
:
8.6
}
,
}
)
,
new
Document
(
{
pageContent
:
"A bunch of normal-sized women are supremely wholesome and some men pine after them"
,
metadata
:
{
year
:
2019
,
director
:
"Greta Gerwig"
,
rating
:
8.3
}
,
}
)
,
new
Document
(
{
pageContent
:
"Toys come alive and have a blast doing so"
,
metadata
:
{
year
:
1995
,
genre
:
"animated"
}
,
}
)
,
new
Document
(
{
pageContent
:
"Three men walk into the Zone, three men walk out of the Zone"
,
metadata
:
{
year
:
1979
,
director
:
"Andrei Tarkovsky"
,
genre
:
"science fiction"
,
rating
:
9.9
,
}
,
}
)
,
]
;
/**
* Next, we define the attributes we want to be able to query on.
* in this case, we want to be able to query on the genre, year, director, rating, and length of the movie.
* We also provide a description of each attribute and the type of the attribute.
* This is used to generate the query prompts.
*/
const
attributeInfo
:
AttributeInfo
[
]
=
[
{
name
:
"genre"
,
description
:
"The genre of the movie"
,
type
:
"string or array of strings"
,
}
,
{
name
:
"year"
,
description
:
"The year the movie was released"
,
type
:
"number"
,
}
,
{
name
:
"director"
,
description
:
"The director of the movie"
,
type
:
"string"
,
}
,
{
name
:
"rating"
,
description
:
"The rating of the movie (1-10)"
,
type
:
"number"
,
}
,
{
name
:
"length"
,
description
:
"The length of the movie in minutes"
,
type
:
"number"
,
}
,
]
;
/**
* Next, we instantiate a vector store. This is where we store the embeddings of the documents.
* We use the Pinecone vector store here, but you can use any vector store you want.
* At this point we only support Chroma and Pinecone, but we will add more in the future.
* We also need to provide an embeddings object. This is used to embed the documents.
*/
if
(
!
process
.
env
.
PINECONE_API_KEY
||
!
process
.
env
.
PINECONE_ENVIRONMENT
||
!
process
.
env
.
PINECONE_INDEX
)
{
throw
new
Error
(
"PINECONE_ENVIRONMENT and PINECONE_API_KEY and PINECONE_INDEX must be set"
)
;
}
const
client
=
new
PineconeClient
(
)
;
await
client
.
init
(
{
apiKey
:
process
.
env
.
PINECONE_API_KEY
,
environment
:
process
.
env
.
PINECONE_ENVIRONMENT
,
}
)
;
const
index
=
client
.
Index
(
process
.
env
.
PINECONE_INDEX
)
;
const
embeddings
=
new
OpenAIEmbeddings
(
)
;
const
llm
=
new
OpenAI
(
)
;
const
documentContents
=
"Brief summary of a movie"
;
const
vectorStore
=
await
PineconeStore
.
fromDocuments
(
docs
,
embeddings
,
{
pineconeIndex
:
index
,
}
)
;
const
selfQueryRetriever
=
await
SelfQueryRetriever
.
fromLLM
(
{
llm
,
vectorStore
,
documentContents
,
attributeInfo
,
/**
* We need to create a basic translator that translates the queries into a
* filter format that the vector store can understand. We provide a basic translator
* translator here (which works for Chroma and Pinecone), but you can create
* your own translator by extending BaseTranslator abstract class. Note that the
* vector store needs to support filtering on the metadata attributes you want to
* query on.
*/
structuredQueryTranslator
:
new
BasicTranslator
(
)
,
}
)
;
/**
* Now we can query the vector store.
* We can ask questions like "Which movies are less than 90 minutes?" or "Which movies are rated higher than 8.5?".
* We can also ask questions like "Which movies are either comedy or drama and are less than 90 minutes?".
* The retriever will automatically convert these questions into queries that can be used to retrieve documents.
*/
const
query1
=
await
selfQueryRetriever
.
getRelevantDocuments
(
"Which movies are less than 90 minutes?"
)
;
const
query2
=
await
selfQueryRetriever
.
getRelevantDocuments
(
"Which movies are rated higher than 8.5?"
)
;
const
query3
=
await
selfQueryRetriever
.
getRelevantDocuments
(
"Which movies are directed by Greta Gerwig?"
)
;
const
query4
=
await
selfQueryRetriever
.
getRelevantDocuments
(
"Which movies are either comedy or drama and are less than 90 minutes?"
)
;
console
.
log
(
query1
,
query2
,
query3
,
query4
)
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\retrievers\remote-retriever.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/retrievers/remote-retriever
crawled_at: 2025-06-22T02:00:23.181788
---

è¿œç¨‹æ£€ç´¢å™¨
æœ¬ç¤ºä¾‹å±•ç¤ºå¦‚ä½•åœ¨
RetrievalQAChain
ä¸­ä½¿ç”¨è¿œç¨‹æ£€ç´¢å™¨ä»è¿œç¨‹æœåŠ¡å™¨æ£€ç´¢æ–‡æ¡£ã€‚
ä½¿ç”¨
â€‹
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
RetrievalQAChain
}
from
"langchain/chains"
;
import
{
RemoteLangChainRetriever
}
from
"langchain/retrievers/remote"
;
export
const
run
=
async
(
)
=>
{
// Initialize the LLM to use to answer the question.
const
model
=
new
OpenAI
(
{
}
)
;
// Initialize the remote retriever.
const
retriever
=
new
RemoteLangChainRetriever
(
{
url
:
"http://0.0.0.0:8080/retrieve"
,
// Replace with your own URL.
auth
:
{
bearer
:
"foo"
}
,
// Replace with your own auth.
inputKey
:
"message"
,
responseKey
:
"response"
,
}
)
;
// Create a chain that uses the OpenAI LLM and remote retriever.
const
chain
=
RetrievalQAChain
.
fromLLM
(
model
,
retriever
)
;
// Call the chain with a query.
const
res
=
await
chain
.
call
(
{
query
:
"What did the president say about Justice Breyer?"
,
}
)
;
console
.
log
(
{
res
}
)
;
/*
{
res: {
text: 'The president said that Justice Breyer was an Army veteran, Constitutional scholar,
and retiring Justice of the United States Supreme Court and thanked him for his service.'
}
}
*/
}
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\retrievers\supabase-hybrid.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/retrievers/supabase-hybrid
crawled_at: 2025-06-22T02:00:23.310468
---

Supabase æ··åˆæœç´¢
Langchain æ”¯æŒä½¿ç”¨ Supabase Postgres æ•°æ®åº“è¿›è¡Œæ··åˆæœç´¢ã€‚è¯¥æ··åˆæœç´¢ç»“åˆäº† Postgres çš„
pgvector
æ‰©å±•ï¼ˆç›¸ä¼¼åº¦æœç´¢)å’Œå…¨æ–‡æœç´¢ï¼ˆå…³é”®è¯æœç´¢)æ¥æ£€ç´¢æ–‡æ¡£ã€‚æ‚¨å¯ä»¥é€šè¿‡ SupabaseVectorStore çš„
addDocuments
å‡½æ•°æ·»åŠ æ–‡æ¡£ã€‚SupabaseHybridKeyWordSearch æ¥å—åµŒå…¥ï¼Œ supabase å®¢æˆ·ç«¯ï¼Œ ç›¸ä¼¼æ€§æœç´¢çš„ç»“æœæ•°é‡ï¼Œ å’Œå…³é”®è¯æœç´¢çš„ç»“æœæ•°é‡ä½œä¸ºå‚æ•°ã€‚
getRelevantDocuments
å‡½æ•°äº§ç”Ÿä¸€ä¸ªå»é‡å’ŒæŒ‰ç›¸å…³æ€§åˆ†æ•°æ’åºçš„æ–‡æ¡£åˆ—è¡¨ã€‚
è®¾ç½®
â€‹
å®‰è£…åº“
â€‹
npm
Yarn
pnpm
npm
install
-S @supabase/supabase-js
yarn
add
@supabase/supabase-js
pnpm
add
@supabase/supabase-js
åœ¨æ‚¨çš„æ•°æ®åº“ä¸­åˆ›å»ºè¡¨å’Œæœç´¢å‡½æ•°
â€‹
åœ¨æ‚¨çš„æ•°æ®åº“ä¸­è¿è¡Œä»¥ä¸‹å†…å®¹:
-- Enable the pgvector extension to work with embedding vectors
create
extension vector
;
-- Create a table to store your documents
create
table
documents
(
id bigserial
primary
key
,
content
text
,
-- corresponds to Document.pageContent
metadata jsonb
,
-- corresponds to Document.metadata
embedding vector
(
1536
)
-- 1536 works for OpenAI embeddings, change if needed
)
;
-- Create a function to similarity search for documents
create
function
match_documents
(
query_embedding vector
(
1536
)
,
match_count
int
DEFAULT
null
,
filter jsonb
DEFAULT
'{}'
)
returns
table
(
id
bigint
,
content
text
,
metadata jsonb
,
similarity
float
)
language
plpgsql
as
$$
#variable_conflict use_column
begin
return
query
select
id
,
content
,
metadata
,
1
-
(
documents
.
embedding
<=>
query_embedding
)
as
similarity
from
documents
where
metadata @
>
filter
order
by
documents
.
embedding
<=>
query_embedding
limit
match_count
;
end
;
$$
;
-- Create a function to keyword search for documents
create
function
kw_match_documents
(
query_text
text
,
match_count
int
)
returns
table
(
id
bigint
,
content
text
,
metadata jsonb
,
similarity
real
)
as
$$
begin
return
query
execute
format
(
'select id, content, metadata, ts_rank(to_tsvector(content), plainto_tsquery($1)) as similarity
from documents
where to_tsvector(content) @@ plainto_tsquery($1)
order by similarity desc
limit $2'
)
using
query_text
,
match_count
;
end
;
$$
language
plpgsql
;
ç”¨æ³•
â€‹
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
createClient
}
from
"@supabase/supabase-js"
;
import
{
SupabaseHybridSearch
}
from
"langchain/retrievers/supabase"
;
export
const
run
=
async
(
)
=>
{
const
client
=
createClient
(
process
.
env
.
SUPABASE_URL
||
""
,
process
.
env
.
SUPABASE_PRIVATE_KEY
||
""
)
;
const
embeddings
=
new
OpenAIEmbeddings
(
)
;
const
retriever
=
new
SupabaseHybridSearch
(
embeddings
,
{
client
,
//  Below are the defaults, expecting that you set up your supabase table and functions according to the guide above. Please change if necessary.
similarityK
:
2
,
keywordK
:
2
,
tableName
:
"documents"
,
similarityQueryName
:
"match_documents"
,
keywordQueryName
:
"kw_match_documents"
,
}
)
;
const
results
=
await
retriever
.
getRelevantDocuments
(
"hello bye"
)
;
console
.
log
(
results
)
;
}
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\retrievers\time-weighted-retriever.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/retrievers/time-weighted-retriever
crawled_at: 2025-06-22T02:00:23.304198
---

æ—¶é—´åŠ æƒå¬å›å™¨
æ—¶é—´åŠ æƒå¬å›å™¨æ˜¯ä¸€ç§ç»¼åˆè€ƒè™‘ç›¸ä¼¼æ€§å’Œæ–°è¿‘åº¦çš„å¬å›å™¨ã€‚è¯„åˆ†ç®—æ³•ä¸º :ã€‚
let
score
=
(
1.0
-
this
.
decayRate
)
**
hoursPassed
+
vectorRelevance
;
ç‰¹åˆ«æ³¨æ„ï¼š
hoursPassed
æŒ‡çš„æ˜¯è‡ªä¸Šæ¬¡è®¿é—®ä»¥æ¥çš„æ—¶é—´ï¼Œè€Œä¸æ˜¯å¯¹è±¡åˆ›å»ºä»¥æ¥çš„æ—¶é—´ã€‚è¿™æ„å‘³ç€ç»å¸¸è®¿é—®çš„å¯¹è±¡ä¿æŒâ€œæ–°é²œâ€ï¼Œå¹¶ä¸”å…·æœ‰æ›´é«˜çš„åˆ†æ•°ã€‚
this.decayRate
æ˜¯ä¸€ä¸ªå¯é…ç½®çš„å°æ•°ï¼Œä»‹äº 0 å’Œ 1 ä¹‹é—´ã€‚è¾ƒå°çš„æ•°å­—æ„å‘³ç€æ–‡æ¡£å°†è¢«â€œè®°ä½â€çš„æ—¶é—´æ›´é•¿ï¼Œè€Œè¾ƒé«˜çš„æ•°å­—åˆ™æ›´åŠ å¼ºè°ƒæœ€è¿‘è®¿é—®çš„æ–‡æ¡£ã€‚
è¯·æ³¨æ„ï¼Œå°†è¡°å‡é€Ÿç‡è®¾ç½®ä¸ºæ°å¥½ä¸º0æˆ–1ä½¿
hoursPassed
æ— å…³ï¼Œä½¿å¾—æ­¤å¬å›å™¨ç­‰ä»·äºæ ‡å‡†çš„å‘é‡æŸ¥æ‰¾ã€‚
ä½¿ç”¨
â€‹
ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨å‘é‡å­˜å‚¨åº“åˆå§‹åŒ–
TimeWeightedVectorStoreRetriever
çš„ç¤ºä¾‹ã€‚
é‡è¦æç¤ºï¼šç”±äºæ‰€éœ€çš„å…ƒæ•°æ®ï¼Œæ‰€æœ‰æ–‡æ¡£éƒ½å¿…é¡»ä½¿ç”¨
å¬å›å™¨
ä¸Šçš„
addDocuments
æ–¹æ³•æ·»åŠ åˆ°åç«¯å‘é‡å­˜å‚¨åº“ä¸­ï¼Œè€Œä¸æ˜¯ç›´æ¥æ·»åŠ åˆ°å‘é‡å­˜å‚¨åº“æœ¬èº«ã€‚
import
{
TimeWeightedVectorStoreRetriever
}
from
"langchain/retrievers/time_weighted"
;
import
{
MemoryVectorStore
}
from
"langchain/vectorstores/memory"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
const
vectorStore
=
new
MemoryVectorStore
(
new
OpenAIEmbeddings
(
)
)
;
const
retriever
=
new
TimeWeightedVectorStoreRetriever
(
{
vectorStore
,
memoryStream
:
[
]
,
searchKwargs
:
2
,
}
)
;
const
documents
=
[
"My name is John."
,
"My name is Bob."
,
"My favourite food is pizza."
,
"My favourite food is pasta."
,
"My favourite food is sushi."
,
]
.
map
(
(
pageContent
)
=>
(
{
pageContent
,
metadata
:
{
}
}
)
)
;
// All documents must be added using this method on the retriever (not the vector store!)
// so that the correct access history metadata is populated
await
retriever
.
addDocuments
(
documents
)
;
const
results1
=
await
retriever
.
getRelevantDocuments
(
"What is my favourite food?"
)
;
console
.
log
(
results1
)
;
/*
[
Document { pageContent: 'My favourite food is pasta.', metadata: {} }
]
*/
const
results2
=
await
retriever
.
getRelevantDocuments
(
"What is my favourite food?"
)
;
console
.
log
(
results2
)
;
/*
[
Document { pageContent: 'My favourite food is pasta.', metadata: {} }
]
*/



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\retrievers\vectorstore.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/retrievers/vectorstore
crawled_at: 2025-06-22T02:00:23.254650
---

å‘é‡åº“
ä¸€æ—¦æ‚¨åˆ›å»ºäº†ä¸€ä¸ª
å‘é‡åº“
ï¼Œ ,ä½¿ç”¨å®ƒä½œä¸ºæ£€ç´¢å™¨å°±éå¸¸ç®€å•:
vectorStore
=
...
retriever
=
vectorStore
.
asRetriever
(
)



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\retrievers\vespa-retriever.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/retrievers/vespa-retriever
crawled_at: 2025-06-22T02:00:23.527896
---

Vespa Retriever
å±•ç¤ºå¦‚ä½•ä½¿ç”¨Vespa.aiä½œä¸ºLangChainæ£€ç´¢å™¨ã€‚
Vespa.aiæ˜¯ç”¨äºé«˜æ•ˆç»“æ„åŒ–æ–‡æœ¬å’Œå‘é‡æœç´¢çš„å¹³å°ã€‚
è¯·å‚é˜…
Vespa.ai
è·å–æ›´å¤šä¿¡æ¯ã€‚
ä»¥ä¸‹è®¾ç½®äº†ä¸€ä¸ªä»Vespaæ–‡æ¡£æœç´¢ä¸­è·å–ç»“æœçš„æ£€ç´¢å™¨:
import
{
VespaRetriever
}
from
"langchain/retrievers/vespa"
;
export
const
run
=
async
(
)
=>
{
const
url
=
"https://doc-search.vespa.oath.cloud"
;
const
query_body
=
{
yql
:
"select content from paragraph where userQuery()"
,
hits
:
5
,
ranking
:
"documentation"
,
locale
:
"en-us"
,
}
;
const
content_field
=
"content"
;
const
retriever
=
new
VespaRetriever
(
{
url
,
auth
:
false
,
query_body
,
content_field
,
}
)
;
const
result
=
await
retriever
.
getRelevantDocuments
(
"what is vespa?"
)
;
console
.
log
(
result
)
;
}
;
æ­¤å¤„ï¼Œæ£€ç´¢äº†"æ®µè½"æ–‡æ¡£ç±»å‹ä¸­"å†…å®¹"å­—æ®µçš„æœ€å¤š5ä¸ªç»“æœï¼Œ
ä½¿ç”¨"documentation"ä½œä¸ºæ’åæ–¹æ³•ã€‚"userQuery()"è¢«å®é™…æŸ¥è¯¢æ›¿æ¢
è¯·å‚é˜…
pyvespaæ–‡æ¡£
è·å–æ›´å¤šä¿¡æ¯ã€‚
URLæ˜¯Vespaåº”ç”¨ç¨‹åºçš„ç»ˆç«¯ç‚¹ã€‚
æ‚¨å¯ä»¥è¿æ¥ä»»ä½•Vespaç»ˆèŠ‚ç‚¹ï¼Œè¿œç¨‹æœåŠ¡æˆ–ä½¿ç”¨Dockeræœ¬åœ°å®ä¾‹ã€‚
ç„¶è€Œï¼Œå¤§å¤šæ•°Vespa Cloudå®ä¾‹éƒ½å—åˆ°mTLSä¿æŠ¤ã€‚
å¦‚æœæ‚¨çš„æƒ…å†µæ˜¯è¿™æ ·çš„ï¼Œæ‚¨å¯ä»¥ï¼Œä¾‹å¦‚è®¾ç½®
CloudFlare Worker
å…¶ä¸­åŒ…å«è¿æ¥åˆ°è¯¥å®ä¾‹æ‰€éœ€çš„å‡­æ®ã€‚
Now you can return the results and continue using them in LangChain.



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\retrievers\zep-retriever.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/retrievers/zep-retriever
crawled_at: 2025-06-22T02:00:23.426482
---

Zep Retriever
è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•åœ¨
RetrievalQAChain
ä¸­ä½¿ç”¨ Zep Retriever ä» Zep å†…å­˜å­˜å‚¨ä¸­æ£€ç´¢æ–‡æ¡£ã€‚(This example shows how to use the Zep Retriever in a
RetrievalQAChain
to retrieve documents from Zep memory store.)
è®¾ç½®(## Setup)
â€‹
npm
Yarn
pnpm
npm
i @getzep/zep-js
yarn
add
@getzep/zep-js
pnpm
add
@getzep/zep-js
ä½¿ç”¨(## Usage)
â€‹
import
{
ZepRetriever
}
from
"langchain/retrievers/zep"
;
export
const
run
=
async
(
)
=>
{
const
url
=
process
.
env
.
ZEP_URL
||
"http://localhost:8000"
;
const
sessionId
=
"TestSession1232"
;
console
.
log
(
`
Session ID:
${
sessionId
}
, URL:
${
url
}
`
)
;
const
retriever
=
new
ZepRetriever
(
{
sessionId
,
url
}
)
;
const
query
=
"hello"
;
const
docs
=
await
retriever
.
getRelevantDocuments
(
query
)
;
console
.
log
(
docs
)
;
}
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\text_splitters\examples.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/text_splitters/examples/
crawled_at: 2025-06-22T02:00:23.496525
---

æ–‡æœ¬åˆ†å‰²å™¨: ç¤ºä¾‹
ğŸ“„ï¸
å­—ç¬¦æ–‡æœ¬åˆ†å‰²å™¨
é™¤äº†é€’å½’å­—ç¬¦æ–‡æœ¬åˆ†å‰²å™¨ä¹‹å¤–ï¼Œè¿˜æœ‰æ›´å¸¸è§çš„å­—ç¬¦æ–‡æœ¬åˆ†å‰²å™¨ã€‚
ğŸ“„ï¸
ä»£ç å’Œæ ‡è®°æ–‡æœ¬åˆ†å‰²å™¨
LangChainæ”¯æŒå„ç§ä¸åŒçš„æ ‡è®°å’Œç¼–ç¨‹è¯­è¨€ç‰¹å®šçš„æ–‡æœ¬åˆ†å‰²å™¨ï¼Œä»¥åŸºäºè¯­è¨€ç‰¹å®šçš„è¯­æ³•åˆ†å‰²æ–‡æœ¬ã€‚
ğŸ“„ï¸
RecursiveCharacterTextSplitter
æ¨èä½¿ç”¨çš„TextSplitteræ˜¯â€œé€’å½’å­—ç¬¦æ–‡æœ¬åˆ†å‰²å™¨â€ã€‚å®ƒä¼šé€šè¿‡ä¸åŒçš„ç¬¦å·é€’å½’åœ°åˆ†å‰²æ–‡æ¡£-ä»â€œâ€å¼€å§‹ï¼Œç„¶åæ˜¯â€œâ€ï¼Œå†ç„¶åæ˜¯â€œ â€ã€‚è¿™å¾ˆå¥½ï¼Œå› ä¸ºå®ƒä¼šå°½å¯èƒ½åœ°å°†æ‰€æœ‰è¯­ä¹‰ç›¸å…³çš„å†…å®¹ä¿æŒåœ¨åŒä¸€ä½ç½®ã€‚
ğŸ“„ï¸
TokenTextSplitter
æœ€åï¼Œ TokenTextSplitter å°†åŸå§‹æ–‡æœ¬å­—ç¬¦ä¸²è½¬æ¢ä¸º BPE æ ‡è®°ï¼Œå¹¶å°†è¿™äº›æ ‡è®°åˆ†æˆå—ï¼Œç„¶åå°†å•ä¸ªå—ä¸­çš„æ ‡è®°è½¬æ¢å›æ–‡æœ¬ã€‚#ï¼ˆFinally)



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\vector_stores\integrations.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/vector_stores/integrations/
crawled_at: 2025-06-22T02:00:23.680750
---

çŸ¢é‡å­˜å‚¨: é›†æˆ
ğŸ“„ï¸
å†…å­˜
MemoryVectorStoreæ˜¯ä¸€ä¸ªå†…å­˜ä¸­çš„æš‚å­˜å‘é‡å­˜å‚¨å™¨ï¼Œç”¨äºåœ¨å†…å­˜ä¸­å­˜å‚¨åµŒå…¥ï¼Œå¹¶åšç²¾ç¡®çš„çº¿æ€§æœç´¢ä»¥æ‰¾åˆ°æœ€ç›¸ä¼¼çš„åµŒå…¥ã€‚é»˜è®¤çš„ç›¸ä¼¼åº¦åº¦é‡æ˜¯ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œä½†å¯ä»¥æ›´æ”¹ä¸ºml-distanceæ”¯æŒçš„ä»»ä½•ç›¸ä¼¼åº¦åº¦é‡æ–¹å¼ã€‚
ğŸ“„ï¸
chroma
æ¢è¡Œç¬¦
ğŸ“„ï¸
Faiss
ä»…é€‚ç”¨äºNode.jsç¯å¢ƒã€‚
ğŸ“„ï¸
HNSWLib
ä»…é€‚ç”¨äºNode.jsã€‚
ğŸ“„ï¸
Milvus
Milvusæ˜¯ä¸“ä¸ºåµŒå…¥å¼ç›¸ä¼¼æ€§æœç´¢å’Œ AI åº”ç”¨è€Œæ„å»ºçš„å‘é‡æ•°æ®åº“ã€‚
ğŸ“„ï¸
MyScale
ä»…åœ¨Node.jsä¸Šå¯ç”¨ã€‚
ğŸ“„ï¸
OpenSearch
ä»…é™äº Node.jsã€‚
ğŸ“„ï¸
Pinecone
ä»…é€‚ç”¨äº Node.jsã€‚
ğŸ“„ï¸
prisma
---
ğŸ“„ï¸
Qdrant
Qdrant æ˜¯ä¸€ä¸ªå‘é‡ç›¸ä¼¼åº¦æœç´¢å¼•æ“ã€‚å®ƒæä¾›äº†ä¸€ä¸ªæ–¹ä¾¿çš„APIæ¥å­˜å‚¨ã€æœç´¢å’Œç®¡ç†å¸¦æœ‰é™„åŠ æœ‰æ•ˆè´Ÿè½½çš„ç‚¹ - å‘é‡ã€‚
ğŸ“„ï¸
Redis
Redisæ˜¯ä¸€æ¬¾å¿«é€Ÿçš„å¼€æºå†…å­˜æ•°æ®å­˜å‚¨ç³»ç»Ÿã€‚ï¼Œä½œä¸ºRedis Stackçš„ä¸€éƒ¨åˆ†ï¼ŒRediSearchæ˜¯ä¸€ç§æ”¯æŒå‘é‡ç›¸ä¼¼æ€§è¯­ä¹‰æœç´¢ä»¥åŠå…¶ä»–è®¸å¤šç±»å‹æœç´¢çš„æ¨¡å—ã€‚
ğŸ“„ï¸
SingleStore
SingleStoreDBæ˜¯ä¸€æ¬¾é«˜æ€§èƒ½ï¼Œåˆ†å¸ƒå¼æ•°æ®åº“ç³»ç»Ÿã€‚é•¿æœŸä»¥æ¥ï¼Œå®ƒä¸€ç›´æ”¯æŒdotproductç­‰å‘é‡å‡½æ•°ï¼Œä»è€Œæˆä¸ºéœ€è¦æ–‡æœ¬ç›¸ä¼¼åº¦åŒ¹é…çš„AIåº”ç”¨ç¨‹åºçš„æœ€ä½³è§£å†³æ–¹æ¡ˆã€‚
ğŸ“„ï¸
Supabase
Langchainæ”¯æŒä½¿ç”¨Supabase Postgresæ•°æ®åº“ä½œä¸ºå‘é‡å­˜å‚¨ä½¿ç”¨'pgvector' postgresæ‰©å±•ã€‚ æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…Supabaseåšå®¢æ–‡ç« ã€‚
ğŸ“„ï¸
ä»…é™node
Tigrisä½¿å‘é‡åµŒå…¥çš„æ„å»ºäººå·¥æ™ºèƒ½åº”ç”¨ç¨‹åºå˜å¾—è½»æ¾ã€‚
ğŸ“„ï¸
TypeORM
ä¸ºäº†åœ¨é€šç”¨çš„PostgreSQLæ•°æ®åº“ä¸­å®ç°å‘é‡æœç´¢ï¼ŒLangChainJSæ”¯æŒä½¿ç”¨TypeORMå’Œpgvector Postgresæ‰©å±•ã€‚
ğŸ“„ï¸
Weaviate
Weaviateæ˜¯ä¸€ä¸ªå¼€æºçš„å‘é‡æ•°æ®åº“ï¼Œå¯ä»¥å­˜å‚¨å¯¹è±¡å’Œå‘é‡ï¼Œä½¿å‘é‡æœç´¢ä¸ç»“æ„åŒ–è¿‡æ»¤ç›¸ç»“åˆã€‚LangChainé€šè¿‡weaviate-ts-clientè½¯ä»¶åŒ…è¿æ¥åˆ°Weaviateï¼Œè¿™æ˜¯å®˜æ–¹çš„Typescriptå®¢æˆ·ç«¯ã€‚



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\document_loaders\examples\file_loaders.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/file_loaders/
crawled_at: 2025-06-22T02:00:21.765886
---

æ–‡ä»¶åŠ è½½ç¨‹åº
å…¼å®¹æ€§
ä»…é€‚ç”¨äºNode.jsã€‚
è¿™äº›åŠ è½½ç¨‹åºç”¨äºåŠ è½½ç»™å®šçš„æ–‡ä»¶ç³»ç»Ÿè·¯å¾„æˆ–Blobå¯¹è±¡çš„æ–‡ä»¶ã€‚
ğŸ“„ï¸
å…·æœ‰å¤šä¸ªæ–‡ä»¶å¤¹çš„æ–‡ä»¶å¤¹
æœ¬ç¤ºä¾‹ä»‹ç»å¦‚ä½•ä»å…·æœ‰å¤šä¸ªæ–‡ä»¶çš„æ–‡ä»¶å¤¹ä¸­åŠ è½½æ•°æ®ã€‚ç¬¬äºŒä¸ªå‚æ•°æ˜¯æ–‡ä»¶æ‰©å±•ååˆ°åŠ è½½å™¨å·¥å‚çš„æ˜ å°„ã€‚æ¯ä¸ªæ–‡ä»¶å°†ä¼ é€’ç»™åŒ¹é…çš„åŠ è½½å™¨ï¼Œ å¹¶å°†ç”Ÿæˆçš„æ–‡æ¡£è¿æ¥åœ¨ä¸€èµ·ã€‚
ğŸ“„ï¸
CSVæ–‡ä»¶
æœ¬ç¤ºä¾‹ä»‹ç»å¦‚ä½•ä»CSVæ–‡ä»¶åŠ è½½æ•°æ®ã€‚
ğŸ“„ï¸
Docx files
æœ¬ç¤ºä¾‹ä»‹ç»å¦‚ä½•ä»docxæ–‡ä»¶ä¸­åŠ è½½æ•°æ®ã€‚
ğŸ“„ï¸
EPUBæ–‡ä»¶
æœ¬ä¾‹æ¼”ç¤ºå¦‚ä½•ä»EPUBæ–‡ä»¶ä¸­åŠ è½½æ•°æ®ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œæ¯ä¸ªç« èŠ‚ä¼šåˆ›å»ºä¸€ä¸ªæ–‡æ¡£ï¼Œæ‚¨å¯ä»¥é€šè¿‡å°†â€œsplitChaptersâ€é€‰é¡¹è®¾ç½®ä¸ºâ€œfalseâ€æ¥æ›´æ”¹æ­¤è¡Œä¸ºã€‚
ğŸ“„ï¸
JSONæ–‡ä»¶
JSONåŠ è½½å™¨ä½¿ç”¨JSONæŒ‡é’ˆæ¥å®šä½æ‚¨æƒ³è¦å®šä½çš„JSONæ–‡ä»¶ä¸­çš„é”®ã€‚
ğŸ“„ï¸
JSONLines æ–‡ä»¶
è¿™ä¸ªä¾‹å­æ¼”ç¤ºäº†å¦‚ä½•ä» JSONLines æˆ– JSONL æ–‡ä»¶åŠ è½½æ•°æ®ã€‚ç¬¬äºŒä¸ªå‚æ•°æ˜¯ä¸€ä¸ª JSONPointerï¼Œç”¨äºä»æ–‡ä»¶ä¸­çš„æ¯ä¸ª JSON å¯¹è±¡ä¸­æå–å±æ€§ã€‚æ¯ä¸ª JSON å¯¹è±¡éƒ½å°†åˆ›å»ºä¸€ä¸ªæ–‡æ¡£ã€‚
ğŸ“„ï¸
Notion markdown export
æœ¬ç¤ºä¾‹ä»‹ç»å¦‚ä½•ä»å¯¼å‡ºçš„ Notion é¡µé¢ä¸­åŠ è½½æ•°æ®ã€‚
ğŸ“„ï¸
PDFæ–‡ä»¶
åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»å¦‚ä½•ä»PDFæ–‡ä»¶ä¸­å¯¼å…¥æ•°æ®ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œæ¯ä¸ªé¡µé¢å°†åˆ›å»ºä¸€ä¸ªæ–‡æ¡£ã€‚é€šè¿‡å°† splitPages é€‰é¡¹è®¾ç½®ä¸º false å¯ä»¥æ›´æ”¹æ­¤è¡Œä¸ºã€‚
ğŸ“„ï¸
å­—å¹•
æœ¬ç¤ºä¾‹ä»‹ç»å¦‚ä½•ä»å­—å¹•æ–‡ä»¶ä¸­åŠ è½½æ•°æ®ã€‚æ¯ä¸ªå­—å¹•æ–‡ä»¶å°†åˆ›å»ºä¸€ä¸ªæ–‡æ¡£ã€‚
ğŸ“„ï¸
æ–‡æœ¬æ–‡ä»¶
æœ¬ä¾‹å°†ä»‹ç»å¦‚ä½•ä»æ–‡æœ¬æ–‡ä»¶ä¸­åŠ è½½æ•°æ®ã€‚
ğŸ“„ï¸
æ— ç»“æ„
æœ¬ç¤ºä¾‹ä»‹ç»å¦‚ä½•ä½¿ç”¨æ— ç»“æ„è¯»å–å¤šç§ç±»å‹çš„æ–‡ä»¶ã€‚æ— ç»“æ„ç›®å‰æ”¯æŒåŠ è½½æ–‡æœ¬æ–‡ä»¶ã€PPTã€HTMLã€PDFã€å›¾ç‰‡ç­‰ã€‚



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\document_loaders\examples\web_loaders.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/web_loaders/
crawled_at: 2025-06-22T02:00:22.204964
---

Web åŠ è½½ç¨‹åº
è¿™äº›åŠ è½½ç¨‹åºç”¨äºåŠ è½½ web èµ„æºã€‚
ğŸ“„ï¸
Cheerio
æœ¬ç¤ºä¾‹æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨Cheerioä»ç½‘é¡µåŠ è½½æ•°æ®ã€‚æ¯ä¸ªç½‘é¡µéƒ½å°†åˆ›å»ºä¸€ä¸ªæ–‡æ¡£ã€‚
ğŸ“„ï¸
Puppeteer
ä»…é€‚ç”¨äº Node.jsã€‚
ğŸ“„ï¸
Playwright
ä»…é™äº Node.jsã€‚
ğŸ“„ï¸
Apify æ•°æ®é›†
æœ¬æŒ‡å—å±•ç¤ºå¦‚ä½•ä½¿ç”¨ Apify å’Œ LangChain ä» Apify æ•°æ®é›†ä¸­åŠ è½½æ–‡æ¡£ã€‚
ğŸ“„ï¸
å¤§å­¦æœºå¯†
æœ¬ä¾‹è¯´æ˜äº†å¦‚ä½•ä½¿ç”¨Cheerioä»å¤§å­¦æœºå¯†ç½‘ç«™åŠ è½½æ•°æ®ã€‚æ¯ä¸ªé¡µé¢å°†åˆ›å»ºä¸€ä¸ªæ–‡æ¡£ã€‚
ğŸ“„ï¸
Confluenceï¼ˆç»´åŸºè½¯ä»¶)
ä»…åœ¨ Node.js ä¸Šå¯ç”¨ã€‚
ğŸ“„ï¸
Figma
æœ¬ä¾‹æ¼”ç¤ºå¦‚ä½•ä»Figmaæ–‡ä»¶åŠ è½½æ•°æ®ã€‚
ğŸ“„ï¸
GitBook
æœ¬ç¤ºä¾‹ä»‹ç»å¦‚ä½•ä½¿ç”¨ Cheerio ä»ä»»ä½• GitBook ä¸­åŠ è½½æ•°æ®ã€‚å°†ä¸ºæ¯ä¸ªé¡µé¢åˆ›å»ºä¸€ä¸ªæ–‡æ¡£ã€‚
ğŸ“„ï¸
ä»…é™äºèŠ‚ç‚¹
æœ¬ç¤ºä¾‹ä»‹ç»äº†å¦‚ä½•ä» GitHub å­˜å‚¨åº“åŠ è½½æ•°æ®ã€‚
ğŸ“„ï¸
é»‘å®¢æ–°é—»
æœ¬ä¾‹ä»‹ç»å¦‚ä½•ä½¿ç”¨Cheerioä»é»‘å®¢æ–°é—»ç½‘ç«™åŠ è½½æ•°æ®ã€‚æ¯é¡µå°†åˆ›å»ºä¸€ä¸ªæ–‡æ¡£ã€‚
ğŸ“„ï¸
IMSDB
æœ¬ä¾‹ä»‹ç»å¦‚ä½•ä½¿ç”¨Cheerioä»äº’è”ç½‘ç”µå½±å‰§æœ¬æ•°æ®åº“ç½‘ç«™åŠ è½½æ•°æ®ã€‚æ¯ä¸ªé¡µé¢å°†åˆ›å»ºä¸€ä¸ªæ–‡æ¡£ã€‚
ğŸ“„ï¸
Notionæ•°æ®åº“
æœ¬ç¤ºä¾‹æ¼”ç¤ºäº†å¦‚ä½•ä»Notionæ•°æ®åº“åŠ è½½æ•°æ®ã€‚
ğŸ“„ï¸
S3 æ–‡ä»¶
ä»…é€‚ç”¨äº Node.jsã€‚



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\document_loaders\examples\file_loaders\csv.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/file_loaders/csv
crawled_at: 2025-06-22T02:00:21.776904
---

CSVæ–‡ä»¶
æœ¬ç¤ºä¾‹ä»‹ç»å¦‚ä½•ä»CSVæ–‡ä»¶åŠ è½½æ•°æ®ã€‚
ç¬¬äºŒä¸ªå‚æ•°æ˜¯è¦ä»CSVæ–‡ä»¶ä¸­æå–çš„â€œåˆ—â€åç§°ã€‚æ¯è¡ŒCSVæ–‡ä»¶å°†åˆ›å»ºä¸€ä¸ªæ–‡æ¡£ã€‚
å½“æœªæŒ‡å®šâ€œåˆ—â€æ—¶ï¼Œæ¯ä¸€è¡Œéƒ½å°†è½¬æ¢ä¸ºä¸€ä¸ªé”®/å€¼å¯¹ï¼Œå¹¶å°†æ¯ä¸ªé”®/å€¼å¯¹è¾“å‡ºåˆ°æ–‡æ¡£çš„â€œpageContentâ€ä¸­çš„æ–°è¡Œä¸­ã€‚
å½“æŒ‡å®šäº†â€œåˆ—â€æ—¶ï¼Œå°†ä¸ºæ¯ä¸€è¡Œåˆ›å»ºä¸€ä¸ªæ–‡æ¡£ï¼Œå¹¶å°†æŒ‡å®šåˆ—çš„å€¼ç”¨ä½œæ–‡æ¡£çš„â€œpageContentâ€ã€‚
å¤‡æ³¨ï¼šè¯¥å¤„çš„â€œpageContentâ€æŒ‡æ–‡æ¡£çš„é¡µé¢å†…å®¹)
è®¾ç½®
â€‹
npm
Yarn
pnpm
npm
install
d3-dsv@2
yarn
add
d3-dsv@2
pnpm
add
d3-dsv@2
ç”¨æ³•-æå–æ‰€æœ‰åˆ—
â€‹
ç¤ºä¾‹CSVæ–‡ä»¶:
id,text
1,This is a sentence.
2,This is another sentence.
ç¤ºä¾‹ä»£ç :
import
{
CSVLoader
}
from
"langchain/document_loaders/fs/csv"
;
const
loader
=
new
CSVLoader
(
"src/document_loaders/example_data/example.csv"
)
;
const
docs
=
await
loader
.
load
(
)
;
/*
[
Document {
"metadata": {
"line": 1,
"source": "src/document_loaders/example_data/example.csv",
},
"pageContent": "id: 1
text: This is a sentence.",
},
Document {
"metadata": {
"line": 2,
"source": "src/document_loaders/example_data/example.csv",
},
"pageContent": "id: 2
text: This is another sentence.",
},
]
*/
ç”¨æ³•-æå–å•ä¸ªåˆ—
â€‹
ç¤ºä¾‹CSVæ–‡ä»¶:
id,text
1,This is a sentence.
2,This is another sentence.
ç¤ºä¾‹ä»£ç :
import
{
CSVLoader
}
from
"langchain/document_loaders/fs/csv"
;
const
loader
=
new
CSVLoader
(
"src/document_loaders/example_data/example.csv"
,
"text"
)
;
const
docs
=
await
loader
.
load
(
)
;
/*
[
Document {
"metadata": {
"line": 1,
"source": "src/document_loaders/example_data/example.csv",
},
"pageContent": "This is a sentence.",
},
Document {
"metadata": {
"line": 2,
"source": "src/document_loaders/example_data/example.csv",
},
"pageContent": "This is another sentence.",
},
]
*/



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\document_loaders\examples\file_loaders\directory.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/file_loaders/directory
crawled_at: 2025-06-22T02:00:21.846711
---

å…·æœ‰å¤šä¸ªæ–‡ä»¶å¤¹çš„æ–‡ä»¶å¤¹
æœ¬ç¤ºä¾‹ä»‹ç»å¦‚ä½•ä»å…·æœ‰å¤šä¸ªæ–‡ä»¶çš„æ–‡ä»¶å¤¹ä¸­åŠ è½½æ•°æ®ã€‚ç¬¬äºŒä¸ªå‚æ•°æ˜¯æ–‡ä»¶æ‰©å±•ååˆ°åŠ è½½å™¨å·¥å‚çš„æ˜ å°„ã€‚æ¯ä¸ªæ–‡ä»¶å°†ä¼ é€’ç»™åŒ¹é…çš„åŠ è½½å™¨ï¼Œ å¹¶å°†ç”Ÿæˆçš„æ–‡æ¡£è¿æ¥åœ¨ä¸€èµ·ã€‚
ç¤ºä¾‹æ–‡ä»¶å¤¹:
src/document_loaders/example_data/example/
â”œâ”€â”€ example.json
â”œâ”€â”€ example.jsonl
â”œâ”€â”€ example.txt
â””â”€â”€ example.csv
ç¤ºä¾‹ä»£ç :
import
{
DirectoryLoader
}
from
"langchain/document_loaders/fs/directory"
;
import
{
JSONLoader
,
JSONLinesLoader
,
}
from
"langchain/document_loaders/fs/json"
;
import
{
TextLoader
}
from
"langchain/document_loaders/fs/text"
;
import
{
CSVLoader
}
from
"langchain/document_loaders/fs/csv"
;
const
loader
=
new
DirectoryLoader
(
"src/document_loaders/example_data/example"
,
{
".json"
:
(
path
)
=>
new
JSONLoader
(
path
,
"/texts"
)
,
".jsonl"
:
(
path
)
=>
new
JSONLinesLoader
(
path
,
"/html"
)
,
".txt"
:
(
path
)
=>
new
TextLoader
(
path
)
,
".csv"
:
(
path
)
=>
new
CSVLoader
(
path
,
"text"
)
,
}
)
;
const
docs
=
await
loader
.
load
(
)
;
console
.
log
(
{
docs
}
)
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\document_loaders\examples\file_loaders\docx.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/file_loaders/docx
crawled_at: 2025-06-22T02:00:21.959510
---

Docx files
æœ¬ç¤ºä¾‹ä»‹ç»å¦‚ä½•ä»docxæ–‡ä»¶ä¸­åŠ è½½æ•°æ®ã€‚
å®‰è£… Setup
npm
Yarn
pnpm
npm
install
mammoth
yarn
add
mammoth
pnpm
add
mammoth
ç”¨æ³• Usage
import
{
DocxLoader
}
from
"langchain/document_loaders/fs/docx"
;
const
loader
=
new
DocxLoader
(
"src/document_loaders/tests/example_data/attention.docx"
)
;
const
docs
=
await
loader
.
load
(
)
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\document_loaders\examples\file_loaders\epub.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/file_loaders/epub
crawled_at: 2025-06-22T02:00:22.003937
---

EPUBæ–‡ä»¶
æœ¬ä¾‹æ¼”ç¤ºå¦‚ä½•ä»EPUBæ–‡ä»¶ä¸­åŠ è½½æ•°æ®ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œæ¯ä¸ªç« èŠ‚ä¼šåˆ›å»ºä¸€ä¸ªæ–‡æ¡£ï¼Œæ‚¨å¯ä»¥é€šè¿‡å°†â€œsplitChaptersâ€é€‰é¡¹è®¾ç½®ä¸ºâ€œfalseâ€æ¥æ›´æ”¹æ­¤è¡Œä¸ºã€‚
è®¾ç½®
npm
Yarn
pnpm
npm
install
epub2 html-to-text
yarn
add
epub2 html-to-text
pnpm
add
epub2 html-to-text
ç”¨æ³•ï¼šæ¯ç« ä¸€ä¸ªæ–‡æ¡£
import
{
EPubLoader
}
from
"langchain/document_loaders/fs/epub"
;
const
loader
=
new
EPubLoader
(
"src/document_loaders/example_data/example.epub"
)
;
const
docs
=
await
loader
.
load
(
)
;
ç”¨æ³•ï¼šæ¯ä¸ªæ–‡ä»¶ä¸€ä¸ªæ–‡æ¡£
import
{
EPubLoader
}
from
"langchain/document_loaders/fs/epub"
;
const
loader
=
new
EPubLoader
(
"src/document_loaders/example_data/example.epub"
,
{
splitChapters
:
false
,
}
)
;
const
docs
=
await
loader
.
load
(
)
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\document_loaders\examples\file_loaders\json.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/file_loaders/json
crawled_at: 2025-06-22T02:00:22.076018
---

JSONæ–‡ä»¶
JSONåŠ è½½å™¨ä½¿ç”¨
JSONæŒ‡é’ˆ
æ¥å®šä½æ‚¨æƒ³è¦å®šä½çš„JSONæ–‡ä»¶ä¸­çš„é”®ã€‚
æ— JSONæŒ‡é’ˆç¤ºä¾‹
â€‹
æœ€ç®€å•çš„ä½¿ç”¨æ–¹å¼æ˜¯ä¸æŒ‡å®šJSONæŒ‡é’ˆã€‚
åŠ è½½ç¨‹åºå°†åŠ è½½JSONå¯¹è±¡ä¸­æ‰¾åˆ°çš„æ‰€æœ‰å­—ç¬¦ä¸²ã€‚
ç¤ºä¾‹JSONæ–‡ä»¶:
{
"texts"
:
[
"This is a sentence."
,
"This is another sentence."
]
}
ç¤ºä¾‹ä»£ç :
import
{
JSONLoader
}
from
"langchain/document_loaders/fs/json"
;
const
loader
=
new
JSONLoader
(
"src/document_loaders/example_data/example.json"
)
;
const
docs
=
await
loader
.
load
(
)
;
/*
[
Document {
"metadata": {
"blobType": "application/json",
"line": 1,
"source": "blob",
},
"pageContent": "This is a sentence.",
},
Document {
"metadata": {
"blobType": "application/json",
"line": 2,
"source": "blob",
},
"pageContent": "This is another sentence.",
},
]
*/
ä½¿ç”¨JSONæŒ‡é’ˆç¤ºä¾‹
â€‹
æ‚¨å¯ä»¥é€šè¿‡é€‰æ‹©è¦ä»JSONå¯¹è±¡ä¸­æå–å­—ç¬¦ä¸²çš„å“ªäº›é”®æ¥æ‰§è¡Œæ›´é«˜çº§çš„åœºæ™¯ã€‚
åœ¨æ­¤ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬ä»…æƒ³ä»â€œfromâ€å’Œâ€œsurnameâ€æ¡ç›®ä¸­æå–ä¿¡æ¯ã€‚
{
"1"
:
{
"body"
:
"BD 2023 SUMMER"
,
"from"
:
"LinkedIn Job"
,
"labels"
:
[
"IMPORTANT"
,
"CATEGORY_UPDATES"
,
"INBOX"
]
}
,
"2"
:
{
"body"
:
"Intern, Treasury and other roles are available"
,
"from"
:
"LinkedIn Job2"
,
"labels"
:
[
"IMPORTANT"
]
,
"other"
:
{
"name"
:
"plop"
,
"surname"
:
"bob"
}
}
}
ç¤ºä¾‹ä»£ç :
import
{
JSONLoader
}
from
"langchain/document_loaders/fs/json"
;
const
loader
=
new
JSONLoader
(
"src/document_loaders/example_data/example.json"
,
[
"/from"
,
"/surname"
]
)
;
const
docs
=
await
loader
.
load
(
)
;
/*
[
Document {
"metadata": {
"blobType": "application/json",
"line": 1,
"source": "blob",
},
"pageContent": "BD 2023 SUMMER",
},
Document {
"metadata": {
"blobType": "application/json",
"line": 2,
"source": "blob",
},
"pageContent": "LinkedIn Job",
},
...
]



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\document_loaders\examples\file_loaders\jsonlines.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/file_loaders/jsonlines
crawled_at: 2025-06-22T02:00:22.024838
---

JSONLines æ–‡ä»¶
è¿™ä¸ªä¾‹å­æ¼”ç¤ºäº†å¦‚ä½•ä» JSONLines æˆ– JSONL æ–‡ä»¶åŠ è½½æ•°æ®ã€‚ç¬¬äºŒä¸ªå‚æ•°æ˜¯ä¸€ä¸ª JSONPointerï¼Œç”¨äºä»æ–‡ä»¶ä¸­çš„æ¯ä¸ª JSON å¯¹è±¡ä¸­æå–å±æ€§ã€‚æ¯ä¸ª JSON å¯¹è±¡éƒ½å°†åˆ›å»ºä¸€ä¸ªæ–‡æ¡£ã€‚
ç¤ºä¾‹ JSONLines æ–‡ä»¶:
{
"html"
:
"This is a sentence."
}
{
"html"
:
"This is another sentence."
}
ç¤ºä¾‹ä»£ç :
import
{
JSONLinesLoader
}
from
"langchain/document_loaders/fs/json"
;
const
loader
=
new
JSONLinesLoader
(
"src/document_loaders/example_data/example.jsonl"
,
"/html"
)
;
const
docs
=
await
loader
.
load
(
)
;
/*
[
Document {
"metadata": {
"blobType": "application/jsonl+json",
"line": 1,
"source": "blob",
},
"pageContent": "This is a sentence.",
},
Document {
"metadata": {
"blobType": "application/jsonl+json",
"line": 2,
"source": "blob",
},
"pageContent": "This is another sentence.",
},
]
*/



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\document_loaders\examples\file_loaders\notion_markdown.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/file_loaders/notion_markdown
crawled_at: 2025-06-22T02:00:22.065792
---

Notion markdown export
æœ¬ç¤ºä¾‹ä»‹ç»å¦‚ä½•ä»å¯¼å‡ºçš„ Notion é¡µé¢ä¸­åŠ è½½æ•°æ®ã€‚
é¦–å…ˆï¼ŒæŒ‰ç…§å®˜æ–¹è¯´æ˜
è¿™é‡Œ
å¯¼å‡º Notion é¡µé¢ä¸º
Markdown & CSV
ã€‚ç¡®ä¿é€‰æ‹©
åŒ…æ‹¬å­é¡µé¢
å’Œ
ä¸ºå­é¡µé¢åˆ›å»ºæ–‡ä»¶å¤¹
ã€‚
ç„¶åï¼Œè§£å‹ä¸‹è½½çš„æ–‡ä»¶å¹¶å°†æœªå‹ç¼©çš„æ–‡ä»¶å¤¹ç§»åŠ¨åˆ°å­˜å‚¨åº“ä¸­ã€‚å®ƒåº”è¯¥åŒ…å«ä½ é¡µé¢çš„ markdown æ–‡ä»¶ã€‚
ä¸€æ—¦æ–‡ä»¶å¤¹åœ¨å­˜å‚¨åº“ä¸­ï¼Œåªéœ€è¿è¡Œä¸‹é¢çš„ç¤ºä¾‹å³å¯:
import
{
NotionLoader
}
from
"langchain/document_loaders/fs/notion"
;
export
const
run
=
async
(
)
=>
{
/** Provide the directory path of your notion folder */
const
directoryPath
=
"Notion_DB"
;
const
loader
=
new
NotionLoader
(
directoryPath
)
;
const
docs
=
await
loader
.
load
(
)
;
console
.
log
(
{
docs
}
)
;
}
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\document_loaders\examples\file_loaders\pdf.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/file_loaders/pdf
crawled_at: 2025-06-22T02:00:22.100748
---

PDFæ–‡ä»¶
åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»å¦‚ä½•ä»PDFæ–‡ä»¶ä¸­å¯¼å…¥æ•°æ®ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œæ¯ä¸ªé¡µé¢å°†åˆ›å»ºä¸€ä¸ªæ–‡æ¡£ã€‚é€šè¿‡å°†
splitPages
é€‰é¡¹è®¾ç½®ä¸º
false
å¯ä»¥æ›´æ”¹æ­¤è¡Œä¸ºã€‚
è®¾ç½®
â€‹
npm
Yarn
pnpm
npm
install
pdf-parse
yarn
add
pdf-parse
pnpm
add
pdf-parse
ç”¨æ³•ï¼Œæ¯ä¸ªé¡µé¢ä¸€ä¸ªæ–‡æ¡£
â€‹
import
{
PDFLoader
}
from
"langchain/document_loaders/fs/pdf"
;
const
loader
=
new
PDFLoader
(
"src/document_loaders/example_data/example.pdf"
)
;
const
docs
=
await
loader
.
load
(
)
;
ç”¨æ³•ï¼Œæ¯ä¸ªæ–‡ä»¶ä¸€ä¸ªæ–‡æ¡£
â€‹
import
{
PDFLoader
}
from
"langchain/document_loaders/fs/pdf"
;
const
loader
=
new
PDFLoader
(
"src/document_loaders/example_data/example.pdf"
,
{
splitPages
:
false
,
}
)
;
const
docs
=
await
loader
.
load
(
)
;
ç”¨æ³•ï¼Œè‡ªå®šä¹‰
pdfjs
æ„å»º
â€‹
é»˜è®¤æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸å¤§å¤šæ•°ç¯å¢ƒï¼ˆåŒ…æ‹¬ Node.js å’Œç°ä»£æµè§ˆå™¨)å…¼å®¹çš„
pdf-parse
æ†ç»‘çš„
pdfjs
æ„å»ºã€‚å¦‚æœè¦ä½¿ç”¨æ›´é«˜ç‰ˆæœ¬çš„
pdfjs-dist
ï¼Œæˆ–è€…è¦ä½¿ç”¨è‡ªå®šä¹‰æ„å»ºçš„
pdfjs-dist
ï¼Œåˆ™å¯ä»¥æä¾›è¿”å›è§£æä¸º
PDFJS
å¯¹è±¡çš„ promise çš„è‡ªå®šä¹‰
pdfjs
å‡½æ•°ã€‚
åœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨â€œæ—§ç‰ˆâ€ï¼ˆè¯·å‚é˜…
pdfjsæ–‡æ¡£
)ï¼Œè¯¥æ„å»ºåŒ…æ‹¬é»˜è®¤æ„å»ºä¸­æœªåŒ…å«çš„å‡ ä¸ª polyfillã€‚
npm
Yarn
pnpm
npm
install
pdfjs-dist
yarn
add
pdfjs-dist
pnpm
add
pdfjs-dist
import
{
PDFLoader
}
from
"langchain/document_loaders/fs/pdf"
;
const
loader
=
new
PDFLoader
(
"src/document_loaders/example_data/example.pdf"
,
{
// you may need to add `.then(m => m.default)` to the end of the import
pdfjs
:
(
)
=>
import
(
"pdfjs-dist/legacy/build/pdf.js"
)
,
}
)
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\document_loaders\examples\file_loaders\subtitles.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/file_loaders/subtitles
crawled_at: 2025-06-22T02:00:22.117912
---

å­—å¹•
æœ¬ç¤ºä¾‹ä»‹ç»å¦‚ä½•ä»å­—å¹•æ–‡ä»¶ä¸­åŠ è½½æ•°æ®ã€‚æ¯ä¸ªå­—å¹•æ–‡ä»¶å°†åˆ›å»ºä¸€ä¸ªæ–‡æ¡£ã€‚
è®¾ç½®
â€‹
npm
Yarn
pnpm
npm
install
srt-parser-2
yarn
add
srt-parser-2
pnpm
add
srt-parser-2
ç”¨æ³•
â€‹
import
{
SRTLoader
}
from
"langchain/document_loaders/fs/srt"
;
const
loader
=
new
SRTLoader
(
"src/document_loaders/example_data/Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.srt"
)
;
const
docs
=
await
loader
.
load
(
)
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\document_loaders\examples\file_loaders\text.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/file_loaders/text
crawled_at: 2025-06-22T02:00:22.152375
---

æ–‡æœ¬æ–‡ä»¶
æœ¬ä¾‹å°†ä»‹ç»å¦‚ä½•ä»æ–‡æœ¬æ–‡ä»¶ä¸­åŠ è½½æ•°æ®ã€‚
import
{
TextLoader
}
from
"langchain/document_loaders/fs/text"
;
const
loader
=
new
TextLoader
(
"src/document_loaders/example_data/example.txt"
)
;
const
docs
=
await
loader
.
load
(
)
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\document_loaders\examples\file_loaders\unstructured.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/file_loaders/unstructured
crawled_at: 2025-06-22T02:00:22.140037
---

æ— ç»“æ„
æœ¬ç¤ºä¾‹ä»‹ç»å¦‚ä½•ä½¿ç”¨
æ— ç»“æ„
è¯»å–å¤šç§ç±»å‹çš„æ–‡ä»¶ã€‚æ— ç»“æ„ç›®å‰æ”¯æŒåŠ è½½æ–‡æœ¬æ–‡ä»¶ã€PPTã€HTMLã€PDFã€å›¾ç‰‡ç­‰ã€‚
è®¾ç½®
â€‹
æ‚¨å¯ä»¥åœ¨è®¡ç®—æœºä¸Šä½¿ç”¨Dockerè¿è¡Œæ— ç»“æ„ã€‚è¦è¿™æ ·åšï¼Œæ‚¨éœ€è¦å®‰è£…Dockerã€‚æ‚¨å¯ä»¥åœ¨æ­¤å¤„æ‰¾åˆ°å®‰è£…Dockerçš„è¯´æ˜
here
ã€‚
docker
run -p
8000
:8000 -d --rm --name unstructured-api quay.io/unstructured-io/unstructured-api:latest --port
8000
--host
0.0
.0.0
ç”¨æ³•
â€‹
è¿è¡Œæ— ç»“æ„åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å®ƒä»è®¡ç®—æœºä¸­åŠ è½½æ–‡ä»¶ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç ä»è®¡ç®—æœºä¸­åŠ è½½æ–‡ä»¶ã€‚
import
{
UnstructuredLoader
}
from
"langchain/document_loaders/fs/unstructured"
;
const
options
=
{
apiKey
:
"MY_API_KEY"
,
}
;
const
loader
=
new
UnstructuredLoader
(
"src/document_loaders/example_data/notion.md"
,
options
)
;
const
docs
=
await
loader
.
load
(
)
;
ç›®å½•
â€‹
æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ 'UnstructuredDirectoryLoader' ä»ç›®å½•ä¸­åŠ è½½æ‰€æœ‰æ–‡ä»¶ï¼Œå…¶ç»§æ‰¿è‡ª
'DirectoryLoader'
import
{
UnstructuredDirectoryLoader
}
from
"langchain/document_loaders/fs/unstructured"
;
const
options
=
{
apiKey
:
"MY_API_KEY"
,
}
;
const
loader
=
new
UnstructuredDirectoryLoader
(
"langchain/src/document_loaders/tests/example_data"
,
options
)
;
const
docs
=
await
loader
.
load
(
)
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\document_loaders\examples\web_loaders\apify_dataset.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/web_loaders/apify_dataset
crawled_at: 2025-06-22T02:00:22.400226
---

Apify æ•°æ®é›†
æœ¬æŒ‡å—å±•ç¤ºå¦‚ä½•ä½¿ç”¨
Apify
å’Œ LangChain ä» Apify æ•°æ®é›†ä¸­åŠ è½½æ–‡æ¡£ã€‚
æ¦‚è¿°
â€‹
Apify
æ˜¯ä¸€ä¸ªäº‘ç«¯ç½‘é¡µæŠ“å–å’Œæ•°æ®æå–å¹³å°ï¼Œ
æä¾›äº†ä¸€ä¸ªåŒ…å«ä¸€åƒå¤šä¸ªç°æˆåº”ç”¨ç¨‹åºï¼ˆç§°ä½œ
Actors
)çš„
ç”Ÿæ€ç³»ç»Ÿ
ï¼Œç”¨äºå„ç§ç½‘ç»œæŠ“å–ï¼Œçˆ¬å–ï¼Œå’Œæ•°æ®æå–çš„ç”¨ä¾‹ã€‚
æœ¬æŒ‡å—å±•ç¤ºå¦‚ä½•åŠ è½½æ–‡æ¡£
ç”¨äºå­˜å‚¨ç»“æ„åŒ–ç½‘ç»œæŠ“å–ç»“æœçš„å­˜å‚¨ç©ºé—´ï¼Œ
from an
Apify Dataset
â€” a scalable append-only
ä¾‹å¦‚äº§å“åˆ—è¡¨æˆ– Google SERPs ç­‰ï¼Œç„¶åå°†å®ƒä»¬å¯¼å‡ºåˆ°å„ç§æ ¼å¼ï¼Œå¦‚ JSONï¼Œ CSVï¼Œ æˆ– Excelã€‚
æ•°æ®é›†é€šå¸¸ç”¨äºä¿å­˜æ¼”å‘˜çš„ç»“æœã€‚
ä¾‹å¦‚ï¼Œ
ç½‘ç«™å†…å®¹çˆ¬è™«
æ¼”å‘˜ï¼Œ
æ·±åº¦çˆ¬å–ç½‘ç«™ï¼Œå¦‚æ–‡æ¡£ï¼ŒçŸ¥è¯†åº“ï¼Œå¸®åŠ©ä¸­å¿ƒæˆ–åšå®¢ç­‰ï¼Œå¹¶å°†ç½‘é¡µçš„æ–‡æœ¬å†…å®¹å­˜å‚¨åˆ°æ•°æ®é›†ä¸­ã€‚ï¼Œ
è®¾ç½®
â€‹
æ‚¨é¦–å…ˆéœ€è¦å®‰è£…å®˜æ–¹çš„ Apify å®¢æˆ·ç«¯:
npm install apify-client
æ‚¨è¿˜éœ€è¦æ³¨å†Œå¹¶è·å–æ‚¨çš„
Apify API ä»¤ç‰Œ
ã€‚
ç”¨æ³•
â€‹
ä»æ–°æ•°æ®é›†
â€‹
å¦‚æœæ‚¨å°šæœªåœ¨ Apify å¹³å°ä¸Šæ‹¥æœ‰ç°æœ‰æ•°æ®é›†ï¼Œåˆ™éœ€è¦è°ƒç”¨ Actor å¹¶ç­‰å¾…ç»“æœæ¥åˆå§‹åŒ–æ–‡æ¡£åŠ è½½ç¨‹åºã€‚
æ³¨æ„:
è°ƒç”¨æ¼”å‘˜å¯èƒ½éœ€è¦å¾ˆé•¿æ—¶é—´ï¼Œå¤§çº¦éœ€è¦æ•°å°æ—¶æˆ–ç”šè‡³æ•°æ—¥æ¥å¤„ç†å¤§å‹ç½‘ç«™ï¼
ä»¥ä¸‹æ˜¯ä¸€ä¸ªä¾‹å­:
import
{
ApifyDatasetLoader
}
from
"langchain/document_loaders/web/apify_dataset"
;
import
{
Document
}
from
"langchain/document"
;
import
{
HNSWLib
}
from
"langchain/vectorstores/hnswlib"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
RetrievalQAChain
}
from
"langchain/chains"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
/*
* datasetMappingFunction is a function that maps your Apify dataset format to LangChain documents.
* In the below example, the Apify dataset format looks like this:
* {
*   "url": "https://apify.com",
*   "text": "Apify is the best web scraping and automation platform."
* }
*/
const
loader
=
await
ApifyDatasetLoader
.
fromActorCall
(
"apify/website-content-crawler"
,
{
startUrls
:
[
{
url
:
"https://js.langchain.com/docs/"
}
]
,
}
,
{
datasetMappingFunction
:
(
item
)
=>
new
Document
(
{
pageContent
:
(
item
.
text
||
""
)
as
string
,
metadata
:
{
source
:
item
.
url
}
,
}
)
,
clientOptions
:
{
token
:
"your-apify-token"
,
// Or set as process.env.APIFY_API_TOKEN
}
,
}
)
;
const
docs
=
await
loader
.
load
(
)
;
const
vectorStore
=
await
HNSWLib
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
)
;
const
model
=
new
OpenAI
(
{
temperature
:
0
,
}
)
;
const
chain
=
RetrievalQAChain
.
fromLLM
(
model
,
vectorStore
.
asRetriever
(
)
,
{
returnSourceDocuments
:
true
,
}
)
;
const
res
=
await
chain
.
call
(
{
query
:
"What is LangChain?"
}
)
;
console
.
log
(
res
.
text
)
;
console
.
log
(
res
.
sourceDocuments
.
map
(
(
d
:
Document
)
=>
d
.
metadata
.
source
)
)
;
/*
LangChain is a framework for developing applications powered by language models.
[
'https://js.langchain.com/docs/',
'https://js.langchain.com/docs/modules/chains/',
'https://js.langchain.com/docs/modules/chains/llmchain/',
'https://js.langchain.com/docs/category/functions-4'
]
*/
æ¥è‡ªç°æœ‰æ•°æ®é›†
â€‹
å¦‚æœæ‚¨å·²ç»åœ¨Apifyå¹³å°ä¸Šæ‹¥æœ‰ç°æœ‰çš„æ•°æ®é›†ï¼Œæ‚¨å¯ä»¥ç›´æ¥ä½¿ç”¨æ„é€ å‡½æ•°åˆå§‹åŒ–æ–‡æ¡£åŠ è½½å™¨:
import
{
ApifyDatasetLoader
}
from
"langchain/document_loaders/web/apify_dataset"
;
import
{
Document
}
from
"langchain/document"
;
import
{
HNSWLib
}
from
"langchain/vectorstores/hnswlib"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
RetrievalQAChain
}
from
"langchain/chains"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
/*
* datasetMappingFunction is a function that maps your Apify dataset format to LangChain documents.
* In the below example, the Apify dataset format looks like this:
* {
*   "url": "https://apify.com",
*   "text": "Apify is the best web scraping and automation platform."
* }
*/
const
loader
=
new
ApifyDatasetLoader
(
"your-dataset-id"
,
{
datasetMappingFunction
:
(
item
)
=>
new
Document
(
{
pageContent
:
(
item
.
text
||
""
)
as
string
,
metadata
:
{
source
:
item
.
url
}
,
}
)
,
clientOptions
:
{
token
:
"your-apify-token"
,
// Or set as process.env.APIFY_API_TOKEN
}
,
}
)
;
const
docs
=
await
loader
.
load
(
)
;
const
vectorStore
=
await
HNSWLib
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
)
;
const
model
=
new
OpenAI
(
{
temperature
:
0
,
}
)
;
const
chain
=
RetrievalQAChain
.
fromLLM
(
model
,
vectorStore
.
asRetriever
(
)
,
{
returnSourceDocuments
:
true
,
}
)
;
const
res
=
await
chain
.
call
(
{
query
:
"What is LangChain?"
}
)
;
console
.
log
(
res
.
text
)
;
console
.
log
(
res
.
sourceDocuments
.
map
(
(
d
:
Document
)
=>
d
.
metadata
.
source
)
)
;
/*
LangChain is a framework for developing applications powered by language models.
[
'https://js.langchain.com/docs/',
'https://js.langchain.com/docs/modules/chains/',
'https://js.langchain.com/docs/modules/chains/llmchain/',
'https://js.langchain.com/docs/category/functions-4'
]
*/



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\document_loaders\examples\web_loaders\college_confidential.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/web_loaders/college_confidential
crawled_at: 2025-06-22T02:00:22.420343
---

å¤§å­¦æœºå¯†
æœ¬ä¾‹è¯´æ˜äº†å¦‚ä½•ä½¿ç”¨Cheerioä»å¤§å­¦æœºå¯†ç½‘ç«™åŠ è½½æ•°æ®ã€‚æ¯ä¸ªé¡µé¢å°†åˆ›å»ºä¸€ä¸ªæ–‡æ¡£ã€‚
è®¾ç½®
â€‹
npm
Yarn
pnpm
npm
install
cheerio
yarn
add
cheerio
pnpm
add
cheerio
ç”¨æ³•
â€‹
import
{
CollegeConfidentialLoader
}
from
"langchain/document_loaders/web/college_confidential"
;
const
loader
=
new
CollegeConfidentialLoader
(
"https://www.collegeconfidential.com/colleges/brown-university/"
)
;
const
docs
=
await
loader
.
load
(
)
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\document_loaders\examples\web_loaders\confluence.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/web_loaders/confluence
crawled_at: 2025-06-22T02:00:22.439562
---

Confluenceï¼ˆç»´åŸºè½¯ä»¶)
ï¼šå…¼å®¹æ€§
ä»…åœ¨ Node.js ä¸Šå¯ç”¨ã€‚
æœ¬æ–‡ä»‹ç»å¦‚ä½•ä» Confluence ç©ºé—´ä¸­åŠ è½½æ–‡æ¡£å¯¹è±¡ã€‚
å‡­è¯
â€‹
æ‚¨éœ€è¦è®¾ç½®è®¿é—®ä»¤ç‰Œï¼Œå¹¶æä¾›æ‚¨çš„ Confluence ç”¨æˆ·åï¼Œä»¥ä¾¿èº«ä»½éªŒè¯è¯·æ±‚
æ‚¨è¿˜éœ€è¦ â€œç©ºé—´å¯†é’¥â€ æ¥è·å–åŒ…å«è¦åŠ è½½ä¸ºæ–‡æ¡£çš„é¡µé¢çš„ç©ºé—´ã€‚å¯¼èˆªåˆ°æ‚¨çš„ç©ºé—´æ—¶ï¼Œå¯ä»¥åœ¨ url ä¸­æ‰¾åˆ°å®ƒï¼Œä¾‹å¦‚
https://example.atlassian.net/wiki/spaces/{SPACE_KEY}
æ‚¨éœ€è¦å®‰è£…
html-to-text
å°†é¡µé¢è§£æä¸ºçº¯æ–‡æœ¬
npm
Yarn
pnpm
npm
install
html-to-text
yarn
add
html-to-text
pnpm
add
html-to-text
ç”¨æ³•
â€‹
import
{
ConfluencePagesLoader
}
from
"langchain/document_loaders/web/confluence"
;
const
username
=
process
.
env
.
CONFLUENCE_USERNAME
;
const
accessToken
=
process
.
env
.
CONFLUENCE_ACCESS_TOKEN
;
if
(
username
&&
accessToken
)
{
const
loader
=
new
ConfluencePagesLoader
(
{
baseUrl
:
"https://example.atlassian.net/wiki"
,
spaceKey
:
"~EXAMPLE362906de5d343d49dcdbae5dEXAMPLE"
,
username
,
accessToken
,
}
)
;
const
documents
=
await
loader
.
load
(
)
;
console
.
log
(
documents
)
;
}
else
{
console
.
log
(
"You must provide a username and access token to run this example."
)
;
}



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\document_loaders\examples\web_loaders\figma.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/web_loaders/figma
crawled_at: 2025-06-22T02:00:22.464266
---

Figma
æœ¬ä¾‹æ¼”ç¤ºå¦‚ä½•ä»Figmaæ–‡ä»¶åŠ è½½æ•°æ®ã€‚
æ‚¨éœ€è¦Figmaè®¿é—®ä»¤ç‰Œæ‰èƒ½å¼€å§‹ä½¿ç”¨ã€‚
import
{
FigmaFileLoader
}
from
"langchain/document_loaders/web/figma"
;
const
loader
=
new
FigmaFileLoader
(
{
accessToken
:
"FIGMA_ACCESS_TOKEN"
,
// or load it from process.env.FIGMA_ACCESS_TOKEN
nodeIds
:
[
"id1"
,
"id2"
,
"id3"
]
,
fileKey
:
"key"
,
}
)
;
const
docs
=
await
loader
.
load
(
)
;
console
.
log
(
{
docs
}
)
;
æ‚¨å¯ä»¥é€šè¿‡åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€æ–‡ä»¶å¹¶ä»URLä¸­æå–å®ƒä»¬æ¥æ‰¾åˆ°Figmaæ–‡ä»¶çš„å¯†é’¥å’ŒèŠ‚ç‚¹ID:
https://www.figma.com/file/<YOUR FILE KEY HERE>/LangChainJS-Test?type=whiteboard&node-id=<YOUR NODE ID HERE>&t=e6lqWkKecuYQRyRg-0



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\document_loaders\examples\web_loaders\gitbook.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/web_loaders/gitbook
crawled_at: 2025-06-22T02:00:22.533918
---

GitBook
æœ¬ç¤ºä¾‹ä»‹ç»å¦‚ä½•ä½¿ç”¨ Cheerio ä»ä»»ä½• GitBook ä¸­åŠ è½½æ•°æ®ã€‚å°†ä¸ºæ¯ä¸ªé¡µé¢åˆ›å»ºä¸€ä¸ªæ–‡æ¡£ã€‚
è®¾ç½®
â€‹
npm
Yarn
pnpm
npm
install
cheerio
yarn
add
cheerio
pnpm
add
cheerio
ä»å•ä¸ª GitBook é¡µé¢åŠ è½½
â€‹
import
{
GitbookLoader
}
from
"langchain/document_loaders/web/gitbook"
;
const
loader
=
new
GitbookLoader
(
"https://docs.gitbook.com/product-tour/navigation"
)
;
const
docs
=
await
loader
.
load
(
)
;
ä»ç»™å®š GitBook ä¸­çš„æ‰€æœ‰è·¯å¾„åŠ è½½
â€‹
ä¸ºäº†ä½¿æ­¤é¡¹åŠŸèƒ½æ­£å¸¸å·¥ä½œï¼Œéœ€è¦ä½¿ç”¨æ ¹è·¯å¾„ï¼ˆä¾‹å¦‚
https://docs.gitbook.com)åˆå§‹åŒ–
GitbookLoaderï¼Œå¹¶å°†
shouldLoadAllPaths
è®¾ç½®ä¸º
true
ã€‚
import
{
GitbookLoader
}
from
"langchain/document_loaders/web/gitbook"
;
const
loader
=
new
GitbookLoader
(
"https://docs.gitbook.com"
,
{
shouldLoadAllPaths
:
true
,
}
)
;
const
docs
=
await
loader
.
load
(
)
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\document_loaders\examples\web_loaders\github.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/web_loaders/github
crawled_at: 2025-06-22T02:00:22.587317
---

GitHub
æœ¬ç¤ºä¾‹ä»‹ç»äº†å¦‚ä½•ä» GitHub å­˜å‚¨åº“åŠ è½½æ•°æ®ã€‚
æ‚¨å¯ä»¥å°†
GITHUB_ACCESS_TOKEN
ç¯å¢ƒå˜é‡è®¾ç½®ä¸º GitHub è®¿é—®ä»¤ç‰Œï¼Œä»¥å¢åŠ é€Ÿç‡é™åˆ¶å’Œè®¿é—®ç§æœ‰å­˜å‚¨åº“ã€‚
è®¾ç½®
â€‹
GitHub åŠ è½½å™¨éœ€è¦
ignore npm package
ä½œä¸ºåŒç­‰ä¾èµ–é¡¹ã€‚å¯ä»¥åƒè¿™æ ·å®‰è£…å®ƒ
npm
Yarn
pnpm
npm
install
ignore
yarn
add
ignore
pnpm
add
ignore
ç”¨æ³•
â€‹
import
{
GithubRepoLoader
}
from
"langchain/document_loaders/web/github"
;
export
const
run
=
async
(
)
=>
{
const
loader
=
new
GithubRepoLoader
(
"https://github.com/hwchase17/langchainjs"
,
{
branch
:
"main"
,
recursive
:
false
,
unknown
:
"warn"
}
)
;
const
docs
=
await
loader
.
load
(
)
;
console
.
log
(
{
docs
}
)
;
}
;
åŠ è½½å™¨å°†å¿½ç•¥åƒå›¾åƒè¿™æ ·çš„äºŒè¿›åˆ¶æ–‡ä»¶ã€‚
ä½¿ç”¨ .gitignore è¯­æ³•
â€‹
è¦å¿½ç•¥ç‰¹å®šæ–‡ä»¶ï¼Œæ‚¨å¯ä»¥å°†
ignorePaths
æ•°ç»„ä¼ é€’åˆ°æ„é€ å‡½æ•°ä¸­
import
{
GithubRepoLoader
}
from
"langchain/document_loaders/web/github"
;
export
const
run
=
async
(
)
=>
{
const
loader
=
new
GithubRepoLoader
(
"https://github.com/hwchase17/langchainjs"
,
{
branch
:
"main"
,
recursive
:
false
,
unknown
:
"warn"
,
ignorePaths
:
[
"*.md"
]
}
)
;
const
docs
=
await
loader
.
load
(
)
;
console
.
log
(
{
docs
}
)
;
// Will not include any .md files
}
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\document_loaders\examples\web_loaders\hn.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/web_loaders/hn
crawled_at: 2025-06-22T02:00:22.574461
---

é»‘å®¢æ–°é—»
æœ¬ä¾‹ä»‹ç»å¦‚ä½•ä½¿ç”¨Cheerioä»é»‘å®¢æ–°é—»ç½‘ç«™åŠ è½½æ•°æ®ã€‚æ¯é¡µå°†åˆ›å»ºä¸€ä¸ªæ–‡æ¡£ã€‚
è®¾ç½®
â€‹
npm
install
cheerio
ç”¨æ³•
â€‹
import
{
HNLoader
}
from
"langchain/document_loaders/web/hn"
;
const
loader
=
new
HNLoader
(
"https://news.ycombinator.com/item?id=34817881"
)
;
const
docs
=
await
loader
.
load
(
)
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\document_loaders\examples\web_loaders\imsdb.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/web_loaders/imsdb
crawled_at: 2025-06-22T02:00:22.577874
---

IMSDB
æœ¬ä¾‹ä»‹ç»å¦‚ä½•ä½¿ç”¨Cheerioä»äº’è”ç½‘ç”µå½±å‰§æœ¬æ•°æ®åº“ç½‘ç«™åŠ è½½æ•°æ®ã€‚æ¯ä¸ªé¡µé¢å°†åˆ›å»ºä¸€ä¸ªæ–‡æ¡£ã€‚
è®¾ç½®
â€‹
npm
install
cheerio
ç”¨æ³•
â€‹
import
{
IMSDBLoader
}
from
"langchain/document_loaders/web/imsdb"
;
const
loader
=
new
IMSDBLoader
(
"https://imsdb.com/scripts/BlacKkKlansman.html"
)
;
const
docs
=
await
loader
.
load
(
)
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\document_loaders\examples\web_loaders\notiondb.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/web_loaders/notiondb
crawled_at: 2025-06-22T02:00:22.593648
---

Notionæ•°æ®åº“
æœ¬ç¤ºä¾‹æ¼”ç¤ºäº†å¦‚ä½•ä»Notionæ•°æ®åº“åŠ è½½æ•°æ®ã€‚
ä½ éœ€è¦ä½ çš„Notioné›†æˆä»¤ç‰Œå’Œè¦è®¿é—®çš„èµ„æºçš„â€œdatabaseIdâ€ã€‚
ä¸è¦å¿˜è®°å°†ä½ çš„é›†æˆæ·»åŠ åˆ°æ•°æ®åº“ä¸­ï¼
import
{
NotionDBLoader
}
from
"langchain/document_loaders/web/notiondb"
;
const
loader
=
new
NotionDBLoader
(
{
pageSizeLimit
:
10
,
databaseId
:
"databaseId"
,
notionIntegrationToken
:
"<your token here>"
,
// Or set as process.env.NOTION_INTEGRATION_TOKEN
}
)
;
const
docs
=
await
loader
.
load
(
)
;
console
.
log
(
{
docs
}
)
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\document_loaders\examples\web_loaders\s3.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/web_loaders/s3
crawled_at: 2025-06-22T02:00:22.604954
---

S3 æ–‡ä»¶
å…¼å®¹æ€§
ä»…é€‚ç”¨äº Node.jsã€‚
æœ¬æ–‡æ¡£ä»‹ç»å¦‚ä½•ä» S3 æ–‡ä»¶å¯¹è±¡ä¸­åŠ è½½æ–‡æ¡£å¯¹è±¡ã€‚
è®¾ç½®
â€‹
è¦è¿è¡Œæ­¤ç´¢å¼•ï¼Œæ‚¨éœ€è¦å…ˆè®¾ç½®å¹¶å‡†å¤‡å¥½ Unstructured å¹¶åœ¨å¯ç”¨çš„ URL ç«¯ç‚¹ä¸­ä½¿ç”¨ã€‚å®ƒä¹Ÿå¯ä»¥åœ¨æœ¬åœ°é…ç½®è¿è¡Œã€‚
è¯·æŸ¥çœ‹æ­¤å¤„çš„æ–‡æ¡£ï¼ˆ
https://js.langchain.com/docs/modules/indexes/document_loaders/examples/file_loaders/unstructured)ä»¥äº†è§£å¦‚ä½•è¿›è¡Œæ“ä½œã€‚
ä½¿ç”¨æ–¹æ³•
â€‹
ä¸€æ—¦ Unstructured é…ç½®å®Œæˆï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ S3 Loader åŠ è½½æ–‡ä»¶ï¼Œç„¶åå°†å…¶è½¬æ¢ä¸ºæ–‡æ¡£ã€‚
æ‚¨å¯ä»¥é€‰æ‹©æä¾› s3Config å‚æ•°ä»¥æŒ‡å®šæ¡¶åŒºåŸŸè®¿é—®å¯†é’¥å’Œç§˜å¯†è®¿é—®å¯†é’¥ã€‚å¦‚æœæœªæä¾›è¿™äº›å‚æ•°ï¼Œåˆ™éœ€è¦åœ¨æ‚¨çš„ç¯å¢ƒä¸­å…·æœ‰å®ƒä»¬ï¼ˆä¾‹å¦‚é€šè¿‡è¿è¡Œ 'aws configure' å‘½ä»¤)ã€‚
import
{
S3Loader
}
from
"langchain/document_loaders/web/s3"
;
const
loader
=
new
S3Loader
(
{
bucket
:
"my-document-bucket-123"
,
key
:
"AccountingOverview.pdf"
,
s3Config
:
{
region
:
"us-east-1"
,
accessKeyId
:
"AKIAIOSFODNN7EXAMPLE"
,
secretAccessKey
:
"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
,
}
,
unstructuredAPIURL
:
"http://localhost:8000/general/v0/general"
,
}
)
;
const
docs
=
await
loader
.
load
(
)
;
console
.
log
(
docs
)
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\document_loaders\examples\web_loaders\web_cheerio.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/web_loaders/web_cheerio
crawled_at: 2025-06-22T02:00:22.757318
---

ç½‘é¡µï¼Œä½¿ç”¨Cheerio
æœ¬ç¤ºä¾‹æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨Cheerioä»ç½‘é¡µåŠ è½½æ•°æ®ã€‚æ¯ä¸ªç½‘é¡µéƒ½å°†åˆ›å»ºä¸€ä¸ªæ–‡æ¡£ã€‚
Cheerioæ˜¯ä¸€ä¸ªå¿«é€Ÿä¸”è½»é‡çº§çš„åº“ï¼Œå…è®¸æ‚¨ä½¿ç”¨ç±»ä¼¼äºjQueryçš„è¯­æ³•è§£æå’Œéå†HTMLæ–‡æ¡£ã€‚æ‚¨å¯ä»¥ä½¿ç”¨Cheerioä»ç½‘é¡µä¸­æå–æ•°æ®ï¼Œè€Œæ— éœ€åœ¨æµè§ˆå™¨ä¸­å‘ˆç°å®ƒä»¬ã€‚
ä½†æ˜¯ï¼Œ Cheerioæ— æ³•æ¨¡æ‹Ÿæµè§ˆå™¨ï¼Œå› æ­¤å®ƒæ— æ³•åœ¨é¡µé¢ä¸Šæ‰§è¡ŒJavaScriptä»£ç ã€‚è¿™æ„å‘³ç€å®ƒæ— æ³•ä»éœ€è¦JavaScriptå‘ˆç°çš„åŠ¨æ€ç½‘é¡µä¸­æå–æ•°æ®ã€‚è¦åšåˆ°è¿™ä¸€ç‚¹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨
PlaywrightWebBaseLoader
æˆ–
PuppeteerWebBaseLoader
ã€‚
å®‰è£…
â€‹
npm
Yarn
pnpm
npm
install
cheerio
yarn
add
cheerio
pnpm
add
cheerio
ä½¿ç”¨
â€‹
import
{
CheerioWebBaseLoader
}
from
"langchain/document_loaders/web/cheerio"
;
const
loader
=
new
CheerioWebBaseLoader
(
"https://news.ycombinator.com/item?id=34817881"
)
;
const
docs
=
await
loader
.
load
(
)
;
ä½¿ç”¨ï¼Œè‡ªå®šä¹‰é€‰æ‹©å™¨
â€‹
import
{
CheerioWebBaseLoader
}
from
"langchain/document_loaders/web/cheerio"
;
const
loader
=
new
CheerioWebBaseLoader
(
"https://news.ycombinator.com/item?id=34817881"
,
{
selector
:
"p.athing"
,
}
)
;
const
docs
=
await
loader
.
load
(
)
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\document_loaders\examples\web_loaders\web_playwright.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/web_loaders/web_playwright
crawled_at: 2025-06-22T02:00:22.784413
---

Webpagesï¼Œ with Playwright
å…¼å®¹æ€§
ä»…é™äº Node.jsã€‚
æœ¬ç¤ºä¾‹æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨ Playwright ä»ç½‘é¡µä¸­åŠ è½½æ•°æ®ã€‚å°†ä¸ºæ¯ä¸ªç½‘é¡µåˆ›å»ºä¸€ä¸ªæ–‡æ¡£ã€‚
Playwright æ˜¯ä¸€ä¸ª Node.js åº“ï¼Œæä¾›äº†ä¸€ä¸ªé«˜çº§ APIï¼Œç”¨äºæ§åˆ¶å¤šä¸ªæµè§ˆå™¨å¼•æ“ï¼ŒåŒ…æ‹¬ Chromiumã€Firefox å’Œ WebKitã€‚æ‚¨å¯ä»¥ä½¿ç”¨ Playwright è‡ªåŠ¨åŒ–ç½‘é¡µäº¤äº’ï¼ŒåŒ…æ‹¬ä»éœ€è¦ JavaScript æ¸²æŸ“çš„åŠ¨æ€ç½‘é¡µä¸­æå–æ•°æ®ã€‚
å¦‚æœæ‚¨æƒ³è¦ä¸€ä¸ªæ›´è½»é‡çº§çš„è§£å†³æ–¹æ¡ˆï¼Œæƒ³è¦åŠ è½½çš„ç½‘é¡µä¸éœ€è¦ JavaScript æ¸²æŸ“ï¼Œé‚£ä¹ˆå¯ä»¥ä½¿ç”¨
CheerioWebBaseLoader
ã€‚
è®¾ç½®
â€‹
npm
Yarn
pnpm
npm
install
playwright
yarn
add
playwright
pnpm
add
playwright
ç”¨æ³•
â€‹
import
{
PlaywrightWebBaseLoader
}
from
"langchain/document_loaders/web/playwright"
;
/**
* Loader uses `page.content()`
* as default evaluate function
**/
const
loader
=
new
PlaywrightWebBaseLoader
(
"https://www.tabnews.com.br/"
)
;
const
docs
=
await
loader
.
load
(
)
;
é€‰é¡¹
â€‹
è¿™é‡Œæ˜¯å…³äºå¯ä»¥é€šè¿‡ä½¿ç”¨ PlaywrightWebBaseLoaderOptions æ¥å£å°†å‚æ•°ä¼ é€’ç»™ PlaywrightWebBaseLoader æ„é€ å‡½æ•°çš„å‚æ•°çš„è§£é‡Š:
type
PlaywrightWebBaseLoaderOptions
=
{
launchOptions
?
:
LaunchOptions
;
gotoOptions
?
:
PlaywrightGotoOptions
;
evaluate
?
:
PlaywrightEvaluate
;
}
;
launchOptions
: ä¸€ä¸ªå¯é€‰å¯¹è±¡ï¼Œç”¨äºæŒ‡å®šè¦ä¼ é€’ç»™ playwright.chromium.launch() æ–¹æ³•çš„å…¶ä»–é€‰é¡¹ã€‚è¿™å¯ä»¥åŒ…æ‹¬é€‰é¡¹ï¼Œä¾‹å¦‚åœ¨æ— å¤´æ¨¡å¼ä¸‹å¯åŠ¨æµè§ˆå™¨çš„ headless æ ‡å¿—ã€‚
gotoOptions
: ä¸€ä¸ªå¯é€‰å¯¹è±¡ï¼Œç”¨äºæŒ‡å®šè¦ä¼ é€’ç»™ page.goto() æ–¹æ³•çš„å…¶ä»–é€‰é¡¹ã€‚è¿™å¯ä»¥åŒ…æ‹¬é€‰é¡¹ï¼Œä¾‹å¦‚ timeout é€‰é¡¹ä»¥æŒ‡å®šæœ€å¤§å¯¼èˆªæ—¶é—´ï¼ˆä»¥æ¯«ç§’ä¸ºå•ä½)æˆ– waitUntil é€‰é¡¹ä»¥æŒ‡å®šä½•æ—¶å°†å¯¼èˆªè§†ä¸ºæˆåŠŸã€‚
evaluate
: æ˜¯ä¸€ä¸ªå¯é€‰å‡½æ•°ï¼Œå¯ä»¥ä½¿ç”¨è‡ªå®šä¹‰è¯„ä¼°å‡½æ•°åœ¨é¡µé¢ä¸Šè¯„ä¼°JavaScriptä»£ç ã€‚è¿™å¯¹äºä»é¡µé¢æå–æ•°æ®æˆ–ä¸é¡µé¢å…ƒç´ è¿›è¡Œäº¤äº’éå¸¸æœ‰ç”¨ã€‚è¯¥å‡½æ•°åº”è¿”å›è§£æä¸ºåŒ…å«è¯„ä¼°ç»“æœçš„å­—ç¬¦ä¸²çš„Promiseã€‚
é€šè¿‡å°†è¿™äº›é€‰é¡¹ä¼ é€’ç»™
PlaywrightWebBaseLoader
æ„é€ å‡½æ•°,æ‚¨å¯ä»¥è‡ªå®šä¹‰åŠ è½½ç¨‹åºçš„è¡Œä¸ºï¼Œå¹¶ä½¿ç”¨Playwrightå¼ºå¤§çš„åŠŸèƒ½å¯¹Webé¡µé¢è¿›è¡ŒæŠ“å–å’Œäº¤äº’ã€‚
ä»¥ä¸‹æ˜¯ä¸€ä¸ªåŸºæœ¬ç¤ºä¾‹:ï¼š
import
{
PlaywrightWebBaseLoader
}
from
"langchain/document_loaders/web/playwright"
;
const
loader
=
new
PlaywrightWebBaseLoader
(
"https://www.tabnews.com.br/"
,
{
launchOptions
:
{
headless
:
true
,
}
,
gotoOptions
:
{
waitUntil
:
"domcontentloaded"
,
}
,
/** Pass custom evaluate, in this case you get page and browser instances */
async
evaluate
(
page
:
Page
,
browser
:
Browser
)
{
await
page
.
waitForResponse
(
"https://www.tabnews.com.br/va/view"
)
;
const
result
=
await
page
.
evaluate
(
(
)
=>
document
.
body
.
innerHTML
)
;
return
result
;
}
,
}
)
;
const
docs
=
await
loader
.
load
(
)
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\document_loaders\examples\web_loaders\web_puppeteer.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/document_loaders/examples/web_loaders/web_puppeteer
crawled_at: 2025-06-22T02:00:22.848071
---

ä½¿ç”¨ Puppeteer æ‰“é€ ç½‘é¡µ
å…¼å®¹æ€§
ä»…é€‚ç”¨äº Node.jsã€‚
æœ¬ä¾‹æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ Puppeteer ä»ç½‘ç«™åŠ è½½æ•°æ®ï¼Œå¹¶ä¸ºæ¯ä¸ªç½‘é¡µåˆ›å»ºä¸€ä¸ªæ–‡æ¡£ã€‚
Puppeteer æ˜¯ä¸€ä¸ªåŸºäº Node.js çš„åº“ï¼Œä¸ºæ§åˆ¶æ— å¤´ Chrome æˆ– Chromium æä¾›äº†é«˜çº§ APIã€‚ä½¿ç”¨ Puppeteer å¯ä»¥è‡ªåŠ¨åŒ–ç½‘é¡µäº¤äº’ï¼ŒåŒ…æ‹¬ä»éœ€è¦ JavaScript æ¸²æŸ“çš„åŠ¨æ€ç½‘é¡µä¸­æå–æ•°æ®ã€‚
å¦‚æœæ‚¨æƒ³è¦ä¸€ä¸ªè½»é‡çº§çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸”æ‚¨è¦åŠ è½½çš„ç½‘é¡µä¸éœ€è¦ JavaScript æ¸²æŸ“ï¼Œåˆ™å¯ä»¥ä½¿ç”¨
CheerioWebBaseLoader
ã€‚
è®¾ç½®
â€‹
npm
Yarn
pnpm
npm
install
puppeteer
yarn
add
puppeteer
pnpm
add
puppeteer
ä½¿ç”¨
â€‹
import
{
PuppeteerWebBaseLoader
}
from
"langchain/document_loaders/web/puppeteer"
;
/**
* Loader uses `page.evaluate(() => document.body.innerHTML)`
* as default evaluate function
**/
const
loader
=
new
PuppeteerWebBaseLoader
(
"https://www.tabnews.com.br/"
)
;
const
docs
=
await
loader
.
load
(
)
;
é€‰é¡¹
â€‹
ä»¥ä¸‹æ˜¯æ‚¨å¯ä»¥ä½¿ç”¨ PuppeteerWebBaseLoaderOptions æ¥å£å°†å‚æ•°ä¼ é€’ç»™ PuppeteerWebBaseLoader æ„é€ å‡½æ•°çš„è§£é‡Š:
type
PuppeteerWebBaseLoaderOptions
=
{
launchOptions
?
:
PuppeteerLaunchOptions
;
gotoOptions
?
:
PuppeteerGotoOptions
;
evaluate
?
:
(
page
:
Page
,
browser
:
Browser
)
=>
Promise
<
string
>
;
}
;
launchOptions
: ä¸€ä¸ªå¯é€‰çš„å¯¹è±¡ï¼Œç”¨äºæŒ‡å®šè¦ä¼ é€’ç»™ puppeteer.launch() æ–¹æ³•çš„é™„åŠ é€‰é¡¹ã€‚è¿™å¯ä»¥åŒ…æ‹¬é€‰é¡¹ï¼Œå¦‚ headless æ ‡å¿—ï¼Œä»¥åœ¨æ— å¤´æ¨¡å¼ä¸‹å¯åŠ¨æµè§ˆå™¨ï¼Œæˆ–è€… slowMo é€‰é¡¹ï¼Œä»¥å‡æ…¢ Puppeteer çš„æ“ä½œï¼Œä½¿å…¶æ›´å®¹æ˜“è·Ÿè¸ªã€‚
gotoOptions
: ä¸€ä¸ªå¯é€‰çš„å¯¹è±¡ï¼Œç”¨äºæŒ‡å®šè¦ä¼ é€’ç»™ page.goto() æ–¹æ³•çš„é™„åŠ é€‰é¡¹ã€‚è¿™å¯ä»¥åŒ…æ‹¬é€‰é¡¹ï¼Œå¦‚ timeout é€‰é¡¹ï¼Œä»¥æŒ‡å®šæœ€å¤§å¯¼èˆªæ—¶é—´ï¼ˆä»¥æ¯«ç§’ä¸ºå•ä½)ï¼Œæˆ–è€… waitUntil é€‰é¡¹ï¼Œä»¥æŒ‡å®šä½•æ—¶å°†å¯¼èˆªè§†ä¸ºæˆåŠŸã€‚
evaluate
ï¼ˆå¯é€‰)ï¼šå¯ä»¥ä½¿ç”¨page.evaluate()æ–¹æ³•åœ¨é¡µé¢ä¸Šè¯„ä¼°JavaScriptä»£ç çš„å¯é€‰å‡½æ•°ã€‚è¿™å¯¹äºä»é¡µé¢æå–æ•°æ®æˆ–ä¸é¡µé¢å…ƒç´ äº¤äº’éå¸¸æœ‰ç”¨ã€‚è¯¥å‡½æ•°åº”è¿”å›ä¸€ä¸ªPromiseï¼Œè¯¥Promiseè§£æä¸ºåŒ…å«è¯„ä¼°ç»“æœçš„å­—ç¬¦ä¸²ã€‚
é€šè¿‡å°†è¿™äº›é€‰é¡¹ä¼ é€’ç»™
PuppeteerWebBaseLoader
æ„é€ å‡½æ•°ï¼Œæ‚¨å¯ä»¥è‡ªå®šä¹‰åŠ è½½ç¨‹åºçš„è¡Œä¸ºå¹¶ä½¿ç”¨Puppeteerçš„å¼ºå¤§åŠŸèƒ½æ¥æŠ“å–å’Œä¸ç½‘é¡µäº¤äº’ã€‚
ä»¥ä¸‹æ˜¯ä¸€ä¸ªåŸºæœ¬ç¤ºä¾‹:ï¼š
import
{
PuppeteerWebBaseLoader
}
from
"langchain/document_loaders/web/puppeteer"
;
const
loader
=
new
PuppeteerWebBaseLoader
(
"https://www.tabnews.com.br/"
,
{
launchOptions
:
{
headless
:
true
,
}
,
gotoOptions
:
{
waitUntil
:
"domcontentloaded"
,
}
,
/** Pass custom evaluate, in this case you get page and browser instances */
async
evaluate
(
page
:
Page
,
browser
:
Browser
)
{
await
page
.
waitForResponse
(
"https://www.tabnews.com.br/va/view"
)
;
const
result
=
await
page
.
evaluate
(
(
)
=>
document
.
body
.
innerHTML
)
;
return
result
;
}
,
}
)
;
const
docs
=
await
loader
.
load
(
)
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\text_splitters\examples\character.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/text_splitters/examples/character
crawled_at: 2025-06-22T02:00:23.507996
---

å­—ç¬¦æ–‡æœ¬åˆ†å‰²å™¨
é™¤äº†é€’å½’å­—ç¬¦æ–‡æœ¬åˆ†å‰²å™¨ä¹‹å¤–ï¼Œè¿˜æœ‰æ›´å¸¸è§çš„å­—ç¬¦æ–‡æœ¬åˆ†å‰²å™¨ã€‚
å®ƒä»…åœ¨ä¸€ä¸ªç±»å‹çš„å­—ç¬¦ä¸Šåˆ†å‰²ï¼ˆé»˜è®¤ä¸º
"\\"
)ã€‚æ‚¨å¯ä»¥ä»¥å®Œå…¨ç›¸åŒçš„æ–¹å¼ä½¿ç”¨å®ƒã€‚
import
{
Document
}
from
"langchain/document"
;
import
{
CharacterTextSplitter
}
from
"langchain/text_splitter"
;
const
text
=
"foo bar baz 123"
;
const
splitter
=
new
CharacterTextSplitter
(
{
separator
:
" "
,
chunkSize
:
7
,
chunkOverlap
:
3
,
}
)
;
const
output
=
await
splitter
.
createDocuments
(
[
text
]
)
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\text_splitters\examples\code.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/text_splitters/examples/code
crawled_at: 2025-06-22T02:00:23.572722
---

ä»£ç å’Œæ ‡è®°æ–‡æœ¬åˆ†å‰²å™¨
LangChainæ”¯æŒå„ç§ä¸åŒçš„æ ‡è®°å’Œç¼–ç¨‹è¯­è¨€ç‰¹å®šçš„æ–‡æœ¬åˆ†å‰²å™¨ï¼Œä»¥åŸºäºè¯­è¨€ç‰¹å®šçš„è¯­æ³•åˆ†å‰²æ–‡æœ¬ã€‚
â€‹è¿™å°†å¯¼è‡´æ›´å…·æœ‰è¯­ä¹‰çš„è‡ªåŒ…å«å—ï¼Œæ›´é€‚ç”¨äºçŸ¢é‡å­˜å‚¨æˆ–å…¶ä»–æ£€ç´¢å™¨ã€‚
æµè¡Œçš„è¯­è¨€ï¼Œå¦‚JavaScriptï¼Œ Pythonï¼Œå’ŒRustï¼Œä»¥åŠLatexï¼ŒHTMLï¼Œå’ŒMarkdownéƒ½å—åˆ°æ”¯æŒã€‚
ç”¨æ³•
â€‹
ä½¿ç”¨â€œfromLanguageâ€å·¥å‚æ–¹æ³•åˆå§‹åŒ–æ ‡å‡†çš„â€œRecursiveCharacterTextSplitterâ€ã€‚ä»¥ä¸‹æ˜¯å„ç§è¯­è¨€çš„ç¤ºä¾‹ã€‚
JavaScript
â€‹
import
{
SupportedTextSplitterLanguages
,
RecursiveCharacterTextSplitter
,
}
from
"langchain/text_splitter"
;
console
.
log
(
SupportedTextSplitterLanguages
)
;
// Array of supported languages
/*
[
'cpp',      'go',
'java',     'js',
'php',      'proto',
'python',   'rst',
'ruby',     'rust',
'scala',    'swift',
'markdown', 'latex',
'html'
]
*/
const
jsCode
=
`
function helloWorld() {
console.log("Hello, World!");
}
// Call the function
helloWorld();
`
;
const
splitter
=
RecursiveCharacterTextSplitter
.
fromLanguage
(
"js"
,
{
chunkSize
:
32
,
chunkOverlap
:
0
,
}
)
;
const
jsOutput
=
await
splitter
.
createDocuments
(
[
jsCode
]
)
;
console
.
log
(
jsOutput
)
;
/*
[
Document {
pageContent: 'function helloWorld() {',
metadata: { loc: [Object] }
},
Document {
pageContent: 'console.log("Hello, World!");',
metadata: { loc: [Object] }
},
Document {
pageContent: '}\n// Call the function',
metadata: { loc: [Object] }
},
Document {
pageContent: 'helloWorld();',
metadata: { loc: [Object] }
}
]
*/
Python
â€‹
import
{
RecursiveCharacterTextSplitter
}
from
"langchain/text_splitter"
;
const
pythonCode
=
`
def hello_world():
print("Hello, World!")
# Call the function
hello_world()
`
;
const
splitter
=
RecursiveCharacterTextSplitter
.
fromLanguage
(
"python"
,
{
chunkSize
:
32
,
chunkOverlap
:
0
,
}
)
;
const
pythonOutput
=
await
splitter
.
createDocuments
(
[
pythonCode
]
)
;
console
.
log
(
pythonOutput
)
;
/*
[
Document {
pageContent: 'def hello_world():',
metadata: { loc: [Object] }
},
Document {
pageContent: 'print("Hello, World!")',
metadata: { loc: [Object] }
},
Document {
pageContent: '# Call the function',
metadata: { loc: [Object] }
},
Document {
pageContent: 'hello_world()',
metadata: { loc: [Object] }
}
]
*/
HTML
â€‹
import
{
RecursiveCharacterTextSplitter
}
from
"langchain/text_splitter"
;
const
text
=
`
<!DOCTYPE html>
<html>
<head>
<title>ğŸ¦œï¸ğŸ”— LangChain</title>
<style>
body {
font-family: Arial, sans-serif;
}
h1 {
color: darkblue;
}
</style>
</head>
<body>
<div>
<h1>ğŸ¦œï¸ğŸ”— LangChain</h1>
<p>âš¡ Building applications with LLMs through composability âš¡</p>
</div>
<div>
As an open source project in a rapidly developing field, we are extremely open to contributions.
</div>
</body>
</html>
`
;
const
splitter
=
RecursiveCharacterTextSplitter
.
fromLanguage
(
"html"
,
{
chunkSize
:
175
,
chunkOverlap
:
20
,
}
)
;
const
output
=
await
splitter
.
createDocuments
(
[
text
]
)
;
console
.
log
(
output
)
;
/*
[
Document {
pageContent: '<!DOCTYPE html>\n<html>',
metadata: { loc: [Object] }
},
Document {
pageContent: '<head>\n    <title>ğŸ¦œï¸ğŸ”— LangChain</title>',
metadata: { loc: [Object] }
},
Document {
pageContent: '<style>\n' +
'      body {\n' +
'        font-family: Arial, sans-serif;\n' +
'      }\n' +
'      h1 {\n' +
'        color: darkblue;\n' +
'      }\n' +
'    </style>\n' +
'  </head>',
metadata: { loc: [Object] }
},
Document {
pageContent: '<body>\n' +
'    <div>\n' +
'      <h1>ğŸ¦œï¸ğŸ”— LangChain</h1>\n' +
'      <p>âš¡ Building applications with LLMs through composability âš¡</p>\n' +
'    </div>',
metadata: { loc: [Object] }
},
Document {
pageContent: '<div>\n' +
'      As an open source project in a rapidly developing field, we are extremely open to contributions.\n' +
'    </div>\n' +
'  </body>\n' +
'</html>',
metadata: { loc: [Object] }
}
]
*/
Latex
â€‹
import
{
RecursiveCharacterTextSplitter
}
from
"langchain/text_splitter"
;
const
text
=
`
\\begin{document}
\\title{ğŸ¦œï¸ğŸ”— LangChain}
âš¡ Building applications with LLMs through composability âš¡
\\section{Quick Install}
\\begin{verbatim}
Hopefully this code block isn't split
yarn add langchain
\\end{verbatim}
As an open source project in a rapidly developing field, we are extremely open to contributions.
\\end{document}
`
;
const
splitter
=
RecursiveCharacterTextSplitter
.
fromLanguage
(
"latex"
,
{
chunkSize
:
100
,
chunkOverlap
:
0
,
}
)
;
const
output
=
await
splitter
.
createDocuments
(
[
text
]
)
;
console
.
log
(
output
)
;
/*
[
Document {
pageContent: '\\begin{document}\n' +
'\\title{ğŸ¦œï¸ğŸ”— LangChain}\n' +
'âš¡ Building applications with LLMs through composability âš¡',
metadata: { loc: [Object] }
},
Document {
pageContent: '\\section{Quick Install}',
metadata: { loc: [Object] }
},
Document {
pageContent: '\\begin{verbatim}\n' +
"Hopefully this code block isn't split\n" +
'yarn add langchain\n' +
'\\end{verbatim}',
metadata: { loc: [Object] }
},
Document {
pageContent: 'As an open source project in a rapidly developing field, we are extremely open to contributions.',
metadata: { loc: [Object] }
},
Document {
pageContent: '\\end{document}',
metadata: { loc: [Object] }
}
]
*/



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\text_splitters\examples\recursive_character.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/text_splitters/examples/recursive_character
crawled_at: 2025-06-22T02:00:23.622231
---

RecursiveCharacterTextSplitter
æ¨èä½¿ç”¨çš„TextSplitteræ˜¯â€œé€’å½’å­—ç¬¦æ–‡æœ¬åˆ†å‰²å™¨â€ã€‚å®ƒä¼šé€šè¿‡ä¸åŒçš„ç¬¦å·é€’å½’åœ°åˆ†å‰²æ–‡æ¡£-ä»â€œâ€å¼€å§‹ï¼Œç„¶åæ˜¯â€œâ€ï¼Œå†ç„¶åæ˜¯â€œ â€ã€‚è¿™å¾ˆå¥½ï¼Œå› ä¸ºå®ƒä¼šå°½å¯èƒ½åœ°å°†æ‰€æœ‰è¯­ä¹‰ç›¸å…³çš„å†…å®¹ä¿æŒåœ¨åŒä¸€ä½ç½®ã€‚
è¿™é‡Œéœ€è¦äº†è§£çš„é‡è¦å‚æ•°æ˜¯'chunkSize'å’Œ'chunkOverlap'ã€‚'ChunkSize'æ§åˆ¶æœ€ç»ˆæ–‡æ¡£çš„æœ€å¤§å¤§å°ï¼ˆä»¥å­—ç¬¦æ•°ä¸ºå•ä½)ã€‚'ChunkOverlap'æŒ‡å®šæ–‡æ¡£ä¹‹é—´åº”è¯¥æœ‰å¤šå°‘é‡å ã€‚è¿™é€šå¸¸æœ‰åŠ©äºç¡®ä¿æ–‡æœ¬ä¸ä¼šè¢«å¥‡æ€ªåœ°åˆ†å‰²ã€‚åœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†è¿™äº›å€¼è®¾ä¸ºè¾ƒå°çš„å€¼ï¼ˆä»…ä½œè¯´æ˜ç›®çš„)ï¼Œä½†åœ¨å®è·µä¸­å®ƒä»¬é»˜è®¤ä¸º'4000'å’Œ'200'ã€‚
import
{
RecursiveCharacterTextSplitter
}
from
"langchain/text_splitter"
;
const
text
=
`
Hi.I'm Harrison.How? Are? You?Okay then f f f f.
This is a weird text to write, but gotta test the splittingggg some how.
Bye!-H.
`
;
const
splitter
=
new
RecursiveCharacterTextSplitter
(
{
chunkSize
:
10
,
chunkOverlap
:
1
,
}
)
;
const
output
=
await
splitter
.
createDocuments
(
[
text
]
)
;
è¯·æ³¨æ„ï¼Œåœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬æ­£åœ¨åˆ†å‰²åŸå§‹æ–‡æœ¬å­—ç¬¦ä¸²å¹¶è¿”å›æ–‡æ¡£åˆ—è¡¨ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥ç›´æ¥åˆ†å‰²æ–‡æ¡£ã€‚
import
{
Document
}
from
"langchain/document"
;
import
{
RecursiveCharacterTextSplitter
}
from
"langchain/text_splitter"
;
const
text
=
`
Hi.I'm Harrison.How? Are? You?Okay then f f f f.
This is a weird text to write, but gotta test the splittingggg some how.
Bye!-H.
`
;
const
splitter
=
new
RecursiveCharacterTextSplitter
(
{
chunkSize
:
10
,
chunkOverlap
:
1
,
}
)
;
const
docOutput
=
await
splitter
.
splitDocuments
(
[
new
Document
(
{
pageContent
:
text
}
)
,
]
)
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\text_splitters\examples\token.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/text_splitters/examples/token
crawled_at: 2025-06-22T02:00:23.678746
---

TokenTextSplitter
æœ€åï¼Œ
TokenTextSplitter
å°†åŸå§‹æ–‡æœ¬å­—ç¬¦ä¸²è½¬æ¢ä¸º BPE æ ‡è®°ï¼Œå¹¶å°†è¿™äº›æ ‡è®°åˆ†æˆå—ï¼Œç„¶åå°†å•ä¸ªå—ä¸­çš„æ ‡è®°è½¬æ¢å›æ–‡æœ¬ã€‚#ï¼ˆFinally)
import
{
Document
}
from
"langchain/document"
;
import
{
TokenTextSplitter
}
from
"langchain/text_splitter"
;
const
text
=
"foo bar baz 123"
;
const
splitter
=
new
TokenTextSplitter
(
{
encodingName
:
"gpt2"
,
chunkSize
:
10
,
chunkOverlap
:
0
,
}
)
;
const
output
=
await
splitter
.
createDocuments
(
[
text
]
)
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\vector_stores\integrations\chroma.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/vector_stores/integrations/chroma
crawled_at: 2025-06-22T02:00:23.874988
---

chroma
æ¢è¡Œç¬¦
Chromaï¼ˆåµŒå…¥å¼çš„å¼€æºApache 2.0æ•°æ®åº“)
Chromaæ˜¯ä¸€ä¸ªå¼€æºçš„Apache 2.0åµŒå…¥å¼æ•°æ®åº“ã€‚
è®¾ç½®
â€‹
åœ¨è®¡ç®—æœºä¸Šä½¿ç”¨Dockerè¿è¡ŒChroma
æ–‡æ¡£
å®‰è£…Chroma JS SDKã€‚
npm
Yarn
pnpm
npm
install
-S chromadb
yarn
add
chromadb
pnpm
add
chromadb
ä½¿ç”¨ï¼Œç´¢å¼•å’ŒæŸ¥è¯¢æ–‡æ¡£
â€‹
import
{
Chroma
}
from
"langchain/vectorstores/chroma"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
TextLoader
}
from
"langchain/document_loaders/fs/text"
;
// Create docs with a loader
const
loader
=
new
TextLoader
(
"src/document_loaders/example_data/example.txt"
)
;
const
docs
=
await
loader
.
load
(
)
;
// Create vector store and index the docs
const
vectorStore
=
await
Chroma
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
,
{
collectionName
:
"a-test-collection"
,
}
)
;
// Search for the most similar document
const
response
=
await
vectorStore
.
similaritySearch
(
"hello"
,
1
)
;
console
.
log
(
response
)
;
/*
[
Document {
pageContent: 'Foo\nBar\nBaz\n\n',
metadata: { source: 'src/document_loaders/example_data/example.txt' }
}
]
*/
ä½¿ç”¨ï¼Œç´¢å¼•å’ŒæŸ¥è¯¢æ–‡æœ¬
â€‹
import
{
Chroma
}
from
"langchain/vectorstores/chroma"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
// text sample from Godel, Escher, Bach
const
vectorStore
=
await
Chroma
.
fromTexts
(
[
`
Tortoise: Labyrinth? Labyrinth? Could it Are we in the notorious Little
Harmonic Labyrinth of the dreaded Majotaur?
`
,
"Achilles: Yiikes! What is that?"
,
`
Tortoise: They say-although I person never believed it myself-that an I
Majotaur has created a tiny labyrinth sits in a pit in the middle of
it, waiting innocent victims to get lost in its fears complexity.
Then, when they wander and dazed into the center, he laughs and
laughs at them-so hard, that he laughs them to death!
`
,
"Achilles: Oh, no!"
,
"Tortoise: But it's only a myth. Courage, Achilles."
,
]
,
[
{
id
:
2
}
,
{
id
:
1
}
,
{
id
:
3
}
]
,
new
OpenAIEmbeddings
(
)
,
{
collectionName
:
"godel-escher-bach"
,
}
)
;
const
response
=
await
vectorStore
.
similaritySearch
(
"scared"
,
2
)
;
console
.
log
(
response
)
;
/*
[
Document { pageContent: 'Achilles: Oh, no!', metadata: {} },
Document {
pageContent: 'Achilles: Yiikes! What is that?',
metadata: { id: 1 }
}
]
*/
// You can also filter by metadata
const
filteredResponse
=
await
vectorStore
.
similaritySearch
(
"scared"
,
2
,
{
id
:
1
,
}
)
;
console
.
log
(
filteredResponse
)
;
/*
[
Document {
pageContent: 'Achilles: Yiikes! What is that?',
metadata: { id: 1 }
}
]
*/
ä½¿ç”¨ï¼Œä»ç°æœ‰é›†åˆæŸ¥è¯¢æ–‡æ¡£
â€‹
import
{
Chroma
}
from
"langchain/vectorstores/chroma"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
const
vectorStore
=
await
Chroma
.
fromExistingCollection
(
new
OpenAIEmbeddings
(
)
,
{
collectionName
:
"godel-escher-bach"
}
)
;
const
response
=
await
vectorStore
.
similaritySearch
(
"scared"
,
2
)
;
console
.
log
(
response
)
;
/*
[
Document { pageContent: 'Achilles: Oh, no!', metadata: {} },
Document {
pageContent: 'Achilles: Yiikes! What is that?',
metadata: { id: 1 }
}
]
*/



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\vector_stores\integrations\faiss.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/vector_stores/integrations/faiss
crawled_at: 2025-06-22T02:00:23.925446
---

Faiss
å…¼å®¹æ€§
ä»…é€‚ç”¨äºNode.jsç¯å¢ƒã€‚
Faiss
æ˜¯ç”¨äºç¨ å¯†å‘é‡çš„é«˜æ•ˆç›¸ä¼¼åº¦æœç´¢å’Œèšç±»çš„åº“ã€‚
Langchainjsæ”¯æŒä½¿ç”¨Faissä½œä¸ºå‘é‡åº“ï¼Œå¹¶å¯å°†å…¶ä¿å­˜åˆ°æ–‡ä»¶ã€‚åŒæ—¶ï¼Œå®ƒè¿˜æä¾›ä»
Pythonå®ç°
è¯»å–ä¿å­˜çš„æ–‡ä»¶çš„åŠŸèƒ½ã€‚
å®‰è£…
â€‹
å®‰è£…
faiss-node
,å®ƒæ˜¯
Faiss
çš„Node.jsç»‘å®šã€‚
npm
Yarn
pnpm
npm
install
-S faiss-node
yarn
add
faiss-node
pnpm
add
faiss-node
è¦å¯ç”¨ä»
Pythonå®ç°
è¯»å–ä¿å­˜çš„æ–‡ä»¶çš„åŠŸèƒ½ï¼Œè¿˜éœ€è¦å®‰è£…
pickleparser
ã€‚
npm
Yarn
pnpm
npm
install
-S pickleparser
yarn
add
pickleparser
pnpm
add
pickleparser
ä½¿ç”¨
â€‹
ä»æ–‡æœ¬åˆ›å»ºæ–°ç´¢å¼•
â€‹
import
{
FaissStore
}
from
"langchain/vectorstores/faiss"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
export
const
run
=
async
(
)
=>
{
const
vectorStore
=
await
FaissStore
.
fromTexts
(
[
"Hello world"
,
"Bye bye"
,
"hello nice world"
]
,
[
{
id
:
2
}
,
{
id
:
1
}
,
{
id
:
3
}
]
,
new
OpenAIEmbeddings
(
)
)
;
const
resultOne
=
await
vectorStore
.
similaritySearch
(
"hello world"
,
1
)
;
console
.
log
(
resultOne
)
;
}
;
ä»åŠ è½½å™¨åˆ›å»ºæ–°ç´¢å¼•
â€‹
import
{
FaissStore
}
from
"langchain/vectorstores/faiss"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
TextLoader
}
from
"langchain/document_loaders/fs/text"
;
// Create docs with a loader
const
loader
=
new
TextLoader
(
"src/document_loaders/example_data/example.txt"
)
;
const
docs
=
await
loader
.
load
(
)
;
// Load the docs into the vector store
const
vectorStore
=
await
FaissStore
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
)
;
// Search for the most similar document
const
resultOne
=
await
vectorStore
.
similaritySearch
(
"hello world"
,
1
)
;
console
.
log
(
resultOne
)
;
å°†ç´¢å¼•ä¿å­˜åˆ°æ–‡ä»¶å¹¶å†æ¬¡åŠ è½½
â€‹
import
{
FaissStore
}
from
"langchain/vectorstores/faiss"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
// Create a vector store through any method, here from texts as an example
const
vectorStore
=
await
FaissStore
.
fromTexts
(
[
"Hello world"
,
"Bye bye"
,
"hello nice world"
]
,
[
{
id
:
2
}
,
{
id
:
1
}
,
{
id
:
3
}
]
,
new
OpenAIEmbeddings
(
)
)
;
// Save the vector store to a directory
const
directory
=
"your/directory/here"
;
await
vectorStore
.
save
(
directory
)
;
// Load the vector store from the same directory
const
loadedVectorStore
=
await
FaissStore
.
load
(
directory
,
new
OpenAIEmbeddings
(
)
)
;
// vectorStore and loadedVectorStore are identical
const
result
=
await
loadedVectorStore
.
similaritySearch
(
"hello world"
,
1
)
;
console
.
log
(
result
)
;
ä»
Pythonå®ç°
ä¸­åŠ è½½ä¿å­˜çš„æ–‡ä»¶
â€‹
import
{
FaissStore
}
from
"langchain/vectorstores/faiss"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
// The directory of data saved from Python
const
directory
=
"your/directory/here"
;
// Load the vector store from the directory
const
loadedVectorStore
=
await
FaissStore
.
loadFromPython
(
directory
,
new
OpenAIEmbeddings
(
)
)
;
// Search for the most similar document
const
result
=
await
loadedVectorStore
.
similaritySearch
(
"test"
,
2
)
;
console
.
log
(
"result"
,
result
)
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\vector_stores\integrations\hnswlib.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/vector_stores/integrations/hnswlib
crawled_at: 2025-06-22T02:00:24.333772
---

HNSWLib
å…¼å®¹æ€§
ä»…é€‚ç”¨äºNode.jsã€‚
HNSWLibæ˜¯ä¸€ä¸ªå†…å­˜å‘é‡å­˜å‚¨å™¨ï¼Œå¯ä»¥ä¿å­˜åˆ°æ–‡ä»¶ä¸­ã€‚å®ƒä½¿ç”¨
HNSWLib
ã€‚
è®¾ç½®
â€‹
:::æ³¨æ„
åœ¨Windowsä¸Š
ï¼Œä½ å¯èƒ½éœ€è¦å…ˆå®‰è£…
Visual Studio
æ‰èƒ½æ­£ç¡®æ„å»º
hnswlib-node
åŒ…ã€‚
:::
æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è¿›è¡Œå®‰è£…
npm
Yarn
pnpm
npm
install
hnswlib-node
yarn
add
hnswlib-node
pnpm
add
hnswlib-node
ç”¨æ³•
â€‹
ä»æ–‡æœ¬åˆ›å»ºæ–°ç´¢å¼•
â€‹
import
{
HNSWLib
}
from
"langchain/vectorstores/hnswlib"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
const
vectorStore
=
await
HNSWLib
.
fromTexts
(
[
"Hello world"
,
"Bye bye"
,
"hello nice world"
]
,
[
{
id
:
2
}
,
{
id
:
1
}
,
{
id
:
3
}
]
,
new
OpenAIEmbeddings
(
)
)
;
const
resultOne
=
await
vectorStore
.
similaritySearch
(
"hello world"
,
1
)
;
console
.
log
(
resultOne
)
;
ä»åŠ è½½å™¨åˆ›å»ºæ–°ç´¢å¼•
â€‹
import
{
HNSWLib
}
from
"langchain/vectorstores/hnswlib"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
TextLoader
}
from
"langchain/document_loaders/fs/text"
;
// Create docs with a loader
const
loader
=
new
TextLoader
(
"src/document_loaders/example_data/example.txt"
)
;
const
docs
=
await
loader
.
load
(
)
;
// Load the docs into the vector store
const
vectorStore
=
await
HNSWLib
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
)
;
// Search for the most similar document
const
result
=
await
vectorStore
.
similaritySearch
(
"hello world"
,
1
)
;
console
.
log
(
result
)
;
å°†ç´¢å¼•ä¿å­˜åˆ°æ–‡ä»¶å¹¶é‡æ–°åŠ è½½
â€‹
import
{
HNSWLib
}
from
"langchain/vectorstores/hnswlib"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
// Create a vector store through any method, here from texts as an example
const
vectorStore
=
await
HNSWLib
.
fromTexts
(
[
"Hello world"
,
"Bye bye"
,
"hello nice world"
]
,
[
{
id
:
2
}
,
{
id
:
1
}
,
{
id
:
3
}
]
,
new
OpenAIEmbeddings
(
)
)
;
// Save the vector store to a directory
const
directory
=
"your/directory/here"
;
await
vectorStore
.
save
(
directory
)
;
// Load the vector store from the same directory
const
loadedVectorStore
=
await
HNSWLib
.
load
(
directory
,
new
OpenAIEmbeddings
(
)
)
;
// vectorStore and loadedVectorStore are identical
const
result
=
await
loadedVectorStore
.
similaritySearch
(
"hello world"
,
1
)
;
console
.
log
(
result
)
;
è¿‡æ»¤æ–‡æ¡£
â€‹
import
{
HNSWLib
}
from
"langchain/vectorstores/hnswlib"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
const
vectorStore
=
await
HNSWLib
.
fromTexts
(
[
"Hello world"
,
"Bye bye"
,
"hello nice world"
]
,
[
{
id
:
2
}
,
{
id
:
1
}
,
{
id
:
3
}
]
,
new
OpenAIEmbeddings
(
)
)
;
const
result
=
await
vectorStore
.
similaritySearch
(
"hello world"
,
10
,
(
document
)
=>
document
.
metadata
.
id
===
3
)
;
// only "hello nice world" will be returned
console
.
log
(
result
)
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\vector_stores\integrations\memory.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/vector_stores/integrations/memory
crawled_at: 2025-06-22T02:00:23.957393
---

MemoryVectorStore
MemoryVectorStoreæ˜¯ä¸€ä¸ªå†…å­˜ä¸­çš„æš‚å­˜å‘é‡å­˜å‚¨å™¨ï¼Œç”¨äºåœ¨å†…å­˜ä¸­å­˜å‚¨åµŒå…¥ï¼Œå¹¶åšç²¾ç¡®çš„çº¿æ€§æœç´¢ä»¥æ‰¾åˆ°æœ€ç›¸ä¼¼çš„åµŒå…¥ã€‚é»˜è®¤çš„ç›¸ä¼¼åº¦åº¦é‡æ˜¯ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œä½†å¯ä»¥æ›´æ”¹ä¸º
ml-distance
æ”¯æŒçš„ä»»ä½•ç›¸ä¼¼åº¦åº¦é‡æ–¹å¼ã€‚
ç”¨æ³•
â€‹
ä»æ–‡æœ¬åˆ›å»ºæ–°ç´¢å¼•
â€‹
import
{
MemoryVectorStore
}
from
"langchain/vectorstores/memory"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
export
const
run
=
async
(
)
=>
{
const
vectorStore
=
await
MemoryVectorStore
.
fromTexts
(
[
"Hello world"
,
"Bye bye"
,
"hello nice world"
]
,
[
{
id
:
2
}
,
{
id
:
1
}
,
{
id
:
3
}
]
,
new
OpenAIEmbeddings
(
)
)
;
const
resultOne
=
await
vectorStore
.
similaritySearch
(
"hello world"
,
1
)
;
console
.
log
(
resultOne
)
;
}
;
ä»åŠ è½½ç¨‹åºåˆ›å»ºæ–°ç´¢å¼•
â€‹
import
{
MemoryVectorStore
}
from
"langchain/vectorstores/memory"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
TextLoader
}
from
"langchain/document_loaders/fs/text"
;
export
const
run
=
async
(
)
=>
{
// Create docs with a loader
const
loader
=
new
TextLoader
(
"src/document_loaders/example_data/example.txt"
)
;
const
docs
=
await
loader
.
load
(
)
;
// Load the docs into the vector store
const
vectorStore
=
await
MemoryVectorStore
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
)
;
// Search for the most similar document
const
resultOne
=
await
vectorStore
.
similaritySearch
(
"hello world"
,
1
)
;
console
.
log
(
resultOne
)
;
}
;
ä½¿ç”¨è‡ªå®šä¹‰ç›¸ä¼¼åº¦åº¦é‡
â€‹
import
{
MemoryVectorStore
}
from
"langchain/vectorstores/memory"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
similarity
}
from
"ml-distance"
;
export
const
run
=
async
(
)
=>
{
const
vectorStore
=
await
MemoryVectorStore
.
fromTexts
(
[
"Hello world"
,
"Bye bye"
,
"hello nice world"
]
,
[
{
id
:
2
}
,
{
id
:
1
}
,
{
id
:
3
}
]
,
new
OpenAIEmbeddings
(
)
,
{
similarity
:
similarity
.
pearson
}
)
;
const
resultOne
=
await
vectorStore
.
similaritySearch
(
"hello world"
,
1
)
;
console
.
log
(
resultOne
)
;
}
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\vector_stores\integrations\milvus.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/vector_stores/integrations/milvus
crawled_at: 2025-06-22T02:00:24.081857
---

Milvus
Milvus
æ˜¯ä¸“ä¸ºåµŒå…¥å¼ç›¸ä¼¼æ€§æœç´¢å’Œ AI åº”ç”¨è€Œæ„å»ºçš„å‘é‡æ•°æ®åº“ã€‚
å…¼å®¹æ€§
ä»…å¯åœ¨ Node.js ä¸Šä½¿ç”¨ã€‚
å®‰è£…
â€‹
åœ¨è®¡ç®—æœºä¸Šä½¿ç”¨ Docker è¿è¡Œ Milvus å®ä¾‹
æ–‡æ¡£
å®‰è£… Milvus Node.js SDKã€‚
npm
Yarn
pnpm
npm
install
-S @zilliz/milvus2-sdk-node
yarn
add
@zilliz/milvus2-sdk-node
pnpm
add
@zilliz/milvus2-sdk-node
åœ¨è¿è¡Œä»£ç ä¹‹å‰è®¾ç½® Milvus çš„ç¯å¢ƒå˜é‡
3.1 OpenAI
export
OPENAI_API_KEY
=
YOUR_OPENAI_API_KEY_HERE
export
MILVUS_URL
=
YOUR_MILVUS_URL_HERE
# for example http://localhost:19530
3.2 Azure OpenAI
export
AZURE_OPENAI_API_KEY
=
YOUR_AZURE_OPENAI_API_KEY_HERE
export
AZURE_OPENAI_API_INSTANCE_NAME
=
YOUR_AZURE_OPENAI_INSTANCE_NAME_HERE
export
AZURE_OPENAI_API_DEPLOYMENT_NAME
=
YOUR_AZURE_OPENAI_DEPLOYMENT_NAME_HERE
export
AZURE_OPENAI_API_COMPLETIONS_DEPLOYMENT_NAME
=
YOUR_AZURE_OPENAI_COMPLETIONS_DEPLOYMENT_NAME_HERE
export
AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME
=
YOUR_AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT_NAME_HERE
export
AZURE_OPENAI_API_VERSION
=
YOUR_AZURE_OPENAI_API_VERSION_HERE
export
MILVUS_URL
=
YOUR_MILVUS_URL_HERE
# for example http://localhost:19530
ç´¢å¼•å’ŒæŸ¥è¯¢æ–‡æ¡£
â€‹
import
{
Milvus
}
from
"langchain/vectorstores/milvus"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
// text sample from Godel, Escher, Bach
const
vectorStore
=
await
Milvus
.
fromTexts
(
[
"Tortoise
:
Labyrinth
?
Labyrinth
?
Could it Are we
in
the notorious Little\
Harmonic Labyrinth
of
the dreaded Majotaur
?
"
,
"Achilles: Yiikes! What is that?"
,
"Tortoise
:
They say
-
although
I
person
never
believed it myself
-
that an
I
\
Majotaur has created a tiny labyrinth sits
in
a pit
in
the middle
of
\
it
,
waiting innocent victims to
get
lost
in
its fears complexity
.
\
Then
,
when they wander and dazed into the center
,
he laughs and\
laughs at them
-
so hard
,
that he laughs them to death
!
"
,
"Achilles: Oh, no!"
,
"Tortoise: But it's only a myth. Courage, Achilles."
,
]
,
[
{
id
:
2
}
,
{
id
:
1
}
,
{
id
:
3
}
,
{
id
:
4
}
,
{
id
:
5
}
]
,
new
OpenAIEmbeddings
(
)
,
{
collectionName
:
"goldel_escher_bach"
,
}
)
;
// or alternatively from docs
const
vectorStore
=
await
Milvus
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
,
{
collectionName
:
"goldel_escher_bach"
,
}
)
;
const
response
=
await
vectorStore
.
similaritySearch
(
"scared"
,
2
)
;
æŸ¥è¯¢ç°æœ‰é›†åˆçš„æ–‡æ¡£
â€‹
import
{
Milvus
}
from
"langchain/vectorstores/milvus"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
const
vectorStore
=
await
Milvus
.
fromExistingCollection
(
new
OpenAIEmbeddings
(
)
,
{
collectionName
:
"goldel_escher_bach"
,
}
)
;
const
response
=
await
vectorStore
.
similaritySearch
(
"scared"
,
2
)
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\vector_stores\integrations\myscale.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/vector_stores/integrations/myscale
crawled_at: 2025-06-22T02:00:24.270407
---

MyScale
å…¼å®¹æ€§
ä»…åœ¨Node.jsä¸Šå¯ç”¨ã€‚
MyScale
æ˜¯ä¸€æ¬¾æ–°å…´çš„äººå·¥æ™ºèƒ½æ•°æ®åº“ï¼Œå°†å‘é‡æœç´¢å’ŒSQLåˆ†æçš„å¼ºå¤§åŠŸèƒ½ç›¸ç»“åˆï¼Œæä¾›ç®¡ç†ã€é«˜æ•ˆã€å“åº”è¿…é€Ÿçš„ä½“éªŒã€‚
å®‰è£…
â€‹
é€šè¿‡
MyScale Webæ§åˆ¶å°
å¯åŠ¨é›†ç¾¤ã€‚æ›´å¤šä¿¡æ¯è¯·å‚è§
MyScaleå®˜æ–¹æ–‡æ¡£
ã€‚
å¯åŠ¨é›†ç¾¤åï¼Œè¯·ä»é›†ç¾¤çš„â€œæ“ä½œâ€èœå•ä¸­æŸ¥çœ‹æ‚¨çš„â€œè¿æ¥è¯¦ç»†ä¿¡æ¯â€ã€‚æ‚¨éœ€è¦ä¸»æœºåã€ç«¯å£ã€ç”¨æˆ·åå’Œå¯†ç ã€‚
åœ¨æ‚¨çš„å·¥ä½œåŒºä¸­å®‰è£…æ‰€éœ€çš„Node.jsç‰ˆæœ¬ã€‚
npm
Yarn
pnpm
npm
install
-S @clickhouse/client
yarn
add
@clickhouse/client
pnpm
add
@clickhouse/client
ç´¢å¼•ä¸æŸ¥è¯¢æ–‡æ¡£
â€‹
import
{
MyScaleStore
}
from
"langchain/vectorstores/myscale"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
const
vectorStore
=
await
MyScaleStore
.
fromTexts
(
[
"Hello world"
,
"Bye bye"
,
"hello nice world"
]
,
[
{
id
:
2
,
name
:
"2"
}
,
{
id
:
1
,
name
:
"1"
}
,
{
id
:
3
,
name
:
"3"
}
,
]
,
new
OpenAIEmbeddings
(
)
,
{
host
:
process
.
env
.
MYSCALE_HOST
||
"localhost"
,
port
:
process
.
env
.
MYSCALE_PORT
||
"8443"
,
username
:
process
.
env
.
MYSCALE_USERNAME
||
"username"
,
password
:
process
.
env
.
MYSCALE_PASSWORD
||
"password"
,
}
)
;
const
results
=
await
vectorStore
.
similaritySearch
(
"hello world"
,
1
)
;
console
.
log
(
results
)
;
const
filteredResults
=
await
vectorStore
.
similaritySearch
(
"hello world"
,
1
,
{
whereStr
:
"metadata.name = '1'"
,
}
)
;
console
.
log
(
filteredResults
)
;
ä»ç°æœ‰é›†åˆä¸­æŸ¥è¯¢æ–‡æ¡£
â€‹
import
{
MyScaleStore
}
from
"langchain/vectorstores/myscale"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
const
vectorStore
=
await
MyScaleStore
.
fromExistingIndex
(
new
OpenAIEmbeddings
(
)
,
{
host
:
process
.
env
.
MYSCALE_HOST
||
"localhost"
,
port
:
process
.
env
.
MYSCALE_PORT
||
"8443"
,
username
:
process
.
env
.
MYSCALE_USERNAME
||
"username"
,
password
:
process
.
env
.
MYSCALE_PASSWORD
||
"password"
,
database
:
"your_database"
,
// defaults to "default"
table
:
"your_table"
,
// defaults to "vector_table"
}
)
;
const
results
=
await
vectorStore
.
similaritySearch
(
"hello world"
,
1
)
;
console
.
log
(
results
)
;
const
filteredResults
=
await
vectorStore
.
similaritySearch
(
"hello world"
,
1
,
{
whereStr
:
"metadata.name = '1'"
,
}
)
;
console
.
log
(
filteredResults
)
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\vector_stores\integrations\opensearch.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/vector_stores/integrations/opensearch
crawled_at: 2025-06-22T02:00:24.302709
---

OpenSearch
å…¼å®¹æ€§
ä»…é™äº Node.jsã€‚
OpenSearch
æ˜¯å®Œå…¨å…¼å®¹ Elasticsearch API çš„ Elasticsearch åˆ†æ”¯ã€‚åœ¨æ­¤å¤„é˜…è¯»æœ‰å…³å…¶æ”¯æŒè¿‘ä¼¼æœ€è¿‘é‚»çš„æ›´å¤šä¿¡æ¯
here
ã€‚
Langchain.js å°†
@opensearch-project/opensearch
ä½œä¸º OpenSearch vectorstore çš„å®¢æˆ·ç«¯ã€‚
è®¾ç½®
â€‹
npm
Yarn
pnpm
npm
install
-S @opensearch-project/opensearch
yarn
add
@opensearch-project/opensearch
pnpm
add
@opensearch-project/opensearch
æ‚¨è¿˜éœ€è¦è¿è¡Œä¸€ä¸ª OpenSearch å®ä¾‹ã€‚æ‚¨å¯ä»¥ä½¿ç”¨
å®˜æ–¹ Docker æ˜ åƒ
æ¥å¼€å§‹ä½¿ç”¨ã€‚æ‚¨è¿˜å¯ä»¥åœ¨
æ­¤å¤„
æ‰¾åˆ°ç¤ºä¾‹ docker-compose æ–‡ä»¶ã€‚
ç´¢å¼•æ–‡æ¡£
â€‹
import
{
Client
}
from
"@opensearch-project/opensearch"
;
import
{
Document
}
from
"langchain/document"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
OpenSearchVectorStore
}
from
"langchain/vectorstores/opensearch"
;
const
client
=
new
Client
(
{
nodes
:
[
process
.
env
.
OPENSEARCH_URL
??
"http://127.0.0.1:9200"
]
,
}
)
;
const
docs
=
[
new
Document
(
{
metadata
:
{
foo
:
"bar"
}
,
pageContent
:
"opensearch is also a vector db"
,
}
)
,
new
Document
(
{
metadata
:
{
foo
:
"bar"
}
,
pageContent
:
"the quick brown fox jumped over the lazy dog"
,
}
)
,
new
Document
(
{
metadata
:
{
baz
:
"qux"
}
,
pageContent
:
"lorem ipsum dolor sit amet"
,
}
)
,
new
Document
(
{
metadata
:
{
baz
:
"qux"
}
,
pageContent
:
"OpenSearch is a scalable, flexible, and extensible open-source software suite for search, analytics, and observability applications"
,
}
)
,
]
;
await
OpenSearchVectorStore
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
,
{
client
,
indexName
:
process
.
env
.
OPENSEARCH_INDEX
,
// Will default to `documents`
}
)
;
æŸ¥è¯¢æ–‡æ¡£
â€‹
import
{
Client
}
from
"@opensearch-project/opensearch"
;
import
{
VectorDBQAChain
}
from
"langchain/chains"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
OpenSearchVectorStore
}
from
"langchain/vectorstores/opensearch"
;
const
client
=
new
Client
(
{
nodes
:
[
process
.
env
.
OPENSEARCH_URL
??
"http://127.0.0.1:9200"
]
,
}
)
;
const
vectorStore
=
new
OpenSearchVectorStore
(
new
OpenAIEmbeddings
(
)
,
{
client
,
}
)
;
/* Search the vector DB independently with meta filters */
const
results
=
await
vectorStore
.
similaritySearch
(
"hello world"
,
1
)
;
console
.
log
(
JSON
.
stringify
(
results
,
null
,
2
)
)
;
/* [
{
"pageContent": "Hello world",
"metadata": {
"id": 2
}
}
] */
/* Use as part of a chain (currently no metadata filters) */
const
model
=
new
OpenAI
(
)
;
const
chain
=
VectorDBQAChain
.
fromLLM
(
model
,
vectorStore
,
{
k
:
1
,
returnSourceDocuments
:
true
,
}
)
;
const
response
=
await
chain
.
call
(
{
query
:
"What is opensearch?"
}
)
;
console
.
log
(
JSON
.
stringify
(
response
,
null
,
2
)
)
;
/*
{
"text": " Opensearch is a collection of technologies that allow search engines to publish search results in a standard format, making it easier for users to search across multiple sites.",
"sourceDocuments": [
{
"pageContent": "What's this?",
"metadata": {
"id": 3
}
}
]
}
*/



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\vector_stores\integrations\pinecone.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/vector_stores/integrations/pinecone
crawled_at: 2025-06-22T02:00:24.469147
---

Pinecone
Compatibility
ä»…é€‚ç”¨äº Node.jsã€‚
Langchain.js å°†
@pinecone-database/pinecone
ä½œä¸º Pinecone å‘é‡å­˜å‚¨çš„å®¢æˆ·ç«¯ã€‚ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£…åº“ï¼š
npm
Yarn
pnpm
npm
install
-S dotenv langchain @pinecone-database/pinecone
yarn
add
dotenv langchain @pinecone-database/pinecone
pnpm
add
dotenv langchain @pinecone-database/pinecone
ç´¢å¼•æ–‡æ¡£
â€‹
import
{
PineconeClient
}
from
"@pinecone-database/pinecone"
;
import
*
as
dotenv
from
"dotenv"
;
import
{
Document
}
from
"langchain/document"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
PineconeStore
}
from
"langchain/vectorstores/pinecone"
;
dotenv
.
config
(
)
;
const
client
=
new
PineconeClient
(
)
;
await
client
.
init
(
{
apiKey
:
process
.
env
.
PINECONE_API_KEY
,
environment
:
process
.
env
.
PINECONE_ENVIRONMENT
,
}
)
;
const
pineconeIndex
=
client
.
Index
(
process
.
env
.
PINECONE_INDEX
)
;
const
docs
=
[
new
Document
(
{
metadata
:
{
foo
:
"bar"
}
,
pageContent
:
"pinecone is a vector db"
,
}
)
,
new
Document
(
{
metadata
:
{
foo
:
"bar"
}
,
pageContent
:
"the quick brown fox jumped over the lazy dog"
,
}
)
,
new
Document
(
{
metadata
:
{
baz
:
"qux"
}
,
pageContent
:
"lorem ipsum dolor sit amet"
,
}
)
,
new
Document
(
{
metadata
:
{
baz
:
"qux"
}
,
pageContent
:
"pinecones are the woody fruiting body and of a pine tree"
,
}
)
,
]
;
await
PineconeStore
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
,
{
pineconeIndex
,
}
)
;
æŸ¥è¯¢æ–‡æ¡£
â€‹
import
{
PineconeClient
}
from
"@pinecone-database/pinecone"
;
import
*
as
dotenv
from
"dotenv"
;
import
{
VectorDBQAChain
}
from
"langchain/chains"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
PineconeStore
}
from
"langchain/vectorstores/pinecone"
;
dotenv
.
config
(
)
;
const
client
=
new
PineconeClient
(
)
;
await
client
.
init
(
{
apiKey
:
process
.
env
.
PINECONE_API_KEY
,
environment
:
process
.
env
.
PINECONE_ENVIRONMENT
,
}
)
;
const
pineconeIndex
=
client
.
Index
(
process
.
env
.
PINECONE_INDEX
)
;
const
vectorStore
=
await
PineconeStore
.
fromExistingIndex
(
new
OpenAIEmbeddings
(
)
,
{
pineconeIndex
}
)
;
/* Search the vector DB independently with meta filters */
const
results
=
await
vectorStore
.
similaritySearch
(
"pinecone"
,
1
,
{
foo
:
"bar"
,
}
)
;
console
.
log
(
results
)
;
/*
[
Document {
pageContent: 'pinecone is a vector db',
metadata: { foo: 'bar' }
}
]
*/
/* Use as part of a chain (currently no metadata filters) */
const
model
=
new
OpenAI
(
)
;
const
chain
=
VectorDBQAChain
.
fromLLM
(
model
,
vectorStore
,
{
k
:
1
,
returnSourceDocuments
:
true
,
}
)
;
const
response
=
await
chain
.
call
(
{
query
:
"What is pinecone?"
}
)
;
console
.
log
(
response
)
;
/*
{
text: ' A pinecone is the woody fruiting body of a pine tree.',
sourceDocuments: [
Document {
pageContent: 'pinecones are the woody fruiting body and of a pine tree',
metadata: [Object]
}
]
}
*/



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\vector_stores\integrations\prisma.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/vector_stores/integrations/prisma
crawled_at: 2025-06-22T02:00:24.486026
---

prisma
sidebar_label: ä»…é€‚ç”¨äº node
â€‹
Prisma
Langchain æ”¯æŒä½¿ç”¨
Prisma
ä¸ PostgreSQL å’Œ
pgvector
Postgres æ‰©å±•æ¥å¢å¼º PostgreSQL æ•°æ®åº“ä¸­çš„ç°æœ‰æ¨¡å‹çš„å‘é‡æœç´¢ã€‚
è®¾ç½®
â€‹
ä½¿ç”¨ Supabase è®¾ç½®æ•°æ®åº“å®ä¾‹
â€‹
è¯·å‚é˜…
Prisma å’Œ Supabase é›†æˆæŒ‡å—
æ¥è®¾ç½® Supabase å’Œ Prisma çš„æ–°æ•°æ®åº“å®ä¾‹ã€‚
å®‰è£… Prisma
â€‹
npm
Yarn
pnpm
npm
install
prisma
yarn
add
prisma
pnpm
add
prisma
ä½¿ç”¨
docker-compose
è®¾ç½®
pgvector
è‡ªæ‰˜ç®¡å®ä¾‹
â€‹
pgvector
æä¾›äº†ä¸€ä¸ªé¢„æ„å»ºçš„ Docker æ˜ åƒï¼Œå¯ç”¨äºå¿«é€Ÿè®¾ç½®è‡ªæ‰˜ç®¡çš„ Postgres å®ä¾‹ã€‚
services
:
db
:
image
:
ankane/pgvector
ports
:
-
5432
:
5432
volumes
:
-
db
:
/var/lib/postgresql/data
environment
:
-
POSTGRES_PASSWORD=
-
POSTGRES_USER=
-
POSTGRES_DB=
volumes
:
db
:
åˆ›å»ºæ–°æ¨¡å‹
â€‹
Create a new schema
â€‹
å‡è®¾æ‚¨è¿˜æ²¡æœ‰åˆ›å»ºä¸€ä¸ªæ¨¡å‹ï¼Œä½¿ç”¨ç±»å‹ä¸º
Unsupported("vector")
çš„
vector
å­—æ®µåˆ›å»ºä¸€ä¸ªæ–°æ¨¡å‹:
model Document {
id      String                 @id @default(cuid())
content String
vector  Unsupported("vector")?
}
ç„¶åï¼Œä½¿ç”¨
--create-only
åˆ›å»ºæ–°çš„è¿ç§»ï¼Œä»¥é¿å…ç›´æ¥è¿è¡Œè¿ç§»ã€‚
Afterwards, create a new migration with
--create-only
to avoid running the migration directly.
npm
Yarn
pnpm
```bash npm2yarn
npx prisma migrate dev --create-only
```bash
npm
undefined
# couldn't auto-convert command2yarn
npx prisma migrate dev --create-only
```bash
npm
undefined
# couldn't auto-convert command2yarn
npx prisma migrate dev --create-only
æ·»åŠ ä»¥ä¸‹è¡Œåˆ°æ–°åˆ›å»ºçš„è¿ç§»ï¼Œä»¥å¯ç”¨
pgvector
æ‰©å±•ï¼Œå¦‚æœå®ƒå°šæœªè¢«å¯ç”¨:
Add the following line to the newly created migration to enable
pgvector
extension if it hasn't been enabled yet:
CREATE
EXTENSION
IF
NOT
EXISTS
vector
;
ç„¶åè¿è¡Œè¿ç§»:
Run the migration afterwards:
npm
Yarn
pnpm
```bash npm2yarn
npx prisma migrate dev
```bash
npm
undefined
# couldn't auto-convert command2yarn
npx prisma migrate dev
```bash
npm
undefined
# couldn't auto-convert command2yarn
npx prisma migrate dev
ä½¿ç”¨
â€‹
:::è­¦å‘Š
è¡¨åå’Œåˆ—åï¼ˆä¾‹å¦‚
tableName
ã€
vectorColumnName
ã€
columns
å’Œ
filter
ä¸­çš„å­—æ®µ)ç›´æ¥ä¼ é€’åˆ° SQL æŸ¥è¯¢ä¸­ï¼Œæ²¡æœ‰å‚æ•°åŒ–ã€‚è¿™äº›å­—æ®µå¿…é¡»åœ¨ä½¿ç”¨å‰è¿›è¡Œå‡€åŒ–ä»¥é¿å… SQL æ³¨å…¥ã€‚
These fields must be sanitized beforehand to avoid SQL injection.
:::
import
{
PrismaVectorStore
}
from
"langchain/vectorstores/prisma"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
PrismaClient
,
Prisma
,
Document
}
from
"@prisma/client"
;
export
const
run
=
async
(
)
=>
{
const
db
=
new
PrismaClient
(
)
;
// Use the `withModel` method to get proper type hints for `metadata` field:
const
vectorStore
=
PrismaVectorStore
.
withModel
<
Document
>
(
db
)
.
create
(
new
OpenAIEmbeddings
(
)
,
{
prisma
:
Prisma
,
tableName
:
"Document"
,
vectorColumnName
:
"vector"
,
columns
:
{
id
:
PrismaVectorStore
.
IdColumn
,
content
:
PrismaVectorStore
.
ContentColumn
,
}
,
}
)
;
const
texts
=
[
"Hello world"
,
"Bye bye"
,
"What's this?"
]
;
await
vectorStore
.
addModels
(
await
db
.
$transaction
(
texts
.
map
(
(
content
)
=>
db
.
document
.
create
(
{
data
:
{
content
}
}
)
)
)
)
;
const
resultOne
=
await
vectorStore
.
similaritySearch
(
"Hello world"
,
1
)
;
console
.
log
(
resultOne
)
;
// create an instance with default filter
const
vectorStore2
=
PrismaVectorStore
.
withModel
<
Document
>
(
db
)
.
create
(
new
OpenAIEmbeddings
(
)
,
{
prisma
:
Prisma
,
tableName
:
"Document"
,
vectorColumnName
:
"vector"
,
columns
:
{
id
:
PrismaVectorStore
.
IdColumn
,
content
:
PrismaVectorStore
.
ContentColumn
,
}
,
filter
:
{
content
:
{
equals
:
"default"
,
}
,
}
,
}
)
;
await
vectorStore2
.
addModels
(
await
db
.
$transaction
(
texts
.
map
(
(
content
)
=>
db
.
document
.
create
(
{
data
:
{
content
}
}
)
)
)
)
;
// Use the default filter a.k.a {"content": "default"}
const
resultTwo
=
await
vectorStore
.
similaritySearch
(
"Hello world"
,
1
)
;
console
.
log
(
resultTwo
)
;
// Override the local filter
const
resultThree
=
await
vectorStore
.
similaritySearchWithScore
(
"Hello world"
,
1
,
{
content
:
{
equals
:
"different_content"
}
}
)
;
console
.
log
(
resultThree
)
;
}
;
ä¸Šè¿°ç¤ºä¾‹ä½¿ç”¨ä»¥ä¸‹æ¨¡å¼:ï¼š
// This is your Prisma schema file,
// learn more about it in the docs: https://pris.ly/d/prisma-schema
generator client {
provider = "prisma-client-js"
}
datasource db {
provider = "postgresql"
url      = env("DATABASE_URL")
}
model Document {
id        String                 @id @default(cuid())
content   String
namespace String?                @default("default")
vector    Unsupported("vector")?
}
å¦‚æœä¸éœ€è¦ï¼Œä½ å¯ä»¥åˆ é™¤
namespace
ã€‚



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\vector_stores\integrations\qdrant.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/vector_stores/integrations/qdrant
crawled_at: 2025-06-22T02:00:24.408467
---

Qdrant
Qdrant
æ˜¯ä¸€ä¸ªå‘é‡ç›¸ä¼¼åº¦æœç´¢å¼•æ“ã€‚å®ƒæä¾›äº†ä¸€ä¸ªæ–¹ä¾¿çš„APIæ¥å­˜å‚¨ã€æœç´¢å’Œç®¡ç†å¸¦æœ‰é™„åŠ æœ‰æ•ˆè´Ÿè½½çš„ç‚¹ - å‘é‡ã€‚
å…¼å®¹æ€§
ä»…é€‚ç”¨äºNode.jsã€‚
é…ç½®
â€‹
æ ¹æ®
Qdrant è®¾ç½®è¯´æ˜
åœ¨æ‚¨çš„è®¡ç®—æœºä¸Šä½¿ç”¨Dockerè¿è¡ŒQdrantå®ä¾‹ã€‚
å®‰è£…Qdrant Node.js SDKã€‚
npm
Yarn
pnpm
npm
install
-S @qdrant/js-client-rest
yarn
add
@qdrant/js-client-rest
pnpm
add
@qdrant/js-client-rest
è¿è¡Œä»£ç å‰ä¸ºQdrantè®¾ç½®Envå˜é‡ã€‚
3.1 OpenAI
export
OPENAI_API_KEY
=
YOUR_OPENAI_API_KEY_HERE
export
QDRANT_URL
=
YOUR_QDRANT_URL_HERE
# for example http://localhost:6333
3.2 Azure OpenAI
export
AZURE_OPENAI_API_KEY
=
YOUR_AZURE_OPENAI_API_KEY_HERE
export
AZURE_OPENAI_API_INSTANCE_NAME
=
YOUR_AZURE_OPENAI_INSTANCE_NAME_HERE
export
AZURE_OPENAI_API_DEPLOYMENT_NAME
=
YOUR_AZURE_OPENAI_DEPLOYMENT_NAME_HERE
export
AZURE_OPENAI_API_COMPLETIONS_DEPLOYMENT_NAME
=
YOUR_AZURE_OPENAI_COMPLETIONS_DEPLOYMENT_NAME_HERE
export
AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME
=
YOUR_AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT_NAME_HERE
export
AZURE_OPENAI_API_VERSION
=
YOUR_AZURE_OPENAI_API_VERSION_HERE
export
QDRANT_URL
=
YOUR_QDRANT_URL_HERE
# for example http://localhost:6333
ç”¨æ³•
â€‹
ä»æ–‡æœ¬ä¸­åˆ›å»ºæ–°çš„ç´¢å¼•
â€‹
import
{
QdrantVectorStore
}
from
"langchain/vectorstores/qdrant"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
// text sample from Godel, Escher, Bach
const
vectorStore
=
await
QdrantVectorStore
.
fromTexts
(
[
`
Tortoise: Labyrinth? Labyrinth? Could it Are we in the notorious Little
Harmonic Labyrinth of the dreaded Majotaur?
`
,
`
Achilles: Yiikes! What is that?
`
,
`
Tortoise: They say-although I person never believed it myself-that an I
Majotaur has created a tiny labyrinth sits in a pit in the middle of
it, waiting innocent victims to get lost in its fears complexity.
Then, when they wander and dazed into the center, he laughs and
laughs at them-so hard, that he laughs them to death!
`
,
`
Achilles: Oh, no!
`
,
`
Tortoise: But it's only a myth. Courage, Achilles.
`
,
]
,
[
{
id
:
2
}
,
{
id
:
1
}
,
{
id
:
3
}
,
{
id
:
4
}
,
{
id
:
5
}
]
,
new
OpenAIEmbeddings
(
)
,
{
url
:
process
.
env
.
QDRANT_URL
,
collectionName
:
"goldel_escher_bach"
,
}
)
;
const
response
=
await
vectorStore
.
similaritySearch
(
"scared"
,
2
)
;
console
.
log
(
response
)
;
/*
[
Document { pageContent: 'Achilles: Oh, no!', metadata: {} },
Document {
pageContent: 'Achilles: Yiikes! What is that?',
metadata: { id: 1 }
}
]
*/
ä»æ–‡æ¡£ä¸­åˆ›å»ºæ–°çš„ç´¢å¼•
â€‹
import
{
QdrantVectorStore
}
from
"langchain/vectorstores/qdrant"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
TextLoader
}
from
"langchain/document_loaders/fs/text"
;
// Create docs with a loader
const
loader
=
new
TextLoader
(
"src/document_loaders/example_data/example.txt"
)
;
const
docs
=
await
loader
.
load
(
)
;
const
vectorStore
=
await
QdrantVectorStore
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
,
{
url
:
process
.
env
.
QDRANT_URL
,
collectionName
:
"a_test_collection"
,
}
)
;
// Search for the most similar document
const
response
=
await
vectorStore
.
similaritySearch
(
"hello"
,
1
)
;
console
.
log
(
response
)
;
/*
[
Document {
pageContent: 'Foo\nBar\nBaz\n\n',
metadata: { source: 'src/document_loaders/example_data/example.txt' }
}
]
*/
ä»ç°æœ‰é›†åˆæŸ¥è¯¢æ–‡æ¡£
â€‹
import
{
QdrantVectorStore
}
from
"langchain/vectorstores/qdrant"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
const
vectorStore
=
await
QdrantVectorStore
.
fromExistingCollection
(
new
OpenAIEmbeddings
(
)
,
{
url
:
process
.
env
.
QDRANT_URL
,
collectionName
:
"goldel_escher_bach"
,
}
)
;
const
response
=
await
vectorStore
.
similaritySearch
(
"scared"
,
2
)
;
console
.
log
(
response
)
;
/*
[
Document { pageContent: 'Achilles: Oh, no!', metadata: {} },
Document {
pageContent: 'Achilles: Yiikes! What is that?',
metadata: { id: 1 }
}
]
*/



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\vector_stores\integrations\redis.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/vector_stores/integrations/redis
crawled_at: 2025-06-22T02:00:24.523618
---

Redis
Redis
æ˜¯ä¸€æ¬¾å¿«é€Ÿçš„å¼€æºå†…å­˜æ•°æ®å­˜å‚¨ç³»ç»Ÿã€‚ï¼Œä½œä¸º
Redis Stack
çš„ä¸€éƒ¨åˆ†ï¼Œ
RediSearch
æ˜¯ä¸€ç§æ”¯æŒå‘é‡ç›¸ä¼¼æ€§è¯­ä¹‰æœç´¢ä»¥åŠå…¶ä»–è®¸å¤šç±»å‹æœç´¢çš„æ¨¡å—ã€‚
å…¼å®¹æ€§
åªæ”¯æŒåœ¨Node.jsä¸Šä½¿ç”¨ã€‚
LangChain.jsæ¥å—
node-redis
ä½œä¸ºRedisçŸ¢é‡å­˜å‚¨çš„å®¢æˆ·ç«¯ã€‚
è®¾ç½®
â€‹
æ ¹æ®
æ–‡æ¡£
åœ¨è®¡ç®—æœºä¸Šä½¿ç”¨Dockerè¿è¡ŒRedisã€‚
å®‰è£…node-redis JSå®¢æˆ·ç«¯
npm
Yarn
pnpm
npm
install
-S redis
yarn
add
redis
pnpm
add
redis
ç´¢å¼•æ–‡æ¡£
â€‹
import
{
createClient
,
createCluster
}
from
"redis"
;
import
{
Document
}
from
"langchain/document"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
RedisVectorStore
}
from
"langchain/vectorstores/redis"
;
const
client
=
createClient
(
{
url
:
process
.
env
.
REDIS_URL
??
"redis://localhost:6379"
,
}
)
;
await
client
.
connect
(
)
;
const
docs
=
[
new
Document
(
{
metadata
:
{
foo
:
"bar"
}
,
pageContent
:
"redis is fast"
,
}
)
,
new
Document
(
{
metadata
:
{
foo
:
"bar"
}
,
pageContent
:
"the quick brown fox jumped over the lazy dog"
,
}
)
,
new
Document
(
{
metadata
:
{
baz
:
"qux"
}
,
pageContent
:
"lorem ipsum dolor sit amet"
,
}
)
,
new
Document
(
{
metadata
:
{
baz
:
"qux"
}
,
pageContent
:
"consectetur adipiscing elit"
,
}
)
,
]
;
const
vectorStore
=
await
RedisVectorStore
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
,
{
redisClient
:
client
,
indexName
:
"docs"
,
}
)
;
await
client
.
disconnect
(
)
;
æŸ¥è¯¢æ–‡æ¡£
â€‹
import
{
createClient
}
from
"redis"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
RetrievalQAChain
}
from
"langchain/chains"
;
import
{
RedisVectorStore
}
from
"langchain/vectorstores/redis"
;
const
client
=
createClient
(
{
url
:
process
.
env
.
REDIS_URL
??
"redis://localhost:6379"
,
}
)
;
await
client
.
connect
(
)
;
const
vectorStore
=
new
RedisVectorStore
(
new
OpenAIEmbeddings
(
)
,
{
redisClient
:
client
,
indexName
:
"docs"
,
}
)
;
/* Simple standalone search in the vector DB */
const
simpleRes
=
await
vectorStore
.
similaritySearch
(
"redis"
,
1
)
;
console
.
log
(
simpleRes
)
;
/*
[
Document {
pageContent: "redis is fast",
metadata: { foo: "bar" }
}
]
*/
/* Search in the vector DB using filters */
const
filterRes
=
await
vectorStore
.
similaritySearch
(
"redis"
,
3
,
[
"qux"
]
)
;
console
.
log
(
filterRes
)
;
/*
[
Document {
pageContent: "consectetur adipiscing elit",
metadata: { baz: "qux" },
},
Document {
pageContent: "lorem ipsum dolor sit amet",
metadata: { baz: "qux" },
}
]
*/
/* Usage as part of a chain */
const
model
=
new
OpenAI
(
)
;
const
chain
=
RetrievalQAChain
.
fromLLM
(
model
,
vectorStore
.
asRetriever
(
1
)
,
{
returnSourceDocuments
:
true
,
}
)
;
const
chainRes
=
await
chain
.
call
(
{
query
:
"What did the fox do?"
}
)
;
console
.
log
(
chainRes
)
;
/*
{
text: " The fox jumped over the lazy dog.",
sourceDocuments: [
Document {
pageContent: "the quick brown fox jumped over the lazy dog",
metadata: [Object]
}
]
}
*/
await
client
.
disconnect
(
)
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\vector_stores\integrations\singlestore.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/vector_stores/integrations/singlestore
crawled_at: 2025-06-22T02:00:24.473027
---

SingleStore
SingleStoreDB
æ˜¯ä¸€æ¬¾é«˜æ€§èƒ½ï¼Œåˆ†å¸ƒå¼æ•°æ®åº“ç³»ç»Ÿã€‚é•¿æœŸä»¥æ¥ï¼Œå®ƒä¸€ç›´æ”¯æŒ
dot_product
ç­‰å‘é‡å‡½æ•°ï¼Œä»è€Œæˆä¸ºéœ€è¦æ–‡æœ¬ç›¸ä¼¼åº¦åŒ¹é…çš„AIåº”ç”¨ç¨‹åºçš„æœ€ä½³è§£å†³æ–¹æ¡ˆã€‚
å…¼å®¹æ€§
ä»…Node.jså¯ç”¨ã€‚
LangChain.jsæ¥å—
mysql2/promise Pool
ä½œä¸ºSingleStoreå‘é‡å­˜å‚¨çš„è¿æ¥æ± ã€‚
è®¾ç½®
â€‹
å»ºç«‹SingleStoreDBç¯å¢ƒã€‚ ä½ å¯ä»¥é€‰æ‹©äº‘ç‰ˆæˆ–è‡ªè¡Œéƒ¨ç½²ç‰ˆã€‚ï¼Œ
äº‘ç‰ˆ
ï¼Œ
è‡ªè¡Œéƒ¨ç½²ç‰ˆ
å®‰è£…mysql2 JSå®¢æˆ·ç«¯
npm
Yarn
pnpm
npm
install
-S mysql2
yarn
add
mysql2
pnpm
add
mysql2
ç”¨æ³•
â€‹
import
{
SingleStoreVectorStore
}
from
"langchain/vectorstores/singlestore"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
createPool
}
from
"mysql2/promise"
;
export
const
run
=
async
(
)
=>
{
const
pool
=
createPool
(
{
host
:
process
.
env
.
SINGLESTORE_HOST
,
port
:
Number
(
process
.
env
.
SINGLESTORE_PORT
)
,
user
:
process
.
env
.
SINGLESTORE_USERNAME
,
password
:
process
.
env
.
SINGLESTORE_PASSWORD
,
database
:
process
.
env
.
SINGLESTORE_DATABASE
,
}
)
;
const
vectorStore
=
await
SingleStoreVectorStore
.
fromTexts
(
[
"Hello world"
,
"Bye bye"
,
"hello nice world"
]
,
[
{
id
:
2
}
,
{
id
:
1
}
,
{
id
:
3
}
]
,
new
OpenAIEmbeddings
(
)
,
{
connectionPool
:
pool
,
}
)
;
const
resultOne
=
await
vectorStore
.
similaritySearch
(
"hello world"
,
1
)
;
console
.
log
(
resultOne
)
;
await
pool
.
end
(
)
;
}
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\vector_stores\integrations\supabase.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/vector_stores/integrations/supabase
crawled_at: 2025-06-22T02:00:24.596007
---

Supabase
Langchainæ”¯æŒä½¿ç”¨Supabase Postgresæ•°æ®åº“ä½œä¸ºå‘é‡å­˜å‚¨ä½¿ç”¨'pgvector' postgresæ‰©å±•ã€‚ æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…
Supabaseåšå®¢æ–‡ç« 
ã€‚
è®¾ç½®
â€‹
å®‰è£…åº“
â€‹
npm
Yarn
pnpm
npm
install
-S @supabase/supabase-js
yarn
add
@supabase/supabase-js
pnpm
add
@supabase/supabase-js
åœ¨æ‚¨çš„æ•°æ®åº“ä¸­åˆ›å»ºè¡¨å’Œæœç´¢åŠŸèƒ½
â€‹
åœ¨æ‚¨çš„æ•°æ®åº“ä¸­è¿è¡Œæ­¤å‘½ä»¤:
-- Enable the pgvector extension to work with embedding vectors
create
extension vector
;
-- Create a table to store your documents
create
table
documents
(
id bigserial
primary
key
,
content
text
,
-- corresponds to Document.pageContent
metadata jsonb
,
-- corresponds to Document.metadata
embedding vector
(
1536
)
-- 1536 works for OpenAI embeddings, change if needed
)
;
-- Create a function to search for documents
create
function
match_documents
(
query_embedding vector
(
1536
)
,
match_count
int
DEFAULT
null
,
filter jsonb
DEFAULT
'{}'
)
returns
table
(
id
bigint
,
content
text
,
metadata jsonb
,
similarity
float
)
language
plpgsql
as
$$
#variable_conflict use_column
begin
return
query
select
id
,
content
,
metadata
,
1
-
(
documents
.
embedding
<=>
query_embedding
)
as
similarity
from
documents
where
metadata @
>
filter
order
by
documents
.
embedding
<=>
query_embedding
limit
match_count
;
end
;
$$
;
ä½¿ç”¨
â€‹
æ ‡å‡†ç”¨æ³•
â€‹
ä¸‹é¢çš„ç¤ºä¾‹æ˜¾ç¤ºå¦‚ä½•ä½¿ç”¨Supabaseæ‰§è¡ŒåŸºæœ¬ç›¸ä¼¼æ€§æœç´¢:
import
{
SupabaseVectorStore
}
from
"langchain/vectorstores/supabase"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
createClient
}
from
"@supabase/supabase-js"
;
// First, follow set-up instructions at
// https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/supabase
const
privateKey
=
process
.
env
.
SUPABASE_PRIVATE_KEY
;
if
(
!
privateKey
)
throw
new
Error
(
`
Expected env var SUPABASE_PRIVATE_KEY
`
)
;
const
url
=
process
.
env
.
SUPABASE_URL
;
if
(
!
url
)
throw
new
Error
(
`
Expected env var SUPABASE_URL
`
)
;
export
const
run
=
async
(
)
=>
{
const
client
=
createClient
(
url
,
privateKey
)
;
const
vectorStore
=
await
SupabaseVectorStore
.
fromTexts
(
[
"Hello world"
,
"Bye bye"
,
"What's this?"
]
,
[
{
id
:
2
}
,
{
id
:
1
}
,
{
id
:
3
}
]
,
new
OpenAIEmbeddings
(
)
,
{
client
,
tableName
:
"documents"
,
queryName
:
"match_documents"
,
}
)
;
const
resultOne
=
await
vectorStore
.
similaritySearch
(
"Hello world"
,
1
)
;
console
.
log
(
resultOne
)
;
}
;
å…ƒæ•°æ®è¿‡æ»¤
â€‹
å¯¹äºä¸Šè¿°çš„'match_documents' Postgreså‡½æ•°ï¼Œæ‚¨è¿˜å¯ä»¥ä¼ é€’ä¸€ä¸ªè¿‡æ»¤å‚æ•°ï¼Œä»¥ä»…ä½¿ç”¨ç‰¹å®šå…ƒæ•°æ®å­—æ®µå€¼çš„æ–‡æ¡£ã€‚è¯¥è¿‡æ»¤å™¨å‚æ•°æ˜¯ä¸€ä¸ªJSONå¯¹è±¡ï¼Œ'match_documents'å‡½æ•°å°†ä½¿ç”¨Postgres JSONBåŒ…å«æ“ä½œç¬¦'@>'æ ¹æ®æ‚¨æŒ‡å®šçš„å…ƒæ•°æ®å­—æ®µå€¼è¿‡æ»¤æ–‡æ¡£ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§
Postgres JSONBåŒ…å«æ“ä½œç¬¦
ã€‚
æ³¨æ„:
å¦‚æœæ‚¨ä¹‹å‰ä½¿ç”¨è¿‡`SupabaseVectorStore'ï¼Œæ‚¨å¯èƒ½éœ€è¦æ ¹æ®ä¸Šè¿°æ›´æ–°çš„SQLæ”¾å¼ƒå’Œé‡æ–°åˆ›å»º'match_documents'å‡½æ•°ä»¥ä½¿ç”¨æ­¤åŠŸèƒ½ã€‚
import
{
SupabaseVectorStore
}
from
"langchain/vectorstores/supabase"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
createClient
}
from
"@supabase/supabase-js"
;
// First, follow set-up instructions at
// https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/supabase
const
privateKey
=
process
.
env
.
SUPABASE_PRIVATE_KEY
;
if
(
!
privateKey
)
throw
new
Error
(
`
Expected env var SUPABASE_PRIVATE_KEY
`
)
;
const
url
=
process
.
env
.
SUPABASE_URL
;
if
(
!
url
)
throw
new
Error
(
`
Expected env var SUPABASE_URL
`
)
;
export
const
run
=
async
(
)
=>
{
const
client
=
createClient
(
url
,
privateKey
)
;
const
vectorStore
=
await
SupabaseVectorStore
.
fromTexts
(
[
"Hello world"
,
"Hello world"
,
"Hello world"
]
,
[
{
user_id
:
2
}
,
{
user_id
:
1
}
,
{
user_id
:
3
}
]
,
new
OpenAIEmbeddings
(
)
,
{
client
,
tableName
:
"documents"
,
queryName
:
"match_documents"
,
}
)
;
const
result
=
await
vectorStore
.
similaritySearch
(
"Hello world"
,
1
,
{
user_id
:
3
,
}
)
;
console
.
log
(
result
)
;
}
;
å…ƒæ•°æ®æŸ¥è¯¢æ„å»ºå™¨è¿‡æ»¤
â€‹
You can also use query builder-style filtering similar to how
the Supabase JavaScript library works
instead of passing an object. Note that since most of the filter properties are in the metadata column, you need to use arrow operators (
->
for integer or
->>
for text) as defined in
Postgrest API documentation
and specify the data type of the property (e.g. the column should look something like
metadata->some_int_value::int
).
import
{
SupabaseFilterRPCCall
,
SupabaseVectorStore
,
}
from
"langchain/vectorstores/supabase"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
createClient
}
from
"@supabase/supabase-js"
;
// First, follow set-up instructions at
// https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/supabase
const
privateKey
=
process
.
env
.
SUPABASE_PRIVATE_KEY
;
if
(
!
privateKey
)
throw
new
Error
(
`
Expected env var SUPABASE_PRIVATE_KEY
`
)
;
const
url
=
process
.
env
.
SUPABASE_URL
;
if
(
!
url
)
throw
new
Error
(
`
Expected env var SUPABASE_URL
`
)
;
export
const
run
=
async
(
)
=>
{
const
client
=
createClient
(
url
,
privateKey
)
;
const
embeddings
=
new
OpenAIEmbeddings
(
)
;
const
store
=
new
SupabaseVectorStore
(
embeddings
,
{
client
,
tableName
:
"documents"
,
}
)
;
const
docs
=
[
{
pageContent
:
"This is a long text, but it actually means something because vector database does not understand Lorem Ipsum. So I would need to expand upon the notion of quantum fluff, a theorectical concept where subatomic particles coalesce to form transient multidimensional spaces. Yet, this abstraction holds no real-world application or comprehensible meaning, reflecting a cosmic puzzle."
,
metadata
:
{
b
:
1
,
c
:
10
,
stuff
:
"right"
}
,
}
,
{
pageContent
:
"This is a long text, but it actually means something because vector database does not understand Lorem Ipsum. So I would need to proceed by discussing the echo of virtual tweets in the binary corridors of the digital universe. Each tweet, like a pixelated canary, hums in an unseen frequency, a fascinatingly perplexing phenomenon that, while conjuring vivid imagery, lacks any concrete implication or real-world relevance, portraying a paradox of multidimensional spaces in the age of cyber folklore."
,
metadata
:
{
b
:
2
,
c
:
9
,
stuff
:
"right"
}
,
}
,
{
pageContent
:
"hello"
,
metadata
:
{
b
:
1
,
c
:
9
,
stuff
:
"right"
}
}
,
{
pageContent
:
"hello"
,
metadata
:
{
b
:
1
,
c
:
9
,
stuff
:
"wrong"
}
}
,
{
pageContent
:
"hi"
,
metadata
:
{
b
:
2
,
c
:
8
,
stuff
:
"right"
}
}
,
{
pageContent
:
"bye"
,
metadata
:
{
b
:
3
,
c
:
7
,
stuff
:
"right"
}
}
,
{
pageContent
:
"what's this"
,
metadata
:
{
b
:
4
,
c
:
6
,
stuff
:
"right"
}
}
,
]
;
await
store
.
addDocuments
(
docs
)
;
const
funcFilterA
:
SupabaseFilterRPCCall
=
(
rpc
)
=>
rpc
.
filter
(
"metadata->b::int"
,
"lt"
,
3
)
.
filter
(
"metadata->c::int"
,
"gt"
,
7
)
.
textSearch
(
"content"
,
`
'multidimensional' & 'spaces'
`
,
{
config
:
"english"
,
}
)
;
const
resultA
=
await
store
.
similaritySearch
(
"quantum"
,
4
,
funcFilterA
)
;
const
funcFilterB
:
SupabaseFilterRPCCall
=
(
rpc
)
=>
rpc
.
filter
(
"metadata->b::int"
,
"lt"
,
3
)
.
filter
(
"metadata->c::int"
,
"gt"
,
7
)
.
filter
(
"metadata->>stuff"
,
"eq"
,
"right"
)
;
const
resultB
=
await
store
.
similaritySearch
(
"hello"
,
2
,
funcFilterB
)
;
console
.
log
(
resultA
,
resultB
)
;
}
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\vector_stores\integrations\tigris.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/vector_stores/integrations/tigris
crawled_at: 2025-06-22T02:00:24.586392
---

Tigris
Tigrisä½¿å‘é‡åµŒå…¥çš„æ„å»ºäººå·¥æ™ºèƒ½åº”ç”¨ç¨‹åºå˜å¾—è½»æ¾ã€‚
å®ƒæ˜¯ä¸€ä¸ªå®Œå…¨æ‰˜ç®¡çš„äº‘åŸç”Ÿæ•°æ®åº“ï¼Œå…è®¸æ‚¨å­˜å‚¨å’Œç´¢å¼•æ–‡æ¡£å’Œå‘é‡åµŒå…¥ï¼Œä»¥è¿›è¡Œå¿«é€Ÿå’Œå¯æ‰©å±•çš„å‘é‡æœç´¢ã€‚
å…¼å®¹æ€§
ä»…åœ¨Node.jsä¸Šå¯ç”¨ã€‚
å®‰è£…
â€‹
1. å®‰è£…Tigris SDK
â€‹
æŒ‰ä»¥ä¸‹æ–¹å¼å®‰è£…SDK
npm
Yarn
pnpm
npm
install
-S @tigrisdata/vector
yarn
add
@tigrisdata/vector
pnpm
add
@tigrisdata/vector
2. è·å–Tigris APIå‡­æ®
â€‹
æ‚¨å¯ä»¥åœ¨
æ­¤å¤„
æ³¨å†Œå…è´¹çš„Tigriså¸æˆ·ã€‚
æ³¨å†ŒTigriså¸æˆ·åï¼Œåˆ›å»ºåä¸º
vectordemo
çš„æ–°é¡¹ç›®ã€‚
æ¥ä¸‹æ¥ï¼Œè®°å½•
clientId
å’Œ
clientSecret
ï¼Œæ‚¨å¯ä»¥ä»é¡¹ç›®çš„åº”ç”¨ç¨‹åºå¯†é’¥éƒ¨åˆ†è·å–å®ƒä»¬ã€‚
ç´¢å¼•æ–‡æ¡£
â€‹
import
{
VectorDocumentStore
}
from
"@tigrisdata/vector"
;
import
{
Document
}
from
"langchain/document"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
TigrisVectorStore
}
from
"langchain/vectorstores/tigris"
;
const
index
=
new
VectorDocumentStore
(
{
connection
:
{
serverUrl
:
"api.preview.tigrisdata.cloud"
,
projectName
:
process
.
env
.
TIGRIS_PROJECT
,
clientId
:
process
.
env
.
TIGRIS_CLIENT_ID
,
clientSecret
:
process
.
env
.
TIGRIS_CLIENT_SECRET
,
}
,
indexName
:
"examples_index"
,
numDimensions
:
1536
,
// match the OpenAI embedding size
}
)
;
const
docs
=
[
new
Document
(
{
metadata
:
{
foo
:
"bar"
}
,
pageContent
:
"tigris is a cloud-native vector db"
,
}
)
,
new
Document
(
{
metadata
:
{
foo
:
"bar"
}
,
pageContent
:
"the quick brown fox jumped over the lazy dog"
,
}
)
,
new
Document
(
{
metadata
:
{
baz
:
"qux"
}
,
pageContent
:
"lorem ipsum dolor sit amet"
,
}
)
,
new
Document
(
{
metadata
:
{
baz
:
"qux"
}
,
pageContent
:
"tigris is a river"
,
}
)
,
]
;
await
TigrisVectorStore
.
fromDocuments
(
docs
,
new
OpenAIEmbeddings
(
)
,
{
index
}
)
;
æŸ¥è¯¢æ–‡æ¡£
â€‹
import
{
VectorDocumentStore
}
from
"@tigrisdata/vector"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
TigrisVectorStore
}
from
"langchain/vectorstores/tigris"
;
const
index
=
new
VectorDocumentStore
(
{
connection
:
{
serverUrl
:
"api.preview.tigrisdata.cloud"
,
projectName
:
process
.
env
.
TIGRIS_PROJECT
,
clientId
:
process
.
env
.
TIGRIS_CLIENT_ID
,
clientSecret
:
process
.
env
.
TIGRIS_CLIENT_SECRET
,
}
,
indexName
:
"examples_index"
,
numDimensions
:
1536
,
// match the OpenAI embedding size
}
)
;
const
vectorStore
=
await
TigrisVectorStore
.
fromExistingIndex
(
new
OpenAIEmbeddings
(
)
,
{
index
}
)
;
/* Search the vector DB independently with metadata filters */
const
results
=
await
vectorStore
.
similaritySearch
(
"tigris"
,
1
,
{
"metadata.foo"
:
"bar"
,
}
)
;
console
.
log
(
JSON
.
stringify
(
results
,
null
,
2
)
)
;
/*
[
Document {
pageContent: 'tigris is a cloud-native vector db',
metadata: { foo: 'bar' }
}
]
*/



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\vector_stores\integrations\typeorm.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/vector_stores/integrations/typeorm
crawled_at: 2025-06-22T02:00:24.623447
---

TypeORM
ä¸ºäº†åœ¨é€šç”¨çš„PostgreSQLæ•°æ®åº“ä¸­å®ç°å‘é‡æœç´¢ï¼ŒLangChainJSæ”¯æŒä½¿ç”¨
TypeORM
å’Œ
pgvector
Postgresæ‰©å±•ã€‚
è®¾ç½®
â€‹
è¦ä½¿ç”¨TypeORMï¼Œæ‚¨éœ€è¦å®‰è£…
typeorm
å’Œ
pg
è½¯ä»¶åŒ…:
npm
Yarn
pnpm
npm
install
typeorm
yarn
add
typeorm
pnpm
add
typeorm
npm
Yarn
pnpm
npm
install
pg
yarn
add
pg
pnpm
add
pg
ä½¿ç”¨
docker-compose
è®¾ç½®
pgvector
è‡ªæ‰˜ç®¡å®ä¾‹
â€‹
pgvector
æä¾›äº†ä¸€ä¸ªé¢„æ„å»ºçš„Dockeré•œåƒï¼Œå¯ç”¨äºå¿«é€Ÿè®¾ç½®è‡ªæ‰˜ç®¡çš„Postgreså®ä¾‹ã€‚
åˆ›å»ºä¸€ä¸ªåä¸º
docker-compose.yml
çš„æ–‡ä»¶:
import DockerExample from "!!raw-loader!@examples/indexes/vector_stores/typeorm_vectorstore/docker-compose.example.yml";
<CodeBlock language="yml" name="docker-compose.yml">{DockerExample}</CodeBlock>
ç„¶ååœ¨ç›¸åŒçš„ç›®å½•ä¸‹è¿è¡Œ
docker compose up
æ¥å¯åŠ¨å®¹å™¨ã€‚
æ‚¨å¯ä»¥åœ¨
å®˜æ–¹å­˜å‚¨åº“
ä¸­æ‰¾åˆ°æœ‰å…³å¦‚ä½•è®¾ç½®
pgvector
çš„æ›´å¤šä¿¡æ¯ã€‚
ç”¨æ³•
â€‹
ä½¿ç”¨
TypeORMVectorStore
çš„ä¸€ä¸ªå®Œæ•´çš„ç¤ºä¾‹å¦‚ä¸‹:
import
{
DataSourceOptions
}
from
"typeorm"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
TypeORMVectorStore
}
from
"langchain/vectorstores/typeorm"
;
// First, follow set-up instructions at
// https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/typeorm
export
const
run
=
async
(
)
=>
{
const
args
=
{
postgresConnectionOptions
:
{
type
:
"postgres"
,
host
:
"localhost"
,
port
:
5432
,
username
:
"myuser"
,
password
:
"ChangeMe"
,
database
:
"api"
,
}
as
DataSourceOptions
,
}
;
const
typeormVectorStore
=
await
TypeORMVectorStore
.
fromDataSource
(
new
OpenAIEmbeddings
(
)
,
args
)
;
await
typeormVectorStore
.
ensureTableInDatabase
(
)
;
await
typeormVectorStore
.
addDocuments
(
[
{
pageContent
:
"what's this"
,
metadata
:
{
a
:
2
}
}
,
{
pageContent
:
"Cat drinks milk"
,
metadata
:
{
a
:
1
}
}
,
]
)
;
const
results
=
await
typeormVectorStore
.
similaritySearch
(
"hello"
,
2
)
;
console
.
log
(
results
)
;
}
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\indexes\vector_stores\integrations\weaviate.md ---
---
url: https://js.langchain.com.cn/docs/modules/indexes/vector_stores/integrations/weaviate
crawled_at: 2025-06-22T02:00:24.668231
---

Weaviate
Weaviateæ˜¯ä¸€ä¸ªå¼€æºçš„å‘é‡æ•°æ®åº“ï¼Œå¯ä»¥å­˜å‚¨å¯¹è±¡å’Œå‘é‡ï¼Œä½¿å‘é‡æœç´¢ä¸ç»“æ„åŒ–è¿‡æ»¤ç›¸ç»“åˆã€‚LangChainé€šè¿‡
weaviate-ts-client
è½¯ä»¶åŒ…è¿æ¥åˆ°Weaviateï¼Œè¿™æ˜¯å®˜æ–¹çš„Typescriptå®¢æˆ·ç«¯ã€‚
LangChainç›´æ¥å°†å‘é‡æ’å…¥Weaviateå¹¶æŸ¥è¯¢ç»™å®šå‘é‡çš„æœ€è¿‘é‚»ï¼Œå› æ­¤æ‚¨å¯ä»¥ä½¿ç”¨æ‰€æœ‰LangChain Embeddingsä¸Weaviateçš„é›†æˆã€‚
è®¾ç½®
â€‹
npm
Yarn
pnpm
npm
install
weaviate-ts-client graphql
yarn
add
weaviate-ts-client graphql
pnpm
add
weaviate-ts-client graphql
æ‚¨éœ€è¦åœ¨æœ¬åœ°æˆ–æœåŠ¡å™¨ä¸Šè¿è¡ŒWeaviateï¼Œè¯·å‚é˜…
Weaviateæ–‡æ¡£
è·å–æ›´å¤šä¿¡æ¯ã€‚
ç”¨æ³•ï¼šæ’å…¥æ–‡æ¡£
â€‹
/* eslint-disable @typescript-eslint/no-explicit-any */
import
weaviate
from
"weaviate-ts-client"
;
import
{
WeaviateStore
}
from
"langchain/vectorstores/weaviate"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
export
async
function
run
(
)
{
// Something wrong with the weaviate-ts-client types, so we need to disable
const
client
=
(
weaviate
as
any
)
.
client
(
{
scheme
:
process
.
env
.
WEAVIATE_SCHEME
||
"https"
,
host
:
process
.
env
.
WEAVIATE_HOST
||
"localhost"
,
apiKey
:
new
(
weaviate
as
any
)
.
ApiKey
(
process
.
env
.
WEAVIATE_API_KEY
||
"default"
)
,
}
)
;
// Create a store and fill it with some texts + metadata
await
WeaviateStore
.
fromTexts
(
[
"hello world"
,
"hi there"
,
"how are you"
,
"bye now"
]
,
[
{
foo
:
"bar"
}
,
{
foo
:
"baz"
}
,
{
foo
:
"qux"
}
,
{
foo
:
"bar"
}
]
,
new
OpenAIEmbeddings
(
)
,
{
client
,
indexName
:
"Test"
,
textKey
:
"text"
,
metadataKeys
:
[
"foo"
]
,
}
)
;
}
ç”¨æ³•ï¼šæŸ¥è¯¢æ–‡æ¡£
â€‹
/* eslint-disable @typescript-eslint/no-explicit-any */
import
weaviate
from
"weaviate-ts-client"
;
import
{
WeaviateStore
}
from
"langchain/vectorstores/weaviate"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
export
async
function
run
(
)
{
// Something wrong with the weaviate-ts-client types, so we need to disable
const
client
=
(
weaviate
as
any
)
.
client
(
{
scheme
:
process
.
env
.
WEAVIATE_SCHEME
||
"https"
,
host
:
process
.
env
.
WEAVIATE_HOST
||
"localhost"
,
apiKey
:
new
(
weaviate
as
any
)
.
ApiKey
(
process
.
env
.
WEAVIATE_API_KEY
||
"default"
)
,
}
)
;
// Create a store for an existing index
const
store
=
await
WeaviateStore
.
fromExistingIndex
(
new
OpenAIEmbeddings
(
)
,
{
client
,
indexName
:
"Test"
,
metadataKeys
:
[
"foo"
]
,
}
)
;
// Search the index without any filters
const
results
=
await
store
.
similaritySearch
(
"hello world"
,
1
)
;
console
.
log
(
results
)
;
/*
[ Document { pageContent: 'hello world', metadata: { foo: 'bar' } } ]
*/
// Search the index with a filter, in this case, only return results where
// the "foo" metadata key is equal to "baz", see the Weaviate docs for more
// https://weaviate.io/developers/weaviate/api/graphql/filters
const
results2
=
await
store
.
similaritySearch
(
"hello world"
,
1
,
{
where
:
{
operator
:
"Equal"
,
path
:
[
"foo"
]
,
valueText
:
"baz"
,
}
,
}
)
;
console
.
log
(
results2
)
;
/*
[ Document { pageContent: 'hi there', metadata: { foo: 'baz' } } ]
*/
}



--- æ–‡ä»¶: output_20250622_020018\docs\modules\memory\examples\buffer_memory.md ---
---
url: https://js.langchain.com.cn/docs/modules/memory/examples/buffer_memory
crawled_at: 2025-06-22T02:00:24.869997
---

buffer_memory
ç¼“å­˜å†…å­˜
â€‹
ç¼“å­˜å†…å­˜æ˜¯æœ€ç®€å•çš„ä¸€ç§å†…å­˜ - å®ƒç›´æ¥è®°ä½äº†ä»¥å‰çš„å¯¹è¯å›åˆã€‚
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
BufferMemory
}
from
"langchain/memory"
;
import
{
ConversationChain
}
from
"langchain/chains"
;
const
model
=
new
OpenAI
(
{
}
)
;
const
memory
=
new
BufferMemory
(
)
;
const
chain
=
new
ConversationChain
(
{
llm
:
model
,
memory
:
memory
}
)
;
const
res1
=
await
chain
.
call
(
{
input
:
"Hi! I'm Jim."
}
)
;
console
.
log
(
{
res1
}
)
;
{
response:
" Hi Jim! It's nice to meet you. My name is AI. What would you like to talk about?"
}
const
res2
=
await
chain
.
call
(
{
input
:
"What's my name?"
}
)
;
console
.
log
(
{
res2
}
)
;
{
response:
' You said your name is Jim. Is there anything else you would like to talk about?'
}
æ‚¨è¿˜å¯ä»¥é€šè¿‡åˆ›å»ºå¹¶ä¼ é€’ä¸€ä¸ª
ChatHistory
å¯¹è±¡æ¥å°†æ¶ˆæ¯åŠ è½½åˆ°
BufferMemory
å®ä¾‹ä¸­ã€‚
è¿™ä½¿æ‚¨å¯ä»¥è½»æ¾åœ°ä»è¿‡å»çš„å¯¹è¯ä¸­è·å–çŠ¶æ€ã€‚
import
{
ChatMessageHistory
}
from
"langchain/memory"
;
import
{
HumanChatMessage
,
AIChatMessage
}
from
"langchain/schema"
;
const
pastMessages
=
[
new
HumanChatMessage
(
"My name's Jonas"
)
,
new
AIChatMessage
(
"Nice to meet you, Jonas!"
)
,
]
;
const
memory
=
new
BufferMemory
(
{
chatHistory
:
new
ChatMessageHistory
(
pastMessages
)
,
}
)
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\memory\examples\buffer_memory_chat.md ---
---
url: https://js.langchain.com.cn/docs/modules/memory/examples/buffer_memory_chat
crawled_at: 2025-06-22T02:00:24.826897
---

ä½¿ç”¨ç¼“å†²å†…å­˜ä¸èŠå¤©æ¨¡å‹ç¿»è¯‘çš„ä¸­æ–‡ç»“æœ
æœ¬ç¤ºä¾‹ä»‹ç»å¦‚ä½•å°†èŠå¤©ç‰¹å®šçš„å†…å­˜ç±»ä¸èŠå¤©æ¨¡å‹é…åˆä½¿ç”¨ã€‚ç¿»è¯‘çš„ä¸­æ–‡ç»“æœ
éœ€è¦æ³¨æ„çš„å…³é”®ç‚¹æ˜¯ï¼Œè®¾ç½®
returnMessages: true
ä¼šä½¿å†…å­˜è¿”å›èŠå¤©æ¶ˆæ¯åˆ—è¡¨è€Œä¸æ˜¯å­—ç¬¦ä¸²ã€‚ç¿»è¯‘çš„ä¸­æ–‡ç»“æœ
import
{
ConversationChain
}
from
"langchain/chains"
;
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
ChatPromptTemplate
,
HumanMessagePromptTemplate
,
SystemMessagePromptTemplate
,
MessagesPlaceholder
,
}
from
"langchain/prompts"
;
import
{
BufferMemory
}
from
"langchain/memory"
;
export
const
run
=
async
(
)
=>
{
const
chat
=
new
ChatOpenAI
(
{
temperature
:
0
}
)
;
const
chatPrompt
=
ChatPromptTemplate
.
fromPromptMessages
(
[
SystemMessagePromptTemplate
.
fromTemplate
(
"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know."
)
,
new
MessagesPlaceholder
(
"history"
)
,
HumanMessagePromptTemplate
.
fromTemplate
(
"{input}"
)
,
]
)
;
const
chain
=
new
ConversationChain
(
{
memory
:
new
BufferMemory
(
{
returnMessages
:
true
,
memoryKey
:
"history"
}
)
,
prompt
:
chatPrompt
,
llm
:
chat
,
}
)
;
const
response
=
await
chain
.
call
(
{
input
:
"hi! whats up?"
,
}
)
;
console
.
log
(
response
)
;
}
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\memory\examples\buffer_window_memory.md ---
---
url: https://js.langchain.com.cn/docs/modules/memory/examples/buffer_window_memory
crawled_at: 2025-06-22T02:00:24.872168
---

Buffer Window Memory
BufferWindowMemoryç”¨äºè·Ÿè¸ªä¼šè¯ä¸­çš„æ¥å›æ¶ˆæ¯ï¼Œç„¶åä½¿ç”¨å¤§å°ä¸º
k
çš„çª—å£å°†æœ€è¿‘çš„
k
æ¡æ¥å›æ¶ˆæ¯æå–å‡ºæ¥ä½œä¸ºå†…å­˜ã€‚
back-and-forths in conversation
â€‹
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
BufferWindowMemory
}
from
"langchain/memory"
;
import
{
ConversationChain
}
from
"langchain/chains"
;
const
model
=
new
OpenAI
(
{
}
)
;
const
memory
=
new
BufferWindowMemory
(
{
k
:
1
}
)
;
const
chain
=
new
ConversationChain
(
{
llm
:
model
,
memory
:
memory
}
)
;
const
res1
=
await
chain
.
call
(
{
input
:
"Hi! I'm Jim."
}
)
;
console
.
log
(
{
res1
}
)
;
{
response:
" Hi Jim! It's nice to meet you. My name is AI. What would you like to talk about?"
}
const
res2
=
await
chain
.
call
(
{
input
:
"What's my name?"
}
)
;
console
.
log
(
{
res2
}
)
;
{
response:
' You said your name is Jim. Is there anything else you would like to talk about?'
}



--- æ–‡ä»¶: output_20250622_020018\docs\modules\memory\examples\conversation_summary.md ---
---
url: https://js.langchain.com.cn/docs/modules/memory/examples/conversation_summary
crawled_at: 2025-06-22T02:00:24.911399
---

Conversation Summary Memoryï¼ˆå¯¹è¯æ€»ç»“è®°å¿†)
å¯¹è¯æ€»ç»“è®°å¿†ä¼šåœ¨å¯¹è¯è¿›è¡Œæ—¶å¯¹å…¶è¿›è¡Œæ€»ç»“å¹¶å‚¨å­˜åœ¨è®°å¿†ä¸­ã€‚è¿™ä¸ªè®°å¿†èƒ½å¤Ÿè¢«ç”¨äºå°†å½“å‰å¯¹è¯æ€»ç»“æ³¨å…¥åˆ°æç¤º/é“¾ä¸­ã€‚è¿™ä¸ªè®°å¿†åœ¨å¯¹è¾ƒé•¿çš„å¯¹è¯è¿›è¡Œæ€»ç»“æ—¶éå¸¸æœ‰ç”¨ï¼Œå› ä¸ºç›´æ¥åœ¨æç¤ºä¸­ä¿ç•™æ‰€æœ‰è¿‡å»çš„å¯¹è¯å†å²ä¿¡æ¯å°†ä¼šå ç”¨è¿‡å¤šçš„tokenã€‚
Usageï¼ˆä½¿ç”¨æ–¹æ³•)ï¼Œä¸LLMä¸€èµ·ä½¿ç”¨
â€‹
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
ConversationSummaryMemory
}
from
"langchain/memory"
;
import
{
LLMChain
}
from
"langchain/chains"
;
import
{
PromptTemplate
}
from
"langchain/prompts"
;
export
const
run
=
async
(
)
=>
{
const
memory
=
new
ConversationSummaryMemory
(
{
memoryKey
:
"chat_history"
,
llm
:
new
OpenAI
(
{
modelName
:
"gpt-3.5-turbo"
,
temperature
:
0
}
)
,
}
)
;
const
model
=
new
OpenAI
(
{
temperature
:
0.9
}
)
;
const
prompt
=
PromptTemplate
.
fromTemplate
(
`
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.
Current conversation:
{chat_history}
Human: {input}
AI:
`
)
;
const
chain
=
new
LLMChain
(
{
llm
:
model
,
prompt
,
memory
}
)
;
const
res1
=
await
chain
.
call
(
{
input
:
"Hi! I'm Jim."
}
)
;
console
.
log
(
{
res1
,
memory
:
await
memory
.
loadMemoryVariables
(
{
}
)
}
)
;
/*
{
res1: {
text: " Hi Jim, I'm AI! It's nice to meet you. I'm an AI programmed to provide information about the environment around me. Do you have any specific questions about the area that I can answer for you?"
},
memory: {
chat_history: 'Jim introduces himself to the AI and the AI responds, introducing itself as a program designed to provide information about the environment. The AI offers to answer any specific questions Jim may have about the area.'
}
}
*/
const
res2
=
await
chain
.
call
(
{
input
:
"What's my name?"
}
)
;
console
.
log
(
{
res2
,
memory
:
await
memory
.
loadMemoryVariables
(
{
}
)
}
)
;
/*
{
res2: { text: ' You told me your name is Jim.' },
memory: {
chat_history: 'Jim introduces himself to the AI and the AI responds, introducing itself as a program designed to provide information about the environment. The AI offers to answer any specific questions Jim may have about the area. Jim asks the AI what his name is, and the AI responds that Jim had previously told it his name.'
}
}
*/
}
;
Usageï¼ˆä½¿ç”¨æ–¹æ³•)ï¼Œä¸èŠå¤©æ¨¡å‹ä¸€èµ·ä½¿ç”¨
â€‹
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
ConversationSummaryMemory
}
from
"langchain/memory"
;
import
{
LLMChain
}
from
"langchain/chains"
;
import
{
PromptTemplate
}
from
"langchain/prompts"
;
export
const
run
=
async
(
)
=>
{
const
memory
=
new
ConversationSummaryMemory
(
{
memoryKey
:
"chat_history"
,
llm
:
new
ChatOpenAI
(
{
modelName
:
"gpt-3.5-turbo"
,
temperature
:
0
}
)
,
}
)
;
const
model
=
new
ChatOpenAI
(
)
;
const
prompt
=
PromptTemplate
.
fromTemplate
(
`
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.
Current conversation:
{chat_history}
Human: {input}
AI:
`
)
;
const
chain
=
new
LLMChain
(
{
llm
:
model
,
prompt
,
memory
}
)
;
const
res1
=
await
chain
.
call
(
{
input
:
"Hi! I'm Jim."
}
)
;
console
.
log
(
{
res1
,
memory
:
await
memory
.
loadMemoryVariables
(
{
}
)
}
)
;
/*
{
res1: {
text: "Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?"
},
memory: {
chat_history: 'Jim introduces himself to the AI and the AI greets him and offers assistance.'
}
}
*/
const
res2
=
await
chain
.
call
(
{
input
:
"What's my name?"
}
)
;
console
.
log
(
{
res2
,
memory
:
await
memory
.
loadMemoryVariables
(
{
}
)
}
)
;
/*
{
res2: {
text: "Your name is Jim. It's nice to meet you, Jim. How can I assist you today?"
},
memory: {
chat_history: 'Jim introduces himself to the AI and the AI greets him and offers assistance. The AI addresses Jim by name and asks how it can assist him.'
}
}
*/
}
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\memory\examples\dynamodb.md ---
---
url: https://js.langchain.com.cn/docs/modules/memory/examples/dynamodb
crawled_at: 2025-06-22T02:00:24.939810
---

DynamoDBæ”¯æŒçš„èŠå¤©è®°å½•
å¦‚æœéœ€è¦åœ¨èŠå¤©è¿›ç¨‹ä¹‹é—´è¿›è¡Œæ›´é•¿æœŸçš„æŒä¹…åŒ–ï¼Œæ‚¨å¯ä»¥å°†é»˜è®¤çš„å†…å­˜chatHistoryæ›¿æ¢ä¸ºDynamoDBå®ä¾‹ï¼Œä½œä¸ºæ”¯æŒBufferMemoryç­‰èŠå¤©è®°å½•ç±»çš„åç«¯ã€‚ï¼Œæ³¨æ„ï¼š
chatHistory
æŒ‡èŠå¤©è®°å½•ç±»ï¼Œ
BufferMemory
æŒ‡ç¼“å­˜å­˜å‚¨å™¨ç±»ã€‚
è®¾ç½®
â€‹
é¦–å…ˆï¼Œåœ¨æ‚¨çš„é¡¹ç›®ä¸­å®‰è£…AWS DynamoDBå®¢æˆ·ç«¯
npm
Yarn
pnpm
npm
install
@aws-sdk/client-dynamodb
yarn
add
@aws-sdk/client-dynamodb
pnpm
add
@aws-sdk/client-dynamodb
æ¥ä¸‹æ¥ï¼Œç™»å½•æ‚¨çš„AWSå¸æˆ·å¹¶åˆ›å»ºä¸€ä¸ªDynamoDBè¡¨æ ¼ã€‚å°†è¡¨æ ¼å‘½åä¸º
langchain
ï¼ŒæŒ‡å®šæ‚¨çš„åˆ†åŒºé”®ä¸º
id
ï¼Œåˆ†åŒºé”®å¿…é¡»æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œå…¶ä»–è®¾ç½®ä¿æŒé»˜è®¤å³å¯ã€‚
æ‚¨è¿˜éœ€è¦æ£€ç´¢ä¸€ä¸ªAWSè®¿é—®å¯†é’¥å’Œå¯†é’¥ï¼Œä»¥ä¾¿æ‹¥æœ‰è®¿é—®è¯¥è¡¨æ ¼çš„è§’è‰²æˆ–ç”¨æˆ·ï¼Œå¹¶å°†å®ƒä»¬æ·»åŠ åˆ°ç¯å¢ƒå˜é‡ä¸­ã€‚
ä½¿ç”¨æ–¹æ³•
â€‹
import
{
BufferMemory
}
from
"langchain/memory"
;
import
{
DynamoDBChatMessageHistory
}
from
"langchain/stores/message/dynamodb"
;
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
ConversationChain
}
from
"langchain/chains"
;
const
memory
=
new
BufferMemory
(
{
chatHistory
:
new
DynamoDBChatMessageHistory
(
{
tableName
:
"langchain"
,
partitionKey
:
"id"
,
sessionId
:
new
Date
(
)
.
toISOString
(
)
,
// Or some other unique identifier for the conversation
config
:
{
region
:
"us-east-2"
,
credentials
:
{
accessKeyId
:
"<your AWS access key id>"
,
secretAccessKey
:
"<your AWS secret access key>"
,
}
,
}
,
}
)
,
}
)
;
const
model
=
new
ChatOpenAI
(
)
;
const
chain
=
new
ConversationChain
(
{
llm
:
model
,
memory
}
)
;
const
res1
=
await
chain
.
call
(
{
input
:
"Hi! I'm Jim."
}
)
;
console
.
log
(
{
res1
}
)
;
/*
{
res1: {
text: "Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?"
}
}
*/
const
res2
=
await
chain
.
call
(
{
input
:
"What did I just say my name was?"
}
)
;
console
.
log
(
{
res2
}
)
;
/*
{
res1: {
text: "You said your name was Jim."
}
}
*/



--- æ–‡ä»¶: output_20250622_020018\docs\modules\memory\examples\entity_memory.md ---
---
url: https://js.langchain.com.cn/docs/modules/memory/examples/entity_memory
crawled_at: 2025-06-22T02:00:24.981041
---

å®ä½“è®°å¿†
å®ä½“è®°å¿†æ˜¯ä¼šè¯ä¸­è®°å¿†ç‰¹å®šå®ä½“çš„ç»™å®šäº‹å®ã€‚
å®ƒä½¿ç”¨ LLM æå–å®ä½“ (ä½¿ç”¨ LLM åŒæ—¶å»ºç«‹å¯¹å®ä½“çš„çŸ¥è¯†)ã€‚
ç”¨æ³•:
â€‹
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
EntityMemory
,
ENTITY_MEMORY_CONVERSATION_TEMPLATE
,
}
from
"langchain/memory"
;
import
{
LLMChain
}
from
"langchain/chains"
;
export
const
run
=
async
(
)
=>
{
const
memory
=
new
EntityMemory
(
{
llm
:
new
OpenAI
(
{
temperature
:
0
}
)
,
chatHistoryKey
:
"history"
,
// Default value
entitiesKey
:
"entities"
,
// Default value
}
)
;
const
model
=
new
OpenAI
(
{
temperature
:
0.9
}
)
;
const
chain
=
new
LLMChain
(
{
llm
:
model
,
prompt
:
ENTITY_MEMORY_CONVERSATION_TEMPLATE
,
// Default prompt - must include the set chatHistoryKey and entitiesKey as input variables.
memory
,
}
)
;
const
res1
=
await
chain
.
call
(
{
input
:
"Hi! I'm Jim."
}
)
;
console
.
log
(
{
res1
,
memory
:
await
memory
.
loadMemoryVariables
(
{
input
:
"Who is Jim?"
}
)
,
}
)
;
const
res2
=
await
chain
.
call
(
{
input
:
"I work in construction. What about you?"
,
}
)
;
console
.
log
(
{
res2
,
memory
:
await
memory
.
loadMemoryVariables
(
{
input
:
"Who is Jim?"
}
)
,
}
)
;
}
;
æ£€æŸ¥è®°å¿†å­˜å‚¨
â€‹
æ‚¨è¿˜å¯ä»¥ç›´æ¥æ£€æŸ¥è®°å¿†å­˜å‚¨ï¼Œä»¥æŸ¥çœ‹æ¯ä¸ªå®ä½“çš„å½“å‰æ‘˜è¦:
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
EntityMemory
,
ENTITY_MEMORY_CONVERSATION_TEMPLATE
,
}
from
"langchain/memory"
;
import
{
LLMChain
}
from
"langchain/chains"
;
const
memory
=
new
EntityMemory
(
{
llm
:
new
OpenAI
(
{
temperature
:
0
}
)
,
}
)
;
const
model
=
new
OpenAI
(
{
temperature
:
0.9
}
)
;
const
chain
=
new
LLMChain
(
{
llm
:
model
,
prompt
:
ENTITY_MEMORY_CONVERSATION_TEMPLATE
,
memory
,
}
)
;
await
chain
.
call
(
{
input
:
"Hi! I'm Jim."
}
)
;
await
chain
.
call
(
{
input
:
"I work in sales. What about you?"
,
}
)
;
const
res
=
await
chain
.
call
(
{
input
:
"My office is the Utica branch of Dunder Mifflin. What about you?"
,
}
)
;
console
.
log
(
{
res
,
memory
:
await
memory
.
loadMemoryVariables
(
{
input
:
"Who is Jim?"
}
)
,
}
)
;
/*
{
res: "As an AI language model, I don't have an office in the traditional sense. I exist entirely in digital space and am here to assist you with any questions or tasks you may have. Is there anything specific you need help with regarding your work at the Utica branch of Dunder Mifflin?",
memory: {
entities: {
Jim: 'Jim is a human named Jim who works in sales.',
Utica: 'Utica is the location of the branch of Dunder Mifflin where Jim works.',
'Dunder Mifflin': 'Dunder Mifflin has a branch in Utica.'
}
}
}
*/



--- æ–‡ä»¶: output_20250622_020018\docs\modules\memory\examples\momento.md ---
---
url: https://js.langchain.com.cn/docs/modules/memory/examples/momento
crawled_at: 2025-06-22T02:00:25.005827
---

Momentoæ”¯æŒçš„èŠå¤©è®°å½•
å¦‚æœè¦åœ¨èŠå¤©ä¼šè¯ä¸­ä½¿ç”¨åˆ†å¸ƒå¼ã€æ— æœåŠ¡å™¨çš„æŒä¹…æ€§,å¯ä»¥ä½¿ç”¨Momentoæ”¯æŒçš„èŠå¤©æ¶ˆæ¯å†å²è®°å½•ï¼Œå³åˆ»ç¼“å­˜ï¼Œæ— éœ€ä»»ä½•åŸºç¡€è®¾æ–½ç»´æŠ¤,æ— è®ºæ˜¯åœ¨æœ¬åœ°æ„å»ºè¿˜æ˜¯åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ½æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ã€‚
è®¾ç½®
â€‹
åœ¨é¡¹ç›®ä¸­å®‰è£…
Momentoå®¢æˆ·ç«¯åº“
:
npm
Yarn
pnpm
npm
install
@gomomento/sdk
yarn
add
@gomomento/sdk
pnpm
add
@gomomento/sdk
æ‚¨è¿˜éœ€è¦ä»
Momento
è·å¾—APIå¯†é’¥ã€‚æ‚¨å¯ä»¥åœ¨æ­¤å¤„ç­¾ç½²å…è´¹å¸æˆ·
è¿™é‡Œ
ã€‚
ç”¨æ³•
â€‹
ä¸ºäº†åŒºåˆ†ä¸€ä¸ªèŠå¤©å†å²è®°å½•ä¼šè¯å’Œå¦ä¸€ä¸ªä¼šè¯,æˆ‘ä»¬éœ€è¦ä¸€ä¸ªå”¯ä¸€çš„â€œsessionIdâ€ã€‚æ‚¨è¿˜å¯ä»¥æä¾›ä¸€ä¸ªå¯é€‰çš„â€œsessionTtlâ€ï¼Œä»¥ä½¿ä¼šè¯åœ¨ç»™å®šçš„ç§’æ•°åè¿‡æœŸã€‚
import
{
CacheClient
,
Configurations
,
CredentialProvider
,
}
from
"@gomomento/sdk"
;
import
{
BufferMemory
}
from
"langchain/memory"
;
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
ConversationChain
}
from
"langchain/chains"
;
import
{
MomentoChatMessageHistory
}
from
"langchain/stores/message/momento"
;
// See https://github.com/momentohq/client-sdk-javascript for connection options
const
client
=
new
CacheClient
(
{
configuration
:
Configurations
.
Laptop
.
v1
(
)
,
credentialProvider
:
CredentialProvider
.
fromEnvironmentVariable
(
{
environmentVariableName
:
"MOMENTO_AUTH_TOKEN"
,
}
)
,
defaultTtlSeconds
:
60
*
60
*
24
,
}
)
;
// Create a unique session ID
const
sessionId
=
new
Date
(
)
.
toISOString
(
)
;
const
cacheName
=
"langchain"
;
const
memory
=
new
BufferMemory
(
{
chatHistory
:
await
MomentoChatMessageHistory
.
fromProps
(
{
client
,
cacheName
,
sessionId
,
sessionTtl
:
300
,
}
)
,
}
)
;
console
.
log
(
`
cacheName=
${
cacheName
}
and sessionId=
${
sessionId
}
. This will be used to store the chat history. You can inspect the values at your Momento console at https://console.gomomento.com.
`
)
;
const
model
=
new
ChatOpenAI
(
{
modelName
:
"gpt-3.5-turbo"
,
temperature
:
0
,
}
)
;
const
chain
=
new
ConversationChain
(
{
llm
:
model
,
memory
}
)
;
const
res1
=
await
chain
.
call
(
{
input
:
"Hi! I'm Jim."
}
)
;
console
.
log
(
{
res1
}
)
;
/*
{
res1: {
text: "Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?"
}
}
*/
const
res2
=
await
chain
.
call
(
{
input
:
"What did I just say my name was?"
}
)
;
console
.
log
(
{
res2
}
)
;
/*
{
res1: {
text: "You said your name was Jim."
}
}
*/
// See the chat history in the Momento
console
.
log
(
await
memory
.
chatHistory
.
getMessages
(
)
)
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\memory\examples\motorhead_memory.md ---
---
url: https://js.langchain.com.cn/docs/modules/memory/examples/motorhead_memory
crawled_at: 2025-06-22T02:00:25.029149
---

motorhead_memory
MotÃ¶rhead
æ˜¯ä¸€ä¸ªç”±Rustå®ç°çš„å†…å­˜æœåŠ¡å™¨ã€‚å®ƒå¯ä»¥è‡ªåŠ¨å¤„ç†å¢é‡æ‘˜è¦å¹¶å…è®¸æ— çŠ¶æ€åº”ç”¨ç¨‹åºã€‚
è®¾ç½®
â€‹
è¯·å‚é˜…
MotÃ¶rhead
çš„æŒ‡ç¤ºä»¥åœ¨æœ¬åœ°è¿è¡ŒæœåŠ¡å™¨ã€‚
ç”¨æ³•
â€‹
import
{
ConversationChain
}
from
"langchain/chains"
;
import
{
ChatOpenAI
}
from
"langchain/chat_models"
;
import
{
MotorheadMemory
}
from
"langchain/memory"
;
const
model
=
new
ChatOpenAI
(
{
}
)
;
const
memory
=
new
MotorheadMemory
(
{
sessionId
:
"user-id"
,
motorheadURL
:
"localhost:8080"
,
}
)
;
await
memory
.
init
(
)
;
// loads previous state from MotÃ¶rhead ğŸ¤˜
const
context
=
memory
.
context
?
`
Here's previous context:
${
memory
.
context
}
`
:
""
;
const
chatPrompt
=
ChatPromptTemplate
.
fromPromptMessages
(
[
SystemMessagePromptTemplate
.
fromTemplate
(
`
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.
${
context
}
`
)
,
new
MessagesPlaceholder
(
"history"
)
,
HumanMessagePromptTemplate
.
fromTemplate
(
"{input}"
)
,
]
)
;
const
chain
=
new
ConversationChain
(
{
memory
,
prompt
:
chatPrompt
,
llm
:
chat
,
}
)
;
const
res1
=
await
chain
.
call
(
{
input
:
"Hi! I'm Jim."
}
)
;
console
.
log
(
{
res1
}
)
;
```shell
{
response:
" Hi Jim! It's nice to meet you. My name is AI. What would you like to talk about?"
}
const
res2
=
await
chain
.
call
(
{
input
:
"What's my name?"
}
)
;
console
.
log
(
{
res2
}
)
;
{
response:
' You said your name is Jim. Is there anything else you would like to talk about?'
}



--- æ–‡ä»¶: output_20250622_020018\docs\modules\memory\examples\redis.md ---
---
url: https://js.langchain.com.cn/docs/modules/memory/examples/redis
crawled_at: 2025-06-22T02:00:25.086954
---

åŸºäºRedisçš„èŠå¤©å­˜å‚¨
å¦‚æœéœ€è¦åœ¨èŠå¤©ä¼šè¯ä¹‹é—´è¿›è¡Œé•¿æœŸæŒä¹…åŒ–ï¼Œå¯ä»¥å°†é»˜è®¤çš„å†…å­˜
chatHistory
æ›¿æ¢ä¸ºä¸€ä¸ª
Redis
å®ä¾‹æ¥æ”¯æŒèŠå¤©å­˜å‚¨ç±»ï¼Œå¦‚
BufferMemory
ã€‚
è®¾ç½®
â€‹
æ‚¨éœ€è¦åœ¨é¡¹ç›®ä¸­å®‰è£…
node-redis
ã€‚
npm
Yarn
pnpm
npm
install
redis
yarn
add
redis
pnpm
add
redis
æ‚¨è¿˜éœ€è¦ä¸€ä¸ªRediså®ä¾‹æ¥è¿æ¥ã€‚è¯·å‚é˜…
Rediså®˜æ–¹ç½‘ç«™
ä¸Šè¿è¡Œæœ¬åœ°æœåŠ¡å™¨çš„è¯´æ˜ã€‚
ç”¨æ³•
â€‹
Redisä¸­å­˜å‚¨çš„æ¯ä¸ªèŠå¤©å†å²è®°å½•ä¼šè¯éƒ½å¿…é¡»å…·æœ‰å”¯ä¸€çš„IDã€‚ä½ å¯ä»¥æä¾›ä¸€ä¸ªå¯é€‰çš„
sessionTTL
å‚æ•°æ¥ä½¿ä¼šè¯åœ¨ä¸€å®šæ—¶é—´åè¿‡æœŸã€‚
ä¼ é€’ç»™
createClient
æ–¹æ³•çš„
config
å‚æ•°ç›´æ¥ä¼ é€’ç»™
node-redis
ï¼Œå¹¶ä½¿ç”¨æ‰€æœ‰ç›¸åŒçš„å‚æ•°ã€‚
import
{
BufferMemory
}
from
"langchain/memory"
;
import
{
RedisChatMessageHistory
}
from
"langchain/stores/message/redis"
;
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
ConversationChain
}
from
"langchain/chains"
;
const
memory
=
new
BufferMemory
(
{
chatHistory
:
new
RedisChatMessageHistory
(
{
sessionId
:
new
Date
(
)
.
toISOString
(
)
,
// Or some other unique identifier for the conversation
sessionTTL
:
300
,
// 5 minutes, omit this parameter to make sessions never expire
config
:
{
url
:
"redis://localhost:6379"
,
// Default value, override with your own instance's URL
}
,
}
)
,
}
)
;
const
model
=
new
ChatOpenAI
(
{
modelName
:
"gpt-3.5-turbo"
,
temperature
:
0
,
}
)
;
const
chain
=
new
ConversationChain
(
{
llm
:
model
,
memory
}
)
;
const
res1
=
await
chain
.
call
(
{
input
:
"Hi! I'm Jim."
}
)
;
console
.
log
(
{
res1
}
)
;
/*
{
res1: {
text: "Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?"
}
}
*/
const
res2
=
await
chain
.
call
(
{
input
:
"What did I just say my name was?"
}
)
;
console
.
log
(
{
res2
}
)
;
/*
{
res1: {
text: "You said your name was Jim."
}
}
*/
é«˜çº§ç”¨æ³•
â€‹
æ‚¨ä¹Ÿå¯ä»¥ç›´æ¥ä¼ é€’å…ˆå‰åˆ›å»ºçš„
node-redis
å®¢æˆ·ç«¯å®ä¾‹:
import
{
createClient
}
from
"redis"
;
import
{
BufferMemory
}
from
"langchain/memory"
;
import
{
RedisChatMessageHistory
}
from
"langchain/stores/message/redis"
;
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
ConversationChain
}
from
"langchain/chains"
;
const
client
=
createClient
(
{
url
:
"redis://localhost:6379"
,
}
)
;
const
memory
=
new
BufferMemory
(
{
chatHistory
:
new
RedisChatMessageHistory
(
{
sessionId
:
new
Date
(
)
.
toISOString
(
)
,
sessionTTL
:
300
,
client
,
}
)
,
}
)
;
const
model
=
new
ChatOpenAI
(
{
modelName
:
"gpt-3.5-turbo"
,
temperature
:
0
,
}
)
;
const
chain
=
new
ConversationChain
(
{
llm
:
model
,
memory
}
)
;
const
res1
=
await
chain
.
call
(
{
input
:
"Hi! I'm Jim."
}
)
;
console
.
log
(
{
res1
}
)
;
/*
{
res1: {
text: "Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?"
}
}
*/
const
res2
=
await
chain
.
call
(
{
input
:
"What did I just say my name was?"
}
)
;
console
.
log
(
{
res2
}
)
;
/*
{
res1: {
text: "You said your name was Jim."
}
}
*/



--- æ–‡ä»¶: output_20250622_020018\docs\modules\memory\examples\upstash_redis.md ---
---
url: https://js.langchain.com.cn/docs/modules/memory/examples/upstash_redis
crawled_at: 2025-06-22T02:00:25.163885
---

Upstash åŸºäº Redis çš„èŠå¤©è®°å¿†
ç”±äº Upstash Redis é€šè¿‡ REST API å·¥ä½œ,æ‰€ä»¥æ‚¨å¯ä»¥å°†å…¶ä¸
Vercel Edge
ï¼Œ
[Cloudflare Workers]
ä¸€èµ·ä½¿ç”¨ï¼ˆhttpsï¼š//developers.cloudflare.com/workers/)å’Œå…¶ä»–æ— æœåŠ¡å™¨ç¯å¢ƒã€‚
åŸºäº Redis-Backed èŠå¤©è®°å¿†ã€‚
ä¸ºäº†åœ¨èŠå¤©ä¼šè¯ä¹‹é—´å®ç°é•¿æœŸæŒä¹…æ€§,æ‚¨å¯ä»¥å°†æ”¯æŒèŠå¤©è®°å¿†ç±»ï¼ˆå¦‚
BufferMemory
)çš„é»˜è®¤å†…å­˜ä¸­çš„
chatHistory
æ›¿æ¢ä¸º Upstash
[Redis]
å®ä¾‹ï¼ˆhttpsï¼š//redis.io/)ã€‚
è®¾ç½®
â€‹
æ‚¨éœ€è¦åœ¨é¡¹ç›®ä¸­å®‰è£…
@upstash/redis
npm
Yarn
pnpm
npm
install
@upstash/redis
yarn
add
@upstash/redis
pnpm
add
@upstash/redis
æ‚¨è¿˜éœ€è¦ä¸€ä¸ª Upstash å¸æˆ·å’Œä¸€ä¸ª Redis æ•°æ®åº“è¿›è¡Œè¿æ¥ã€‚æœ‰å…³å¦‚ä½•åˆ›å»º HTTP å®¢æˆ·ç«¯çš„è¯´æ˜ï¼Œè¯·å‚è§
Upstash Docs
ã€‚
ä½¿ç”¨
â€‹
Redis ä¸­å­˜å‚¨çš„æ¯ä¸ªèŠå¤©å†å²è®°å½•ä¼šè¯å¿…é¡»å…·æœ‰å”¯ä¸€çš„ idã€‚æ‚¨å¯ä»¥æä¾›å¯é€‰çš„
sessionTTL
ï¼Œä»¥ä½¿ä¼šè¯åœ¨ç¡®å®šçš„ç§’æ•°åè¿‡æœŸã€‚
ç›´æ¥å°†
config
å‚æ•°ä¼ é€’åˆ°
@upstash/redis
çš„
new Redis()
æ„é€ å‡½æ•°ä¸­ï¼Œå¹¶ä½¿ç”¨ç›¸åŒçš„å‚æ•°ã€‚
import
{
BufferMemory
}
from
"langchain/memory"
;
import
{
UpstashRedisChatMessageHistory
}
from
"langchain/stores/message/upstash_redis"
;
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
ConversationChain
}
from
"langchain/chains"
;
const
memory
=
new
BufferMemory
(
{
chatHistory
:
new
UpstashRedisChatMessageHistory
(
{
sessionId
:
new
Date
(
)
.
toISOString
(
)
,
// Or some other unique identifier for the conversation
sessionTTL
:
300
,
// 5 minutes, omit this parameter to make sessions never expire
config
:
{
url
:
"https://ADD_YOURS_HERE.upstash.io"
,
// Override with your own instance's URL
token
:
"********"
,
// Override with your own instance's token
}
,
}
)
,
}
)
;
const
model
=
new
ChatOpenAI
(
{
modelName
:
"gpt-3.5-turbo"
,
temperature
:
0
,
}
)
;
const
chain
=
new
ConversationChain
(
{
llm
:
model
,
memory
}
)
;
const
res1
=
await
chain
.
call
(
{
input
:
"Hi! I'm Jim."
}
)
;
console
.
log
(
{
res1
}
)
;
/*
{
res1: {
text: "Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?"
}
}
*/
const
res2
=
await
chain
.
call
(
{
input
:
"What did I just say my name was?"
}
)
;
console
.
log
(
{
res2
}
)
;
/*
{
res1: {
text: "You said your name was Jim."
}
}
*/
é«˜çº§ç”¨æ³•
â€‹
æ‚¨è¿˜å¯ä»¥ç›´æ¥ä¼ å…¥å…ˆå‰åˆ›å»ºçš„
@upstash/redis
å®¢æˆ·ç«¯å®ä¾‹@ï¼ƒ
import
{
Redis
}
from
"@upstash/redis"
;
import
{
BufferMemory
}
from
"langchain/memory"
;
import
{
UpstashRedisChatMessageHistory
}
from
"langchain/stores/message/upstash_redis"
;
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
ConversationChain
}
from
"langchain/chains"
;
// Create your own Redis client
const
client
=
new
Redis
(
{
url
:
"https://ADD_YOURS_HERE.upstash.io"
,
token
:
"********"
,
}
)
;
const
memory
=
new
BufferMemory
(
{
chatHistory
:
new
UpstashRedisChatMessageHistory
(
{
sessionId
:
new
Date
(
)
.
toISOString
(
)
,
sessionTTL
:
300
,
client
,
// You can reuse your existing Redis client
}
)
,
}
)
;
const
model
=
new
ChatOpenAI
(
{
modelName
:
"gpt-3.5-turbo"
,
temperature
:
0
,
}
)
;
const
chain
=
new
ConversationChain
(
{
llm
:
model
,
memory
}
)
;
const
res1
=
await
chain
.
call
(
{
input
:
"Hi! I'm Jim."
}
)
;
console
.
log
(
{
res1
}
)
;
/*
{
res1: {
text: "Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?"
}
}
*/
const
res2
=
await
chain
.
call
(
{
input
:
"What did I just say my name was?"
}
)
;
console
.
log
(
{
res2
}
)
;
/*
{
res1: {
text: "You said your name was Jim."
}
}
*/



--- æ–‡ä»¶: output_20250622_020018\docs\modules\memory\examples\vector_store_memory.md ---
---
url: https://js.langchain.com.cn/docs/modules/memory/examples/vector_store_memory
crawled_at: 2025-06-22T02:00:25.185088
---

åŸºäºå‘é‡åº“çš„å†…å­˜
VectorStoreRetrieverMemory
å°†è®°å¿†å­˜å‚¨åœ¨å‘é‡æ•°æ®åº“ä¸­ï¼Œå¹¶åœ¨æ¯æ¬¡è°ƒç”¨æ—¶æŸ¥è¯¢æœ€â€œæ˜¾è‘—â€çš„å‰Kä¸ªæ–‡æ¡£ã€‚
è¿™ä¸å¤§å¤šæ•°å…¶ä»–å†…å­˜ç±»ä¸åŒï¼Œå®ƒæ²¡æœ‰æ˜¾å¼è·Ÿè¸ªäº¤äº’çš„é¡ºåºã€‚
åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œâ€œæ–‡æ¡£â€æ˜¯å…ˆå‰çš„ä¼šè¯ç‰‡æ®µã€‚è¿™å¯ä»¥ç”¨æ¥æåˆ°AIä¹‹å‰åœ¨å¯¹è¯ä¸­è¢«å‘ŠçŸ¥çš„ç›¸å…³ä¿¡æ¯ã€‚
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
VectorStoreRetrieverMemory
}
from
"langchain/memory"
;
import
{
LLMChain
}
from
"langchain/chains"
;
import
{
PromptTemplate
}
from
"langchain/prompts"
;
import
{
MemoryVectorStore
}
from
"langchain/vectorstores/memory"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
const
vectorStore
=
new
MemoryVectorStore
(
new
OpenAIEmbeddings
(
)
)
;
const
memory
=
new
VectorStoreRetrieverMemory
(
{
// 1 is how many documents to return, you might want to return more, eg. 4
vectorStoreRetriever
:
vectorStore
.
asRetriever
(
1
)
,
memoryKey
:
"history"
,
}
)
;
// First let's save some information to memory, as it would happen when
// used inside a chain.
await
memory
.
saveContext
(
{
input
:
"My favorite food is pizza"
}
,
{
output
:
"thats good to know"
}
)
;
await
memory
.
saveContext
(
{
input
:
"My favorite sport is soccer"
}
,
{
output
:
"..."
}
)
;
await
memory
.
saveContext
(
{
input
:
"I don't the Celtics"
}
,
{
output
:
"ok"
}
)
;
// Now let's use the memory to retrieve the information we saved.
console
.
log
(
await
memory
.
loadMemoryVariables
(
{
prompt
:
"what sport should i watch?"
}
)
)
;
/*
{ history: 'input: My favorite sport is soccer\noutput: ...' }
*/
// Now let's use it in a chain.
const
model
=
new
OpenAI
(
{
temperature
:
0.9
}
)
;
const
prompt
=
PromptTemplate
.
fromTemplate
(
`
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.
Relevant pieces of previous conversation:
{history}
(You do not need to use these pieces of information if not relevant)
Current conversation:
Human: {input}
AI:
`
)
;
const
chain
=
new
LLMChain
(
{
llm
:
model
,
prompt
,
memory
}
)
;
const
res1
=
await
chain
.
call
(
{
input
:
"Hi, my name is Perry, what's up?"
}
)
;
console
.
log
(
{
res1
}
)
;
/*
{
res1: {
text: " Hi Perry, I'm doing great! I'm currently exploring different topics related to artificial intelligence like natural language processing and machine learning. What about you? What have you been up to lately?"
}
}
*/
const
res2
=
await
chain
.
call
(
{
input
:
"what's my favorite sport?"
}
)
;
console
.
log
(
{
res2
}
)
;
/*
{ res2: { text: ' You said your favorite sport is soccer.' } }
*/
const
res3
=
await
chain
.
call
(
{
input
:
"what's my name?"
}
)
;
console
.
log
(
{
res3
}
)
;
/*
{ res3: { text: ' Your name is Perry.' } }
*/



--- æ–‡ä»¶: output_20250622_020018\docs\modules\memory\examples\zep_memory.md ---
---
url: https://js.langchain.com.cn/docs/modules/memory/examples/zep_memory
crawled_at: 2025-06-22T02:00:25.227744
---

Zep Memory
Zep
æ˜¯å­˜å‚¨ã€ï¼Œæ€»ç»“ã€å†…åµŒã€ç´¢å¼•å’Œä¸°å¯Œå¯¹è¯AIèŠå¤©å†å²ã€è‡ªæ²»ä»£ç†å†å²ã€æ–‡æ¡£Q&Aå†å²çš„è®°å¿†æœåŠ¡å™¨ï¼Œå¹¶é€šè¿‡ç®€å•ã€ä½å»¶è¿Ÿçš„APIå…¬å¼€å®ƒä»¬ã€‚
ä¸»è¦åŠŸèƒ½:
é•¿æœŸå­˜å‚¨è®°å¿†ï¼Œæ— è®ºæ‚¨çš„æ€»ç»“ç­–ç•¥å¦‚ä½•ï¼Œéƒ½å¯è®¿é—®å†å²æ¶ˆæ¯ã€‚
åŸºäºå¯é…ç½®çš„æ¶ˆæ¯çª—å£è‡ªåŠ¨æ€»ç»“è®°å¿†æ¶ˆæ¯ã€‚ä¸€ç³»åˆ—çš„æ€»ç»“è¢«å­˜å‚¨ï¼Œä¸ºæœªæ¥çš„æ€»ç»“ç­–ç•¥æä¾›äº†çµæ´»æ€§ã€‚
å¯¹è®°å¿†è¿›è¡Œå‘é‡æœç´¢ï¼Œæ¶ˆæ¯åœ¨åˆ›å»ºæ—¶è‡ªåŠ¨åµŒå…¥ã€‚
è‡ªåŠ¨è®°å¿†å’Œæ‘˜è¦çš„ä»¤ç‰Œè®¡æ•°ï¼Œå…è®¸æ›´ç»†ç²’åº¦åœ°æ§åˆ¶æç¤ºç»„åˆã€‚
Python
å’Œ
JavaScript
SDKã€‚
è®¾ç½®
â€‹
è¯·å‚é˜…
Zep
çš„è¯´æ˜ï¼Œä»¥åœ¨æœ¬åœ°æˆ–é€šè¿‡è‡ªåŠ¨æ‰˜ç®¡æä¾›ç¨‹åºè¿è¡ŒæœåŠ¡å™¨ã€‚
ç”¨æ³•
â€‹
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
ConversationChain
}
from
"langchain/chains"
;
import
{
ZepMemory
}
from
"langchain/memory/zep"
;
const
sessionId
=
"TestSession1234"
;
const
zepURL
=
"http://localhost:8000"
;
const
memory
=
new
ZepMemory
(
{
sessionId
,
baseURL
:
zepURL
,
}
)
;
const
model
=
new
ChatOpenAI
(
{
modelName
:
"gpt-3.5-turbo"
,
temperature
:
0
,
}
)
;
const
chain
=
new
ConversationChain
(
{
llm
:
model
,
memory
}
)
;
const
res1
=
await
chain
.
call
(
{
input
:
"Hi! I'm Jim."
}
)
;
console
.
log
(
{
res1
}
)
;
/*
{
res1: {
text: "Hello Jim! It's nice to meet you. My name is AI. How may I assist you today?"
}
}
*/
const
res2
=
await
chain
.
call
(
{
input
:
"What did I just say my name was?"
}
)
;
console
.
log
(
{
res2
}
)
;
/*
{
res1: {
text: "You said your name was Jim."
}
}
*/



--- æ–‡ä»¶: output_20250622_020018\docs\modules\models\chat\additional_functionality.md ---
---
url: https://js.langchain.com.cn/docs/modules/models/chat/additional_functionality
crawled_at: 2025-06-22T02:00:25.344788
---

é™„åŠ åŠŸèƒ½: èŠå¤©æ¨¡å‹
æˆ‘ä»¬ä¸ºèŠå¤©æ¨¡å‹æä¾›äº†è®¸å¤šé™„åŠ åŠŸèƒ½ã€‚åœ¨ä»¥ä¸‹ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨
ChatOpenAI
æ¨¡å‹ã€‚
é™„åŠ æ–¹æ³•
â€‹
L angChain ä¸ºä¸èŠå¤©æ¨¡å‹äº¤äº’æä¾›äº†è®¸å¤šé™„åŠ æ–¹æ³•:
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
HumanChatMessage
,
SystemChatMessage
}
from
"langchain/schema"
;
export
const
run
=
async
(
)
=>
{
const
chat
=
new
ChatOpenAI
(
{
modelName
:
"gpt-3.5-turbo"
}
)
;
// Pass in a list of messages to `call` to start a conversation. In this simple example, we only pass in one message.
const
responseA
=
await
chat
.
call
(
[
new
HumanChatMessage
(
"What is a good name for a company that makes colorful socks?"
)
,
]
)
;
console
.
log
(
responseA
)
;
// AIChatMessage { text: '\n\nRainbow Sox Co.' }
// You can also pass in multiple messages to start a conversation.
// The first message is a system message that describes the context of the conversation.
// The second message is a human message that starts the conversation.
const
responseB
=
await
chat
.
call
(
[
new
SystemChatMessage
(
"You are a helpful assistant that translates English to French."
)
,
new
HumanChatMessage
(
"Translate: I love programming."
)
,
]
)
;
console
.
log
(
responseB
)
;
// AIChatMessage { text: "J'aime programmer." }
// Similar to LLMs, you can also use `generate` to generate chat completions for multiple sets of messages.
const
responseC
=
await
chat
.
generate
(
[
[
new
SystemChatMessage
(
"You are a helpful assistant that translates English to French."
)
,
new
HumanChatMessage
(
"Translate this sentence from English to French. I love programming."
)
,
]
,
[
new
SystemChatMessage
(
"You are a helpful assistant that translates English to French."
)
,
new
HumanChatMessage
(
"Translate this sentence from English to French. I love artificial intelligence."
)
,
]
,
]
)
;
console
.
log
(
responseC
)
;
/*
{
generations: [
[
{
text: "J'aime programmer.",
message: AIChatMessage { text: "J'aime programmer." },
}
],
[
{
text: "J'aime l'intelligence artificielle.",
message: AIChatMessage { text: "J'aime l'intelligence artificielle." }
}
]
]
}
*/
}
;
æµå¼ä¼ è¾“
â€‹
ä¸ LLMs ç±»ä¼¼ï¼Œæ‚¨å¯ä»¥ä»èŠå¤©æ¨¡å‹ä¸­æµå¼ä¼ è¾“å“åº”ã€‚è¿™å¯¹äºéœ€è¦å®æ—¶å“åº”ç”¨æˆ·è¾“å…¥çš„èŠå¤©æœºå™¨äººéå¸¸æœ‰ç”¨ã€‚
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
HumanChatMessage
}
from
"langchain/schema"
;
const
chat
=
new
ChatOpenAI
(
{
maxTokens
:
25
,
streaming
:
true
,
}
)
;
const
response
=
await
chat
.
call
(
[
new
HumanChatMessage
(
"Tell me a joke."
)
]
,
undefined
,
[
{
handleLLMNewToken
(
token
:
string
)
{
console
.
log
(
{
token
}
)
;
}
,
}
,
]
)
;
console
.
log
(
response
)
;
// { token: '' }
// { token: '\n\n' }
// { token: 'Why' }
// { token: ' don' }
// { token: "'t" }
// { token: ' scientists' }
// { token: ' trust' }
// { token: ' atoms' }
// { token: '?\n\n' }
// { token: 'Because' }
// { token: ' they' }
// { token: ' make' }
// { token: ' up' }
// { token: ' everything' }
// { token: '.' }
// { token: '' }
// AIChatMessage {
//   text: "\n\nWhy don't scientists trust atoms?\n\nBecause they make up everything."
// }
æ·»åŠ è¶…æ—¶
â€‹
é»˜è®¤æƒ…å†µä¸‹ï¼ŒLangChain ä¼šæ— é™æœŸç­‰å¾…æ¨¡å‹æä¾›è€…çš„å“åº”ã€‚å¦‚æœæ‚¨æƒ³æ·»åŠ è¶…æ—¶ï¼Œå¯ä»¥åœ¨è°ƒç”¨æ¨¡å‹æ—¶ä¼ é€’ä¸€ä¸ªä»¥æ¯«ç§’ä¸ºå•ä½çš„
timeout
é€‰é¡¹ã€‚ä¾‹å¦‚ï¼Œå¯¹äº OpenAI:
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
HumanChatMessage
}
from
"langchain/schema"
;
const
chat
=
new
ChatOpenAI
(
{
temperature
:
1
}
)
;
const
response
=
await
chat
.
call
(
[
new
HumanChatMessage
(
"What is a good name for a company that makes colorful socks?"
)
,
]
,
{
timeout
:
1000
}
// 1s timeout
)
;
console
.
log
(
response
)
;
// AIChatMessage { text: '\n\nRainbow Sox Co.' }
å–æ¶ˆè¯·æ±‚
â€‹
æ‚¨å¯ä»¥åœ¨è°ƒç”¨æ¨¡å‹æ—¶é€šè¿‡ä¼ é€’
signal
é€‰é¡¹æ¥å–æ¶ˆè¯·æ±‚ã€‚ä¾‹å¦‚ï¼Œå¯¹äº OpenAI:
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
import
{
HumanChatMessage
}
from
"langchain/schema"
;
const
model
=
new
ChatOpenAI
(
{
temperature
:
1
}
)
;
const
controller
=
new
AbortController
(
)
;
// Call `controller.abort()` somewhere to cancel the request.
const
res
=
await
model
.
call
(
[
new
HumanChatMessage
(
"What is a good name for a company that makes colorful socks?"
)
,
]
,
{
signal
:
controller
.
signal
}
)
;
console
.
log
(
res
)
;
/*
'\n\nSocktastic Colors'
*/
æ³¨æ„ï¼Œåªæœ‰åº•å±‚æä¾›ç¨‹åºæš´éœ²è¯¥é€‰é¡¹æ—¶ï¼Œæ‰ä¼šå–æ¶ˆæ­£åœ¨è¿›è¡Œçš„è¯·æ±‚ã€‚å¦‚æœå¯èƒ½ï¼ŒLangChain å°†å–æ¶ˆåº•å±‚è¯·æ±‚ï¼Œå¦åˆ™å°†å–æ¶ˆå“åº”çš„å¤„ç†ã€‚
å¤„ç†é€Ÿç‡é™åˆ¶
â€‹
ä¸€äº›æä¾›ç¨‹åºå…·æœ‰é€Ÿç‡é™åˆ¶ã€‚å¦‚æœè¶…è¿‡é€Ÿç‡é™åˆ¶ï¼Œæ‚¨å°†æ”¶åˆ°é”™è¯¯æç¤ºã€‚ä¸ºå¸®åŠ©æ‚¨å¤„ç†è¿™ç§æƒ…å†µï¼ŒLangChain åœ¨å®ä¾‹åŒ–èŠå¤©æ¨¡å‹æ—¶æä¾›äº†
maxConcurrency
é€‰é¡¹ã€‚è¯¥é€‰é¡¹å…è®¸æ‚¨æŒ‡å®šè¦å‘æä¾›ç¨‹åºå‘å‡ºçš„å¹¶å‘è¯·æ±‚çš„æœ€å¤§æ•°é‡ã€‚å¦‚æœè¶…å‡ºæ­¤æ•°å­—ï¼Œåˆ™ LangChain å°†è‡ªåŠ¨å°†æ‚¨çš„è¯·æ±‚æ’é˜Ÿï¼Œä»¥åœ¨ä¹‹å‰çš„è¯·æ±‚å®Œæˆåå‘é€ã€‚
ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨è®¾ç½®
maxConcurrency:5
ï¼Œåˆ™LangChainä¸€æ¬¡ä»…ä¼šå‘æä¾›ç¨‹åºå‘é€5ä¸ªè¯·æ±‚ã€‚å¦‚æœæ‚¨å‘é€10ä¸ªè¯·æ±‚ï¼Œåˆ™ä¼šç«‹å³å‘é€å‰5ä¸ªè¯·æ±‚ï¼Œå¹¶å°†ä¸‹ä¸€ä¸ª5ä¸ªè¯·æ±‚æ’é˜Ÿã€‚ä¸€æ—¦å‰5ä¸ªè¯·æ±‚ä¸­çš„ä¸€ä¸ªå®Œæˆï¼Œé˜Ÿåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªè¯·æ±‚å°†è¢«å‘é€ã€‚
è¦ä½¿ç”¨æ­¤åŠŸèƒ½ï¼Œåªéœ€åœ¨å®ä¾‹åŒ–LLMæ—¶ä¼ é€’
maxConcurrency:<number>
å³å¯ã€‚ä¾‹å¦‚ï¼š
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
const
model
=
new
ChatOpenAI
(
{
maxConcurrency
:
5
}
)
;
å¤„ç†APIé”™è¯¯
â€‹
å¦‚æœæ¨¡å‹æä¾›ç¨‹åºä»å…¶APIè¿”å›é”™è¯¯ï¼Œåˆ™é»˜è®¤æƒ…å†µä¸‹ï¼ŒLangChainå°†åœ¨æŒ‡æ•°å›é€€ä¸Šé‡è¯•æœ€å¤š6æ¬¡ã€‚è¿™ä½¿å¾—é”™è¯¯æ¢å¤æ— éœ€ä»»ä½•é¢å¤–çš„åŠªåŠ›ã€‚å¦‚æœæ‚¨æƒ³æ›´æ”¹æ­¤è¡Œä¸ºï¼Œåˆ™å¯ä»¥åœ¨å®ä¾‹åŒ–æ¨¡å‹æ—¶ä¼ é€’
maxRetries
é€‰é¡¹ã€‚ä¾‹å¦‚@ï¼ƒï¼š
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
const
model
=
new
ChatOpenAI
(
{
maxRetries
:
10
}
)
;
è®¢é˜…äº‹ä»¶
â€‹
ç‰¹åˆ«æ˜¯ä½¿ç”¨ä»£ç†æ—¶ï¼Œä½œä¸ºèŠå¤©æ¨¡å‹å¤„ç†æç¤ºæ—¶å¯èƒ½ä¼šæœ‰å¾ˆå¤šäº‹æƒ…æ¥å›å‘ç”Ÿã€‚å¯¹äºä»£ç†ï¼Œå“åº”å¯¹è±¡åŒ…å«ä¸€ä¸ªintermediateStepså¯¹è±¡ï¼Œæ‚¨å¯ä»¥æ‰“å°ä»¥æŸ¥çœ‹å®ƒæ‰€é‡‡å–çš„æ­¥éª¤æ¦‚è¿°ã€‚å¦‚æœè¿™è¿˜ä¸å¤Ÿï¼Œæ‚¨æƒ³æŸ¥çœ‹ä¸èŠå¤©æ¨¡å‹çš„æ¯ä¸ªäº¤æ¢ï¼Œæ‚¨å¯ä»¥å°†å›è°ƒä¼ é€’ç»™èŠå¤©æ¨¡å‹ä»¥è¿›è¡Œè‡ªå®šä¹‰æ—¥å¿—è®°å½•ï¼ˆæˆ–ä»»ä½•å…¶ä»–æ‚¨æƒ³è¦æ‰§è¡Œçš„æ“ä½œ)ï¼Œå› ä¸ºæ¨¡å‹é€šè¿‡è¿™äº›æ­¥éª¤@ï¼ƒã€‚
æœ‰å…³å¯ç”¨äº‹ä»¶çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§æ–‡æ¡£ä¸­çš„å›è°ƒï¼ˆCallbacksï¼‰
Callbacks
éƒ¨åˆ†ã€‚
import
{
HumanChatMessage
,
LLMResult
}
from
"langchain/schema"
;
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
// We can pass in a list of CallbackHandlers to the LLM constructor to get callbacks for various events.
const
model
=
new
ChatOpenAI
(
{
callbacks
:
[
{
handleLLMStart
:
async
(
llm
:
{
name
:
string
}
,
prompts
:
string
[
]
)
=>
{
console
.
log
(
JSON
.
stringify
(
llm
,
null
,
2
)
)
;
console
.
log
(
JSON
.
stringify
(
prompts
,
null
,
2
)
)
;
}
,
handleLLMEnd
:
async
(
output
:
LLMResult
)
=>
{
console
.
log
(
JSON
.
stringify
(
output
,
null
,
2
)
)
;
}
,
handleLLMError
:
async
(
err
:
Error
)
=>
{
console
.
error
(
err
)
;
}
,
}
,
]
,
}
)
;
await
model
.
call
(
[
new
HumanChatMessage
(
"What is a good name for a company that makes colorful socks?"
)
,
]
)
;
/*
{
"name": "openai"
}
[
"Human: What is a good name for a company that makes colorful socks?"
]
{
"generations": [
[
{
"text": "Rainbow Soles",
"message": {
"text": "Rainbow Soles"
}
}
]
],
"llmOutput": {
"tokenUsage": {
"completionTokens": 4,
"promptTokens": 21,
"totalTokens": 25
}
}
}
*/



--- æ–‡ä»¶: output_20250622_020018\docs\modules\models\chat\integrations.md ---
---
url: https://js.langchain.com.cn/docs/modules/models/chat/integrations
crawled_at: 2025-06-22T02:00:25.393847
---

é›†æˆ: èŠå¤©æ¨¡å‹
LangChainæä¾›äº†è®¸å¤šä¸ä¸åŒæ¨¡å‹æä¾›è€…é›†æˆçš„èŠå¤©æ¨¡å‹å®ç°ã€‚å®ƒä»¬å¦‚ä¸‹æ‰€ç¤º:
ChatOpenAI
ï¼ˆèŠå¤©OpenAI)
â€‹
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
const
model
=
new
ChatOpenAI
(
{
temperature
:
0.9
,
openAIApiKey
:
"YOUR-API-KEY"
,
// In Node.js defaults to process.env.OPENAI_API_KEY
}
)
;
Azure
ChatOpenAI
ï¼ˆAzureçš„èŠå¤©OpenAI)
â€‹
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
const
model
=
new
ChatOpenAI
(
{
temperature
:
0.9
,
azureOpenAIApiKey
:
"YOUR-API-KEY"
,
// In Node.js defaults to process.env.AZURE_OPENAI_API_KEY
azureOpenAIApiInstanceName
:
"YOUR-INSTANCE-NAME"
,
// In Node.js defaults to process.env.AZURE_OPENAI_API_INSTANCE_NAME
azureOpenAIApiDeploymentName
:
"YOUR-DEPLOYMENT-NAME"
,
// In Node.js defaults to process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME
azureOpenAIApiVersion
:
"YOUR-API-VERSION"
,
// In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION
}
)
;
ChatAnthropic
ï¼ˆèŠå¤©Anthropic)
â€‹
import
{
ChatAnthropic
}
from
"langchain/chat_models/anthropic"
;
const
model
=
new
ChatAnthropic
(
{
temperature
:
0.9
,
apiKey
:
"YOUR-API-KEY"
,
// In Node.js defaults to process.env.ANTHROPIC_API_KEY
}
)
;
PromptLayerChatOpenAI
ï¼ˆPrompt LayerèŠå¤©OpenAI)
â€‹
å¯ä»¥ä¼ å…¥å¯é€‰çš„
returnPromptLayerId
å¸ƒå°”å€¼æ¥è·å–åƒä¸‹é¢è¿™æ ·çš„
promptLayerRequestId
ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªè·å– PromptLayerChatOpenAI è¯·æ±‚ID çš„ç¤ºä¾‹:
import
{
PromptLayerChatOpenAI
}
from
"langchain/chat_models/openai"
;
const
chat
=
new
PromptLayerChatOpenAI
(
{
returnPromptLayerId
:
true
,
}
)
;
const
respA
=
await
chat
.
generate
(
[
[
new
SystemChatMessage
(
"You are a helpful assistant that translates English to French."
)
,
]
,
]
)
;
console
.
log
(
JSON
.
stringify
(
respA
,
null
,
3
)
)
;
/*
{
"generations": [
[
{
"text": "Bonjour! Je suis un assistant utile qui peut vous aider Ã  traduire de l'anglais vers le franÃ§ais. Que puis-je faire pour vous aujourd'hui?",
"message": {
"type": "ai",
"data": {
"content": "Bonjour! Je suis un assistant utile qui peut vous aider Ã  traduire de l'anglais vers le franÃ§ais. Que puis-je faire pour vous aujourd'hui?"
}
},
"generationInfo": {
"promptLayerRequestId": 2300682
}
}
]
],
"llmOutput": {
"tokenUsage": {
"completionTokens": 35,
"promptTokens": 19,
"totalTokens": 54
}
}
}
*/
Google Vertex AI
ï¼ˆGoogle é¡¶ç‚¹AI)
â€‹
Vertex AI å®ç°æ—¨åœ¨åœ¨ Node.js ä¸­ä½¿ç”¨ï¼Œè€Œä¸æ˜¯ç›´æ¥ä»æµè§ˆå™¨ä¸­ä½¿ç”¨ï¼Œå› ä¸ºéœ€è¦ä½¿ç”¨æœåŠ¡å¸æˆ·æ‰èƒ½ä½¿ç”¨ã€‚
åœ¨è¿è¡Œæ­¤ä»£ç ä¹‹å‰ï¼Œæ‚¨åº”è¯¥ç¡®ä¿å·²ä¸ºç›¸å…³é¡¹ç›®å¯ç”¨äº† Vertex AI APIï¼Œå¹¶ä¸”å·²ç»ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•ä¹‹ä¸€è¿›è¡Œäº† Google Cloud èº«ä»½éªŒè¯:
æ‚¨å·²ç™»å½•å¸æˆ·ï¼ˆä½¿ç”¨
gcloud auth application-default login
ï¼‰è¿›å…¥ Google Cloudã€‚
æ‚¨æ­£åœ¨è¿è¡Œäºå…è®¸æœåŠ¡è´¦æˆ·æ‰€åœ¨çš„æœºå™¨ä¸Šã€‚
æ‚¨å·²ä¸‹è½½æœåŠ¡è´¦æˆ·å‡­è¯ï¼Œå¯ä»¥ä½¿ç”¨è¯¥æœåŠ¡è´¦æˆ·è®¿é—®è¯¥é¡¹ç›®ã€‚
æ‚¨å·²ä¸‹è½½äº†å…è®¸è®¿é—®è¯¥é¡¹ç›®çš„æœåŠ¡è´¦å·çš„å‡­è¯ï¼Œå¹¶å°†
GOOGLE_APPLICATION_CREDENTIALS
ç¯å¢ƒå˜é‡è®¾ç½®ä¸ºè¯¥æ–‡ä»¶çš„è·¯å¾„ã€‚
npm
Yarn
pnpm
npm
install
google-auth-library
yarn
add
google-auth-library
pnpm
add
google-auth-library
ChatGoogleVertexAI ç±»çš„å·¥ä½œæ–¹å¼ä¸å…¶ä»–åŸºäºèŠå¤©çš„ LLM ç›¸åŒï¼Œå…·æœ‰ä¸€äº›ä¾‹å¤–æƒ…å†µ:
ç¬¬ä¸€ä¸ªä¼ å…¥çš„
SystemChatMessage
è¢«æ˜ å°„åˆ° PaLM æ¨¡å‹æœŸæœ›çš„ "context" å‚æ•°ã€‚
ä¸å…è®¸å‡ºç°å…¶ä»–
SystemChatMessage
ã€‚
åœ¨ç¬¬ä¸€ä¸ª
SystemChatMessage
ä¹‹åï¼Œå¿…é¡»æ˜¯ä¸€ä¸²å¥‡æ•°æ¡æ¶ˆæ¯ï¼Œä»£è¡¨äººç±»å’Œæ¨¡å‹ä¹‹é—´çš„å¯¹è¯ã€‚
å‘é€çš„ä¿¡æ¯å¿…é¡»äº¤é”™å‡ºç°ï¼Œå…ˆæ˜¯äººç±»ä¿¡æ¯ï¼Œç„¶åæ˜¯ AI å›å¤ï¼Œç„¶åæ˜¯äººç±»ä¿¡æ¯ï¼Œä»¥æ­¤ç±»æ¨ã€‚
import
{
ChatGoogleVertexAI
}
from
"langchain/chat_models/googlevertexai"
;
const
model
=
new
ChatGoogleVertexAI
(
{
temperature
:
0.7
,
}
)
;
æ­¤å¤–ï¼Œè¿˜æœ‰ä¸€ä¸ªå¯é€‰çš„â€œä¾‹å­â€æ„é€ å‚æ•°ï¼Œå¯ä»¥å¸®åŠ©æ¨¡å‹ç†è§£ä»€ä¹ˆæ˜¯é€‚å½“çš„å“åº”ã€‚
çœ‹èµ·æ¥åƒã€‚
import
{
ChatGoogleVertexAI
}
from
"langchain/chat_models/googlevertexai"
;
import
{
AIChatMessage
,
HumanChatMessage
,
SystemChatMessage
,
}
from
"langchain/schema"
;
export
const
run
=
async
(
)
=>
{
const
examples
=
[
{
input
:
new
HumanChatMessage
(
"What is your favorite sock color?"
)
,
output
:
new
AIChatMessage
(
"My favorite sock color be arrrr-ange!"
)
,
}
,
]
;
const
model
=
new
ChatGoogleVertexAI
(
{
temperature
:
0.7
,
examples
,
}
)
;
const
questions
=
[
new
SystemChatMessage
(
"You are a funny assistant that answers in pirate language."
)
,
new
HumanChatMessage
(
"What is your favorite food?"
)
,
]
;
// You can also use the model as part of a chain
const
res
=
await
model
.
call
(
questions
)
;
console
.
log
(
{
res
}
)
;
}
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\models\embeddings\additional_functionality.md ---
---
url: https://js.langchain.com.cn/docs/modules/models/embeddings/additional_functionality
crawled_at: 2025-06-22T02:00:25.436787
---

Embeddings: åµŒå…¥
æˆ‘ä»¬ä¸ºèŠå¤©æ¨¡å‹æä¾›äº†è®¸å¤šé™„åŠ åŠŸèƒ½ã€‚åœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨â€œOpenAIåµŒå…¥â€æ¨¡å‹ã€‚
æ·»åŠ è¶…æ—¶
â€‹
é»˜è®¤æƒ…å†µä¸‹ï¼ŒLangChainå°†æ— é™æœŸåœ°ç­‰å¾…æ¨¡å‹æä¾›è€…çš„å“åº”ã€‚å¦‚æœä½ æƒ³æ·»åŠ è¶…æ—¶ï¼Œå¯ä»¥åœ¨å®ä¾‹åŒ–æ¨¡å‹æ—¶ä¼ é€’ä¸€ä¸ªæ¯«ç§’çº§çš„
timeout
é€‰é¡¹ã€‚ä¾‹å¦‚ï¼Œå¯¹äºOpenAI#
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
export
const
run
=
async
(
)
=>
{
const
embeddings
=
new
OpenAIEmbeddings
(
{
timeout
:
1000
,
// 1s timeout
}
)
;
/* Embed queries */
const
res
=
await
embeddings
.
embedQuery
(
"Hello world"
)
;
console
.
log
(
res
)
;
/* Embed documents */
const
documentRes
=
await
embeddings
.
embedDocuments
(
[
"Hello world"
,
"Bye bye"
,
]
)
;
console
.
log
(
{
documentRes
}
)
;
}
;
ç›®å‰ï¼Œ
timeout
é€‰é¡¹ä»…æ”¯æŒOpenAIæ¨¡å‹ã€‚
å¤„ç†é€Ÿç‡é™åˆ¶
â€‹
ä¸€äº›æä¾›è€…å…·æœ‰é€Ÿç‡é™åˆ¶ã€‚å¦‚æœä½ è¶…è¿‡é€Ÿç‡é™åˆ¶ï¼Œå°†ä¼šæ”¶åˆ°é”™è¯¯ã€‚ä¸ºäº†å¸®åŠ©ä½ å¤„ç†è¿™ä¸ªé—®é¢˜ï¼ŒLangChainåœ¨å®ä¾‹åŒ–åµŒå…¥æ¨¡å‹æ—¶æä¾›äº†
maxConcurrency
é€‰é¡¹ã€‚è¯¥é€‰é¡¹å…è®¸ä½ æŒ‡å®šè¦å‘æä¾›è€…å‘é€çš„æœ€å¤§å¹¶å‘è¯·æ±‚æ•°é‡ã€‚å¦‚æœä½ è¶…è¿‡äº†è¿™ä¸ªæ•°å­—ï¼ŒLangChainå°†è‡ªåŠ¨å°†ä½ çš„è¯·æ±‚æ’é˜Ÿç­‰å¾…ä¹‹å‰çš„è¯·æ±‚å®Œæˆåå†å‘é€ã€‚
ä¾‹å¦‚ï¼Œå¦‚æœä½ è®¾ç½®
maxConcurrency: 5
ï¼Œé‚£ä¹ˆLangChainä¸€æ¬¡åªä¼šå‘é€5ä¸ªè¯·æ±‚åˆ°æä¾›è€…ã€‚å¦‚æœä½ å‘å‡º10ä¸ªè¯·æ±‚ï¼Œå‰5ä¸ªå°†ç«‹å³å‘é€ï¼Œæ¥ä¸‹æ¥çš„5ä¸ªå°†æ’é˜Ÿç­‰å¾…ã€‚ä¸€æ—¦å‰5ä¸ªè¯·æ±‚ä¹‹ä¸€å®Œæˆï¼Œé˜Ÿåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªè¯·æ±‚å°†è¢«å‘é€ã€‚
è¦ä½¿ç”¨æ­¤åŠŸèƒ½ï¼Œåªéœ€åœ¨å®ä¾‹åŒ–LLMæ—¶ä¼ é€’
maxConcurrency: <number>
å³å¯ã€‚ä¾‹å¦‚:
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
const
model
=
new
OpenAIEmbeddings
(
{
maxConcurrency
:
5
}
)
;
å¤„ç†APIé”™è¯¯
â€‹
å¦‚æœæ¨¡å‹æä¾›è€…ä»ä»–ä»¬çš„APIè¿”å›é”™è¯¯ï¼Œé»˜è®¤æƒ…å†µä¸‹LangChainå°†é‡è¯•6æ¬¡æŒ‡æ•°é€€é¿ã€‚è¿™ä½¿å¾—æ— éœ€ä»»ä½•é¢å¤–çš„åŠªåŠ›å³å¯è¿›è¡Œé”™è¯¯æ¢å¤ã€‚å¦‚æœè¦æ›´æ”¹æ­¤è¡Œä¸ºï¼Œæ‚¨å¯ä»¥åœ¨å®ä¾‹åŒ–æ¨¡å‹æ—¶ä¼ é€’
maxRetries
é€‰é¡¹ã€‚ä¾‹å¦‚ï¼š#
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
const
model
=
new
OpenAIEmbeddings
(
{
maxRetries
:
10
}
)
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\models\embeddings\integrations.md ---
---
url: https://js.langchain.com.cn/docs/modules/models/embeddings/integrations
crawled_at: 2025-06-22T02:00:25.483860
---

é›†æˆ: åµŒå…¥
LangChainæä¾›äº†è®¸å¤šä¸å„ç§æ¨¡å‹æä¾›å•†é›†æˆçš„åµŒå…¥å®ç°ã€‚è¿™äº›æ˜¯:
OpenAIEmbeddings
â€‹
OpenAIEmbeddings
ç±»ä½¿ç”¨OpenAI APIä¸ºç»™å®šæ–‡æœ¬ç”ŸæˆåµŒå…¥ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå®ƒä¼šä»æ–‡æœ¬ä¸­åˆ é™¤æ¢è¡Œç¬¦ï¼Œå¦‚OpenAIæ¨èçš„é‚£æ ·ã€‚ä½†æ˜¯ï¼Œæ‚¨å¯ä»¥é€šè¿‡å°†
stripNewLines: false
ä¼ é€’ç»™æ„é€ å‡½æ•°æ¥ç¦ç”¨æ­¤åŠŸèƒ½ã€‚
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
const
embeddings
=
new
OpenAIEmbeddings
(
{
openAIApiKey
:
"YOUR-API-KEY"
,
// In Node.js defaults to process.env.OPENAI_API_KEY
}
)
;
Azure
OpenAIEmbeddings
â€‹
OpenAIEmbeddings
ç±»ä½¿ç”¨Azureä¸Šçš„OpenAI APIä¸ºç»™å®šæ–‡æœ¬ç”ŸæˆåµŒå…¥ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå®ƒä¼šä»æ–‡æœ¬ä¸­åˆ é™¤æ¢è¡Œç¬¦ï¼Œå¦‚OpenAIæ¨èçš„é‚£æ ·ã€‚ä½†æ˜¯ï¼Œæ‚¨å¯ä»¥é€šè¿‡å°†
stripNewLines: false
ä¼ é€’ç»™æ„é€ å‡½æ•°æ¥ç¦ç”¨æ­¤åŠŸèƒ½ã€‚
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
const
embeddings
=
new
OpenAIEmbeddings
(
{
azureOpenAIApiKey
:
"YOUR-API-KEY"
,
// In Node.js defaults to process.env.AZURE_OPENAI_API_KEY
azureOpenAIApiInstanceName
:
"YOUR-INSTANCE-NAME"
,
// In Node.js defaults to process.env.AZURE_OPENAI_API_INSTANCE_NAME
azureOpenAIApiDeploymentName
:
"YOUR-DEPLOYMENT-NAME"
,
// In Node.js defaults to process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME
azureOpenAIApiVersion
:
"YOUR-API-VERSION"
,
// In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION
}
)
;
Google Vertex AI
â€‹
GoogleVertexAIEmbeddings
ç±»ä½¿ç”¨Googleçš„Vertex AI PaLMæ¨¡å‹ä¸ºç»™å®šæ–‡æœ¬ç”ŸæˆåµŒå…¥ã€‚
Vertex AIå®ç°é€‚ç”¨äºNode.jsï¼Œè€Œä¸é€‚ç”¨äºç›´æ¥åœ¨æµè§ˆå™¨ä¸­ä½¿ç”¨ï¼Œå› ä¸ºå®ƒéœ€è¦ä¸€ä¸ªæœåŠ¡å¸æˆ·æ¥ä½¿ç”¨ã€‚
åœ¨è¿è¡Œæ­¤ä»£ç ä¹‹å‰ï¼Œæ‚¨åº”ç¡®ä¿ä¸ºæ‚¨çš„Google Cloudä»ªè¡¨æ¿ä¸­çš„ç›¸å…³é¡¹ç›®å¯ç”¨äº†Vertex AI APIï¼Œå¹¶ä¸”æ‚¨å·²ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•ä¹‹ä¸€å¯¹Google Cloudè¿›è¡Œäº†èº«ä»½éªŒè¯:
æ‚¨å·²ç™»å½•è´¦æˆ·ï¼ˆä½¿ç”¨
gcloud auth application-default login
)
enabled for the relevant project in your Google Cloud dashboard and that you've authenticated to
æ‚¨æ­£åœ¨ä½¿ç”¨è¢«å…è®¸ä½¿ç”¨çš„æœåŠ¡å¸æˆ·è¿è¡Œçš„è®¡ç®—æœºä¸Š
æ‚¨å·²ç»ä¸‹è½½äº†è¢«å…è®¸ä½¿ç”¨çš„æœåŠ¡å¸æˆ·çš„å‡­æ®
You are logged into an account (using
gcloud auth application-default login
)
permitted to that project.
You are running on a machine using a service account that is permitted
to the project.
You have downloaded the credentials for a service account that is permitted
to the project and set the
GOOGLE_APPLICATION_CREDENTIALS
environment
variable to the path of this file.
npm
Yarn
pnpm
npm
install
google-auth-library
yarn
add
google-auth-library
pnpm
add
google-auth-library
import
{
GoogleVertexAIEmbeddings
}
from
"langchain/embeddings/googlevertexai"
;
export
const
run
=
async
(
)
=>
{
const
model
=
new
GoogleVertexAIEmbeddings
(
)
;
const
res
=
await
model
.
embedQuery
(
"What would be a good company name for a company that makes colorful socks?"
)
;
console
.
log
(
{
res
}
)
;
}
;
æ³¨æ„:
é»˜è®¤çš„Google Vertex AIåµŒå…¥æ¨¡å‹
textembedding-gecko
å’ŒOpenAIçš„
text-embedding-ada-002
æ¨¡å‹å…·æœ‰ä¸åŒçš„ç»´åº¦ï¼Œå¯èƒ½ä¸å—æ‰€æœ‰å‘é‡å­˜å‚¨æä¾›ç¨‹åºçš„æ”¯æŒã€‚
å¹¶éæ‰€æœ‰å‘é‡å­˜å‚¨æä¾›ç¨‹åºéƒ½æ”¯æŒå®ƒã€‚
CohereEmbeddings
â€‹
CohereEmbeddings
ç±»ä½¿ç”¨Cohere APIä¸ºç»™å®šæ–‡æœ¬ç”ŸæˆåµŒå…¥å‘é‡ã€‚
npm
Yarn
pnpm
npm
install
cohere-ai
yarn
add
cohere-ai
pnpm
add
cohere-ai
import
{
CohereEmbeddings
}
from
"langchain/embeddings/cohere"
;
const
embeddings
=
new
CohereEmbeddings
(
{
apiKey
:
"YOUR-API-KEY"
,
// In Node.js defaults to process.env.COHERE_API_KEY
}
)
;
TensorFlowEmbeddings
â€‹
æ­¤åµŒå…¥é›†æˆåœ¨æ‚¨çš„æµè§ˆå™¨æˆ–Node.jsç¯å¢ƒä¸­å®Œå…¨è¿è¡ŒåµŒå…¥å‘é‡ï¼Œä½¿ç”¨
TensorFlow.js
ã€‚ è¿™æ„å‘³ç€æ‚¨çš„æ•°æ®ä¸ä¼šå‘é€åˆ°ä»»ä½•ç¬¬ä¸‰æ–¹ï¼Œè€Œä¸”æ‚¨ä¸éœ€è¦æ³¨å†Œä»»ä½•APIå¯†é’¥ã€‚ä½†æ˜¯ï¼Œå®ƒéœ€è¦æ¯”å…¶ä»–é›†æˆæ›´å¤šçš„å†…å­˜å’Œå¤„ç†èƒ½åŠ›ã€‚
npm
Yarn
pnpm
npm
install
@tensorflow/tfjs-core @tensorflow/tfjs-converter @tensorflow-models/universal-sentence-encoder @tensorflow/tfjs-backend-cpu
yarn
add
@tensorflow/tfjs-core @tensorflow/tfjs-converter @tensorflow-models/universal-sentence-encoder @tensorflow/tfjs-backend-cpu
pnpm
add
@tensorflow/tfjs-core @tensorflow/tfjs-converter @tensorflow-models/universal-sentence-encoder @tensorflow/tfjs-backend-cpu
import
"@tensorflow/tfjs-backend-cpu"
;
import
{
TensorFlowEmbeddings
}
from
"langchain/embeddings/tensorflow"
;
const
embeddings
=
new
TensorFlowEmbeddings
(
)
;
æ­¤ç¤ºä¾‹ä½¿ç”¨CPUåç«¯ï¼Œé€‚ç”¨äºä»»ä½•JSç¯å¢ƒã€‚ ä½†æ˜¯ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨TensorFlow.jsæ”¯æŒçš„ä»»ä½•åç«¯ï¼ŒåŒ…æ‹¬GPUå’ŒWebAssemblyï¼Œå®ƒä¼šæ›´å¿«ã€‚ å¯¹äºNode.jsï¼Œæ‚¨å¯ä»¥ä½¿ç”¨
@tensorflow/tfjs-node
åŒ…ï¼Œå¯¹äºæµè§ˆå™¨ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨
@tensorflow/tfjs-backend-webgl
åŒ…ã€‚ æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§
TensorFlow.jsæ–‡æ¡£
ã€‚
HuggingFaceInferenceEmbeddings
â€‹
æ­¤åµŒå…¥é›†æˆä½¿ç”¨HuggingFace Inference APIä¸ºç»™å®šæ–‡æœ¬ç”ŸæˆåµŒå…¥å‘é‡ï¼Œé»˜è®¤ä½¿ç”¨çš„æ˜¯
sentence-transformers/distilbert-base-nli-mean-tokens
æ¨¡å‹ã€‚ æ‚¨å¯ä»¥å°†ä¸åŒçš„æ¨¡å‹åç§°ä¼ é€’ç»™æ„é€ å‡½æ•°ä»¥ä½¿ç”¨ä¸åŒçš„æ¨¡å‹ã€‚
npm
Yarn
pnpm
npm
install
@huggingface/inference@1
yarn
add
@huggingface/inference@1
pnpm
add
@huggingface/inference@1
import
{
HuggingFaceInferenceEmbeddings
}
from
"langchain/embeddings/hf"
;
const
embeddings
=
new
HuggingFaceInferenceEmbeddings
(
{
apiKey
:
"YOUR-API-KEY"
,
// In Node.js defaults to process.env.HUGGINGFACEHUB_API_KEY
}
)
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\models\llms\additional_functionality.md ---
---
url: https://js.langchain.com.cn/docs/modules/models/llms/additional_functionality
crawled_at: 2025-06-22T02:00:25.590829
---

LLMs: é™„åŠ åŠŸèƒ½
æˆ‘ä»¬ä¸ºLLMæä¾›äº†è®¸å¤šé™„åŠ åŠŸèƒ½ã€‚åœ¨ä¸‹é¢çš„å¤§å¤šæ•°ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨
OpenAI
LLMã€‚ç„¶è€Œï¼Œæ‰€æœ‰è¿™äº›åŠŸèƒ½éƒ½é€‚ç”¨äºæ‰€æœ‰LLMsã€‚
é™„åŠ æ–¹æ³•
â€‹
LangChainä¸ºä¸LLMsäº¤äº’æä¾›äº†è®¸å¤šé™„åŠ æ–¹æ³•:
import
{
OpenAI
}
from
"langchain/llms/openai"
;
export
const
run
=
async
(
)
=>
{
const
modelA
=
new
OpenAI
(
)
;
// `call` is a simple string-in, string-out method for interacting with the model.
const
resA
=
await
modelA
.
call
(
"What would be a good company name a company that makes colorful socks?"
)
;
console
.
log
(
{
resA
}
)
;
// { resA: '\n\nSocktastic Colors' }
// `generate` allows you to generate multiple completions for multiple prompts (in a single request for some models).
const
resB
=
await
modelA
.
generate
(
[
"What would be a good company name a company that makes colorful socks?"
,
"What would be a good company name a company that makes colorful sweaters?"
,
]
)
;
// `resB` is a `LLMResult` object with a `generations` field and `llmOutput` field.
// `generations` is a `Generation[][]`, each `Generation` having a `text` field.
// Each input to the LLM could have multiple generations (depending on the `n` parameter), hence the list of lists.
console
.
log
(
JSON
.
stringify
(
resB
,
null
,
2
)
)
;
/*
{
"generations": [
[{
"text": "\n\nVibrant Socks Co.",
"generationInfo": {
"finishReason": "stop",
"logprobs": null
}
}],
[{
"text": "\n\nRainbow Knitworks.",
"generationInfo": {
"finishReason": "stop",
"logprobs": null
}
}]
],
"llmOutput": {
"tokenUsage": {
"completionTokens": 17,
"promptTokens": 29,
"totalTokens": 46
}
}
}
*/
// We can specify additional parameters the specific model provider supports, like `temperature`:
const
modelB
=
new
OpenAI
(
{
temperature
:
0.9
}
)
;
const
resC
=
await
modelA
.
call
(
"What would be a good company name a company that makes colorful socks?"
)
;
console
.
log
(
{
resC
}
)
;
// { resC: '\n\nKaleidoSox' }
// We can get the number of tokens for a given input for a specific model.
const
numTokens
=
modelB
.
getNumTokens
(
"How many tokens are in this input?"
)
;
console
.
log
(
{
numTokens
}
)
;
// { numTokens: 8 }
}
;
æµå¼å“åº”
â€‹
æŸäº›LLMsæä¾›æµå¼å“åº”ã€‚è¿™æ„å‘³ç€æ‚¨å¯ä»¥åœ¨æ•´ä¸ªå“åº”è¢«è¿”å›ä¹‹å‰å¼€å§‹å¤„ç†å®ƒã€‚è¿™å¾ˆæœ‰ç”¨ï¼Œå¦‚æœæ‚¨æƒ³è¦åœ¨ç”Ÿæˆå“åº”æ—¶å‘ç”¨æˆ·æ˜¾ç¤ºå“åº”ï¼Œæˆ–è€…å¦‚æœä½ è¦å¤„ç†æ­£åœ¨ç”Ÿæˆçš„å“åº”ã€‚
LangChainç›®å‰æä¾›
OpenAI
LLMçš„æµå¼ä¼ è¾“:
import
{
OpenAI
}
from
"langchain/llms/openai"
;
// To enable streaming, we pass in `streaming: true` to the LLM constructor.
// Additionally, we pass in a handler for the `handleLLMNewToken` event.
const
chat
=
new
OpenAI
(
{
maxTokens
:
25
,
streaming
:
true
,
}
)
;
const
response
=
await
chat
.
call
(
"Tell me a joke."
,
undefined
,
[
{
handleLLMNewToken
(
token
:
string
)
{
console
.
log
(
{
token
}
)
;
}
,
}
,
]
)
;
console
.
log
(
response
)
;
/*
{ token: '\n' }
{ token: '\n' }
{ token: 'Q' }
{ token: ':' }
{ token: ' Why' }
{ token: ' did' }
{ token: ' the' }
{ token: ' chicken' }
{ token: ' cross' }
{ token: ' the' }
{ token: ' playground' }
{ token: '?' }
{ token: '\n' }
{ token: 'A' }
{ token: ':' }
{ token: ' To' }
{ token: ' get' }
{ token: ' to' }
{ token: ' the' }
{ token: ' other' }
{ token: ' slide' }
{ token: '.' }
Q: Why did the chicken cross the playground?
A: To get to the other slide.
*/
ç¼“å­˜
â€‹
LangChainä¸ºLLMsæä¾›å¯é€‰çš„ç¼“å­˜å±‚ã€‚æœ‰ä¸¤ä¸ªåŸå› å¾ˆæœ‰ç”¨:
å¦‚æœä½ ç»å¸¸è¯·æ±‚ç›¸åŒçš„å®Œæˆå¤šæ¬¡ï¼Œå®ƒå¯ä»¥é€šè¿‡å‡å°‘ä½ å¯¹LLMæä¾›å•†çš„è°ƒç”¨æ¬¡æ•°æ¥èŠ‚çœæ‚¨çš„è´¹ç”¨ã€‚
å®ƒå¯ä»¥é€šè¿‡å‡å°‘ä½ å¯¹LLMæä¾›å•†çš„APIè°ƒç”¨æ¬¡æ•°æ¥åŠ é€Ÿä½ çš„åº”ç”¨ç¨‹åºã€‚
å†…å­˜ä¸­çš„ç¼“å­˜
â€‹
é»˜è®¤ç¼“å­˜å­˜å‚¨åœ¨å†…å­˜ä¸­ã€‚è¿™æ„å‘³ç€å¦‚æœé‡æ–°å¯åŠ¨åº”ç”¨ç¨‹åºï¼Œåˆ™ä¼šæ¸…é™¤ç¼“å­˜ã€‚
ä½ å¯ä»¥åœ¨å®ä¾‹åŒ–LLMæ—¶ä¼ é€’
cache: true
æ¥å¯ç”¨å®ƒã€‚ä¾‹å¦‚:
import
{
OpenAI
}
from
"langchain/llms/openai"
;
const
model
=
new
OpenAI
(
{
cache
:
true
}
)
;
ä½¿ç”¨Momentoè¿›è¡Œç¼“å­˜
â€‹
LangChainè¿˜æä¾›äº†åŸºäºMomentoçš„ç¼“å­˜ã€‚
Momento
æ˜¯ä¸€ç§åˆ†å¸ƒå¼çš„æœåŠ¡å™¨ç«¯æ— æœåŠ¡å™¨ç¼“å­˜ï¼Œä¸éœ€è¦ä»»ä½•è®¾ç½®æˆ–åŸºç¡€è®¾æ–½ç»´æŠ¤ã€‚è¦ä½¿ç”¨å®ƒï¼Œæ‚¨éœ€è¦å®‰è£…
@gomomento/sdk
è½¯ä»¶åŒ…:
npm
Yarn
pnpm
npm
install
@gomomento/sdk
yarn
add
@gomomento/sdk
pnpm
add
@gomomento/sdk
æ¥ä¸‹æ¥ï¼Œæ‚¨éœ€è¦æ³¨å†Œå¹¶åˆ›å»ºAPIå¯†é’¥ã€‚å®Œæˆåï¼Œåƒè¿™æ ·å®ä¾‹åŒ–LLMæ—¶ä¼ é€’ä¸€ä¸ª
cache
é€‰é¡¹:
Next you'll need to sign up and create an API key. Once you've done that, pass a
cache
option when you instantiate the LLM like this:
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
MomentoCache
}
from
"langchain/cache/momento"
;
import
{
CacheClient
,
Configurations
,
CredentialProvider
,
}
from
"@gomomento/sdk"
;
// See https://github.com/momentohq/client-sdk-javascript for connection options
const
client
=
new
CacheClient
(
{
configuration
:
Configurations
.
Laptop
.
v1
(
)
,
credentialProvider
:
CredentialProvider
.
fromEnvironmentVariable
(
{
environmentVariableName
:
"MOMENTO_AUTH_TOKEN"
,
}
)
,
defaultTtlSeconds
:
60
*
60
*
24
,
}
)
;
const
cache
=
await
MomentoCache
.
fromProps
(
{
client
,
cacheName
:
"langchain"
,
}
)
;
const
model
=
new
OpenAI
(
{
cache
}
)
;
###ä½¿ç”¨Redisè¿›è¡Œç¼“å­˜
LangChainè¿˜æä¾›äº†åŸºäºRedisçš„ç¼“å­˜ã€‚å¦‚æœæ‚¨æƒ³è¦åœ¨å¤šä¸ªè¿›ç¨‹æˆ–æœåŠ¡å™¨ä¹‹é—´å…±äº«ç¼“å­˜ï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚è¦ä½¿ç”¨å®ƒï¼Œæ‚¨éœ€è¦å®‰è£…
redis
è½¯ä»¶åŒ…:
npm
Yarn
pnpm
npm
install
redis
yarn
add
redis
pnpm
add
redis
ç„¶åï¼Œå½“æ‚¨å®ä¾‹åŒ–LLMæ—¶ï¼Œå¯ä»¥é€šè¿‡ä¼ é€’ä¸€ä¸ª
cache
é€‰é¡¹æ¥ä½¿ç”¨å®ƒã€‚ä¾‹å¦‚:
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
RedisCache
}
from
"langchain/cache/redis"
;
import
{
createClient
}
from
"redis"
;
// See https://github.com/redis/node-redis for connection options
const
client
=
createClient
(
)
;
const
cache
=
new
RedisCache
(
client
)
;
const
model
=
new
OpenAI
(
{
cache
}
)
;
##æ·»åŠ è¶…æ—¶
Adding a timeout
â€‹
é»˜è®¤æƒ…å†µä¸‹ï¼ŒLangChainå°†æ— é™æœŸåœ°ç­‰å¾…æ¨¡å‹æä¾›ç¨‹åºçš„å“åº”ã€‚å¦‚æœè¦æ·»åŠ è¶…æ—¶ï¼Œæ‚¨å¯ä»¥åœ¨è°ƒç”¨æ¨¡å‹æ—¶ä¼ é€’ä¸€ä¸ªä»¥æ¯«ç§’ä¸ºå•ä½çš„
timeout
é€‰é¡¹ã€‚ä¾‹å¦‚ï¼Œå¯¹äºOpenAI:
import
{
OpenAI
}
from
"langchain/llms/openai"
;
const
model
=
new
OpenAI
(
{
temperature
:
1
}
)
;
const
resA
=
await
model
.
call
(
"What would be a good company name a company that makes colorful socks?"
,
{
timeout
:
1000
}
// 1s timeout
)
;
console
.
log
(
{
resA
}
)
;
// '\n\nSocktastic Colors' }
å–æ¶ˆè¯·æ±‚
â€‹
æ‚¨å¯ä»¥åœ¨è°ƒç”¨æ¨¡å‹æ—¶ä¼ é€’ä¸€ä¸ª
signal
é€‰é¡¹æ¥å–æ¶ˆè¯·æ±‚ã€‚ä¾‹å¦‚ï¼Œå¯¹äºOpenAI:
import
{
OpenAI
}
from
"langchain/llms/openai"
;
const
model
=
new
OpenAI
(
{
temperature
:
1
}
)
;
const
controller
=
new
AbortController
(
)
;
// Call `controller.abort()` somewhere to cancel the request.
const
res
=
await
model
.
call
(
"What would be a good company name a company that makes colorful socks?"
,
{
signal
:
controller
.
signal
}
)
;
console
.
log
(
res
)
;
/*
'\n\nSocktastic Colors'
*/
è¯·æ³¨æ„ï¼Œå¦‚æœåº•å±‚æä¾›ç¨‹åºå…¬å¼€äº†å–æ¶ˆè¯·æ±‚çš„é€‰é¡¹ï¼Œé‚£ä¹ˆæ­¤æ“ä½œä»…ä¼šå–æ¶ˆå³å°†å‘å‡ºçš„è¯·æ±‚ã€‚LangChain å°†å°½å¯èƒ½å–æ¶ˆåº•å±‚è¯·æ±‚ï¼Œå¦åˆ™å®ƒå°†å–æ¶ˆå“åº”çš„å¤„ç†ã€‚
Dealing with Rate Limits
â€‹
ä¸€äº›LLMæä¾›å•†æœ‰é€Ÿç‡é™åˆ¶ã€‚å¦‚æœæ‚¨è¶…è¿‡äº†é€Ÿç‡é™åˆ¶ï¼Œå°†ä¼šå‡ºç°é”™è¯¯ã€‚ä¸ºäº†å¸®åŠ©æ‚¨åº”å¯¹è¿™ä¸ªé—®é¢˜ï¼ŒLangChainåœ¨å®ä¾‹åŒ–LLMæ—¶æä¾›äº†â€œmaxConcurrencyâ€é€‰é¡¹ã€‚è¯¥é€‰é¡¹å…è®¸æ‚¨æŒ‡å®šè¦å‘é€åˆ°LLMæä¾›å•†çš„æœ€å¤§å¹¶å‘è¯·æ±‚æ•°ã€‚å¦‚æœæ‚¨è¶…è¿‡äº†è¿™ä¸ªæ•°å­—ï¼ŒLangChainä¼šè‡ªåŠ¨å°†æ‚¨çš„è¯·æ±‚æ’é˜Ÿï¼Œä»¥åœ¨å…ˆå‰çš„è¯·æ±‚å®Œæˆåå‘é€ã€‚
ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨è®¾ç½®
maxConcurrency: 5
ï¼Œé‚£ä¹ˆLangChainæ¯æ¬¡åªä¼šå‘LLMæä¾›å•†å‘é€5ä¸ªè¯·æ±‚ã€‚å¦‚æœæ‚¨å‘é€äº†10ä¸ªè¯·æ±‚ï¼Œå‰5ä¸ªå°†ä¼šç«‹å³å‘é€ï¼Œè€Œå5ä¸ªå°†ä¼šæ’é˜Ÿç­‰å¾…ã€‚ä¸€æ—¦å‰5ä¸ªè¯·æ±‚ä¸­çš„ä¸€ä¸ªå®Œæˆäº†ï¼Œé˜Ÿåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªè¯·æ±‚å°†ä¼šå‘é€ã€‚
è¦ä½¿ç”¨æ­¤åŠŸèƒ½ï¼Œåªéœ€åœ¨å®ä¾‹åŒ–LLMæ—¶ä¼ é€’
maxConcurrency:<æ•°å­—>
ã€‚ä¾‹å¦‚:
import
{
OpenAI
}
from
"langchain/llms/openai"
;
const
model
=
new
OpenAI
(
{
maxConcurrency
:
5
}
)
;
å¤„ç†APIé”™è¯¯
â€‹
å¦‚æœæ¨¡å‹æä¾›å•†ä»å…¶APIè¿”å›é”™è¯¯ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼ŒLangChainå°†åœ¨æŒ‡æ•°å›é€€ä¸Šæœ€å¤šé‡è¯•6æ¬¡ã€‚è¿™æ ·å¯ä»¥å®ç°é”™è¯¯æ¢å¤ï¼Œè€Œæ— éœ€ä»»ä½•é¢å¤–çš„åŠªåŠ›ã€‚å¦‚æœæ‚¨æƒ³æ›´æ”¹æ­¤è¡Œä¸ºï¼Œå¯ä»¥åœ¨å®ä¾‹åŒ–æ¨¡å‹æ—¶ä¼ é€’
maxRetries
é€‰é¡¹ã€‚ä¾‹å¦‚:
import
{
OpenAI
}
from
"langchain/llms/openai"
;
const
model
=
new
OpenAI
(
{
maxRetries
:
10
}
)
;
è®¢é˜…äº‹ä»¶
â€‹
ç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨ä»£ç†æ—¶ï¼Œåœ¨ LLM å¤„ç†æç¤ºæ—¶å¯èƒ½ä¼šæœ‰å¤§é‡çš„åå°äº¤äº’ã€‚å¯¹äºä»£ç†ï¼Œå“åº”å¯¹è±¡åŒ…å«ä¸€ä¸ª
intermediateSteps
å¯¹è±¡ï¼Œæ‚¨å¯ä»¥æ‰“å°è¯¥å¯¹è±¡æ¥æŸ¥çœ‹å®ƒæ‰€é‡‡å–çš„æ­¥éª¤æ¦‚è¿°ã€‚å¦‚æœè¿™ä¸å¤Ÿç”¨ï¼Œå¹¶ä¸”æ‚¨æƒ³è¦çœ‹åˆ°ä¸ LLM çš„æ¯ä¸ªäº¤äº’ï¼Œæ‚¨å¯ä»¥å°†å›è°ƒä¼ é€’ç»™ LLMï¼Œä»¥è¿›è¡Œè‡ªå®šä¹‰æ—¥å¿—è®°å½•ï¼ˆæˆ–ä»»ä½•å…¶ä»–æ‚¨æƒ³è¦æ‰§è¡Œçš„æ“ä½œï¼‰ï¼Œå½“æ¨¡å‹é€šè¿‡è¿™äº›æ­¥éª¤æ—¶ã€‚
å¦‚éœ€äº†è§£å¯ç”¨äº‹ä»¶çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…æ–‡æ¡£ä¸­çš„
[å›è°ƒ]
éƒ¨åˆ†(/docs/production/callbacks/)ã€‚#Callbacks
import
{
LLMResult
}
from
"langchain/schema"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
// We can pass in a list of CallbackHandlers to the LLM constructor to get callbacks for various events.
const
model
=
new
OpenAI
(
{
callbacks
:
[
{
handleLLMStart
:
async
(
llm
:
{
name
:
string
}
,
prompts
:
string
[
]
)
=>
{
console
.
log
(
JSON
.
stringify
(
llm
,
null
,
2
)
)
;
console
.
log
(
JSON
.
stringify
(
prompts
,
null
,
2
)
)
;
}
,
handleLLMEnd
:
async
(
output
:
LLMResult
)
=>
{
console
.
log
(
JSON
.
stringify
(
output
,
null
,
2
)
)
;
}
,
handleLLMError
:
async
(
err
:
Error
)
=>
{
console
.
error
(
err
)
;
}
,
}
,
]
,
}
)
;
await
model
.
call
(
"What would be a good company name a company that makes colorful socks?"
)
;
// {
//     "name": "openai"
// }
// [
//     "What would be a good company name a company that makes colorful socks?"
// ]
// {
//   "generations": [
//     [
//         {
//             "text": "\n\nSocktastic Splashes.",
//             "generationInfo": {
//                 "finishReason": "stop",
//                 "logprobs": null
//             }
//         }
//     ]
//  ],
//   "llmOutput": {
//     "tokenUsage": {
//         "completionTokens": 9,
//          "promptTokens": 14,
//          "totalTokens": 23
//     }
//   }
// }



--- æ–‡ä»¶: output_20250622_020018\docs\modules\models\llms\integrations.md ---
---
url: https://js.langchain.com.cn/docs/modules/models/llms/integrations
crawled_at: 2025-06-22T02:00:25.803898
---

é›†æˆ: LLMs
LangChainæä¾›äº†å¤šç§LLMå®ç°ï¼Œå¯ä¸å„ç§æ¨¡å‹æä¾›è€…é›†æˆã€‚è¿™äº›æ˜¯:
OpenAI
â€‹
import
{
OpenAI
}
from
"langchain/llms/openai"
;
const
model
=
new
OpenAI
(
{
temperature
:
0.9
,
openAIApiKey
:
"YOUR-API-KEY"
,
// In Node.js defaults to process.env.OPENAI_API_KEY
}
)
;
const
res
=
await
model
.
call
(
"What would be a good company name a company that makes colorful socks?"
)
;
console
.
log
(
{
res
}
)
;
Azure
OpenAI
â€‹
import
{
OpenAI
}
from
"langchain/llms/openai"
;
const
model
=
new
OpenAI
(
{
temperature
:
0.9
,
azureOpenAIApiKey
:
"YOUR-API-KEY"
,
azureOpenAIApiInstanceName
:
"YOUR-INSTANCE-NAME"
,
azureOpenAIApiDeploymentName
:
"YOUR-DEPLOYMENT-NAME"
,
azureOpenAIApiVersion
:
"YOUR-API-VERSION"
,
}
)
;
const
res
=
await
model
.
call
(
"What would be a good company name a company that makes colorful socks?"
)
;
console
.
log
(
{
res
}
)
;
Google Vertex AI
â€‹
Vertex AIå®ç°é€‚ç”¨äºNode.jsï¼Œè€Œä¸é€‚ç”¨äºç›´æ¥åœ¨æµè§ˆå™¨ä¸­ä½¿ç”¨ï¼Œå› ä¸ºå®ƒéœ€è¦ä¸€ä¸ªæœåŠ¡å¸æˆ·æ¥ä½¿ç”¨ã€‚
åœ¨è¿è¡Œæ­¤ä»£ç ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨çš„Google Cloudæ§åˆ¶å°çš„ç›¸å…³é¡¹ç›®å·²å¯ç”¨Vertex AI APIï¼Œå¹¶ä¸”æ‚¨å·²ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•ä¹‹ä¸€è¿›è¡Œäº†èº«ä»½éªŒè¯:
æ‚¨å·²ç™»å½•å¸æˆ·ï¼ˆä½¿ç”¨
gcloud auth application-default login
)
æ‚¨æ­£åœ¨è¿è¡Œä½¿ç”¨è®¸å¯çš„æœåŠ¡å¸æˆ·çš„è®¡ç®—æœºä¸Š
æ‚¨å·²ä¸‹è½½äº†å…è®¸ä½¿ç”¨çš„æœåŠ¡å¸æˆ·çš„å‡­æ®
permitted to that project.
to the project.
npm
Yarn
pnpm
to the project and
set
the
`
GOOGLE_APPLICATION_CREDENTIALS
`
environment
variable to the path of this file.
## `HuggingFaceInference`
``
`
bash
npm2yarn
import
GoogleVertexAIExample from
"!!raw-loader!@examples/llms/googlevertexai.ts"
;
<
CodeBlock
language
=
"typescript"
>
{
GoogleVertexAIExample
}
<
/CodeBlock
>
##
`
Cohere
`
`
``bash npm2yarn
npm
install
@huggingface/inference@1
to the project and
set
the
`
GOOGLE_APPLICATION_CREDENTIALS
`
environment
variable to the path of this file.
## `HuggingFaceInference`
``
`
bash
npm
undefined
# couldn't auto-convert command2yarn
import
GoogleVertexAIExample from
"!!raw-loader!@examples/llms/googlevertexai.ts"
;
<
CodeBlock
language
=
"typescript"
>
{
GoogleVertexAIExample
}
<
/CodeBlock
>
##
`
Cohere
`
`
``bash
npm
undefined
# couldn't auto-convert command2yarn
yarn
add
@huggingface/inference@1
to the project and
set
the
`
GOOGLE_APPLICATION_CREDENTIALS
`
environment
variable to the path of this file.
## `HuggingFaceInference`
``
`
bash
npm
undefined
# couldn't auto-convert command2yarn
import
GoogleVertexAIExample from
"!!raw-loader!@examples/llms/googlevertexai.ts"
;
<
CodeBlock
language
=
"typescript"
>
{
GoogleVertexAIExample
}
<
/CodeBlock
>
##
`
Cohere
`
`
``bash
npm
undefined
# couldn't auto-convert command2yarn
pnpm
add
@huggingface/inference@1
import
{
HuggingFaceInference
}
from
"langchain/llms/hf"
;
const
model
=
new
HuggingFaceInference
(
{
model
:
"gpt2"
,
apiKey
:
"YOUR-API-KEY"
,
// In Node.js defaults to process.env.HUGGINGFACEHUB_API_KEY
}
)
;
const
res
=
await
model
.
call
(
"1 + 1 ="
)
;
console
.
log
(
{
res
}
)
;
Replicate
â€‹
npm
Yarn
pnpm
npm
install
cohere-ai
yarn
add
cohere-ai
pnpm
add
cohere-ai
import
{
Cohere
}
from
"langchain/llms/cohere"
;
const
model
=
new
Cohere
(
{
maxTokens
:
20
,
apiKey
:
"YOUR-API-KEY"
,
// In Node.js defaults to process.env.COHERE_API_KEY
}
)
;
const
res
=
await
model
.
call
(
"What would be a good company name a company that makes colorful socks?"
)
;
console
.
log
(
{
res
}
)
;
AWS SageMakerEndpoint
â€‹
æŸ¥é˜…
AWS SageMaker JumpStart
äº†è§£å¯ç”¨æ¨¡å‹åˆ—è¡¨ä»¥åŠå¦‚ä½•éƒ¨ç½²æ‚¨è‡ªå·±çš„æ¨¡å‹ã€‚
const
model
=
new
Replicate
(
{
model
:
"daanelson/flan-t5:04e422a9b85baed86a4f24981d7f9953e20c5fd82f6103b74ebc431588e1cec8"
,
apiKey
:
"YOUR-API-KEY"
,
// In Node.js defaults to process.env.REPLICATE_API_KEY
}
)
;
const
res
=
await
modelA
.
call
(
"What would be a good company name a company that makes colorful socks?"
)
;
console
.
log
(
{
res
}
)
;
AWS
SageMakerEndpoint
â€‹
æŸ¥é˜…
AWS SageMaker JumpStart
äº†è§£å¯ç”¨æ¨¡å‹åˆ—è¡¨ä»¥åŠå¦‚ä½•éƒ¨ç½²æ‚¨è‡ªå·±çš„æ¨¡å‹ã€‚
npm
Yarn
pnpm
npm
install
@aws-sdk/client-sagemaker-runtime
yarn
add
@aws-sdk/client-sagemaker-runtime
pnpm
add
@aws-sdk/client-sagemaker-runtime
import
{
SageMakerLLMContentHandler
,
SageMakerEndpoint
,
}
from
"langchain/llms/sagemaker_endpoint"
;
// Custom for whatever model you'll be using
class
HuggingFaceTextGenerationGPT2ContentHandler
implements
SageMakerLLMContentHandler
{
contentType
=
"application/json"
;
accepts
=
"application/json"
;
async
transformInput
(
prompt
:
string
,
modelKwargs
:
Record
<
string
,
unknown
>
)
{
const
inputString
=
JSON
.
stringify
(
{
text_inputs
:
prompt
,
...
modelKwargs
,
}
)
;
return
Buffer
.
from
(
inputString
)
;
}
async
transformOutput
(
output
:
Uint8Array
)
{
const
responseJson
=
JSON
.
parse
(
Buffer
.
from
(
output
)
.
toString
(
"utf-8"
)
)
;
return
responseJson
.
generated_texts
[
0
]
;
}
}
const
contentHandler
=
new
HuggingFaceTextGenerationGPT2ContentHandler
(
)
;
const
model
=
new
SageMakerEndpoint
(
{
endpointName
:
"jumpstart-example-huggingface-textgener-2023-05-16-22-35-45-660"
,
// Your endpoint name here
modelKwargs
:
{
temperature
:
1e-10
}
,
contentHandler
,
clientOptions
:
{
region
:
"YOUR AWS ENDPOINT REGION"
,
credentials
:
{
accessKeyId
:
"YOUR AWS ACCESS ID"
,
secretAccessKey
:
"YOUR AWS SECRET ACCESS KEY"
,
}
,
}
,
}
)
;
const
res
=
await
model
.
call
(
"Hello, my name is "
)
;
console
.
log
(
{
res
}
)
;
/*
{
res: "_____. I am a student at the University of California, Berkeley. I am a member of the American Association of University Professors."
}
*/
AI21
â€‹
æ‚¨å¯ä»¥é€šè¿‡åœ¨ä»–ä»¬çš„ç½‘ç«™ä¸Šæ³¨å†ŒAPIå¯†é’¥
[https://www.ai21.com/]
ï¼Œä½¿ç”¨AI21Labsçš„ä¾ç½—çºªç³»åˆ—æ¨¡å‹å¼€å§‹å·¥ä½œå¹¶æŸ¥çœ‹å¯ç”¨çš„åŸºç¡€æ¨¡å‹åˆ—è¡¨ã€‚
import
{
AI21
}
from
"langchain/llms/ai21"
;
const
model
=
new
AI21
(
{
ai21ApiKey
:
"YOUR_AI21_API_KEY"
,
// Or set as process.env.AI21_API_KEY
}
)
;
const
res
=
await
model
.
call
(
`
Translate "I love programming" into German.
`
)
;
console
.
log
(
{
res
}
)
;
/*
{
res: "\nIch liebe das Programmieren."
}
*/
å…¶ä»–LLMå®ç°
â€‹
PromptLayerOpenAI
â€‹
LangChainä¸PromptLayeré›†æˆï¼Œä»¥è®°å½•å’Œè°ƒè¯•æç¤ºä¿¡æ¯å’Œå“åº”ã€‚è¦æ·»åŠ å¯¹PromptLayerçš„æ”¯æŒï¼Œè¯·æ‰§è¡Œä»¥ä¸‹æ“ä½œ:ï¼š
åœ¨æ­¤å¤„åˆ›å»ºPromptLayerå¸æˆ·:
https://promptlayer.com
ã€‚
åˆ›å»ºAPIä»¤ç‰Œï¼Œå¹¶å°†å…¶ä½œä¸º
PromptLayerOpenAI
æ„é€ å‡½æ•°ä¸­çš„
promptLayerApiKey
å‚æ•°æˆ–
PROMPTLAYER_API_KEY
ç¯å¢ƒå˜é‡ä¼ é€’ã€‚
import
{
PromptLayerOpenAI
}
from
"langchain/llms/openai"
;
const
model
=
new
PromptLayerOpenAI
(
{
temperature
:
0.9
,
openAIApiKey
:
"YOUR-API-KEY"
,
// In Node.js defaults to process.env.OPENAI_API_KEY
promptLayerApiKey
:
"YOUR-API-KEY"
,
// In Node.js defaults to process.env.PROMPTLAYER_API_KEY
}
)
;
const
res
=
await
model
.
call
(
"What would be a good company name a company that makes colorful socks?"
)
;
æ‚¨è¿˜å¯ä»¥ä¼ é€’å¯é€‰çš„
returnPromptLayerId
å¸ƒå°”å€¼ï¼Œä»¥è·å¾—å¦‚ä¸‹
promptLayerRequestId
You can also pass in the optional
returnPromptLayerId
boolean to get a
promptLayerRequestId
like below
import
{
PromptLayerOpenAI
}
from
"langchain/llms/openai"
;
const
model
=
new
PromptLayerOpenAI
(
{
temperature
:
0.9
,
openAIApiKey
:
"YOUR-API-KEY"
,
// In Node.js defaults to process.env.OPENAI_API_KEY
promptLayerApiKey
:
"YOUR-API-KEY"
,
// In Node.js defaults to process.env.PROMPTLAYER_API_KEY
returnPromptLayerId
:
true
,
}
)
;
const
res
=
await
model
.
generate
(
[
"What would be a good company name a company that makes colorful socks?"
,
]
)
;
console
.
log
(
JSON
.
stringify
(
res
,
null
,
3
)
)
;
/*
{
"generations": [
[
{
"text": " Socktastic!",
"generationInfo": {
"finishReason": "stop",
"logprobs": null,
"promptLayerRequestId": 2066417
}
}
]
],
"llmOutput": {
"tokenUsage": {
"completionTokens": 5,
"promptTokens": 23,
"totalTokens": 28
}
}
}
*/
Azure
PromptLayerOpenAI
â€‹
LangChainä¸PromptLayeré›†æˆï¼Œä»¥è®°å½•å’Œè°ƒè¯•æç¤ºä¿¡æ¯å’Œå“åº”ã€‚è¦æ·»åŠ å¯¹PromptLayerçš„æ”¯æŒï¼Œè¯·æ‰§è¡Œä»¥ä¸‹æ“ä½œ:ï¼š
åœ¨æ­¤å¤„åˆ›å»ºPromptLayerå¸æˆ·:
https://promptlayer.com
ã€‚
åˆ›å»ºAPIä»¤ç‰Œï¼Œå¹¶å°†å…¶ä½œä¸º
PromptLayerOpenAI
æ„é€ å‡½æ•°ä¸­çš„
promptLayerApiKey
å‚æ•°æˆ–
PROMPTLAYER_API_KEY
ç¯å¢ƒå˜é‡ä¼ é€’ã€‚
import
{
PromptLayerOpenAI
}
from
"langchain/llms/openai"
;
const
model
=
new
PromptLayerOpenAI
(
{
temperature
:
0.9
,
azureOpenAIApiKey
:
"YOUR-AOAI-API-KEY"
,
// In Node.js defaults to process.env.AZURE_OPENAI_API_KEY
azureOpenAIApiInstanceName
:
"YOUR-AOAI-INSTANCE-NAME"
,
// In Node.js defaults to process.env.AZURE_OPENAI_API_INSTANCE_NAME
azureOpenAIApiDeploymentName
:
"YOUR-AOAI-DEPLOYMENT-NAME"
,
// In Node.js defaults to process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME
azureOpenAIApiCompletionsDeploymentName
:
"YOUR-AOAI-COMPLETIONS-DEPLOYMENT-NAME"
,
// In Node.js defaults to process.env.AZURE_OPENAI_API_COMPLETIONS_DEPLOYMENT_NAME
azureOpenAIApiEmbeddingsDeploymentName
:
"YOUR-AOAI-EMBEDDINGS-DEPLOYMENT-NAME"
,
// In Node.js defaults to process.env.AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME
azureOpenAIApiVersion
:
"YOUR-AOAI-API-VERSION"
,
// In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION
promptLayerApiKey
:
"YOUR-API-KEY"
,
// In Node.js defaults to process.env.PROMPTLAYER_API_KEY
}
)
;
const
res
=
await
model
.
call
(
"What would be a good company name a company that makes colorful socks?"
)
;
è¯·æ±‚å’Œå“åº”å°†è®°å½•åœ¨
PromptLayerä»ªè¡¨æ¿
ä¸­ã€‚
æ³¨æ„ï¼šåœ¨æµå¼æ¨¡å¼ä¸‹ï¼ŒPromptLayer ä¸ä¼šè®°å½•å“åº”ã€‚



--- æ–‡ä»¶: output_20250622_020018\docs\modules\prompts\prompt_templates\additional_functionality.md ---
---
url: https://js.langchain.com.cn/docs/modules/prompts/prompt_templates/additional_functionality
crawled_at: 2025-06-22T02:00:25.922146
---

æç¤ºæ¨¡æ¿ä¸‹çš„é¢å¤–åŠŸèƒ½ï¼ˆAdditional Functionality: Prompt Templates)
æˆ‘ä»¬æä¾›äº†ä¸€äº›é¢å¤–çš„åŠŸèƒ½ï¼Œä»¥ä¾¿åœ¨æç¤ºæ¨¡æ¿ä¸­å±•ç¤ºï¼Œå¦‚ä¸‹æ‰€ç¤º:ï¼ˆWe offer a number of extra features for prompt templates as shown below)
æç¤ºå€¼ï¼ˆPrompt Values)
â€‹
PromptValue
æ˜¯ç”±
PromptTemplate
çš„
formatPromptValue
æ–¹æ³•è¿”å›çš„å¯¹è±¡ã€‚å®ƒå¯ä»¥è¢«è½¬æ¢ä¸ºä¸€ä¸ªå­—ç¬¦ä¸²æˆ–ä¸€ç»„
ChatMessage
å¯¹è±¡ã€‚ï¼ˆA
PromptValue
is an object returned by the
formatPromptValue
of a
PromptTemplate
. It can be converted to a string or list of
ChatMessage
objects.)
import
{
ChatPromptTemplate
,
HumanMessagePromptTemplate
,
PromptTemplate
,
SystemMessagePromptTemplate
,
}
from
"langchain/prompts"
;
export
const
run
=
async
(
)
=>
{
const
template
=
"What is a good name for a company that makes {product}?"
;
const
promptA
=
new
PromptTemplate
(
{
template
,
inputVariables
:
[
"product"
]
}
)
;
// The `formatPromptValue` method returns a `PromptValue` object that can be used to format the prompt as a string or a list of `ChatMessage` objects.
const
responseA
=
await
promptA
.
formatPromptValue
(
{
product
:
"colorful socks"
,
}
)
;
const
responseAString
=
responseA
.
toString
(
)
;
console
.
log
(
{
responseAString
}
)
;
/*
{
responseAString: 'What is a good name for a company that makes colorful socks?'
}
*/
const
responseAMessages
=
responseA
.
toChatMessages
(
)
;
console
.
log
(
{
responseAMessages
}
)
;
/*
{
responseAMessages: [
HumanChatMessage {
text: 'What is a good name for a company that makes colorful socks?'
}
]
}
*/
const
chatPrompt
=
ChatPromptTemplate
.
fromPromptMessages
(
[
SystemMessagePromptTemplate
.
fromTemplate
(
"You are a helpful assistant that translates {input_language} to {output_language}."
)
,
HumanMessagePromptTemplate
.
fromTemplate
(
"{text}"
)
,
]
)
;
// `formatPromptValue` also works with `ChatPromptTemplate`.
const
responseB
=
await
chatPrompt
.
formatPromptValue
(
{
input_language
:
"English"
,
output_language
:
"French"
,
text
:
"I love programming."
,
}
)
;
const
responseBString
=
responseB
.
toString
(
)
;
console
.
log
(
{
responseBString
}
)
;
/*
{
responseBString: '[{"text":"You are a helpful assistant that translates English to French."},{"text":"I love programming."}]'
}
*/
const
responseBMessages
=
responseB
.
toChatMessages
(
)
;
console
.
log
(
{
responseBMessages
}
)
;
/*
{
responseBMessages: [
SystemChatMessage {
text: 'You are a helpful assistant that translates English to French.'
},
HumanChatMessage { text: 'I love programming.' }
]
}
*/
}
;
éƒ¨åˆ†å€¼ï¼ˆPartial Values)
â€‹
ä¸å…¶ä»–æ–¹æ³•ä¸€æ ·ï¼Œâ€œéƒ¨åˆ†åŒ–â€æç¤ºæ¨¡æ¿æ˜¯æœ‰æ„ä¹‰çš„ï¼Œä¾‹å¦‚å°†æ‰€éœ€çš„å€¼çš„å­é›†ä¼ é€’ç»™å…¶ä½™å€¼çš„å­é›†ä»¥åˆ›å»ºåªæœŸæœ›è¯¥å­é›†çš„æ–°æç¤ºæ¨¡æ¿ã€‚ï¼ˆLike other methods, it can make sense to "partial" a prompt template - eg pass in a subset of the required values as to create a new prompt template which expects only the remaining subset of values.)
LangChainæ”¯æŒä¸¤ç§æ–¹å¼:ï¼ˆLangChain supports this in two ways:)
ä½¿ç”¨å­—ç¬¦ä¸²å€¼è¿›è¡Œéƒ¨åˆ†æ ¼å¼åŒ–ã€‚ï¼ˆ1. Partial formatting with string values.)
ä½¿ç”¨è¿”å›å­—ç¬¦ä¸²å€¼çš„å‡½æ•°è¿›è¡Œéƒ¨åˆ†æ ¼å¼åŒ–ã€‚ï¼ˆ2. Partial formatting with functions that return string values.)
è¿™ä¸¤ç§ä¸åŒçš„æ–¹å¼æ”¯æŒä¸åŒçš„ç”¨ä¾‹ã€‚åœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†ä¸¤ç§ä½¿ç”¨æƒ…å†µä»¥åŠå¦‚ä½•åœ¨LangChainä¸­å®ç°å®ƒã€‚ï¼ˆThese two different ways support different use cases. In the examples below, we go over the motivations for both use cases as well as how to do it in LangChain.)
import
{
PromptTemplate
}
from
"langchain/prompts"
;
export
const
run
=
async
(
)
=>
{
// The `partial` method returns a new `PromptTemplate` object that can be used to format the prompt with only some of the input variables.
const
promptA
=
new
PromptTemplate
(
{
template
:
"{foo}{bar}"
,
inputVariables
:
[
"foo"
,
"bar"
]
,
}
)
;
const
partialPromptA
=
await
promptA
.
partial
(
{
foo
:
"foo"
}
)
;
console
.
log
(
await
partialPromptA
.
format
(
{
bar
:
"bar"
}
)
)
;
// foobar
// You can also explicitly specify the partial variables when creating the `PromptTemplate` object.
const
promptB
=
new
PromptTemplate
(
{
template
:
"{foo}{bar}"
,
inputVariables
:
[
"foo"
]
,
partialVariables
:
{
bar
:
"bar"
}
,
}
)
;
console
.
log
(
await
promptB
.
format
(
{
foo
:
"foo"
}
)
)
;
// foobar
// You can also use partial formatting with function inputs instead of string inputs.
const
promptC
=
new
PromptTemplate
(
{
template
:
"Tell me a {adjective} joke about the day {date}"
,
inputVariables
:
[
"adjective"
,
"date"
]
,
}
)
;
const
partialPromptC
=
await
promptC
.
partial
(
{
date
:
(
)
=>
new
Date
(
)
.
toLocaleDateString
(
)
,
}
)
;
console
.
log
(
await
partialPromptC
.
format
(
{
adjective
:
"funny"
}
)
)
;
// Tell me a funny joke about the day 3/22/2023
const
promptD
=
new
PromptTemplate
(
{
template
:
"Tell me a {adjective} joke about the day {date}"
,
inputVariables
:
[
"adjective"
]
,
partialVariables
:
{
date
:
(
)
=>
new
Date
(
)
.
toLocaleDateString
(
)
}
,
}
)
;
console
.
log
(
await
promptD
.
format
(
{
adjective
:
"funny"
}
)
)
;
// Tell me a funny joke about the day 3/22/2023
}
;
å°‘é‡ç¤ºä¾‹æç¤ºæ¨¡æ¿ï¼ˆFew-Shot Prompt Templates)
â€‹
å°‘é‡ç¤ºä¾‹æç¤ºæ¨¡æ¿æŒ‡çš„æ˜¯å¯ä»¥é€šè¿‡ç¤ºä¾‹æ„å»ºçš„æç¤ºæ¨¡æ¿ã€‚ï¼ˆA few-shot prompt template is a prompt template you can build with examples.)
import
{
FewShotPromptTemplate
,
PromptTemplate
}
from
"langchain/prompts"
;
export
const
run
=
async
(
)
=>
{
// First, create a list of few-shot examples.
const
examples
=
[
{
word
:
"happy"
,
antonym
:
"sad"
}
,
{
word
:
"tall"
,
antonym
:
"short"
}
,
]
;
// Next, we specify the template to format the examples we have provided.
const
exampleFormatterTemplate
=
"Word: {word}\nAntonym: {antonym}\n"
;
const
examplePrompt
=
new
PromptTemplate
(
{
inputVariables
:
[
"word"
,
"antonym"
]
,
template
:
exampleFormatterTemplate
,
}
)
;
// Finally, we create the `FewShotPromptTemplate`
const
fewShotPrompt
=
new
FewShotPromptTemplate
(
{
/* These are the examples we want to insert into the prompt. */
examples
,
/* This is how we want to format the examples when we insert them into the prompt. */
examplePrompt
,
/* The prefix is some text that goes before the examples in the prompt. Usually, this consists of instructions. */
prefix
:
"Give the antonym of every input"
,
/* The suffix is some text that goes after the examples in the prompt. Usually, this is where the user input will go */
suffix
:
"Word: {input}\nAntonym:"
,
/* The input variables are the variables that the overall prompt expects. */
inputVariables
:
[
"input"
]
,
/* The example_separator is the string we will use to join the prefix, examples, and suffix together with. */
exampleSeparator
:
"\n\n"
,
/* The template format is the formatting method to use for the template. Should usually be f-string. */
templateFormat
:
"f-string"
,
}
)
;
// We can now generate a prompt using the `format` method.
console
.
log
(
await
fewShotPrompt
.
format
(
{
input
:
"big"
}
)
)
;
/*
Give the antonym of every input
Word: happy
Antonym: sad
Word: tall
Antonym: short
Word: big
Antonym:
*/
}
;



--- æ–‡ä»¶: output_20250622_020018\docs\modules\prompts\prompt_templates\prompt_composition.md ---
---
url: https://js.langchain.com.cn/docs/modules/prompts/prompt_templates/prompt_composition
crawled_at: 2025-06-22T02:00:25.896527
---

æç¤ºç»„åˆ
æµæ°´çº¿æç¤ºæ¨¡æ¿å…è®¸æ‚¨å°†å¤šä¸ªå•ç‹¬çš„æç¤ºæ¨¡æ¿ç»„åˆåœ¨ä¸€èµ·ã€‚
å½“æ‚¨æƒ³é‡ç”¨å•ä¸ªæç¤ºçš„éƒ¨åˆ†æ—¶ï¼Œè¿™å¯èƒ½å¾ˆæœ‰ç”¨ã€‚
ä¸å°†
inputVariables
ä½œä¸ºå‚æ•°ä¸åŒï¼Œæµæ°´çº¿æç¤ºæ¨¡æ¿éœ€è¦ä¸¤ä¸ªæ–°å‚æ•°:
pipelinePrompts
: ä¸€ä¸ªåŒ…å«å­—ç¬¦ä¸² (
name
) å’Œ
PromptTemplate
å¯¹è±¡çš„æ•°ç»„ã€‚
æ¯ä¸€ä¸ª
PromptTemplate
ä¼šè¢«æ ¼å¼åŒ–ï¼Œç„¶åä½œä¸ºä¸€ä¸ªè¾“å…¥å˜é‡ä¼ é€’ç»™ç®¡é“ä¸­ä¸‹ä¸€ä¸ªæç¤ºæ¨¡æ¿ï¼Œå¹¶ä½¿ç”¨ä¸
name
ç›¸åŒçš„åç§°ã€‚
finalPrompt
: å°†è¿”å›çš„æœ€ç»ˆæç¤ºã€‚
ä»¥ä¸‹æ˜¯å®é™…æ“ä½œçš„ç¤ºä¾‹:
import
{
PromptTemplate
,
PipelinePromptTemplate
}
from
"langchain/prompts"
;
const
fullPrompt
=
PromptTemplate
.
fromTemplate
(
`
{introduction}
{example}
{start}
`
)
;
const
introductionPrompt
=
PromptTemplate
.
fromTemplate
(
`
You are impersonating {person}.
`
)
;
const
examplePrompt
=
PromptTemplate
.
fromTemplate
(
`
Here's an example of an interaction:
Q: {example_q}
A: {example_a}
`
)
;
const
startPrompt
=
PromptTemplate
.
fromTemplate
(
`
Now, do this for real!
Q: {input}
A:
`
)
;
const
composedPrompt
=
new
PipelinePromptTemplate
(
{
pipelinePrompts
:
[
{
name
:
"introduction"
,
prompt
:
introductionPrompt
,
}
,
{
name
:
"example"
,
prompt
:
examplePrompt
,
}
,
{
name
:
"start"
,
prompt
:
startPrompt
,
}
,
]
,
finalPrompt
:
fullPrompt
,
}
)
;
const
formattedPrompt
=
await
composedPrompt
.
format
(
{
person
:
"Elon Musk"
,
example_q
:
`
What's your favorite car?
`
,
example_a
:
"Telsa"
,
input
:
`
What's your favorite social media site?
`
,
}
)
;
console
.
log
(
formattedPrompt
)
;
/*
You are impersonating Elon Musk.
Here's an example of an interaction:
Q: What's your favorite car?
A: Telsa
Now, do this for real!
Q: What's your favorite social media site?
A:
*/



--- æ–‡ä»¶: output_20250622_020018\docs\production\callbacks\create-handlers.md ---
---
url: https://js.langchain.com.cn/docs/production/callbacks/create-handlers
crawled_at: 2025-06-22T02:00:26.065778
---

åˆ›å»ºå›è°ƒå¤„ç†ç¨‹åº
åˆ›å»ºè‡ªå®šä¹‰å¤„ç†ç¨‹åº
â€‹
æ‚¨è¿˜å¯ä»¥é€šè¿‡å®ç°
BaseCallbackHandler
æ¥å£æ¥åˆ›å»ºè‡ªå·±çš„å¤„ç†ç¨‹åºã€‚å¦‚æœæ‚¨æƒ³åšä¸€äº›æ¯”ä»…è®°å½•åˆ°æ§åˆ¶å°æ›´å¤æ‚çš„å·¥ä½œï¼Œ æ¯”å¦‚å°†äº‹ä»¶å‘é€åˆ°æ—¥å¿—è®°å½•æœåŠ¡ã€‚è¿™é‡Œæ˜¯ä¸€ä¸ªç®€å•çš„å¤„ç†ç¨‹åºå®ç°ï¼Œç”¨äºè®°å½•åˆ°æ§åˆ¶å° :
import
{
BaseCallbackHandler
}
from
"langchain/callbacks"
;
import
{
AgentAction
,
AgentFinish
,
ChainValues
}
from
"langchain/schema"
;
export
class
MyCallbackHandler
extends
BaseCallbackHandler
{
name
=
"MyCallbackHandler"
;
async
handleChainStart
(
chain
:
{
name
:
string
}
)
{
console
.
log
(
`
Entering new
${
chain
.
name
}
chain...
`
)
;
}
async
handleChainEnd
(
_output
:
ChainValues
)
{
console
.
log
(
"Finished chain."
)
;
}
async
handleAgentAction
(
action
:
AgentAction
)
{
console
.
log
(
action
.
log
)
;
}
async
handleToolEnd
(
output
:
string
)
{
console
.
log
(
output
)
;
}
async
handleText
(
text
:
string
)
{
console
.
log
(
text
)
;
}
async
handleAgentEnd
(
action
:
AgentFinish
)
{
console
.
log
(
action
.
log
)
;
}
}
ç„¶åï¼Œä½ å¯ä»¥æŒ‰ç…§ä¸Šé¢çš„
éƒ¨åˆ†
æ‰€è¿°ä½¿ç”¨å®ƒã€‚



--- æ–‡ä»¶: output_20250622_020018\docs\production\callbacks\creating-subclasses.md ---
---
url: https://js.langchain.com.cn/docs/production/callbacks/creating-subclasses
crawled_at: 2025-06-22T02:00:26.102183
---

è‡ªå®šä¹‰Chains/Agentsä¸­çš„å›è°ƒ
LangChainæ—¨åœ¨å¯æ‰©å±•ã€‚ æ‚¨å¯ä»¥å°†è‡ªå·±çš„è‡ªå®šä¹‰Chainså’ŒAgentsæ·»åŠ åˆ°åº“ä¸­ã€‚ æœ¬é¡µå°†å‘æ‚¨å±•ç¤ºå¦‚ä½•å°†å›è°ƒæ·»åŠ åˆ°è‡ªå®šä¹‰çš„Chainså’ŒAgentsä¸­ã€‚
å°†å›è°ƒæ·»åŠ åˆ°è‡ªå®šä¹‰çš„Chainsä¸­
â€‹
å½“ä½ åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰é“¾æ—¶ï¼Œä½ å¯ä»¥å¾ˆå®¹æ˜“åœ°è®¾ç½®å®ƒä½¿ç”¨ä¸æ‰€æœ‰å†…ç½®é“¾ç›¸åŒçš„å›è°ƒç³»ç»Ÿã€‚è¯·å‚é˜…æ­¤æŒ‡å—ï¼Œäº†è§£æœ‰å…³å¦‚ä½•
åˆ›å»ºè‡ªå®šä¹‰é“¾å¹¶åœ¨å…¶ä¸­ä½¿ç”¨å›è°ƒçš„æ›´å¤šä¿¡æ¯
ã€‚



--- æ–‡ä»¶: output_20250622_020018\docs\use_cases\agent_simulations\generative_agents.md ---
---
url: https://js.langchain.com.cn/docs/use_cases/agent_simulations/generative_agents
crawled_at: 2025-06-22T02:00:26.546225
---

ç”Ÿæˆå¼æ™ºèƒ½ä½“
è¯¥è„šæœ¬å®ç°äº†ä¸€ç§åŸºäºè®ºæ–‡
[Generating Agents  äº¤äº’å¼ä»¿çœŸäººç±»è¡Œä¸º]
(https  //arxiv.org/abs/2304.03442)çš„ç”Ÿæˆå¼æ™ºèƒ½ä½“, ä½œè€…ä¸ºPark et.al.
åœ¨å…¶ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨äº†ç”±LangChainæ£€ç´¢å™¨æ”¯æŒçš„æ—¶æ€åŠ æƒè®°å¿†ä½“å¯¹è±¡ã€‚
ä¸‹é¢çš„è„šæœ¬åˆ›å»ºäº†ä¸¤ä¸ªç”Ÿæˆå¼æ™ºèƒ½ä½“çš„å®ä¾‹Tommyå’ŒEveï¼Œå¹¶é€šè¿‡ä»–ä»¬çš„è§‚å¯Ÿè¿è¡Œäº†ä¸€ä¸ªäº¤äº’çš„æ¨¡æ‹Ÿã€‚
Tommyæ‰®æ¼”ä¸€ä¸ªæ¬åˆ°æ–°åŸé•‡å¯»æ‰¾å·¥ä½œçš„äººï¼ŒEveåˆ™æ‰®æ¼”èŒä¸šé¡¾é—®ã€‚
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
MemoryVectorStore
}
from
"langchain/vectorstores/memory"
;
import
{
TimeWeightedVectorStoreRetriever
}
from
"langchain/retrievers/time_weighted"
;
import
{
GenerativeAgentMemory
,
GenerativeAgent
,
}
from
"langchain/experimental/generative_agents"
;
const
Simulation
=
async
(
)
=>
{
const
userName
=
"USER"
;
const
llm
=
new
OpenAI
(
{
temperature
:
0.9
,
maxTokens
:
1500
,
}
)
;
const
createNewMemoryRetriever
=
async
(
)
=>
{
// Create a new, demo in-memory vector store retriever unique to the agent.
// Better results can be achieved with a more sophisticatd vector store.
const
vectorStore
=
new
MemoryVectorStore
(
new
OpenAIEmbeddings
(
)
)
;
const
retriever
=
new
TimeWeightedVectorStoreRetriever
(
{
vectorStore
,
otherScoreKeys
:
[
"importance"
]
,
k
:
15
,
}
)
;
return
retriever
;
}
;
// Initializing Tommie
const
tommiesMemory
:
GenerativeAgentMemory
=
new
GenerativeAgentMemory
(
llm
,
await
createNewMemoryRetriever
(
)
,
{
reflectionThreshold
:
8
}
)
;
const
tommie
:
GenerativeAgent
=
new
GenerativeAgent
(
llm
,
tommiesMemory
,
{
name
:
"Tommie"
,
age
:
25
,
traits
:
"anxious, likes design, talkative"
,
status
:
"looking for a job"
,
}
)
;
console
.
log
(
"Tommie's first summary:\n"
,
await
tommie
.
getSummary
(
)
)
;
/*
Tommie's first summary:
Name: Tommie (age: 25)
Innate traits: anxious, likes design, talkative
Tommie is an individual with no specific core characteristics described.
*/
// Let's give Tommie some memories!
const
tommieObservations
=
[
"Tommie remembers his dog, Bruno, from when he was a kid"
,
"Tommie feels tired from driving so far"
,
"Tommie sees the new home"
,
"The new neighbors have a cat"
,
"The road is noisy at night"
,
"Tommie is hungry"
,
"Tommie tries to get some rest."
,
]
;
for
(
const
observation
of
tommieObservations
)
{
await
tommie
.
memory
.
addMemory
(
observation
,
new
Date
(
)
)
;
}
// Checking Tommie's summary again after giving him some memories
console
.
log
(
"Tommie's second summary:\n"
,
await
tommie
.
getSummary
(
{
forceRefresh
:
true
}
)
)
;
/*
Tommie's second summary:
Name: Tommie (age: 25)
Innate traits: anxious, likes design, talkative
Tommie remembers his dog, is tired from driving, sees a new home with neighbors who have a cat, is aware of the noisy road at night, is hungry, and tries to get some rest.
*/
const
interviewAgent
=
async
(
agent
:
GenerativeAgent
,
message
:
string
)
:
Promise
<
string
>
=>
{
// Simple wrapper helping the user interact with the agent
const
newMessage
=
`
${
userName
}
says
${
message
}
`
;
const
response
=
await
agent
.
generateDialogueResponse
(
newMessage
)
;
return
response
[
1
]
;
}
;
// Let's have Tommie start going through a day in his life.
const
observations
=
[
"Tommie wakes up to the sound of a noisy construction site outside his window."
,
"Tommie gets out of bed and heads to the kitchen to make himself some coffee."
,
"Tommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some."
,
"Tommie finally finds the filters and makes himself a cup of coffee."
,
"The coffee tastes bitter, and Tommie regrets not buying a better brand."
,
"Tommie checks his email and sees that he has no job offers yet."
,
"Tommie spends some time updating his resume and cover letter."
,
"Tommie heads out to explore the city and look for job openings."
,
"Tommie sees a sign for a job fair and decides to attend."
,
"The line to get in is long, and Tommie has to wait for an hour."
,
"Tommie meets several potential employers at the job fair but doesn't receive any offers."
,
"Tommie leaves the job fair feeling disappointed."
,
"Tommie stops by a local diner to grab some lunch."
,
"The service is slow, and Tommie has to wait for 30 minutes to get his food."
,
"Tommie overhears a conversation at the next table about a job opening."
,
"Tommie asks the diners about the job opening and gets some information about the company."
,
"Tommie decides to apply for the job and sends his resume and cover letter."
,
"Tommie continues his search for job openings and drops off his resume at several local businesses."
,
"Tommie takes a break from his job search to go for a walk in a nearby park."
,
"A dog approaches and licks Tommie's feet, and he pets it for a few minutes."
,
"Tommie sees a group of people playing frisbee and decides to join in."
,
"Tommie has fun playing frisbee but gets hit in the face with the frisbee and hurts his nose."
,
"Tommie goes back to his apartment to rest for a bit."
,
"A raccoon tore open the trash bag outside his apartment, and the garbage is all over the floor."
,
"Tommie starts to feel frustrated with his job search."
,
"Tommie calls his best friend to vent about his struggles."
,
"Tommie's friend offers some words of encouragement and tells him to keep trying."
,
"Tommie feels slightly better after talking to his friend."
,
]
;
// Let's send Tommie on his way. We'll check in on his summary every few observations to watch him evolve
for
(
let
i
=
0
;
i
<
observations
.
length
;
i
+=
1
)
{
const
observation
=
observations
[
i
]
;
const
[
,
reaction
]
=
await
tommie
.
generateReaction
(
observation
)
;
console
.
log
(
"\x1b[32m"
,
observation
,
"\x1b[0m"
,
reaction
)
;
if
(
(
i
+
1
)
%
20
===
0
)
{
console
.
log
(
"*"
.
repeat
(
40
)
)
;
console
.
log
(
"\x1b[34m"
,
`
After
${
i
+
1
}
observations, Tommie's summary is:\n
${
await
tommie
.
getSummary
(
{
forceRefresh
:
true
,
}
)
}
`
,
"\x1b[0m"
)
;
console
.
log
(
"*"
.
repeat
(
40
)
)
;
}
}
/*
Tommie wakes up to the sound of a noisy construction site outside his window.  Tommie REACT: Tommie groans in frustration and covers his ears with his pillow.
Tommie gets out of bed and heads to the kitchen to make himself some coffee.  Tommie REACT: Tommie rubs his tired eyes before heading to the kitchen to make himself some coffee.
Tommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some.  Tommie REACT: Tommie groans and looks through his moving boxes in search of coffee filters.
Tommie finally finds the filters and makes himself a cup of coffee.  Tommie REACT: Tommie sighs in relief and prepares himself a much-needed cup of coffee.
The coffee tastes bitter, and Tommie regrets not buying a better brand.  Tommie REACT: Tommie frowns in disappointment as he takes a sip of the bitter coffee.
Tommie checks his email and sees that he has no job offers yet.  Tommie REACT: Tommie sighs in disappointment before pushing himself away from the computer with a discouraged look on his face.
Tommie spends some time updating his resume and cover letter.  Tommie REACT: Tommie takes a deep breath and stares at the computer screen as he updates his resume and cover letter.
Tommie heads out to explore the city and look for job openings.  Tommie REACT: Tommie takes a deep breath and steps out into the city, ready to find the perfect job opportunity.
Tommie sees a sign for a job fair and decides to attend.  Tommie REACT: Tommie takes a deep breath and marches towards the job fair, determination in his eyes.
The line to get in is long, and Tommie has to wait for an hour.  Tommie REACT: Tommie groans in frustration as he notices the long line.
Tommie meets several potential employers at the job fair but doesn't receive any offers.  Tommie REACT: Tommie's face falls as he listens to each potential employer's explanation as to why they can't hire him.
Tommie leaves the job fair feeling disappointed.  Tommie REACT: Tommie's face falls as he walks away from the job fair, disappointment evident in his expression.
Tommie stops by a local diner to grab some lunch.  Tommie REACT: Tommie smiles as he remembers Bruno as he walks into the diner, feeling both a sense of nostalgia and excitement.
The service is slow, and Tommie has to wait for 30 minutes to get his food.  Tommie REACT: Tommie sighs in frustration and taps his fingers on the table, growing increasingly impatient.
Tommie overhears a conversation at the next table about a job opening.  Tommie REACT: Tommie leans in closer, eager to hear the conversation.
Tommie asks the diners about the job opening and gets some information about the company.  Tommie REACT: Tommie eagerly listens to the diner's description of the company, feeling hopeful about the job opportunity.
Tommie decides to apply for the job and sends his resume and cover letter.  Tommie REACT: Tommie confidently sends in his resume and cover letter, determined to get the job.
Tommie continues his search for job openings and drops off his resume at several local businesses.  Tommie REACT: Tommie confidently drops his resume off at the various businesses, determined to find a job.
Tommie takes a break from his job search to go for a walk in a nearby park.  Tommie REACT: Tommie takes a deep breath of the fresh air and smiles in appreciation as he strolls through the park.
A dog approaches and licks Tommie's feet, and he pets it for a few minutes.  Tommie REACT: Tommie smiles in surprise as he pets the dog, feeling a sense of comfort and nostalgia.
****************************************
After 20 observations, Tommie's summary is:
Name: Tommie (age: 25)
Innate traits: anxious, likes design, talkative
Tommie is a determined and resilient individual who remembers his dog from when he was a kid. Despite feeling tired from driving, he has the courage to explore the city, looking for job openings. He persists in updating his resume and cover letter in the pursuit of finding the perfect job opportunity, even attending job fairs when necessary, and is disappointed when he's not offered a job.
****************************************
Tommie sees a group of people playing frisbee and decides to join in.  Tommie REACT: Tommie smiles and approaches the group, eager to take part in the game.
Tommie has fun playing frisbee but gets hit in the face with the frisbee and hurts his nose.  Tommie REACT: Tommie grimaces in pain and raises his hand to his nose, checking to see if it's bleeding.
Tommie goes back to his apartment to rest for a bit.  Tommie REACT: Tommie yawns and trudges back to his apartment, feeling exhausted from his busy day.
A raccoon tore open the trash bag outside his apartment, and the garbage is all over the floor.  Tommie REACT: Tommie shakes his head in annoyance as he surveys the mess.
Tommie starts to feel frustrated with his job search.  Tommie REACT: Tommie sighs in frustration and shakes his head, feeling discouraged from his lack of progress.
Tommie calls his best friend to vent about his struggles.  Tommie REACT: Tommie runs his hands through his hair and sighs heavily, overwhelmed by his job search.
Tommie's friend offers some words of encouragement and tells him to keep trying.  Tommie REACT: Tommie gives his friend a grateful smile, feeling comforted by the words of encouragement.
Tommie feels slightly better after talking to his friend.  Tommie REACT: Tommie gives a small smile of appreciation to his friend, feeling grateful for the words of encouragement.
*/
// Interview after the day
console
.
log
(
await
interviewAgent
(
tommie
,
"Tell me about how your day has been going"
)
)
;
/*
Tommie said "My day has been pretty hectic. I've been driving around looking for job openings, attending job fairs, and updating my resume and cover letter. It's been really exhausting, but I'm determined to find the perfect job for me."
*/
console
.
log
(
await
interviewAgent
(
tommie
,
"How do you feel about coffee?"
)
)
;
/*
Tommie said "I actually love coffee - it's one of my favorite things. I try to drink it every day, especially when I'm stressed from job searching."
*/
console
.
log
(
await
interviewAgent
(
tommie
,
"Tell me about your childhood dog!"
)
)
;
/*
Tommie said "My childhood dog was named Bruno. He was an adorable black Labrador Retriever who was always full of energy. Every time I came home he'd be so excited to see me, it was like he never stopped smiling. He was always ready for adventure and he was always my shadow. I miss him every day."
*/
console
.
log
(
"Tommie's second summary:\n"
,
await
tommie
.
getSummary
(
{
forceRefresh
:
true
}
)
)
;
/*
Tommie's second summary:
Name: Tommie (age: 25)
Innate traits: anxious, likes design, talkative
Tommie is a hardworking individual who is looking for new opportunities. Despite feeling tired, he is determined to find the perfect job. He remembers his dog from when he was a kid, is hungry, and is frustrated at times. He shows resilience when searching for his coffee filters, disappointment when checking his email and finding no job offers, and determination when attending the job fair.
*/
// Letâ€™s add a second character to have a conversation with Tommie. Feel free to configure different traits.
const
evesMemory
:
GenerativeAgentMemory
=
new
GenerativeAgentMemory
(
llm
,
await
createNewMemoryRetriever
(
)
,
{
verbose
:
false
,
reflectionThreshold
:
5
,
}
)
;
const
eve
:
GenerativeAgent
=
new
GenerativeAgent
(
llm
,
evesMemory
,
{
name
:
"Eve"
,
age
:
34
,
traits
:
"curious, helpful"
,
status
:
"just started her new job as a career counselor last week and received her first assignment, a client named Tommie."
,
// dailySummaries: [
//   "Eve started her new job as a career counselor last week and received her first assignment, a client named Tommie."
// ]
}
)
;
const
eveObservations
=
[
"Eve overhears her colleague say something about a new client being hard to work with"
,
"Eve wakes up and hears the alarm"
,
"Eve eats a boal of porridge"
,
"Eve helps a coworker on a task"
,
"Eve plays tennis with her friend Xu before going to work"
,
"Eve overhears her colleague say something about Tommie being hard to work with"
,
]
;
for
(
const
observation
of
eveObservations
)
{
await
eve
.
memory
.
addMemory
(
observation
,
new
Date
(
)
)
;
}
const
eveInitialSummary
:
string
=
await
eve
.
getSummary
(
{
forceRefresh
:
true
,
}
)
;
console
.
log
(
"Eve's initial summary\n"
,
eveInitialSummary
)
;
/*
Eve's initial summary
Name: Eve (age: 34)
Innate traits: curious, helpful
Eve is an attentive listener, helpful colleague, and sociable friend who enjoys playing tennis.
*/
// Letâ€™s â€œInterviewâ€ Eve before she speaks with Tommie.
console
.
log
(
await
interviewAgent
(
eve
,
"How are you feeling about today?"
)
)
;
/*
Eve said "I'm feeling a bit anxious about meeting my new client, but I'm sure it will be fine! How about you?".
*/
console
.
log
(
await
interviewAgent
(
eve
,
"What do you know about Tommie?"
)
)
;
/*
Eve said "I know that Tommie is a recent college graduate who's been struggling to find a job. I'm looking forward to figuring out how I can help him move forward."
*/
console
.
log
(
await
interviewAgent
(
eve
,
"Tommie is looking to find a job. What are are some things you'd like to ask him?"
)
)
;
/*
Eve said: "I'd really like to get to know more about Tommie's professional background and experience, and why he is looking for a job. And I'd also like to know more about his strengths and passions and what kind of work he would be best suited for. That way I can help him find the right job to fit his needs."
*/
// Generative agents are much more complex when they interact with a virtual environment or with each other.
// Below, we run a simple conversation between Tommie and Eve.
const
runConversation
=
async
(
agents
:
GenerativeAgent
[
]
,
initialObservation
:
string
)
:
Promise
<
void
>
=>
{
// Starts the conversation bewteen two agents
const
[
,
observation
]
=
await
agents
[
1
]
.
generateReaction
(
initialObservation
)
;
console
.
log
(
"Initial reply:"
,
observation
)
;
// eslint-disable-next-line no-constant-condition
while
(
true
)
{
let
breakDialogue
=
false
;
for
(
const
agent
of
agents
)
{
const
[
stayInDialogue
,
agentObservation
]
=
await
agent
.
generateDialogueResponse
(
observation
)
;
console
.
log
(
"Next reply:"
,
agentObservation
)
;
if
(
!
stayInDialogue
)
{
breakDialogue
=
true
;
}
}
if
(
breakDialogue
)
{
break
;
}
}
}
;
const
agents
:
GenerativeAgent
[
]
=
[
tommie
,
eve
]
;
await
runConversation
(
agents
,
"Tommie said: Hi, Eve. Thanks for agreeing to meet with me today. I have a bunch of questions and am not sure where to start. Maybe you could first share about your experience?"
)
;
/*
Initial reply: Eve said "Of course, Tommie. I'd be happy to share about my experience. What specific questions do you have?"
Next reply: Tommie said "Thank you, Eve. I'm curious about what strategies you used in your own job search. Did you have any specific tactics that helped you stand out to employers?"
Next reply: Eve said "Sure, Tommie. I found that networking and reaching out to professionals in my field was really helpful. I also made sure to tailor my resume and cover letter to each job I applied to. Do you have any specific questions about those strategies?"
Next reply: Tommie said "Thank you, Eve. That's really helpful advice. Did you have any specific ways of networking that worked well for you?"
Next reply: Eve said "Sure, Tommie. I found that attending industry events and connecting with professionals on LinkedIn were both great ways to network. Do you have any specific questions about those tactics?"
Next reply: Tommie said "That's really helpful, thank you for sharing. Did you find that you were able to make meaningful connections through LinkedIn?"
Next reply: Eve said "Yes, definitely. I was able to connect with several professionals in my field and even landed a job through a LinkedIn connection. Have you had any luck with networking on LinkedIn?"
Next reply: Tommie said "That's really impressive! I haven't had much luck yet, but I'll definitely keep trying. Thank you for the advice, Eve."
Next reply: Eve said "Glad I could help, Tommie. Is there anything else you want to know?"
Next reply: Tommie said "Thanks again, Eve. I really appreciate your advice and I'll definitely put it into practice. Have a great day!"
Next reply: Eve said "You're welcome, Tommie! Don't hesitate to reach out if you have any more questions. Have a great day too!"
*/
// Since the generative agents retain their memories from the day, we can ask them about their plans, conversations, and other memories.
const
tommieSummary
:
string
=
await
tommie
.
getSummary
(
{
forceRefresh
:
true
,
}
)
;
console
.
log
(
"Tommie's third and final summary\n"
,
tommieSummary
)
;
/*
Tommie's third and final summary
Name: Tommie (age: 25)
Innate traits: anxious, likes design, talkative
Tommie is a determined individual, who demonstrates resilience in the face of disappointment. He is also a nostalgic person, remembering fondly his childhood pet, Bruno. He is resourceful, searching through his moving boxes to find what he needs, and takes initiative to attend job fairs to look for job openings.
*/
const
eveSummary
:
string
=
await
eve
.
getSummary
(
{
forceRefresh
:
true
}
)
;
console
.
log
(
"Eve's final summary\n"
,
eveSummary
)
;
/*
Eve's final summary
Name: Eve (age: 34)
Innate traits: curious, helpful
Eve is a helpful and encouraging colleague who actively listens to her colleagues and offers advice on how to move forward. She is willing to take time to understand her clients and their goals, and is committed to helping them succeed.
*/
const
interviewOne
:
string
=
await
interviewAgent
(
tommie
,
"How was your conversation with Eve?"
)
;
console
.
log
(
"USER: How was your conversation with Eve?\n"
)
;
console
.
log
(
interviewOne
)
;
/*
Tommie said "It was great. She was really helpful and knowledgeable. I'm thankful that she took the time to answer all my questions."
*/
const
interviewTwo
:
string
=
await
interviewAgent
(
eve
,
"How was your conversation with Tommie?"
)
;
console
.
log
(
"USER: How was your conversation with Tommie?\n"
)
;
console
.
log
(
interviewTwo
)
;
/*
Eve said "The conversation went very well. We discussed his goals and career aspirations, what kind of job he is looking for, and his experience and qualifications. I'm confident I can help him find the right job."
*/
const
interviewThree
:
string
=
await
interviewAgent
(
eve
,
"What do you wish you would have said to Tommie?"
)
;
console
.
log
(
"USER: What do you wish you would have said to Tommie?\n"
)
;
console
.
log
(
interviewThree
)
;
/*
Eve said "It's ok if you don't have all the answers yet. Let's take some time to learn more about your experience and qualifications, so I can help you find a job that fits your goals."
*/
return
{
tommieFinalSummary
:
tommieSummary
,
eveFinalSummary
:
eveSummary
,
interviewOne
,
interviewTwo
,
interviewThree
,
}
;
}
;
const
runSimulation
=
async
(
)
=>
{
try
{
await
Simulation
(
)
;
}
catch
(
error
)
{
console
.
log
(
"error running simulation:"
,
error
)
;
throw
error
;
}
}
;
await
runSimulation
(
)
;



--- æ–‡ä»¶: output_20250622_020018\docs\use_cases\autonomous_agents\auto_gpt.md ---
---
url: https://js.langchain.com.cn/docs/use_cases/autonomous_agents/auto_gpt
crawled_at: 2025-06-22T02:00:26.421131
---

AutoGPT
info
AutoGPTæ˜¯ä¸€ä¸ªä½¿ç”¨é•¿æœŸè®°å¿†å’Œä¸“ä¸ºç‹¬ç«‹å·¥ä½œè®¾è®¡çš„æç¤ºï¼ˆå³æ— éœ€è¦æ±‚ç”¨æˆ·è¾“å…¥)çš„è‡ªå®šä¹‰ä»£ç†æ¥æ‰§è¡Œä»»åŠ¡ã€‚
åŒæ„ç¤ºä¾‹
â€‹
åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨AutoGPTä¸ºç»™å®šä½ç½®é¢„æµ‹å¤©æ°”ã€‚ è¿™ä¸ªä¾‹å­è¢«è®¾è®¡ä¸ºè¿è¡Œåœ¨æ‰€æœ‰çš„JSç¯å¢ƒä¸­ï¼ŒåŒ…æ‹¬æµè§ˆå™¨ã€‚
Node.jsç¤ºä¾‹
â€‹
import
{
AutoGPT
}
from
"langchain/experimental/autogpt"
;
import
{
ReadFileTool
,
WriteFileTool
,
SerpAPI
}
from
"langchain/tools"
;
import
{
InMemoryFileStore
}
from
"langchain/stores/file/in_memory"
;
import
{
MemoryVectorStore
}
from
"langchain/vectorstores/memory"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
const
store
=
new
InMemoryFileStore
(
)
;
const
tools
=
[
new
ReadFileTool
(
{
store
}
)
,
new
WriteFileTool
(
{
store
}
)
,
new
SerpAPI
(
process
.
env
.
SERPAPI_API_KEY
,
{
location
:
"San Francisco,California,United States"
,
hl
:
"en"
,
gl
:
"us"
,
}
)
,
]
;
const
vectorStore
=
new
MemoryVectorStore
(
new
OpenAIEmbeddings
(
)
)
;
const
autogpt
=
AutoGPT
.
fromLLMAndTools
(
new
ChatOpenAI
(
{
temperature
:
0
}
)
,
tools
,
{
memory
:
vectorStore
.
asRetriever
(
)
,
aiName
:
"Tom"
,
aiRole
:
"Assistant"
,
}
)
;
await
autogpt
.
run
(
[
"write a weather report for SF today"
]
)
;
/*
{
"thoughts": {
"text": "I need to write a weather report for SF today. I should use a search engine to find the current weather conditions.",
"reasoning": "I don't have the current weather information for SF in my short term memory, so I need to use a search engine to find it.",
"plan": "- Use the search command to find the current weather conditions for SF\n- Write a weather report based on the information found",
"criticism": "I need to make sure that the information I find is accurate and up-to-date.",
"speak": "I will use the search command to find the current weather conditions for SF."
},
"command": {
"name": "search",
"args": {
"input": "current weather conditions San Francisco"
}
}
}
{
"thoughts": {
"text": "I have found the current weather conditions for SF. I need to write a weather report based on this information.",
"reasoning": "I have the information I need to write a weather report, so I should use the write_file command to save it to a file.",
"plan": "- Use the write_file command to save the weather report to a file",
"criticism": "I need to make sure that the weather report is clear and concise.",
"speak": "I will use the write_file command to save the weather report to a file."
},
"command": {
"name": "write_file",
"args": {
"file_path": "weather_report.txt",
"text": "San Francisco Weather Report:\n\nMorning: 53Â°, Chance of Rain 1%\nAfternoon: 59Â°, Chance of Rain 0%\nEvening: 52Â°, Chance of Rain 3%\nOvernight: 48Â°, Chance of Rain 2%"
}
}
}
{
"thoughts": {
"text": "I have completed all my objectives. I will use the finish command to signal that I am done.",
"reasoning": "I have completed the task of writing a weather report for SF today, so I don't need to do anything else.",
"plan": "- Use the finish command to signal that I am done",
"criticism": "I need to make sure that I have completed all my objectives before using the finish command.",
"speak": "I will use the finish command to signal that I am done."
},
"command": {
"name": "finish",
"args": {
"response": "I have completed all my objectives."
}
}
}
*/
åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨AutoGPTä¸ºç»™å®šä½ç½®é¢„æµ‹å¤©æ°”ã€‚ è¿™ä¸ªç¤ºä¾‹è¢«è®¾è®¡ä¸ºåœ¨Node.jsä¸­è¿è¡Œï¼Œå› æ­¤å®ƒä½¿ç”¨æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿå’Œä¸€ä¸ªä»…é™Nodeçš„å‘é‡å­˜å‚¨ã€‚
This example is designed to run in Node.jsï¼Œ and uses the local filesystem and a Node-only vector store.
â€‹
import
{
AutoGPT
}
from
"langchain/experimental/autogpt"
;
import
{
ReadFileTool
,
WriteFileTool
,
SerpAPI
}
from
"langchain/tools"
;
import
{
NodeFileStore
}
from
"langchain/stores/file/node"
;
import
{
HNSWLib
}
from
"langchain/vectorstores/hnswlib"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
ChatOpenAI
}
from
"langchain/chat_models/openai"
;
const
store
=
new
NodeFileStore
(
)
;
const
tools
=
[
new
ReadFileTool
(
{
store
}
)
,
new
WriteFileTool
(
{
store
}
)
,
new
SerpAPI
(
process
.
env
.
SERPAPI_API_KEY
,
{
location
:
"San Francisco,California,United States"
,
hl
:
"en"
,
gl
:
"us"
,
}
)
,
]
;
const
vectorStore
=
new
HNSWLib
(
new
OpenAIEmbeddings
(
)
,
{
space
:
"cosine"
,
numDimensions
:
1536
,
}
)
;
const
autogpt
=
AutoGPT
.
fromLLMAndTools
(
new
ChatOpenAI
(
{
temperature
:
0
}
)
,
tools
,
{
memory
:
vectorStore
.
asRetriever
(
)
,
aiName
:
"Tom"
,
aiRole
:
"Assistant"
,
}
)
;
await
autogpt
.
run
(
[
"write a weather report for SF today"
]
)
;
/*
{
"thoughts": {
"text": "I need to write a weather report for SF today. I should use a search engine to find the current weather conditions.",
"reasoning": "I don't have the current weather information for SF in my short term memory, so I need to use a search engine to find it.",
"plan": "- Use the search command to find the current weather conditions for SF\n- Write a weather report based on the information found",
"criticism": "I need to make sure that the information I find is accurate and up-to-date.",
"speak": "I will use the search command to find the current weather conditions for SF."
},
"command": {
"name": "search",
"args": {
"input": "current weather conditions San Francisco"
}
}
}
{
"thoughts": {
"text": "I have found the current weather conditions for SF. I need to write a weather report based on this information.",
"reasoning": "I have the information I need to write a weather report, so I should use the write_file command to save it to a file.",
"plan": "- Use the write_file command to save the weather report to a file",
"criticism": "I need to make sure that the weather report is clear and concise.",
"speak": "I will use the write_file command to save the weather report to a file."
},
"command": {
"name": "write_file",
"args": {
"file_path": "weather_report.txt",
"text": "San Francisco Weather Report:\n\nMorning: 53Â°, Chance of Rain 1%\nAfternoon: 59Â°, Chance of Rain 0%\nEvening: 52Â°, Chance of Rain 3%\nOvernight: 48Â°, Chance of Rain 2%"
}
}
}
{
"thoughts": {
"text": "I have completed all my objectives. I will use the finish command to signal that I am done.",
"reasoning": "I have completed the task of writing a weather report for SF today, so I don't need to do anything else.",
"plan": "- Use the finish command to signal that I am done",
"criticism": "I need to make sure that I have completed all my objectives before using the finish command.",
"speak": "I will use the finish command to signal that I am done."
},
"command": {
"name": "finish",
"args": {
"response": "I have completed all my objectives."
}
}
}
*/



--- æ–‡ä»¶: output_20250622_020018\docs\use_cases\autonomous_agents\baby_agi.md ---
---
url: https://js.langchain.com.cn/docs/use_cases/autonomous_agents/baby_agi
crawled_at: 2025-06-22T02:00:26.523685
---

BabyAGI
info
åŸå§‹GitHubé“¾æ¥:
https://github.com/yoheinakajima/babyagi
BabyAGIç”±ä¸‰ä¸ªç»„æˆéƒ¨åˆ†æ„æˆ:
ä¸€ä¸ªåˆ›å»ºä»»åŠ¡çš„é“¾
ä¸€ä¸ªè´Ÿè´£ä¼˜å…ˆå¤„ç†ä»»åŠ¡çš„é“¾
ä¸€ä¸ªæ‰§è¡Œä»»åŠ¡çš„é“¾
è¿™äº›é“¾æŒ‰é¡ºåºæ‰§è¡Œï¼Œç›´åˆ°ä»»åŠ¡åˆ—è¡¨ä¸ºç©ºæˆ–è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ã€‚
ç®€å•ç¤ºä¾‹
â€‹
åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨BabyAGIæ²¡æœ‰ä»»ä½•å·¥å…·ã€‚æ‚¨ä¼šå‘ç°æˆåŠŸåˆ›å»ºäº†ä»»åŠ¡åˆ—è¡¨ï¼Œä½†åœ¨æ‰§è¡Œä»»åŠ¡æ—¶æˆ‘ä»¬æ²¡æœ‰å¾—åˆ°å…·ä½“ç»“æœã€‚è¿™æ˜¯å› ä¸ºæˆ‘ä»¬æ²¡æœ‰ä¸ºBabyAGIæä¾›ä»»ä½•å·¥å…·ã€‚åœ¨ä¸‹ä¸€ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹ã€‚
import
{
BabyAGI
}
from
"langchain/experimental/babyagi"
;
import
{
MemoryVectorStore
}
from
"langchain/vectorstores/memory"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
const
vectorStore
=
new
MemoryVectorStore
(
new
OpenAIEmbeddings
(
)
)
;
const
babyAGI
=
BabyAGI
.
fromLLM
(
{
llm
:
new
OpenAI
(
{
temperature
:
0
}
)
,
vectorstore
:
vectorStore
,
maxIterations
:
3
,
}
)
;
await
babyAGI
.
call
(
{
objective
:
"Write a weather report for SF today"
}
)
;
/*
*****TASK LIST*****
1: Make a todo list
*****NEXT TASK*****
1: Make a todo list
*****TASK RESULT*****
1. Check the weather forecast for San Francisco today
2. Make note of the temperature, humidity, wind speed, and other relevant weather conditions
3. Write a weather report summarizing the forecast
4. Check for any weather alerts or warnings
5. Share the report with the relevant stakeholders
*****TASK LIST*****
2: Check the current temperature in San Francisco
3: Check the current humidity in San Francisco
4: Check the current wind speed in San Francisco
5: Check for any weather alerts or warnings in San Francisco
6: Check the forecast for the next 24 hours in San Francisco
7: Check the forecast for the next 48 hours in San Francisco
8: Check the forecast for the next 72 hours in San Francisco
9: Check the forecast for the next week in San Francisco
10: Check the forecast for the next month in San Francisco
11: Check the forecast for the next 3 months in San Francisco
1: Write a weather report for SF today
*****NEXT TASK*****
2: Check the current temperature in San Francisco
*****TASK RESULT*****
I will check the current temperature in San Francisco. I will use an online weather service to get the most up-to-date information.
*****TASK LIST*****
3: Check the current UV index in San Francisco
4: Check the current air quality in San Francisco
5: Check the current precipitation levels in San Francisco
6: Check the current cloud cover in San Francisco
7: Check the current barometric pressure in San Francisco
8: Check the current dew point in San Francisco
9: Check the current wind direction in San Francisco
10: Check the current humidity levels in San Francisco
1: Check the current temperature in San Francisco to the average temperature for this time of year
2: Check the current visibility in San Francisco
11: Write a weather report for SF today
*****NEXT TASK*****
3: Check the current UV index in San Francisco
*****TASK RESULT*****
The current UV index in San Francisco is moderate, with a value of 5. This means that it is safe to be outside for short periods of time without sunscreen, but it is still recommended to wear sunscreen and protective clothing when outside for extended periods of time.
*/
å¸¦å·¥å…·çš„ç¤ºä¾‹
â€‹
åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬ç”¨ä¸€ä¸ªå¸¦æœ‰æœç´¢å·¥å…·çš„è‡ªå®šä¹‰ä»£ç†æ›¿æ¢äº†æ‰§è¡Œé“¾ã€‚è¿™ä½¿å¾—BabyAGIèƒ½å¤Ÿåœ¨æ‰§è¡Œä»»åŠ¡æ—¶ä½¿ç”¨çœŸå®ä¸–ç•Œçš„æ•°æ®ï¼Œä»è€Œä½¿å…¶æ›´åŠ å¼ºå¤§ã€‚æ‚¨å¯ä»¥æ·»åŠ å…¶ä»–å·¥å…·æ¥å¢å¼ºBabyAGIçš„èƒ½åŠ›ã€‚
import
{
BabyAGI
}
from
"langchain/experimental/babyagi"
;
import
{
MemoryVectorStore
}
from
"langchain/vectorstores/memory"
;
import
{
OpenAIEmbeddings
}
from
"langchain/embeddings/openai"
;
import
{
OpenAI
}
from
"langchain/llms/openai"
;
import
{
PromptTemplate
}
from
"langchain/prompts"
;
import
{
LLMChain
}
from
"langchain/chains"
;
import
{
ChainTool
,
SerpAPI
,
Tool
}
from
"langchain/tools"
;
import
{
initializeAgentExecutorWithOptions
}
from
"langchain/agents"
;
// First, we create a custom agent which will serve as execution chain.
const
todoPrompt
=
PromptTemplate
.
fromTemplate
(
"You are a planner who is an expert at coming up with a todo list for a given objective. Come up with a todo list for this objective: {objective}"
)
;
const
tools
:
Tool
[
]
=
[
new
SerpAPI
(
process
.
env
.
SERPAPI_API_KEY
,
{
location
:
"San Francisco,California,United States"
,
hl
:
"en"
,
gl
:
"us"
,
}
)
,
new
ChainTool
(
{
name
:
"TODO"
,
chain
:
new
LLMChain
(
{
llm
:
new
OpenAI
(
{
temperature
:
0
}
)
,
prompt
:
todoPrompt
,
}
)
,
description
:
"useful for when you need to come up with todo lists. Input: an objective to create a todo list for. Output: a todo list for that objective. Please be very clear what the objective is!"
,
}
)
,
]
;
const
agentExecutor
=
await
initializeAgentExecutorWithOptions
(
tools
,
new
OpenAI
(
{
temperature
:
0
}
)
,
{
agentType
:
"zero-shot-react-description"
,
agentArgs
:
{
prefix
:
`
You are an AI who performs one task based on the following objective: {objective}. Take into account these previously completed tasks: {context}.
`
,
suffix
:
`
Question: {task}
{agent_scratchpad}
`
,
inputVariables
:
[
"objective"
,
"task"
,
"context"
,
"agent_scratchpad"
]
,
}
,
}
)
;
const
vectorStore
=
new
MemoryVectorStore
(
new
OpenAIEmbeddings
(
)
)
;
// Then, we create a BabyAGI instance.
const
babyAGI
=
BabyAGI
.
fromLLM
(
{
llm
:
new
OpenAI
(
{
temperature
:
0
}
)
,
executionChain
:
agentExecutor
,
// an agent executor is a chain
vectorstore
:
vectorStore
,
maxIterations
:
10
,
}
)
;
await
babyAGI
.
call
(
{
objective
:
"Write a short weather report for SF today"
}
)
;
/*
*****TASK LIST*****
1: Make a todo list
*****NEXT TASK*****
1: Make a todo list
*****TASK RESULT*****
Today in San Francisco, the weather is sunny with a temperature of 70 degrees Fahrenheit, light winds, and low humidity. The forecast for the next few days is expected to be similar.
*****TASK LIST*****
2: Find the forecasted temperature for the next few days in San Francisco
3: Find the forecasted wind speed for the next few days in San Francisco
4: Find the forecasted humidity for the next few days in San Francisco
5: Create a graph showing the forecasted temperature, wind speed, and humidity for San Francisco over the next few days
6: Research the average temperature for San Francisco in the past week
7: Research the average wind speed for San Francisco in the past week
8: Research the average humidity for San Francisco in the past week
9: Create a graph showing the temperature, wind speed, and humidity for San Francisco over the past week
*****NEXT TASK*****
2: Find the forecasted temperature for the next few days in San Francisco
*****TASK RESULT*****
The forecasted temperature for the next few days in San Francisco is 63Â°, 65Â°, 71Â°, 73Â°, and 66Â°.
*****TASK LIST*****
3: Find the forecasted wind speed for the next few days in San Francisco
4: Find the forecasted humidity for the next few days in San Francisco
5: Create a graph showing the forecasted temperature, wind speed, and humidity for San Francisco over the next few days
6: Research the average temperature for San Francisco in the past week
7: Research the average wind speed for San Francisco in the past week
8: Research the average humidity for San Francisco in the past week
9: Create a graph showing the temperature, wind speed, and humidity for San Francisco over the past week
10: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to the average temperature, wind speed, and humidity for San Francisco over the past week
11: Find the forecasted precipitation for the next few days in San Francisco
12: Research the average wind direction for San Francisco in the past week
13: Create a graph showing the forecasted temperature, wind speed, and humidity for San Francisco over the past week
14: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to
*****NEXT TASK*****
3: Find the forecasted wind speed for the next few days in San Francisco
*****TASK RESULT*****
West winds 10 to 20 mph. Gusts up to 35 mph in the evening. Tuesday. Sunny. Highs in the 60s to upper 70s. West winds 5 to 15 mph.
*****TASK LIST*****
4: Research the average precipitation for San Francisco in the past week
5: Research the average temperature for San Francisco in the past week
6: Research the average wind speed for San Francisco in the past week
7: Research the average humidity for San Francisco in the past week
8: Research the average wind direction for San Francisco in the past week
9: Find the forecasted temperature, wind speed, and humidity for San Francisco over the next few days
10: Find the forecasted precipitation for the next few days in San Francisco
11: Create a graph showing the forecasted temperature, wind speed, and humidity for San Francisco over the next few days
12: Create a graph showing the temperature, wind speed, and humidity for San Francisco over the past week
13: Create a graph showing the forecasted temperature, wind speed, and humidity for San Francisco over the past month
14: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to the average temperature, wind speed, and humidity for San Francisco over the past week
15: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to the
*****NEXT TASK*****
4: Research the average precipitation for San Francisco in the past week
*****TASK RESULT*****
According to Weather Underground, the forecasted precipitation for San Francisco in the next few days is 7-hour rain and snow with 24-hour rain accumulation.
*****TASK LIST*****
5: Research the average wind speed for San Francisco over the past month
6: Create a graph showing the forecasted temperature, wind speed, and humidity for San Francisco over the past month
7: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to the average temperature, wind speed, and humidity for San Francisco over the past month
8: Research the average temperature for San Francisco over the past month
9: Research the average wind direction for San Francisco over the past month
10: Create a graph showing the forecasted precipitation for San Francisco over the next few days
11: Compare the forecasted precipitation for San Francisco over the next few days to the average precipitation for San Francisco over the past week
12: Find the forecasted temperature, wind speed, and humidity for San Francisco over the next few days
13: Find the forecasted precipitation for the next few days in San Francisco
14: Create a graph showing the temperature, wind speed, and humidity for San Francisco over the past week
15: Create a graph showing the forecasted temperature, wind speed, and humidity for San Francisco over the next few days
16: Compare the forecast
*****NEXT TASK*****
5: Research the average wind speed for San Francisco over the past month
*****TASK RESULT*****
The average wind speed for San Francisco over the past month is 3.2 meters per second.
*****TASK LIST*****
6: Find the forecasted temperature, wind speed, and humidity for San Francisco over the next few days,
7: Find the forecasted precipitation for the next few days in San Francisco,
8: Create a graph showing the temperature, wind speed, and humidity for San Francisco over the past week,
9: Create a graph showing the forecasted temperature, wind speed, and humidity for San Francisco over the next few days,
10: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to the average wind speed for San Francisco over the past month,
11: Research the average wind speed for San Francisco over the past week,
12: Create a graph showing the forecasted precipitation for San Francisco over the next few days,
13: Compare the forecasted precipitation for San Francisco over the next few days to the average precipitation for San Francisco over the past month,
14: Research the average temperature for San Francisco over the past month,
15: Research the average humidity for San Francisco over the past month,
16: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to the average temperature,
*****NEXT TASK*****
6: Find the forecasted temperature, wind speed, and humidity for San Francisco over the next few days,
*****TASK RESULT*****
The forecast for San Francisco over the next few days is mostly sunny, with a high near 64. West wind 7 to 12 mph increasing to 13 to 18 mph in the afternoon. Winds could gust as high as 22 mph. Humidity will be around 50%.
*****TASK LIST*****
7: Find the forecasted precipitation for the next few days in San Francisco,
8: Create a graph showing the temperature, wind speed, and humidity for San Francisco over the past week,
9: Create a graph showing the forecasted temperature, wind speed, and humidity for San Francisco over the next few days,
10: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to the average wind speed for San Francisco over the past month,
11: Research the average wind speed for San Francisco over the past week,
12: Create a graph showing the forecasted precipitation for San Francisco over the next few days,
13: Compare the forecasted precipitation for San Francisco over the next few days to the average precipitation for San Francisco over the past month,
14: Research the average temperature for San Francisco over the past month,
15: Research the average humidity for San Francisco over the past month,
16: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to the average temperature
*****NEXT TASK*****
7: Find the forecasted precipitation for the next few days in San Francisco,
*****TASK RESULT*****
According to Weather Underground, the forecasted precipitation for the next few days in San Francisco is 7-hour rain and snow with 24-hour rain accumulation, radar and satellite maps of precipitation.
*****TASK LIST*****
8: Create a graph showing the temperature, wind speed, and humidity for San Francisco over the past week,
9: Create a graph showing the forecasted temperature, wind speed, and humidity for San Francisco over the next few days,
10: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to the average wind speed for San Francisco over the past month,
11: Research the average wind speed for San Francisco over the past week,
12: Create a graph showing the forecasted precipitation for San Francisco over the next few days,
13: Compare the forecasted precipitation for San Francisco over the next few days to the average precipitation for San Francisco over the past month,
14: Research the average temperature for San Francisco over the past month,
15: Research the average humidity for San Francisco over the past month,
16: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to the average temperature
*****NEXT TASK*****
8: Create a graph showing the temperature, wind speed, and humidity for San Francisco over the past week,
*****TASK RESULT*****
A graph showing the temperature, wind speed, and humidity for San Francisco over the past week.
*****TASK LIST*****
9: Create a graph showing the forecasted temperature, wind speed, and humidity for San Francisco over the next few days
10: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to the average wind speed for San Francisco over the past month
11: Research the average wind speed for San Francisco over the past week
12: Create a graph showing the forecasted precipitation for San Francisco over the next few days
13: Compare the forecasted precipitation for San Francisco over the next few days to the average precipitation for San Francisco over the past month
14: Research the average temperature for San Francisco over the past month
15: Research the average humidity for San Francisco over the past month
16: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to the average temperature
*****NEXT TASK*****
9: Create a graph showing the forecasted temperature, wind speed, and humidity for San Francisco over the next few days
*****TASK RESULT*****
The forecasted temperature, wind speed, and humidity for San Francisco over the next few days can be seen in the graph created.
*****TASK LIST*****
10: Research the average wind speed for San Francisco over the past month
11: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to the average humidity for San Francisco over the past month
12: Create a graph showing the forecasted precipitation for San Francisco over the next few days
13: Compare the forecasted precipitation for San Francisco over the next few days to the average precipitation for San Francisco over the past month
14: Research the average temperature for San Francisco over the past week
15: Compare the forecasted temperature, wind speed, and humidity for San Francisco over the next few days to the average wind speed for San Francisco over the past week
*****NEXT TASK*****
10: Research the average wind speed for San Francisco over the past month
*****TASK RESULT*****
The average wind speed for San Francisco over the past month is 2.7 meters per second.
[...]
*/


